quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:100,pipeline,pipeline-tools,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,1,['pipeline'],['pipeline-tools']
Deployability,"@ruchim Just fyi, the workflows ran fine with preemptions occuring in 0.21 release",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558:75,release,release,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690#issuecomment-261690558,1,['release'],['release']
Deployability,"@ruchim Thanks for the update. I can find a workaround, e.g. reducing the number of exposed outputs; it's just that this will affect others using the same workflow on e.g. DNAnexus . If I try to make a PR myself, based on what's done for AWS_CROMWELL_INPUTS_GZ, do you think you'd be able to look at it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069:23,update,update,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-445982069,1,['update'],['update']
Deployability,"@ruchim That's fine, this isn't blocking me right now, as long as I have something to tell my boss. However if someone has time to suggest what tests I should add for Postgresql, I can try to have those added by the time you're ready to review the entire mess. (Or should it just be everything you're testing for MySQL? This seems easy enough to add but I'm nervous about bloating your travis-ci runtimes.). Just out of curiosity, what is the timeline for Cromwell releases this year? I am fine using my fork for now but eventually we want to be able to use the official jarfile.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109:465,release,releases,465,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488330109,1,['release'],['releases']
Deployability,@ruchim and @kshakir: Any update on this? This is for our piplines a real blocker to bring our pipelines to production. See also https://github.com/biowdl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-418056753:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050#issuecomment-418056753,2,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,"@ruchim updated the issue, I'm thinking we could add them to centaur too eventually",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4163#issuecomment-425088460:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4163#issuecomment-425088460,1,['update'],['updated']
Deployability,@salonishah11 I believe we now have upgrade tests in our CI which would catch that. @mcovarr @kshakir does that sound right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739:36,upgrade,upgrade,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490140739,1,['upgrade'],['upgrade']
Deployability,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:420,Canary,CanaryTest,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,2,"['Canary', 'canary']","['CanaryTest', 'canary']"
Deployability,@scala-steward - there may be two small bugs happening in this PR:. 1. I didn't expect scala-steward to continue posting updates to this dependency given that there's a `scala-steward:off` line in the file.; 2. This PR is not actually updating the version of sbt - it's just whitespace tidying?. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924:121,update,updates,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924,1,['update'],['updates']
Deployability,@scala-steward this string matching has found an incorrect string comment to update.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524:77,update,update,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524,1,['update'],['update']
Deployability,"@scottfrazer @geoffjentry Should ""getting started"" be updated to point users to our website's documentation, or is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705#issuecomment-212617929:54,update,updated,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705#issuecomment-212617929,1,['update'],['updated']
Deployability,@scottfrazer @mcovarr this PR and Tyburn (please also check broadinstitute/tyburn#16) have been updated. There are now just two things to tidy up in WDL4S and a change in the WDL spec...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-170925365:96,update,updated,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/353#issuecomment-170925365,1,['update'],['updated']
Deployability,"@scottfrazer FWIW my vision of the world was a lot closer to the diagram you drew up but I don't have strong feelings on that. In terms of distributed jars what I'd like to see distributed to the world would be:; - cromwell.jar: full fat jar like we have today which also includes all of the supported backends built in; - cromwell-backend.jar: a jar providing the interface stuff which someone can use to build their own backend jar. I'd be totally okay with (and could see value in):; - cromwell-lite.jar (or something like that): a stripped down fat jar w/o any supported backends or maybe local; - foo-backend.jar for each supported backend. My main concern though is that we always make the most obvious download for a naive user of cromwell to be the one with all of the supported (or perhaps a 'very common supported' subset) backends built in so as to minimize the work someone needs to do to get rolling. The other jars are really an artifact of the multi-project model and can be ignored. The side discussion about `core` is exactly why I was picturing the hierarchy stemming from `backend` all along. Despite the name of `supportedBackends` being my request I'll admit I was just looking at the name when I said that, not thinking the whole thing through critically :). IMO `core` should be code which is shared between all components, I'd call the filesystem concept a component, not something in core itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235:905,rolling,rolling,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235,1,['rolling'],['rolling']
Deployability,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:938,integrat,integration,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['integrat'],['integration']
Deployability,"@scottfrazer Updated w/ your suggestions, appears to work fine in docker",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/42#issuecomment-111221496:13,Update,Updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/42#issuecomment-111221496,1,['Update'],['Updated']
Deployability,@scottfrazer updated to use WdlValues which are properly marshalled as JSON,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-144162941:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-144162941,1,['update'],['updated']
Deployability,"@scottfrazer yes, and I need to update the sample response in the YAML too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143854440:32,update,update,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143854440,1,['update'],['update']
Deployability,"@seandavi I know that GCS != S3, but when I had a brief look at the [source](https://github.com/broadinstitute/cromwell/blob/3b29af0d8f116d63e1fcb85f5b4903fd615a5386/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L89) where that configuration `io` block is being used, `number-of-requests` is used to set a throttle on a fairly low-level Actor that at least at might be used by the AWS batch backend... I haven't looked at the implementation in detail. I'll do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065:244,configurat,configuration,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436944065,1,['configurat'],['configuration']
Deployability,"@sharmaashish that's a great point. I _suspect_ (but haven't checked this) that if we don't specify any driver version then the VM we receive will spin up with a GPU attached, but no driver installed. So it's not ""required"" in that sense, but if you want to use GPUs then perhaps it is... I think it would be appropriate to open an issue to ""Update the default Nvidia GPU version in PAPIv2"" with this suggestion (or maybe re-open this issue one with an updated title and description - I'll leave that up to you). I'll also say that since you've already identified the line in the code and the change you want to make, we always appreciate PRs from motivated contributors!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-492702334:190,install,installed,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-492702334,3,"['Update', 'install', 'update']","['Update', 'installed', 'updated']"
Deployability,@shengqh when you use Google Custom Pipelines (genomics) Cromwell database is not persistent by default. I use a Cromwell server instance configured with an external mysql database. Details can be found in https://cromwell.readthedocs.io/en/latest/Configuring/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409:36,Pipeline,Pipelines,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-450830409,1,['Pipeline'],['Pipelines']
Deployability,"@slnovak Hi - thanks for this! I just had a couple of quick questions on the Homebrew & maintenance front. ; - What's the plan going forward in terms of keeping it up to date, is this something that you plan on doing? If so will you be tracking the official releases (e.g. now 0.16) or updating on some semi-regular (or even not-so-semi) interval? ; - Is there anything we could do in terms of helping out w/ the homebrew angle?; - I'm not at all familiar w/ the homebrew formula stuff. Where is it calling `sbt assemble`? Is that done elsewhere? If so, how does that work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670:258,release,releases,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166030670,1,['release'],['releases']
Deployability,"@slnovak Sounds good, thanks. We need to solidify our release procedure to be less ad hoc now that others are actually using this. We'll try to get that squared away early in the new year - I think what would make sense would be for us to lump the PRs in as part of that procedure but we might come calling for advice/help at some point in the next few weeks :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166332671:54,release,release,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166332671,1,['release'],['release']
Deployability,"@sndrtj To follow up a bit on what @kshakir says here, for any users who do not need call caching I'd highly recommend working with the develop branch. We expect to release Any Day Now (he's been saying for a few weeks) and while it'll have its own problems with life it's already demonstrated itself to be superior in just about every way to 0.19",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302:165,release,release,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-246820302,1,['release'],['release']
Deployability,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1595,integrat,integration,1595,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597,1,['integrat'],['integration']
Deployability,"@tom-dyar Can you try again with latest? I fixed a ton of stuff in the last 24 hours, including a whole new processing pipeline for stdout/stderr and rc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395499661:119,pipeline,pipeline,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395499661,1,['pipeline'],['pipeline']
Deployability,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:89,release,release,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815,4,['release'],['release']
Deployability,@vanajasmy - you need to create a custom AMI for your compute environment that includes a patched ecs-agent that is specific for cromwell. See the following documentation on how to do this:. https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464240535:90,patch,patched,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464240535,1,['patch'],['patched']
Deployability,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:117,release,release,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292,2,['release'],['release']
Deployability,"@vdauwera I spotted the issue but it was @kshakir who ended up resolving it. I believe this had to do with the auth that was used to perform the read_* function, and not having access to the proper google credentials. I checked the [config](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/cromwell_launcher/jes_template.conf) wdl_runner uses and I believe it's missing the goolge.auths key and the engine.filesystem.gcs.auth key in the config, which is probably what Cromwell requires to parse gcs files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165:275,pipeline,pipelines-api-examples,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-295735165,1,['pipeline'],['pipelines-api-examples']
Deployability,"@vdauwera I think it's basically ready to go now, we didn't want to go live until after the 28 release last week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238:95,release,release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980238,1,['release'],['release']
Deployability,@vdauwera My thought was that if you're wiring up something like that that there's probably a good reason to have done so. I could easily see people wanting to munge those values when debugging and such but then would it be an attractive nuisance once released into the wild?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323822585:252,release,released,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323822585,1,['release'],['released']
Deployability,"@vdauwera We deliberately kept this out of the 28 release pending discussion. We'd certainly like to include this in 29, but that release won't go out for a while. I'd be happy to meet with you this week to discuss.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064:50,release,release,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-312980064,2,['release'],['release']
Deployability,@vivster7 is this issue meant for 0.19 hotfix or PBE Cromwell (or both)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947#issuecomment-231108479:39,hotfix,hotfix,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947#issuecomment-231108479,1,['hotfix'],['hotfix']
Deployability,"@vortexing - task input and output data staging is handled by the `ecs-proxy` container that is installed when you create a custom AMI with ""cromwell"" settings. If you are not seeing data move in/out a good place to check for errors is the Cloudwatch log for a task that didn't have it's data staged correctly. Append `-proxy` to the job's cloudwatch log url to get the logging generated by the `ecs-proxy`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845:96,install,installed,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467676845,1,['install'],['installed']
Deployability,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:83,release,released,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053,4,"['configurat', 'release']","['configuration', 'released']"
Deployability,"@vsoch @geoffjentry We did manage to get it working, with some caveats. We also haven't really tested it very extensively yet.; These are the relevant lines from the backend configuration:; ```; submit-docker = """"""; echo ' \; CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"") && \; sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script && \; chmod 775 ${cwd}/execution/script && \; singularity exec --bind /exports:/data/,$CROMWELLROOT:/config docker://${docker} ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; dockerRoot = ""/config""; ```; > This only works if your container has both a /data and /config mount point. I tested this (very shallowly) using biocontainers. Line by line:; 1. `CROMWELLROOT=$(echo ${cwd} | sed ""s/cromwell-executions\\/.*/cromwell-executions/"")` ; 1. If dockerRoot is `/cromwell-executions`; 2. The script will contains paths like: `/cromwell-executions/test/<hash>/call-task/execution/rc`; 3. Therefore we need to have the entire structure under the root of the execution folder mounted, as such, we need to bind the entire execution folder.; 4. This gets the path to the root of the execution folder.; - I also tried setting dockerRoot to be the same as `cwd`: `dockerRoot = ""${cwd}""`, but this resulted in `${cwd}` being placed literally in the execution script. If this had been an option we wouldn't have to bind the execution directory separately (I think), but since it isn't we do have to do so.; 1. `sed -i ""s/\\/exports\\//\\/data\\//g"" ${cwd}/execution/script` ; - This a bit of a nasty workaround to convert absolute paths used in the commands to what their path would be in the container. This is necessary if you have (eg.) a String type output directory in a command. There are other ways of dealing with this, you could make a /data directory which links to /expo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799:174,configurat,configuration,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424631799,1,['configurat'],['configuration']
Deployability,@vsoch Hi - I think what you're looking for is [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) which is where we put examples like this. So if there's a configuration for a backend which works we'd put it in there so we could point people at it. Does that make sense?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397:210,configurat,configuration,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413676397,1,['configurat'],['configuration']
Deployability,"@vsoch Just a heads up, @katevoss did point out that her fix will be on the `/develop` version of RTD shortly but won't be on `/stable` until the next Cromwell release",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402788985:160,release,release,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3862#issuecomment-402788985,1,['release'],['release']
Deployability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several ðŸ”¥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. ðŸ¤ž",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:213,configurat,configuration-reference,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['configurat'],['configuration-reference']
Deployability,"@vsoch That's the theory. Let me know if that doesn't seem to be working for you and we can go from there. The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people **never** want to use actual Docker. However, there are a few buts to the above .... - A Cromwell server can have multiple backends, and workflows can be directed to specific backends, for the case where one sometimes wants it on and sometimes not . - None of this will cover the case where a user **really* wants Singularity (as in an actual Singularity container) instead of the ""use singularity to run a docker container"" model. We'll need to address this separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778:154,configurat,configuration,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413023778,1,['configurat'],['configuration']
Deployability,"@vsoch if you've not already done that, we can always reopen this PR. You all are right, I'm wrong. The reason why I was leaning towards avoiding `cromwell.examples.conf` blurbs is that (as @TMiguelT points out) it's a bit of a mess right now due to having too much stuff in there. TBH work needs to be done to start making that more organized. I sometimes can lean towards throwing the baby out with the bathwater in circumstances like that. I think it's fine to put a number of configurations into `cromwell.examples.conf` as long as the full block is fairly well self contained, and well documented. IOW something which would be easy to peel out into a separate file if/when we get there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389:480,configurat,configurations,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468965389,1,['configurat'],['configurations']
Deployability,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:81,upgrade,upgraded,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463,1,['upgrade'],['upgraded']
Deployability,@wleepang @geoffjentry ; Any updates about PR #5110 ? :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-522934849:29,update,updates,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-522934849,1,['update'],['updates']
Deployability,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:537,install,installed,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,2,['install'],"['install', 'installed']"
Deployability,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:192,update,update,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666,1,['update'],['update']
Deployability,A fix for this just landed on `develop` and will be in the next release of Cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318#issuecomment-434312154:64,release,release,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318#issuecomment-434312154,1,['release'],['release']
Deployability,"A similar patch was tested by @jsotobroad. Once this PR against develop is reviewed & merged, we can submit another PR against master, and _that one_ should make #695 good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179:10,patch,patch,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213246179,1,['patch'],['patch']
Deployability,AC: Confirm that the Pipelines API Request Worker can handle a 502 -- it should be retried.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-490175339:21,Pipeline,Pipelines,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-490175339,1,['Pipeline'],['Pipelines']
Deployability,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2412,configurat,configuration,2412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['configurat'],['configuration']
Deployability,AWS Batch works in a different way than current platforms and we are taking that into account for something we think will be great on release. Stay tuned to #3744 for more details and progress on this item,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745:134,release,release,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395590745,1,['release'],['release']
Deployability,"A_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1504,update,update,1504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['update']
Deployability,"According to the PR description, the WDL 1.0 spec did not have an opinion here, so we let our 1.0 implementation change. The `development` version of WDL says; >In the event that there is no protocol the import is resolved **relative** to the location of the current document. If a protocol-less import starts with `/` it will be interpreted as starting from the root of the host in the resolved URL. so I think it would be most pragmatic to close this issue with a documentation-update PR. @tlangs does this sound fair to you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883:480,update,update,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883,1,['update'],['update']
Deployability,"Actor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transfor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1747,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:61,patch,patch,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523,1,['patch'],['patch']
Deployability,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:35,integrat,integration,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719,1,['integrat'],['integration']
Deployability,Actually one minor request: could you please rebase on `develop` and add an entry to the version 70 release notes for this added functionality with a credit to yourself. ðŸ™‚,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629:100,release,release,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629,1,['release'],['release']
Deployability,"Actually the more I'm digging into this I take it all back. For now. The `zones` field in the Cromwell code doesn't seem to be actually used anywhere except for tests. . Thining about it now I have a recollection that this was part of the cloud formation setup for the batch configuration. I'll need to dig into this unless @wleepang swoops in with some wizardly knowledge. BTW, it could be (and would make sense) that `~/.aws/conf` file is getting picked up via one of the Amazon libraries Cromwell is using. But I see no evidence that it's being directly used by Cromwell itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281:275,configurat,configuration,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493269281,1,['configurat'],['configuration']
Deployability,"Actually what is bugging me I think is precisely the distinction between ""cromwell"" and ""user"" auth I think :smile: ; I think we called them like this because it makes sense in the current way we use them. We use ""cromwell"" auth pretty much always with service-account and ""user"" auth with a ""refreh token mode"" (falling back to the ""cromwell"" auth if there's no ""user"" auth). But really what we need is an auth for pipelines and another one for gcs (they might be the same or not, use service account or refresh tokens, but we shouldn't care from a cromwell perspective).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203478872:416,pipeline,pipelines,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203478872,1,['pipeline'],['pipelines']
Deployability,Added some cleanup after the apt-get install to clean up that layer due to suggestions from @hjfbynara,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/804#issuecomment-218265245:37,install,install,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/804#issuecomment-218265245,1,['install'],['install']
Deployability,"After discussing internally, we unfortunately can't add this code or support the general direction of proxy compatibility. Docker lookups and call caching are pretty sensitive and involved and we're looking to keep the configuration surface area to an absolute minimum.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504:219,configurat,configuration,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504,1,['configurat'],['configuration']
Deployability,"After discussing with @abaumann and @geoffjentry, we are going to plan this for Cromwell 27, our first release of Q4 (April-or-so). Once this is complete, the A-Team will be able to use the Docker Hash library for their own features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090:103,release,release,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2062#issuecomment-285791090,1,['release'],['release']
Deployability,After tech talk discussion this is in standby until after we release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-371150276:61,release,release,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-371150276,1,['release'],['release']
Deployability,"After the changes above, I believe the various Specs should run as expected:; - `sbt alltests:test`; - `sbt notests:test`; - `sbt nodocker:test`; - `sbt dbms:test`; - `sbt integration:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681:172,integrat,integration,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681,1,['integrat'],['integration']
Deployability,Against hotfix instead https://github.com/broadinstitute/cromwell/pull/847,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219846749:8,hotfix,hotfix,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219846749,1,['hotfix'],['hotfix']
Deployability,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:86,integrat,integrate,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055,2,"['integrat', 'pipeline']","['integrate', 'pipeline']"
Deployability,"Ah ha, I believe I've worked it out. The problem is, my docker image has an entrypoint of `/app/fastqc_docker.py`. What this means is that Cromwell generates the full docker command as follows:; ```bash; docker run ; -v /tmp/ggp-556542479:/tmp/ggp-556542479 ; -v /mnt/local-disk:/cromwell_root ; [various -e flags] ; asia.gcr.io/ringed-griffin-220204/dx_fastqc; /tmp/ggp-556542479; ```; So ultimately what my python script sees is this, because the entrypoint and CMD are combined:. ```bash; /app/fastqc_docker.py /tmp/ggp-556542479; ```. So this is why the right command was being run, but it wasn't finding any of the required arguments, like `--read`. Unfortunately this seems to be a longstanding issue in the Google Cloud PAPI v1, (#2461, #2256), and it looks like we're waiting for PAPI v2 to be released before this can be easily fixed. I can't find any obvious workaround for this, and I can't change my entrypoint because that's required elsewhere in my system.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438109764:802,release,released,802,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438109764,1,['release'],['released']
Deployability,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:40,configurat,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039,1,['configurat'],['configuration']
Deployability,"Ah that assumption is indeed faulty -- in a full pipeline we can have upward of 20 inputs that may be used at various points, some only in the last few tasks to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966:49,pipeline,pipeline,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293588966,1,['pipeline'],['pipeline']
Deployability,"Ah, I see. Just a guess, but are you sure your S3 URLs (or more likely your S3 bucket in the configuration file) are in the right format? `s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt` doesn't look valid to me. It should be more like `s3://concr-genomics-results`. Alternatively, maybe the AWS Batch role doesn't have read access to the S3 bucket? The Cromwell server and the Batch instances are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016934,1,['configurat'],['configuration']
Deployability,"Ah, oh well, thanks for the second set of eyes. https://github.com/broadinstitute/cromwell/commit/2682c001d99823098e655acd1dd7a3062a68f495 has your change implemented with some ~nasty~ reflection that hopefully the rest of the DSP-Batcher's will accept, and the test re-enabled. CI running here to see if it breaks anything:; https://travis-ci.com/broadinstitute/cromwell/builds/116462548. Assuming this works, I think the changes will pass all of our existing unit and integration tests!. We can get two others to review. I should abstain due to our collaboration on the code. ðŸ˜‰",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160:470,integrat,integration,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160,1,['integrat'],['integration']
Deployability,"Ah, okay! Thank you for the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-260672380:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-260672380,1,['update'],['update']
Deployability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=â€¦` except one is supposed to use [`-Dlogback.configurationFile=â€¦`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:420,configurat,configurationFile,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,2,['configurat'],"['configuration', 'configurationFile']"
Deployability,Ahh -- disregard. I did not install Akka.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334#issuecomment-165839405:28,install,install,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334#issuecomment-165839405,1,['install'],['install']
Deployability,"Ahh okay, so you did need root permission for what I expected. If you needed root for those 2, arguably another user (or cluster admin) doing the deployment would consider the additional root as trivial. And the benefits to this would be - having the registry deployed on a cluster (still needing root) but without Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811:146,deploy,deployment,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811,2,['deploy'],"['deployed', 'deployment']"
Deployability,"All fixed -- see https://github.com/broadinstitute/mock-jes/pull/1. Since this only works when deployed to App Engine, I've deployed it to our ""mock production"" there and ran 50 workflows... only the /batch endpoint is invoked and everything seems to be working properly",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745:95,deploy,deployed,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257118745,2,['deploy'],['deployed']
Deployability,"Alright, @cjllanwarne and @kshakir I updated the code again... it's changed quite significantly to address all comments so probably needs another look!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223352694:37,update,updated,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223352694,1,['update'],['updated']
Deployability,Also I have not updated the README whatsoever,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203057361:16,update,updated,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203057361,1,['update'],['updated']
Deployability,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:223,release,released,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560,2,"['install', 'release']","['installed', 'released']"
Deployability,"Also have this error, using Cromwell 52, installed using this manual : . https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. logs say : fetch_and_run.is is a directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937:41,install,installed,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-747587937,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"Also, if possible, run something in JES beforehand, then run the liquibase update over the existing data and check the /metadata and /timing can still be accessed for the preexisting tasks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112:75,update,update,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181437112,1,['update'],['update']
Deployability,"Also, regardless of where things stand, this will *not* be part of the upcoming release. I want to give downstream users a chance to more thoroughly vet this in case there are subtle changes not picked up by our testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731:80,release,release,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2380#issuecomment-311174731,1,['release'],['release']
Deployability,"Also, that `hermes` command now needs to be updated for the context of wdl4s. I can do that though if there's a PR for it on wdl4s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/367#issuecomment-170639638:44,update,updated,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/367#issuecomment-170639638,1,['update'],['updated']
Deployability,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:25,release,releases,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057,1,['release'],['releases']
Deployability,"Also. File locks are not that good. But locking via the database would be ideal for horicromtal. If I get some pointers I can implement a database lock. This will require an extra table or something, so I need some pointers on how this should be ideally integrated in cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488:254,integrat,integrated,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488,1,['integrat'],['integrated']
Deployability,"An update...; It looks like the performance of `sync` â€” run on command line entirely outside the context of cromwell â€” that we see on our box that happens to be running cromwell is atypical. Other machines in our compute cluster with same OS and similar amounts of memory execute `sync` in milliseconds, the cromwell server box takes up to a second. we're still trying to identify the cause; there are some configuration differences in the machines, specifically the number, type and state of the mounted filesystems, as well as differences in the applications running in background, but as it stands I'm happy to call this as not a cromwell performance issue, so much as a potential performance issue running cromwell in certain environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989:3,update,update,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285258989,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"Another point that Eric and I came across is that a ""Docker in Docker"" solution - i.e. installing Docker inside the Docker container where he's running Cromwell - is not good either because it necessitates pushing and re-pulling the Docker image he's iterating on, which makes for annoyingly long cycle times and can't work with bad or no Internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900:87,install,installing,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469427900,1,['install'],['installing']
Deployability,Any update about WDL 1.1 support ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500,1,['update'],['update']
Deployability,"Any updates on this front? Weâ€™ve been having issues with occasional spikes in memory usage that donâ€™t abide by a linear model for memory allocation. Currently this requires a lot of â€œbabysittingâ€ for our pipelines (or overprovisioning of memory), reducing their reliability and increasing their cost (on GCP, though the main cost factor is still developersâ€™ time..). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346#issuecomment-485594157,2,"['pipeline', 'update']","['pipelines', 'updates']"
Deployability,Any updates on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-392741040:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2824#issuecomment-392741040,2,['update'],['updates']
Deployability,Any updates on this? I have run into the latter example here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555,1,['update'],['updates']
Deployability,"Apologies, you're totally right. I checked the wrong pipeline for this. This issue came from the WDL in the `gatk3-data-processing` workflow: https://github.com/gatk-workflows/gatk3-data-processing/blob/88fb7e2ba99b251ff917965640a5fe89f02c4dfd/processing-for-variant-discovery-gatk3.wdl#L279. I'll file an issue there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440161270:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4393#issuecomment-440161270,1,['pipeline'],['pipeline']
Deployability,Are there any other changes which will have to be wrapped up in the RELEASE 0.17 umbrella?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646:68,RELEASE,RELEASE,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646,1,['RELEASE'],['RELEASE']
Deployability,"Are there updated acceptance criteria for this ticket, or can this be marked as complete? This currently works on the caas-dev server:. ```; $ curl https://cromwell.caas-dev.broadinstitute.org/engine/v1/status 2>/dev/null | jq .; {; ""ok"": true,; ""systems"": {; ""Cromwell"": {; ""ok"": true; },; ""Sam"": {; ""ok"": true; }; }; }; $ ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4594#issuecomment-458194462:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4594#issuecomment-458194462,1,['update'],['updated']
Deployability,"Are you guys going to do a proper release? Or label an existing tag as; newest release?. On Tue, Jan 10, 2017 at 4:35 PM, Jeff Gentry <notifications@github.com>; wrote:. > @Horneth <https://github.com/Horneth> @LeeTL1220; > <https://github.com/LeeTL1220> That's a good point. The latest version of; > wdltool should always work against the latest wdl4s/Cromwell; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271705111>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk07G1Ukk27IO1RNKJa46Dg9Vs14uks5rQ_mWgaJpZM4Lf57n>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243:34,release,release,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271878243,2,['release'],['release']
Deployability,"Are you running 0.24?. On Sun, Jan 22, 2017 at 1:52 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> In the file you sent me the; > lines look like this /local/cga-fh/cga/Lee_Normal_; > Analysis/Pair/CESC-HSCX1005-TP-NB--/jobs/capture/mut/; > oncotate/job.83173721/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated. In; > the ""fixed"" one you sent me the lines look like this; > /dsde/data/test_dl_oxoq/CESC-HSCX1005-TP-NB--.snp.capture.maf.annotated.; > Further, even running it through dos2unix doesn't change this fact.; >; > Was full_m1_oncotated_list_pc.txt the actual file which caused the; > problem? If so could either this have been a GIGO situation or something; > else in the WDL run putting the wrong paths in your file? I find it hard to; > believe that those paths are what you meant.; >; > Whatever is going on here I don't believe it has to do with DOS-style; > newline chars as that doesn't seem to matter, even when I forcibly insert; > them.; >; > I'm closing this issue as one way or the other it appears to be a; > misnomer. However let's continue to followup either here or in person and; > potentially open a new issue w/ updated info.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274350415>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk29mFygmQZGpLlqpvgZcjpAsa6BCks5rU6VtgaJpZM4LoLVh>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966:1189,update,updated,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-274498966,1,['update'],['updated']
Deployability,Are you running the cromwell 31 release jar or did you build Cromwell from the sources ?; We changed something related to permissions in the develop branch ~1 week ago but it's not in the released 31 jar so I want to (maybe) rule that out.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3500#issuecomment-380213827:32,release,release,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3500#issuecomment-380213827,2,['release'],"['release', 'released']"
Deployability,"Are you using Pipelines API v1?. We are tracking a possible PAPI GPU issue that is limited to v1. If you are able to use the v2 backend instead, I would recommend that as a workaround (v2 is better in other ways too!).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489150608:14,Pipeline,Pipelines,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489150608,1,['Pipeline'],['Pipelines']
Deployability,"As a **Cromwell dev**, I want **to be able to release Cromwell with the same Github account**, so that **I don't have to use my personal github token.**. - effort: small; - risk: small; - business value: small",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648:46,release,release,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-344712648,1,['release'],['release']
Deployability,"As a **Cromwell dev**, I want **wdltool to be released automatically**, so that **when I release Cromwell, wdltool is released and up to date**.; - Effort: Small?; - Risk: Small; - Business Value: Small?; - @Horneth how much time/effort does it take to manually release wdltool? how much risk of human error is there?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089:46,release,released,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-335884089,4,['release'],"['release', 'released']"
Deployability,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:58,release,release,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666,2,['release'],['release']
Deployability,"As a **production pipeline runner**, I want **to write all output files in one directory (rather than hierarchical)**, so that I can **(@ktibbett why is this helpful?)**.; - Effort: **Small**; - Risk: **Medium**; - if files have the same name they could be overwritten; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415:18,pipeline,pipeline,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-326068415,1,['pipeline'],['pipeline']
Deployability,"As a **user running workflows on PAPI**, I want **my workflow to be upgraded from preemptible to regular compute if it is killed after 24 hours rather than retrying**, so that I can **avoid retrying on a job that will need more than 24 hours to run**.; - Effort: **Small**; - Risk: **Small to Medium**; - We'd need a way of being certain that the job was killed due to timeout, rather than another reason, to prevent from upgrading jobs that the user doesn't want upgraded.; - The information about a preemptible VM timing out should come from Google.; - This should be an ""opt-in"" feature, so users do not have the default behavior change from under them.; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096:68,upgrade,upgraded,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-329479096,2,['upgrade'],['upgraded']
Deployability,"As a **user running workflows**, I want to **see my stderr output even when Cromwell gets a ""FileNotFound"" response** so that I can **debug my workflow**.; - Effort: Small to Medium; - We're not sure of the exact way to fix it, so for now we have been patching the issue.; - Risk: Small; - Business value: Small; - There is a workaround, to manually look up the stderr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092:252,patch,patching,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336149092,1,['patch'],['patching']
Deployability,"As a **user setting up Cromwell**, I want **only want to see references to Google Genomics Pipelines API**, so that **I know how to set up the Google backend, not some JES thing.**; - effort: small; - risk: small to medium; - business value: small; - may grow if it becomes confusing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888:91,Pipeline,Pipelines,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2019#issuecomment-344685888,1,['Pipeline'],['Pipelines']
Deployability,"As a **workbench QA**, I want **to know what is being tested**, so that I **don't under- or over-test for each release**. ; - Effort: **Medium**; - Risk: **Small**; - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660:111,release,release,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634#issuecomment-326636660,1,['release'],['release']
Deployability,"As a follow up I did some extra testing. As cromwell evaluates imports from `$PWD`. This means that ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; yields different results depending on the current working directory. In my opinion this is not desirable behavior. The small patch code that I wrote does not solve this issue. If cromwell is run from the same directory as the workflow.wdl it works, but in other cases it does not. . In an ideal case ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; will always lead to the same result no matter what $PWD is. This makes workflows reproducible and easy to be reused. ; This means that cromwell should use the absolute parent path of `workflow.wdl` to evaluate its imports from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775:327,patch,patch,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775,1,['patch'],['patch']
Deployability,"As a short-term workaround you might define different config backends for each Docker configuration you want to support, but that wouldn't scale well if you have a lot of different configurations. Also that requires changes to your conf file to match your WDL which is kind of gross. Do you have any specific suggestions how you'd want this to work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265:86,configurat,configuration,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2494#issuecomment-326077265,2,['configurat'],"['configuration', 'configurations']"
Deployability,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:105,configurat,configuration,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392,1,['configurat'],['configuration']
Deployability,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:647,pipeline,pipelinesRunner,647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969,1,['pipeline'],['pipelinesRunner']
Deployability,"As per TechTalk this morning we're a bit on the fence about whether break develop (so that we're forced to fix it and don't have to hotfix 3 branches) or keep a separate branch and leave develop as is (so that people using it can keep doing that and we have a non-hotfix working branch).; I personally don't have a very strong opinion either way, I may slightly lean towards keeping a separate branch as long as we don't make significant changes to develop. If we do merge to develop I think we should change the default github branch to point to master though so that someone cloning Cromwell doesn't accidentally ends up with the broken version.; @geoffjentry ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328900320:132,hotfix,hotfix,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2613#issuecomment-328900320,2,['hotfix'],['hotfix']
Deployability,"As per the comment @Horneth made I'm going to close this. @rgobbel if you still see this behavior in the newest versions (`develop` or a `hotfix` releas), please reopen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-371209125:138,hotfix,hotfix,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-371209125,1,['hotfix'],['hotfix']
Deployability,"As someone who is having a somewhat similar issue, what did you do to fix the resource management problem? I'm seeing a lot of processes spawned, and memory usage growing out of control. I have a system with a 6-core CPU and 32 GB of RAM, running Linux Mint, and I'm trying to run the processing-for-variant-discovery-gatk4.wdl pipeline (with some interest in running other GATK pipelines as well)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-508219639:328,pipeline,pipeline,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-508219639,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Assigning myself since the MySQL upgrade is the underlying cause. Here is the command sequence I use to create a new local DB for use by a fresh, totally default Cromwell checkout.; ```; docker run --name=mysql1 -p 3306:3306 -d mysql/mysql-server:latest; docker logs mysql1 # copy the auto generated password; docker exec -it mysql1 mysql -uroot -p # paste in the password from the previous stpe; ALTER USER 'root'@'localhost' IDENTIFIED BY '';; CREATE database cromwell_test;; CREATE USER 'root'@'%' IDENTIFIED BY '';; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;; FLUSH PRIVILEGES;; ```; I just validated it still works with recent versions, so it seems my theory about Docker using UTC is correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841:33,upgrade,upgrade,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480417841,1,['upgrade'],['upgrade']
Deployability,"Assuming I'm not too far from the reality, my opinion on this is that ; - I like that the actors don't have to know about the actual reference of the Metadata service. Meaning I'm ok with the push to the event stream approach.; - I like less that metadata is pushed only when actors change state. Their data can be changed (and is changed) when they receive a message but don't necessarily transition. And if I'm understanding this correctly this is not enough to capture that.; For example, the `WorkflowExecutionActor` state are basically `NotRunning`, `Running`, `Done`, we can only update metadata between those states. So there's no update of call status in real-time, outputs etc... Now from your comment I understand you want to do this by having the backend actors publishing metadata, but ; 1) nothing guarantees that they will, or with which format; 2) there are states that could only be known by then engine and that we might want to publish (`JobInitializing`, `Finalizing` etc...). Even more generally, I think the engine is the ultimate decider of what the state of a call is in the workflow, and relying on the backend to get this information seems a bit wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534:586,update,update,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534,2,['update'],['update']
Deployability,"Assuming `PostMVP` tagged tests are filtered out of `sbt test`, then they should never be _added_ to the ignored count. This patch therefore only adds to the ignored count for `sbt alltests:test`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214:125,patch,patch,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1093#issuecomment-229391214,1,['patch'],['patch']
Deployability,"Assuming that putting that in the runtime {cpuPlatform: ""Intel Cascade Lake""} setting in the Cromwell conf file counts as ""in the configuration"", then yes. If we have to hand edit every single one of our WDL task files to put that in a runtime block, not I haven't tried that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598:130,configurat,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598,1,['configurat'],['configuration']
Deployability,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:138,deploy,deployment,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225,2,['deploy'],['deployment']
Deployability,"Aye aye! I don't know scala, but I found the [developer docs](http://cromwell.readthedocs.io/en/develop/Building/) and I know how to use GIthub, so I'm ready to go, lol. I likely won't start this weekend (I have a few projects I'm working on!) but next week for sure. I'll put updates, troubles, and other musings here - thanks in advance for your help :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341:277,update,updates,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412309341,1,['update'],['updates']
Deployability,"Based on the example you shared, would you mind listing what is actually produced inside of the call directory? ; ```s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496508131:145,Pipeline,Pipeline,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-496508131,1,['Pipeline'],['Pipeline']
Deployability,"Based on the need for this use case, I strongly support merging this (or some) basic configuration, and then having documentation with a writeup about these different use cases. @TMiguelT you are spot on about creating the image before hand, and don't forget that in addition to build (which most users might not be able to do on their cluster) you can also just pull from docker:. ```bash; singularity pull docker://<container>; ```; and then create a binary (singularity image) that can be given directly to the run/exec command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105:85,configurat,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461444105,1,['configurat'],['configuration']
Deployability,"Before:; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-02-01 21:13:48.287'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```; After, with no groups excluded: `and (not false)`; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-03-01 20:08:12.447'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (not false) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845:600,update,update,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845,2,['update'],['update']
Deployability,"Besides concurring with the updated-names request, LGTM ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2080/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247:28,update,updated-names,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2080#issuecomment-289107247,1,['update'],['updated-names']
Deployability,"Better still, we'd like to either:. 1) add `cloud-platform` scope, which would allow calling _any_ Google APIs. I understand this may not bode well with the current security model used in Firecloud; however, Google itself recommends migrating away from scopes in favor of IAM, as scopes were introduced before IAM existed [1]. 2) make scopes configurable, ideally at the workflow level, or at least at Cromwell config level. There may be a set of obligatory scopes that is hard-coded in Cromwell (e.g. `genomics` or `compute`), and then `additionalScopes` specified via configuration. This way, we satisfy both the need to restrict the scopes by default, and address other use cases when needed. Our ideal picture for this is that we'd be able to call any Google APIs (e.g. Pub/Sub, Firestore, or BigQuery) from workflows running on CaaS. We don't want to ""wait"" for a new scope to be added upstream each time we have to call a new API. Thanks!. [1] https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#best_practices",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308:570,configurat,configuration,570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4115#issuecomment-424583308,1,['configurat'],['configuration']
Deployability,"Bloom filter is a test of ""have we seen this before?"" on a huge set. We get one for free from Google Guava [here](https://google.github.io/guava/releases/22.0/api/docs/com/google/common/hash/BloomFilter.html)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515:145,release,releases,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332235515,1,['release'],['releases']
Deployability,Bonus awesomeness â€“ removing this backend nerfs the vulnerable JDOM dependency that we would [otherwise have to upgrade](https://broadworkbench.atlassian.net/browse/BW-1228). ```; root(develop)> | 81> whatDependsOn org.jdom jdom2 2.0.6; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] | +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; [info] | ; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890:112,upgrade,upgrade,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890,1,['upgrade'],['upgrade']
Deployability,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:89,integrat,integration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['integrat'],['integration']
Deployability,"Brilliant, thanks for that workaround. That reverts back to the old approach which lets me update to 35 and make use of that temporarily. I'd still love to move to a Java-only version and help with runtimes if there's anything I can do to help debug that, but appreciate the workaround until we have more time to look at that. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426280070:91,update,update,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426280070,1,['update'],['update']
Deployability,"But the job ran successfully. Here is the full logs:. [ec2-user@ip-10-80-199-174 ~]$ java -Dconfig.file=aws.callcache.conf -jar ~/cromwell-35.jar run -i hello_inputs.json hello.wdl; [2018-11-21 15:08:54,14] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,32] [info] Running with database db.url = jdbc:mysql://cromwell-db-rdscluster-6zlvcyvtarfq.cluster-ct1b0hjjpe9q.us-east-1.rds.amazonaws.com/cromwell; [2018-11-21 15:09:03,62] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2018-11-21 15:09:03,91] [info] Slf4jLogger started; [2018-11-21 15:09:04,16] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-23ba05a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-11-21 15:09:04,43] [info] Metadata summary refreshing every 2 seconds.; [2018-11-21 15:09:04,51] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,53] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-11-21 15:09:04,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-11-21 15:09:05,40] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Version 35; [2018-11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-43",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:569,configurat,configuration,569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,2,['configurat'],['configuration']
Deployability,"By the book, the customized branch names have to merge so that tests pass, and then can be updated in a follow-on PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820:91,update,updated,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820,1,['update'],['updated']
Deployability,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2553,configurat,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,2,['configurat'],['configuration']
Deployability,CWL support [was removed](https://github.com/broadinstitute/cromwell/releases/tag/79). References to it in code and documentation are being cleaned up as opportunity arises.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595:69,release,releases,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595,1,['release'],['releases']
Deployability,Can we update the test cases which now work? I suspect `custom_mount_point` at least could be re-enabled?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485560062:7,update,update,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4863#issuecomment-485560062,1,['update'],['update']
Deployability,Can you also patch this in `wdltool` and any other dependencies that are as yet un-cromwell-repo-ified?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904:13,patch,patch,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904,1,['patch'],['patch']
Deployability,"Can you update the issue with an example of what you want to do, using the regex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948:8,update,update,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255241948,1,['update'],['update']
Deployability,"Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; new issues",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-373590817:99,UPDATE,UPDATE,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-373590817,1,['UPDATE'],['UPDATE']
Deployability,Centaur should also be updated run the two tests in #1383 before closing the ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672:23,update,updated,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1392#issuecomment-246056672,1,['update'],['updated']
Deployability,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676,4,"['Update', 'deploy', 'hotfix', 'update']","['Updated', 'deployed', 'hotfix', 'updated']"
Deployability,Check out https://github.com/broadinstitute/wdltool/releases/tag/0.8,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372:52,release,releases,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271909372,1,['release'],['releases']
Deployability,Chris and Adam;; Thanks so much for this fix. I really appreciate having an updated release with this working plus all the goodness since release 36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470983287:76,update,updated,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654#issuecomment-470983287,3,"['release', 'update']","['release', 'updated']"
Deployability,"Chris;; Thanks for working on this and for the test case to iterate with. This example does work for me in the sense that it generates an md5sum, but also demonstrates the underlying issue I'm having with https inputs. I also get them downloaded and staged into my pipeline, but the file names get mangled into random download number. md5sum is cool with this, but many of my real tasks fail because the expected file extensions and associated secondary file extensions get lost with the random file names. Here's the example output I get from running this that demonstrates the file naming issue:; ```; /usr/bin/md5sum '/home/chapmanb/tmp/cromwell/cromwell_work/cromwell-executions/main-http_inputs.cwl/093e2835-e4cc-4731-9248-88d74dec0977/call-sum/inputs/1515144/1710814112361209342' | cut -c1-32; ```; This input should be called `jamie_the_cromwell_pig.png` but instead gets a long number attached to it. Is it possible to preserve initial file names with https like happens with other filesystem types?. In terms of the test cases, it would be great if it also checked that the file extension and name get preserved. Thanks again for looking at this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999:265,pipeline,pipeline,265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439428999,1,['pipeline'],['pipeline']
Deployability,Closing because the update to 2.6.8 is about to be reverted,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430:20,update,update,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430,1,['update'],['update']
Deployability,"Closing on the basis that https://github.com/broadinstitute/cromwell/pull/3996 is on dev and no one seems to think this is important enough to hotfix - in fact, we forgot all about it for a while",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3997#issuecomment-416696267:143,hotfix,hotfix,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3997#issuecomment-416696267,1,['hotfix'],['hotfix']
Deployability,Closing this and including the update to WdlParser in the call element PR that I'm working on.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3322#issuecomment-369031380:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3322#issuecomment-369031380,1,['update'],['update']
Deployability,Closing this as sublime text and vim-wdl were updated. There remains an issue w/ intellij captured [here](https://github.com/broadinstitute/winstanley/issues/10) so that remains open.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323:46,update,updated,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2124#issuecomment-330277323,1,['update'],['updated']
Deployability,Closing this given the fact that the latest QA testing framework in the form of a Jenkins build is now green: https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline/237/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870:205,Pipeline,Pipeline,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870,1,['Pipeline'],['Pipeline']
Deployability,Closing this hotfix PR due to BT-187. Sibling develop PR is here: https://github.com/broadinstitute/cromwell/pull/6218,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507:13,hotfix,hotfix,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507,1,['hotfix'],['hotfix']
Deployability,"Closing this, we've released at least once since Nov '17",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-388163007:20,release,released,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-388163007,1,['release'],['released']
Deployability,"Closing this; I [added a cromwell recipe to bioconda](https://github.com/bioconda/bioconda-recipes/pull/2348), so now it is possible to `conda install cromwell` (if you have the bioconda channel in your config). See here for more info:; https://bioconda.github.io/recipes/cromwell/README.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503:143,install,install,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446#issuecomment-248438503,1,['install'],['install']
Deployability,"Code looks good, it just needs to be updated with the latest state for `develop`. I think Adam's updates should solve the issues Christian had regarding testing his changes in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151:37,update,updated,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151,2,['update'],"['updated', 'updates']"
Deployability,Completed https://github.com/broadinstitute/cromwell/blob/develop/release/release_workflow.wdl#L182-L291,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2574#issuecomment-424937576:66,release,release,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2574#issuecomment-424937576,1,['release'],['release']
Deployability,"Configuring as you suggested (following the instructions on the provided URL) does not even start the process. Apart of some typos in the instructions (e.g., `MYPASSWORD` instead of `MYSQL_PASSWORD`), it looks that there is a conectivity problem with the docker container running mysql. Steps to reproduce:. ```bash; # start mysql-server container; docker run -p 3306:3306 --name cromwell_db -e MYSQL_ROOT_PASSWORD=`cat my_sql.root.pwd` -e MYSQL_DATABASE=cromwell -e MYSQL_USER=cromwell -e MYSQL_PASSWORD=cromwell -d mysql/mysql-server:5.7; ```. The docker server is working and I can access the database using `docker exec -it cromwell_db mysql -u cromwell -p`. Adding to my configuration file:. ```; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?useSSL=false""; user = ""cromwell""; password = ""cromwell""; connectionTimeout = 5000; }; }; ```. And running locally:. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Produces the following log, which is the same even increasing the timeout:. ```; [2018-03-12 11:25:38,45] [info] Running with database db.url = jdbc:mysql://localhost/cromwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:676,configurat,configuration,676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['configurat'],['configuration']
Deployability,"Confirmed that AWS SDK has a dependency on netty version 4.1.22.Final from [line 93 of AWS SDK pom.xml](https://github.com/aws/aws-sdk-java-v2/blob/2.0.0-preview-9/pom.xml#L93). `<netty.version>4.1.22.Final</netty.version>`; ; This is likely coming from very old version of Scala HTTP client `val sttpV = ""0.0.16""` current version is `v.1.1.12` https://github.com/softwaremill/sttp/releases",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850:382,release,releases,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850,1,['release'],['releases']
Deployability,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:115,configurat,configuration,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179,1,['configurat'],['configuration']
Deployability,Considering the release and thus the release notes have already happened I'm going to close this even if it wasn't fully satisfied. @jsotobroad You likely already know this but FWIW the caching is not forward compatible from 0.19 to 0.21 but we expect to be forward compatible from here on out,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/685#issuecomment-253877212:16,release,release,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/685#issuecomment-253877212,2,['release'],['release']
Deployability,"Content seems good ðŸ‘ . ToL: If you don't want to in this PR, I might reorder this so that it goes from least-scary to most-scary, eg:. 1. How do I create my own configuration file for Cromwell; 1. A ""hello world"" example; 1. Description of all the options I can put in my configuration file; 1. What is reference.conf for and where is it?; 1. What is application.conf for and where is it?. Maybe with 2/3 swapped?. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2520/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837:161,configurat,configuration,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2520#issuecomment-320297837,2,['configurat'],['configuration']
Deployability,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:256,pipeline,pipeline,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542,2,['pipeline'],['pipeline']
Deployability,Cool! I've been thinking about how to approach this too. I looked at [shapeless' Typeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L28) (and [docs](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#type-safe-cast)) which co-exists with [ValueTypeable](https://github.com/milessabin/shapeless/blob/master/core/src/main/scala/shapeless/typeable.scala#L53) (declare from -> to types). Dunno if it has any advantages over rolling your own but it's worth a look to see if there are any.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762:504,rolling,rolling,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373570762,1,['rolling'],['rolling']
Deployability,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:155,patch,patch,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653,2,"['hotfix', 'patch']","['hotfixed', 'patch']"
Deployability,Could you hold off a bit? There's an update in flight to the plugin.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778:37,update,update,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778,1,['update'],['update']
Deployability,"Could you post the Cromwell configuration you're using ? Are the samples you're using relatively large?; I'm just wild guessing but one option would be that Cromwell is spending a lot of time trying to calculate md5 hashes for input files.; There's a configuration option to use file paths instead of file content to determine file equivalence for call caching purposes.; If you can make the assumptions that files are immutable and you don't go back and change their content manually this can be something to try.; To try that, you can update your Cromwell configuration: in the `backend.providers.SLURM.config` section (or whatever your backend is named instead of `SLURM`), and add this:; `filesystems.local.caching.hashing-strategy=""path""`; and this; `filesystems.local.caching.duplication-strategy=[""soft-link""]`. If possible I'd also recommend using a MySql DB instead of file based HSQL.; There are instructions here on how to do that in a few lines: http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/#lets-get-started. Let me know if that makes any difference :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902:28,configurat,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386406902,4,"['configurat', 'update']","['configuration', 'update']"
Deployability,Cromwell 0.16 is available now on Macs via `brew install cromwell`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870:49,install,install,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/372#issuecomment-170980870,1,['install'],['install']
Deployability,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:288,pipeline,pipelines,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933,3,"['Pipeline', 'pipeline']","['PipelinesApiBackendLifecycleActorFactory', 'pipelines']"
Deployability,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:125,pipeline,pipeline,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117,1,['pipeline'],['pipeline']
Deployability,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:429,update,updated,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087,1,['update'],['updated']
Deployability,"Cromwell localizes every input file in the call directory when the task gets run. You can specify how it gets localized (soft-link, hard-link, copy) in the [configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/application.conf#L148). Does this make your workflow / call fail ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741:157,configurat,configuration,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140#issuecomment-231414741,1,['configurat'],['configuration']
Deployability,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; â€Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:38,configurat,configurations,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211,1,['configurat'],['configurations']
Deployability,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:37,Pipeline,Pipelines,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898,2,['Pipeline'],['Pipelines']
Deployability,Cromwell requires Java 11 [0][1] and I see 17 there. [0] https://cromwell.readthedocs.io/en/stable/Releases/; [1] https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#60-release-notes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157:99,Release,Releases,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157,2,"['Release', 'release']","['Releases', 'release-notes']"
Deployability,"Current proposal is to support the [`disks` runtime attribute](http://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) using the following rules:. 1. For all tasks, provide a predictable bind mount for `local-disk`. Specifications for disk size and disk type will be ignored, as they are not needed or configurable at runtime for AWS Batch. ; 2. Other mount points that are defined (e.g. `disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""`) will result in additional bind mounts from the host container to the running docker container for the task. The disk size and disk type are ignored. The AWS Batch reference deployment for Cromwell will provide a mount point for each task which the `disks` will be structured under. As an example, assume the following runtime attribute definition:. ```; runtime {; disks: ""local-disk 100 SSD, /mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. Will result in a filesystem tree structure on the host:. ```; /mnt/cromwell_io_mountpoint/; â”œâ”€â”€ $CROMWELL_TASK_ID; â”œâ”€â”€ /cromwell_root; â””â”€â”€ /mnt/; â”œâ”€â”€ /my_mnt; â””â”€â”€ /my_mnt2; ```. And the running container will see the `/cromwell_root`, `/mnt/my_mnt` and `/mnt/my_mnt2` directories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923:625,deploy,deployment,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-398854923,1,['deploy'],['deployment']
Deployability,"Debrief here from a face-to-face w/ @geoffjentry :. So the config flag should be set _AND_ the call should be using docker to do the override. To be clear, the truth table here is . Using Docker on this call | Configuration Flag is set to true | Should reassign; --|--|--; T|T|T; T|F|F; F|T|F; F|F|F. Config flag should be `docker.override_umask_when_creating_directories`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463:210,Configurat,Configuration,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463,1,['Configurat'],['Configuration']
Deployability,Definitely a `Hotfix Candidate`!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3037#issuecomment-350774716:14,Hotfix,Hotfix,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3037#issuecomment-350774716,1,['Hotfix'],['Hotfix']
Deployability,"Depending on the issue, a cloud SQL issue could cause anything from a failed migration and delayed release to data loss and restore from backup. (The first is extremely more likely.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475:99,release,release,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475,1,['release'],['release']
Deployability,"Deployed in the FC dev repo, PR 349.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971:0,Deploy,Deployed,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2073#issuecomment-287393971,1,['Deploy'],['Deployed']
Deployability,"Despite the Github eliding, `StandardCacheHitCopyingActor.scala` and `PipelinesApiBackendCacheHitCopyingActorSpec` do warrant review. ðŸ™‚",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404:70,Pipeline,PipelinesApiBackendCacheHitCopyingActorSpec,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404,1,['Pipeline'],['PipelinesApiBackendCacheHitCopyingActorSpec']
Deployability,"Did I submit the PR to the wrong repo? Is the ""official"" released AWS proxy image built from somewhere other than the `supportedBackends/aws/src/main/resources/ecs-proxy` directory in the cromwell repo?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434253780:57,release,released,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434253780,1,['release'],['released']
Deployability,"Did anyone ever figure out why Cromwell would fail with an `EntityStreamSizeException`? When and how was this issue with the CI fixed?. I managed to make a similar `EntityStreamSizeException` happen using Cromwell release 66 in `run` mode, without providing a configuration, with a workflow input that specified HTTPS URLs for large files as file inputs. Is this exception ever supposed to happen? And, if so, when and why?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494:214,release,release,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4298#issuecomment-892074494,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:956,update,update,956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531,2,['update'],['update']
Deployability,Do we want this for the release ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-202949745:24,release,release,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-202949745,1,['release'],['release']
Deployability,Do we want to keep this? (from Checklist for FC to accept Cromwell doc); ```; Run long-running submissions immediately before a dev deploy and observe that they pass after the new Cromwell version picks them up and completes them.; ```; This was added after the Cromwell 34 release meltdown. There were some changes in it that caused all the running submissions to fail.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490124790:132,deploy,deploy,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941#issuecomment-490124790,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:51,update,updates,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,9,"['Release', 'release', 'update']","['Releases', 'release', 'releases', 'updates']"
Deployability,Don't forget to update `cromiam.yaml`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3945#issuecomment-409299130:16,update,update,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3945#issuecomment-409299130,1,['update'],['update']
Deployability,Draft of blog post is here: https://gatkforums.broadinstitute.org/dsde/discussion/10143/cromwell-29-released-making-way-for-big-changes/p1?new=1. Branch of changelog in github is here: https://github.com/broadinstitute/cromwell/tree/kv_crom29_relnotes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666:100,release,released-making-way-for-big-changes,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2530#issuecomment-321527666,1,['release'],['released-making-way-for-big-changes']
Deployability,Equivalent update applied.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762,1,['update'],['update']
Deployability,Er whoops totally forgot to update the docs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-124223761:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-124223761,1,['update'],['update']
Deployability,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,configurat,configuration,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,2,['configurat'],['configuration']
Deployability,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:88,pipeline,pipeline,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364,2,['pipeline'],['pipeline']
Deployability,"Even so, I'd still update the Lenthall dep if it's updated (so we can easily see it's a SNAPSHOT version)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254530925,2,['update'],"['update', 'updated']"
Deployability,"FYI @katevoss we are seeing this in FireCloud (Alpha environment, ""special snowflake Cromwell 26 hotfix 2"" aka 70741da6). Not often: on the order of 1 out of 10,000.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710:97,hotfix,hotfix,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-298697710,1,['hotfix'],['hotfix']
Deployability,FYI The conformance test that's failing can't pass until the `OutputManipulator` is also updated - I'm doing that today,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3139#issuecomment-357999800:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3139#issuecomment-357999800,1,['update'],['updated']
Deployability,"FYI for reviewers: need to retrofit `/v1` to still fill in `Some(""WDL"")` instead of `None` to fix tests, and will update docs to suggest ""draft-2"" as a workflow type version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2578#issuecomment-325698953:114,update,update,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2578#issuecomment-325698953,1,['update'],['update']
Deployability,"FYI out of curiosity I'm going to also run the full suite of our centaur tests ([removing the `-i includes`](https://github.com/broadinstitute/cromwell/blob/44/src/ci/bin/testCentaurBcs.sh#L19-L20)) to see if additional tests pass with our credentials. If you can see our test results on the Alibaba servers you may see some failures, but as long as the existing tests pass I'll be satisfied. Separately, an entry should be added to the CHANGELOG.md with a short line pointing users to the updated documentation. ([Example](https://github.com/broadinstitute/cromwell/blob/44/CHANGELOG.md#stackdriver-instrumentation)) I've been holding off suggesting this change because that file changes _a lot_ and is subject to frequent merge conflicts. Now that this PR is close enough to merging I think it's time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469:490,update,updated,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4992#issuecomment-512249469,1,['update'],['updated']
Deployability,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:134,update,updated,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['update'],['updated']
Deployability,"FYI- the failing test suite ""centaurJes"" is due to a know limitation in our test setup, but the other three look good, including the ""centaurLocal"" tests. Again, without a dockerized ""centaurTes"" we'll certainly try to avoid issues, but there won't be any guarantees that upgrades to the standard backend API don't break TES. Also, the 87 commits will need to be rebased and squashed correctly to a minimal set, on the order of 1 commit, but otherwise let us know when you're ready for re-review. Let us know if you have more questions or if we can provide any other assistance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897:272,upgrade,upgrades,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-278235897,1,['upgrade'],['upgrades']
Deployability,"False alarm, this was because I was because I was running the upgraded jar in a different directory!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963#issuecomment-348553915:62,upgrade,upgraded,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963#issuecomment-348553915,1,['upgrade'],['upgraded']
Deployability,Figured out that I need to update the contents of `gs://centaur-cwl-conformance` to match the latest commit,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-484288758:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-484288758,1,['update'],['update']
Deployability,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:144,release,release,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['release'],['release']
Deployability,"Filed https://broadinstitute.atlassian.net/browse/DSPTO-710. After a discussion with our devops representatives in channel deploy-horicromtal, I cut the autoscaling part for now because managed instance groups don't work in our env for reasons I don't fully understand (and can certainly be revisited later!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4798#issuecomment-492413869:123,deploy,deploy-horicromtal,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798#issuecomment-492413869,1,['deploy'],['deploy-horicromtal']
Deployability,"Finally I figure out one solution, but it is a little bit ugly and still look for an elegant way:. - Global variables WDL as below:. ```; workflow global {; }. task init {; command { }; output {; String version = ""v1.0""; String reference = ""hg19"". }; }; ```. - Pipeline WDL as below:; ```; import ""global.wdl"" as global. workflow pipeline {; # Global variables; call global.init; String version = init.version; String reference = init.reference; # Pipeline variables; String sample_id = ""Sample_001""; call snp { input: version = version, reference = reference, sample_id = sample_id }; }. task snp {; String version; String reference; String sample_id; command { echo ""SNP_${version} for ${sample_id} on ${reference}!"" }. output { String out = read_string(stdout()) }; }. ```; The final result is:; ```; [2018-11-21 18:23:14,32] [info] BackgroundConfigAsyncJobExecutionActor [a225847apipeline.snp:NA:1]: echo ""SNP_v1.0 for Sample_001 on hg19!""; ```. And you can see global variables are passed to the pipeline WDL while there are some workaround such as empty global workflow and helper task of init.; Is there any other solution?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243:261,Pipeline,Pipeline,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4416#issuecomment-440767243,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,Fixed in a point update. Apologies for the inconvenience. https://github.com/broadinstitute/cromwell/releases/tag/53.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049,2,"['release', 'update']","['releases', 'update']"
Deployability,"Fixed in develop. Question for @katevoss - is this worthy of a hotfix (and if so, should we wait to see if any more WOM issues bubble up in the next few days and patch them all in one go)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349472979:63,hotfix,hotfix,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349472979,2,"['hotfix', 'patch']","['hotfix', 'patch']"
Deployability,Fixed in develop. The fix will appear in 30.2 and the upcoming 31 releases.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3190#issuecomment-360554624:66,release,releases,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190#issuecomment-360554624,1,['release'],['releases']
Deployability,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:54,release,release,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550,1,['release'],['release']
Deployability,"For later when we want to begin implementing this feature:. The [Papi docs](https://cloud.google.com/genomics/docs/how-tos/migration#accessing_the_containers) recommend adding an action that looks like this:. ```json; {; ""pipeline"": {; ""actions"": [; {; ""imageUri"": ""gcr.io/cloud-genomics-pipelines/tools"",; ""entrypoint"": ""ssh-server"",; ""flags"": [; ""RUN_IN_BACKGROUND""; ],; ""portMappings"": {; ""22"": 22; }; }; ]; }; }; ```. via: https://groups.google.com/forum/#!topic/google-genomics-discuss/1nkIxKrqBk0. Permalinks versions of the floating-""master""-links in that thread:; - [""Handling of the command line flag""](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402-L404); - [""Container (""action"") added""](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979:222,pipeline,pipeline,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-492863979,6,['pipeline'],"['pipeline', 'pipelines', 'pipelines-tools']"
Deployability,"For our use cases, Iâ€™d say put responsibility on the pipeline developer. If thereâ€™s a collision, which file â€œwinsâ€ may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, weâ€™d like to distinguish â€œpipeline failureâ€ from â€œoutput failureâ€ in an automated way. So if itâ€™s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467,2,['pipeline'],['pipeline']
Deployability,For some reason the patch coverage works now. Weird that it intermittently works and not works.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240:20,patch,patch,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240,1,['patch'],['patch']
Deployability,"For this one, I basically just worked around it. The other one is still an issue. I'll send it to you with the workaround in place. We can discuss rolling back the workaround when issue #3039 is sorted. I'm hoping that is a typo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3038#issuecomment-350738471:147,rolling,rolling,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3038#issuecomment-350738471,1,['rolling'],['rolling']
Deployability,"For those that come across this PR in the future, a subset of the [docs](https://docs.codecov.io/v4.3.6/docs) referred to in this PR:; - Commit Status; - Project Status; - [Target](https://docs.codecov.io/v4.3.6/docs/commit-status#section-target); - [Threshold](https://docs.codecov.io/v4.3.6/docs/commit-status#section-threshold); - [Base](https://docs.codecov.io/v4.3.6/docs/commit-status#section-base); - [Patch Status](https://docs.codecov.io/v4.3.6/docs/commit-status#section-patch-status); - Coverage Configuration; - [Range](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-range); - [Rounding](https://docs.codecov.io/v4.3.6/docs/coverage-configuration#section-rounding); - Notifications; - [Slack](https://docs.codecov.io/v4.3.6/docs/notifications#section-slack); - Pull Request Comments; - [Disable comment](https://docs.codecov.io/v4.3.6/docs/pull-request-comments#section-disable-comment) (aka Github emails); - CodeCov YAML; - [Default Branch](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-default-branch); - [Validate your repository yaml](https://docs.codecov.io/v4.3.6/docs/codecov-yaml#section-validate-your-repository-yaml)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206:409,Patch,Patch,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2779#issuecomment-339091206,5,"['Configurat', 'Patch', 'configurat', 'patch']","['Configuration', 'Patch', 'configuration', 'patch-status']"
Deployability,"For those who are curious yet lazy, here's a link to see the updated formatting [in-place](https://github.com/broadinstitute/cromwell/blob/aednichols-patch-1/CONTRIBUTING.md)!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4937#issuecomment-489165255:61,update,updated,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4937#issuecomment-489165255,2,"['patch', 'update']","['patch-', 'updated']"
Deployability,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:10,update,update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225,1,['update'],['update']
Deployability,ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorProducer.scala:96); at akka.actor.Props.newActor(Props.scala:213); at akka.actor.ActorCell.newActor(ActorCell.scala:562); at akka.actor.ActorCell.create(ActorCell.scala:588); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:461); ... 8 more; Caused by: java.lang.ExceptionInInitializerError; at cromwell.services.SingletonServicesStore$class.$init$(ServicesStore.scala:28); at cromwell.services.keyvalue.impl.SqlKeyValueServiceActor.<init>(SqlKeyValueServiceActor.scala:16); ... 18 more; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'main'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:43); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 20 more. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:2548,configurat,configuration,2548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['configurat'],['configuration']
Deployability,"Found the forum entry looking for a solution for mounting a docker volumen. In my case I would like to run Ensembl VEP with Cromwell/WDL. Using VEP in cache/offline mode has many advantages, among them much better performance. When running VEP in cache mode it is necessary to have a large set of files locally installed. Downloading these files using the provided INSTALL.pl will be very inefficient. I plan for now to tar everything together and download and untar from a google bucket every time I run the task. However, it would be much better if I could mount a docker volume to the container running the task. The way I see it I would be able to define an snapshot in the runtime section of the task definition. I would also be able to define the mount point (docker run -v *:{mount point}) where this snapshot would be available as a docker volume. In the background Cromwell would provision a disk using the snapshot, mount it to the VM and use the correct `docker run -v /path/to/disk:/requested/mount/point` docker run command. Hope this helps defining this issue. Thanks for considering raising the priority of this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349:311,install,installed,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-334726349,2,"['INSTALL', 'install']","['INSTALL', 'installed']"
Deployability,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:181,integrat,integration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007,1,['integrat'],['integration']
Deployability,"From discussion with @geoffjentry, the purpose is to take this kind of logging out of the code; if someone wants to see these messages, they change the configuration of akka, instead (see http://doc.akka.io/docs/akka/current/java/logging.html#Auxiliary_logging_options). I've updated the PR description to reflect this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423:152,configurat,configuration,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:179,configurat,configuration,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,2,['configurat'],['configuration']
Deployability,"Given that this is being considered an error (I'd prefer ""Failure"") rather than truncation, I'm going to try to push one more time the idea that this should be a configuration option rather than some hard-coded magic number in WDL (which will inevitable need to be raised at some point, and then will different WDL versions have different `read_string` limits even when run on the same Cromwell?). If we were truncating then the worry about different results would be correct, end of story. But what we could be saying now is ""Failure. Your WDL engine isn't able to process this workflow because the file is too large. Try increasing resources or restructuring your WDL""... not really any different from ""Failure. You didn't have enough memory to run that scatter so wide..."", or indeed any other resource constraint. This is just me TOL of course... but it does seem a lot more elegant to me than having a special case for file sizes written in the WDL spec but every other resource limit being implicit and left up to the engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185:162,configurat,configuration,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294767185,1,['configurat'],['configuration']
Deployability,"Given that:; - The WDL spec today is un-opinionated on the size of an `Int`; - The WDL `Int` is defined as 64 bit in the upcoming https://github.com/openwdl/wdl/pull/250; - NB: swap the red and green diffs in your head because Jeff merged it too soon and this is a placeholder ""if we for some reason needed to revert"" PR; - And assuming that people are not currently writing workflows specifically relying on an explosion if their Ints are above 2^16. I suggest you just default to making anything WDL produce`WomLong`s in all new situations, since we'll probably sweep through and update everything else at some point soon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372:582,update,update,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372,1,['update'],['update']
Deployability,Going to merge. Centaur has been updated to address the test failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314:33,update,updated,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1084#issuecomment-229669314,1,['update'],['updated']
Deployability,Good news first: the `centaurPapiV2` build passed ðŸŽ‰ ; Bad news: the other 4 PAPI v2 builds failed ðŸ˜¢ . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:509,upgrade,upgrade,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305,1,['upgrade'],['upgrade']
Deployability,"Good point of discussion. I defaulted to ""`sbt test` by default runs all tests, for now"". Will leave it up to whomever you recommend as second reviewer to decide if integration tests should be excluded from `sbt test` as part of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078:165,integrat,integration,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078,1,['integrat'],['integration']
Deployability,"Good work! For me the problem was caused because I use the conda installation of cromwell and I made a typo in the `_JAVA_OPTIONS` environment variable, so it was running locally instead of on the cluster :man_facepalming: . Still I managed to find the issue because of your issue :smile: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460:65,install,installation,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-645263460,1,['install'],['installation']
Deployability,"Got it. . So in 36 you're a bit stuck in that it's hardcoded into the `v2alpha1` version of Pipelines API. You could use the `v1alpha2` PAPI backend, depending on if you're using PAPIv2 for a specific reason or just because it's newer (side note: Google would really prefer people to be using `v2alpha1`). As of 36.1 (just released today) that docker image is only pulled when running CWL (unclear if you're using CWL or WDL) and is configurable in the configuration file via `CWL.versions.VERSION.output-runtime-extractor.docker-image`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513:92,Pipeline,Pipelines,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513,3,"['Pipeline', 'configurat', 'release']","['Pipelines', 'configuration', 'released']"
Deployability,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681,1,['release'],['release']
Deployability,"Guys, you should update mysql connector! Most of my workflow failures were because of mysql connection loss, it is such a pain to have a pipeline running for >1 day and having stuff crashed because ""cromwell lost connection to mysql""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807:17,update,update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"H. On Fri, Feb 17, 2017, 10:58 AM Thib <notifications@github.com> wrote:. > ------------------------------; > You can view, comment on, or merge this pull request online at:j; >; > https://github.com/broadinstitute/cromwell/pull/2006; > Commit Summary; >; > - fail file hashing if the file does not exist; >; > File Changes; >; > - *M*; > supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala; > <https://github.com/broadinstitute/cromwell/pull/2006/files#diff-0>; > (17); >; > Patch Links:; >; > - https://github.com/broadinstitute/cromwell/pull/2006.patch; > - https://github.com/broadinstitute/cromwell/pull/2006.diff; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/2006>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFIEGBYy1Z6suJGLDtusapP1VvcT0mSfks5rdcOhgaJpZM4MEbxA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369:522,Patch,Patch,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2006#issuecomment-280878369,2,"['Patch', 'patch']","['Patch', 'patch']"
Deployability,"HI, having the same issue. Are there plans for an update? Happy to put in a MR to fix the type here! let me know ! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-1728318541:50,update,update,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3860#issuecomment-1728318541,1,['update'],['update']
Deployability,"Had a convo w/ Seth yesterday and looked into a few similar things (e.g. cwltool's support). I think the proper plan is as follows:. - Explore the path you've been looking at, by changing the configuration of a Cromwell backend to use Singularity instead of docker, but just for docker containers. This would cover the most common use cases; - Separately continue the [conversation at OpenWDL](https://github.com/openwdl/wdl/pull/237) to explore what support for native Singularity containers might look like in WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363:192,configurat,configuration,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412307363,1,['configurat'],['configuration']
Deployability,"Hah, you perturbed (WOTD) the hash, the test needs to be updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165262625,1,['update'],['updated']
Deployability,Happy to back out the `SampleWdl.DeclarationsWorkflow` fix within this PR if it's being updated elsewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/105#issuecomment-123483523:88,update,updated,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/105#issuecomment-123483523,1,['update'],['updated']
Deployability,Has anyone found a solution to this? I've experienced this issue since Cromwell was released for AWS and haven't found a good solution other than transferring all input files and references into the specific Cromwell S3 execution bucket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-601996944:84,release,released,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-601996944,1,['release'],['released']
Deployability,Have y'all tried `womtool womgraph`?. ~My understanding is that `womgraph` is designed to be the successor to `graph`. This is similar to how `wdltool` became `womtool`. We have failed to communicate this migration adequately.~ Not fully correct as it turns out; but `womgraph` may fill a need until `graph` is updated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561257861:311,update,updated,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561257861,1,['update'],['updated']
Deployability,"Haven't used udocker yet myself, but I'm pretty sure `${docker_script}` should be used there also instead of `${script}`. As for HPC and docker, there are some renegades out there running nodes with docker. The very few I've come across are not on-prem, spin up private larger machines on cloud resources, add HPC+docker, and then run whatever scripts on top of that. This includes our CI... until we get one of the non-root LXC implementations going. Related side-note: I would love one day for our CI to also install-then-test udocker, [rootless docker](https://github.com/moby/moby/blob/fc01c2b481097a6057bec3cd1ab2d7b4488c50c4/docs/rootless.md), singularity, etc. If anyone comes across this and knows the magic spell to fully-install-and-configure any of these container tools, or other HPC like HTCondor, GridEngine, etc., (especially on Travis!), please drop a gist or a PR. For example, here's the installation others helped me scrape together for SLURM:; - [Cromwell SLURM CI Installation Script](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/test_slurm.inc.sh); - [Cromwell SLURM CI Conf](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/resources/slurm_application.conf) with commands to run/kill/check jobs; - [Cromwell SLURM CI Test Runner](https://github.com/broadinstitute/cromwell/blob/3c3a3f85ba1229738265b11c3573ccb538b719c2/src/ci/bin/testCentaurSlurm.sh)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981:511,install,install-then-test,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470292981,4,"['Install', 'install']","['Installation', 'install-and-configure', 'install-then-test', 'installation']"
Deployability,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:51,integrat,integration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439,1,['integrat'],['integration']
Deployability,Heads up that updating the cromwell.yaml doesn't update the docs. We can mostly autogenerate docs but @katevoss did some hand editing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342612911:49,update,update,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2832#issuecomment-342612911,1,['update'],['update']
Deployability,"Hearing that the Cromwell team will reject this (and then hence the original PR) is pretty disappointing. The configuration option is specific, doesn't explicitly relate to proxies, but allows us to run Cromwell, despite the lack of proxy support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827:110,configurat,configuration,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827,1,['configurat'],['configuration']
Deployability,"Heh, I've seen it used around github-- and I now use it also-- as a soft version of thumbs-up. ""I don't really understand 100% of what this patch does, but it's small enough so...""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/320#issuecomment-165149877:140,patch,patch,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/320#issuecomment-165149877,1,['patch'],['patch']
Deployability,"Hello @alartin,. The ""local"" backend is essentially the resources on the computer where you're running Cromwell. If one runs local backend without docker, cpu & memory can't be restricted. However, if you are using the local backend with Docker, you can configure Cromwell to add the memory/cpu constraints to the run command. . Here's an example configuration that runs the Local Backend with Memory & CPU constrains applied: ; [Local Backend With Docker](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf). There's a particular stanza that sets [default values for required runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L90-L91). Choosing CPU default to be 1 memory to 4 MB (docker required minimum), so that these are the values applied to tasks that don't have cpu/memory explicitly defined. You can see that cpu/memory have been added as [custom runtime attributes](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L25-L26). And it's also possible to see the exact way they are wired into [docker run command](https://gist.github.com/ruchim/568caa82513099b9d58e9cb89cadee26#file-localwithdocker-conf-L36-L37), where memory is always converted to MB (managed by the usage of the term `memory_mb`).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436:347,configurat,configuration,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-440471436,1,['configurat'],['configuration']
Deployability,Hello @salonishah11 @cjllanwarne . It is currently not possible to mention networks from different GCP project(not the project where pipeline is running). PAPIv2 is accepting networks from from another project as well if we mention the value as; projects/PROJECT_ID_DIFFERENT_FROM_PIPELINE/global/networks/NETWORK_NAME. I know it is not possible to mention whole network using labels in project. Any chance this can be included google backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4806#issuecomment-616204371:133,pipeline,pipeline,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806#issuecomment-616204371,1,['pipeline'],['pipeline']
Deployability,"Hello again,. Just an update: error persists even when I set the `project` name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802,1,['update'],['update']
Deployability,Hello! Is there an update or workaround on this?; I am experiencing the same issue,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-841387132:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-841387132,1,['update'],['update']
Deployability,"Hello! We are still seeing this issue on version `36-fde91e6` using Cromwell-as-a-Service. For example, in the workflow ""d182afeb-33ae-45ac-8e78-ba9e0a7da7ab"", the ""SmartSeq2ZarrConversion"" task has an end time with no seconds or milliseconds but the start time is the full date-time stamp:; ```; ""start"": ""2019-01-28T10:24:40.084Z"" ; ""end"": ""2019-01-28T10:47Z""; ```. We noticed this issue in our pipeline that submits analysis results to the HCA, since the JSON schema that we follow to format the results requires that the timestamps are all in the full datetime format. . We could handle this inconsistency in our pipeline, but having Cromwell return timestamps in a consistent format would be ideal because then the tools that consume this information would not have to implement individual work-arounds to standardize the dates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-459418808:397,pipeline,pipeline,397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2743#issuecomment-459418808,2,['pipeline'],['pipeline']
Deployability,"Hello! We seem to be running into a similar issue on cromwell `34-bda9485`. After humming along without a problem for a while, we all of a sudden stopped being able to run workflows with zipped WDL imports. Looking at the metadata, we get:; ```json; ""failures"": [; {; ""message"": ""Workflow input processing failed"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""pipelines%2Fdna_seq%2FUnmappedBamToAlignedBam.wdl: Name or service not known""; },; {; ""causedBy"": [],; ""message"": ""java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)""; },...; ```; Is this the same issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244:363,pipeline,pipelines,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244,1,['pipeline'],['pipelines']
Deployability,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:104,pipeline,pipeline,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087,1,['pipeline'],['pipeline']
Deployability,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. ðŸ‘Ž. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:474,configurat,configuration,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428,1,['configurat'],['configuration']
Deployability,"Hello,. I think the problem is solved in release 78 of Cromwell. I had this problem when running the [mocha workflow](https://github.com/freeseek/mocha/blob/6679b1fcdacda4096148a75d5bb08ad1241de988/wdl/mocha.wdl#L899-L903) at Cromwell server 74. After updating to 78 the workflow completed the problematic tasks. - Cromwell 74:. ```; +--------------------+---------+------------+---------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +--------------------+---------+------------+---------------------+; | batch_id_lines | 1 | 5m34.003s | Done |; | batch_sorted_tsv | 1 | 4m45.648s | Done |; | csv2bam (Scatter) | - | 10m51.838s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 5m34.003s | Done |; | gtc | 1 | 5m27.897s | Done |; | gtc_reheader | 1 | 5m26.257s | Failed |; | idat | 1 | 5m27.897s | Done |; | idat2gtc (Scatter) | - | 10m58.206s | 0/1 Done | 1 Failed |; | red_idat_lines | 1 | 5m34.002s | Done |; | ref_scatter | 1 | 4m39.394s | Done |; | sample_id_lines | 1 | 5m34.003s | Done |; | sample_sorted_tsv | 1 | 4m42.453s | Done |; +--------------------+---------+------------+---------------------+; â—You have 1 issue:. - Workflow failed; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-gtc_reheader/maps; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-idat2gtc/shard-0/gtcs; ```. - Cromwell 78. ```; +----------------------------+---------+-----------------+-----------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_l",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:41,release,release,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918,1,['release'],['release']
Deployability,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1416,release,released,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921,1,['release'],['released']
Deployability,Here is the [Google Doc](https://docs.google.com/document/d/14cOvS3zdG5R_54R5PuM83piE1QjzTzCk8-NMKOx_Qp0/edit?ts=59692ac4) with existing ideas. . @danbills did you make any other issues for release improvement?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490:190,release,release,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332001490,1,['release'],['release']
Deployability,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:27,pipeline,pipeline,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599,1,['pipeline'],['pipeline']
Deployability,"Hey @EvanTheB -- the docs around backends need more changes, and the updates will be easier to make on my own branch. I'm going to take your changes, adjust surrounding docs and have you review that instead -- hence closing out this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3537#issuecomment-384634147:69,update,updates,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3537#issuecomment-384634147,1,['update'],['updates']
Deployability,"Hey @Horneth -- would you mind replacing the command line 7 of install.sh to say ""sbt assembly""? Just another sneaky fix for Jose!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327:63,install,install,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507#issuecomment-250545327,1,['install'],['install']
Deployability,"Hey @SHuang-Broad, yes you are right it should be pointing to 36.1. There was some conversation going on for forcing HomeBrew to recognize 36.1 as the newest version. But that has became unnecessary now as a new release should be out soon. You can look at the HomeBrew conversation [here](https://github.com/Homebrew/homebrew-core/pull/37175).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4713#issuecomment-470603661:212,release,release,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4713#issuecomment-470603661,1,['release'],['release']
Deployability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:189,install,install-linux,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,8,"['configurat', 'install']","['configuration', 'install', 'install-linux']"
Deployability,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:352,install,install,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232,4,['install'],"['install', 'installable']"
Deployability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:576,configurat,configuration,576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['configurat'],['configuration']
Deployability,"Hey @agraubert, the best way to know that a ticket will be worked on is when the pipeline for it changes to backlog bucket. . Just as an FYI, we are focused on scale improvements and small bug fixes for the near future. Out of curiosity, are you interested in contributing this feature yourself, especially with the guidance of a Cromwell developer? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147:81,pipeline,pipeline,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147,2,['pipeline'],['pipeline']
Deployability,"Hey @antonkulaga, are you running this in Cromwell 30?. The good news: this was indeed a known issue for a long time but I believe it's finally been fixed as of https://github.com/broadinstitute/cromwell/pull/3175. ; The bad news: that won't be available until the next Cromwell release. If you're comfortable building from develop you're welcome to do that and try it out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100:279,release,release,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100,1,['release'],['release']
Deployability,"Hey @dheiman - I've updated the JES behaviour of `write_lines` to match the local and SGE behaviour. . Since this seems to have unearthed a slight difference of opinion regarding the expected behaviour in a variety of situations, I made a list of scenarios in [this doc](https://docs.google.com/a/broadinstitute.org/document/d/1WWxtVwZQKrotvJLXfIflYOwidbasf-arDZUByv0Dt14/edit?usp=sharing) - and it'd be awesome if you would consider adding your opinions too! Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-296700787,2,"['a/b', 'update']","['a/broadinstitute', 'updated']"
Deployability,Hey @geertvandeweyer. I'm preparing a new release with more functionalities based on cromwell 78. It should be ready in the next few days. . Are you in the cromwell slack?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387:42,release,release,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387,1,['release'],['release']
Deployability,"Hey @geoffjentry. The formula downloads the .jar from the Github releases page and [creates a little wrapper script](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L11) that's used to make `cromwell` available as a command-line tool. There's no need to compile from source. In order to update the formula for future releases, you can just submit a PR to Homebrew by updating the [url](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L4), [SHA](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L5), and [install steps](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L9-L12). I'd be happy to do this in the future for future releases -- just include this step in whatever release checklist you may use. Homebrew is a pretty well-established community, so there's not much to contribute on that end. Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076:65,release,releases,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076,6,"['install', 'release', 'update']","['install', 'release', 'releases', 'update']"
Deployability,"Hey @indraniel -- this isn't yet supported --but the plan is to add it eventually. For now, it may be helpful to add something like an rsync at the top of your task, to see what logs are being produced by your task, or if things have gone quiet. It's also possible to add a monitoring script to run in the background that includes info about cpu/memory usage ; https://cromwell.readthedocs.io/en/stable/wf_options/Google/#google-pipelines-api-workflow-options (see ""monitoring_script"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358:429,pipeline,pipelines-api-workflow-options,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358,1,['pipeline'],['pipelines-api-workflow-options']
Deployability,"Hey @mr-c - I'm looking to update things. But this raises another question - since CWL 1.0.X is by definition backwards compatible, why would our reliance on an older version of `schema-salad` be an error on the tester? It shouldn't matter? . For instance we've talked in the past about embedding a particular version of the python code in our JAR just to ensure stability, but if I'm understanding what's causing issues on the CI (and I'm not sure I **am**, but it's my current theory at least) then if we did such a thing this would come up regardless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386,1,['update'],['update']
Deployability,"Hey @rhpvorderman, your changes look fine to me. One thing I noticed is that we've used `${script}` when I believe it should actually be `${docker_script}` to get the container relevant path for the script. Can you confirm in your configurations?. Otherwise I'm happy to click approve from my side (if I'm allowed to do that?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299:231,configurat,configurations,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299,1,['configurat'],['configurations']
Deployability,"Hey @sychan, definitely unfortunate this got lost. 1. This wasn't relevant for me because our pipelines (generated from Janis) inserted the digest at transpilation time so it wasn't relevant. 2. Would make sense to me, but Broad wanted to treat this feature as unstable and generally unsupported. 3. When I made the PR, I was a contributor on this repo, hence the internal branch so it could run tests, unfortunately I'm not on this repo so effectively can't touch the branch, unless I have recreated it in my local fork. 3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948:94,pipeline,pipelines,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948,1,['pipeline'],['pipelines']
Deployability,"Hey All -- I think this is a great feature. If I'm understanding correctly, this should also be a big help locally because this mode will use the same credentials that are used for gcloud. That is, if you have done a 'gcloud login' before, it will use those credentials. . Thinking about that... what if we just remove the 'user' mode for google altogether? It's sort of a pain to do, and having gcloud installed seems a really low bar for someone who wants to run using a google backend as they will likely need gcloud to transfer data, manage their account, etc. If we don't want to remove features... I think we should at least rename them so that 'application-default' sounds more appealing so that is what people do (maybe even call it default).; - k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166117677:403,install,installed,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166117677,1,['install'],['installed']
Deployability,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:160,release,release,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,1,['release'],['release']
Deployability,"Hey Jaeyoung--is there a public version of such an image I can test with on; Google? As for AWS, I think there's a real feature request to make Cromwell; compatible with docker images with an entrypoint on AWS. On Tue, Jun 25, 2019 at 11:39 PM Jaeyoung Chun <notifications@github.com>; wrote:. > I'm definitely having this problem with AWS Backend. Not sure how newest,; > but I believe I had this problem during the HCA Pipeline Surges as well.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2461?email_source=notifications&email_token=ADR7XTPGWVEOCY34LELUVMDP4LQHRA5CNFSM4DTKAP42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYSHD6I#issuecomment-505704953>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ADR7XTLSYROEYXWXSLAFQ2LP4LQHRANCNFSM4DTKAP4Q>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640:421,Pipeline,Pipeline,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505867640,1,['Pipeline'],['Pipeline']
Deployability,"Hey Miguel -- happy to brainstorm/tagdeam on the tools for this if you need help. I think the ""SELECT... FOR UPDATE"" semantics are what we need here. See https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html. Basically when you do the something like . SELECT ... FROM workflow_store WHERE [is new or dead workflow] ORDER BY submission_time LIMIT n FOR UPDATE. will prevent two things from touching that workflow at the same time. Then just make sure the update of status happens in the same transaction and you're all set. . Give a shout... my next two days are shockingly meeting light!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3342#issuecomment-371248824:109,UPDATE,UPDATE,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3342#issuecomment-371248824,3,"['UPDATE', 'update']","['UPDATE', 'update']"
Deployability,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:962,release,released,962,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637,2,['release'],['released']
Deployability,"Hey there, thanks for the bug report! I believe this is actually fixed in our later versions (we just released v22) - could you confirm whether upgrading resolves this issue?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510:102,release,released,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555#issuecomment-253861510,1,['release'],['released']
Deployability,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:192,install,installed,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,1,['install'],['installed']
Deployability,"Hi - sorry I had missed the original question from @myazinn and needed to dig into ad hoc files a bit before opining on first #5057 from @Kirvolque and now #5064. I'm still not in a position where I can answer definitively but my first question would be why not follow the pattern from the [GCP backend](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L716) and [TES backend](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L95) and modify the `mapCommandLineWomFile` function in `AwsAsyncBackendJobExecutionActor`. . It's entirely possible that something about the AWS backend's wiring would lead to that not being the right choice, but that's what I want to understand.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509691549:385,pipeline,pipelines,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509691549,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"Hi - thanks to both of you. In this case my procrastination paid off, I've been meaning to find a way to work this example into something like the examples conf or the docs, so having this updated example is great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438719642:189,update,updated,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438719642,1,['update'],['updated']
Deployability,"Hi - yes, I can confirm this. Cromwell has had WDL 1.0 support for not quite a year now. You're right that this should be updated (pinging @cjllanwarne )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777:122,update,updated,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777,1,['update'],['updated']
Deployability,"Hi @AlekseiLitkovetc,. > I would like to know which two tests should pass with non-default credentials. Back in March when I filed the ticket we were only [including tests for four](https://github.com/broadinstitute/cromwell/blob/38/src/ci/bin/testCentaurAws.sh#L24-L27) of our dozens of Centaur tests. After the AWS hackathon, the test coverage expanded to only [exclude a few](https://github.com/broadinstitute/cromwell/blob/39/src/ci/bin/testCentaurAws.sh#L25-L53) remaining tests. So, the original minimum A/C was to see these tests passing with non-default creds:; - [hello](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/hello.test); - [long_cmd](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/long_cmd.test); - [haplotypecaller.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/haplotypecaller.aws.test); - [singlesample.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/singlesample.aws.test). > could you please prompt where I can find a link to Jenkins?. Our Jenkins servers are only internally accessible to Broad employees because of Jenkins continued problems with security. Perhaps one day we'll move off Travis to another public CI-as-a-Service that will run tests longer than 180 minutes, such as CircleCI or Google Cloud Build. Then we could migrate all tests to one place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164:916,integrat,integrationTestCases,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164,2,['integrat'],['integrationTestCases']
Deployability,"Hi @Redmar-van-den-Berg, you're correct, there appears to be a bug in our draft-2 parser which is failing to catch this. To answer ""which is correct"", the requirement to wrap values in arrays was not being enforced correctly but it now is. In your example you can do this with the array literal syntax, eg:; ```wdl; call ls {; input: files = [ i ]; }; ```. I have added a test for our WDL 1.0 support which **is** catching this properly, so if you're able to upgrade your workflows from WDL draft-2 to WDL 1.0, then `womtool validate` will give you the correct answer. If not, I'll leave this open as a bug since it certainly *should* be picked up by womtool. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219:459,upgrade,upgrade,459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219,1,['upgrade'],['upgrade']
Deployability,"Hi @SergeySdv, we have marked your PR as ""back with originator"" so that you can update it based on the discussion. Once it is ready for review just leave a comment and we will take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574:80,update,update,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574,1,['update'],['update']
Deployability,"Hi @TMiguelT - I fully agree that Cromwell should support some of the more degenerate base images like busybox and alpine. This is partially historical (at one point Pipelines API required Bash so we had no reason to not just use bash ourselves), and going forward it's more been a lack of putting in enough testing of these sorts of containers to keep ourselves honest - we **intend** to be more agnostic but things fall through here and there",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453554372:166,Pipeline,Pipelines,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453554372,1,['Pipeline'],['Pipelines']
Deployability,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:799,integrat,integration,799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170,1,['integrat'],['integration']
Deployability,"Hi @aednichols, . For us there's a large price difference between regular vs Spot VM on GCP hence the pursuit of purely pre-emptible pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346:133,pipeline,pipelines,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346,1,['pipeline'],['pipelines']
Deployability,"Hi @antonkulaga - you're right, and likely more than just Pairs. It's been a while since we've updated the plugin so it's not totally on board with newer WDL constructs at the moment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584:95,update,updated,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-299453584,1,['update'],['updated']
Deployability,"Hi @antonkulaga, your `application.conf` file is missing `--cidfile ${docker_cid}` inside submit-docker, which creates `docker_cid` file. So if you update your `submit-docker` to ""docker run --rm **--cidfile ${docker_cid}** -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"", it should work! You can look at Cromwell's default config for submit-docker [here](https://github.com/broadinstitute/cromwell/blob/e6c7704b9300db8852ae9ac7fbefe39ef6ba71d7/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34). Let me know if this doesn't work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050:148,update,update,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011#issuecomment-418474050,1,['update'],['update']
Deployability,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:42,integrat,integrates,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,1,['integrat'],['integrates']
Deployability,Hi @carolynlawrence. Would a configuration option to remove this privilege re-assignment be sufficient to fix this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606:29,configurat,configuration,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374695606,1,['configurat'],['configuration']
Deployability,"Hi @chapmanb - sorry for the delay in responding here. I was able to get http inputs to work in CWL against a default (ie no custom config specified) instance of Cromwell in server mode. The test case I used is in the linked PR (#4392). I wonder whether you could confirm:; - Whether this test case works for you, and if so:; - Is your use of HTTP inputs different somehow?; - How can I enhance my test case to cover whatever is different?; - Or, whether this test case does not work for you, and if so:; - We might try to work out what is different between your configuration and the default which might be breaking things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089:563,configurat,configuration,563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-439193089,1,['configurat'],['configuration']
Deployability,"Hi @cjllanwarne Thanks for the update. I looked at #5468 and #5554 changes made. They all S3 related changes/enghancements. This fix is EFS specific and hence not addressed by the above two. So this change is needed. ; Thanks,; Vanaja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474,1,['update'],['update']
Deployability,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:142,pipeline,pipelines-tools,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625,6,['pipeline'],"['pipelines', 'pipelines-tools']"
Deployability,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:153,update,updates,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399,2,['update'],['updates']
Deployability,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:184,pipeline,pipelines,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:729,update,updated,729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['update'],['updated']
Deployability,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR ðŸ˜„).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:27,rolling,rolling,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,1,['rolling'],['rolling']
Deployability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:56,patch,patch,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,4,"['configurat', 'patch']","['configuration', 'patch']"
Deployability,Hi @geoffjentry Thanks for your response. I was trying to walk throught `[Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/)`. On the `Start the job` section I wasn't able to choose files. I downloaded 33.1 from [(https://github.com/broadinstitute/cromwell/releases)]. Here is the system version:; ```; $ lsb_release -a; No LSB modules are available.; Distributor ID:	Ubuntu; Description:	Ubuntu 16.04.4 LTS; Release:	16.04; Codename:	xenial; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308:284,release,releases,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308,2,"['Release', 'release']","['Release', 'releases']"
Deployability,"Hi @gn5 , please see `delete_intermediate_output_files` on this page: https://cromwell.readthedocs.io/en/stable/wf_options/Google/. Note that this feature only works on Google Pipelines API v2 at the moment. This is because PAPIv2 is the primary use case for Cromwell within the Broad Institute. When the option is enabled, Cromwell is smart enough to remove the files automatically from being considered in call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620053705:176,Pipeline,Pipelines,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620053705,1,['Pipeline'],['Pipelines']
Deployability,"Hi @henriqueribeiro , . Do you think I can compile this branch with functionality of the latest cromwell release (77) ? I'm now using the branch based on cromwell 58, but more recent versions have retry strategy that's interesting as well: ; - you retry logic handles spot kills; - cromwell retry handles the fetch_and_run is a directory problem. . => both would be great, but since it doesn't get approved, I hope to make a new custom build. However, it says here there are conflicts... . Greetings, ; geert",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775:105,release,release,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775,1,['release'],['release']
Deployability,"Hi @huangzhibo, thanks for your contribution! I have two comments:. ---. 1. I think this feature is OK if the cromwell owner wants to allow it, but not in all cases. ; - Eg if I'm hosting a Cromwell instance I might not want users to be able to specify an arbitrary location on my filesystem to write their workflow contents. ; - I want to allow others to comment on this rather than making any solid statements... personally I think at the very least we'd want to require an opt-in in the configuration to enable this - something like `""allow_workflow_specified_execution_root""`?. ---. 2. Could you consider adding a centaur test for this? This should help get you started:; - Have a look in `centaur/src/main/resources`; - Make a new test directory for the new test; - Add a workflow that can determine the workflow root - off the top of my head, maybe running `pwd` from within the `command` and selecting the workflow root from the output?; - Add a new workflow options file alongside the workflow file and use it to specify an execution root.; - Create a new `.test` file next to the others, and use it to specify your workflow and your workflow options as inputs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028:490,configurat,configuration,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-438388028,1,['configurat'],['configuration']
Deployability,"Hi @jainh,. The runtime section is very backend-implementation specific, but to answer your question in the abstract:. The wdl spec [says](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#runtime-section) that ""Values can be any expression â€¦"". For example:. Valid wdl string:; ```; runtime {; my_key: ""a 'b c' d""; }; ```. Valid wdl array:; ```; runtime {; my_key: [""a"", ""b c"", ""d""]; }; ```. The following however is **invalid** according to the spec as the keys are duplicated:; ```; runtime {; my_key: ""a""; my_key: ""b c""; my_key: ""d""; }; ```. It is then up to the backend to decide and implement what keys and values it will accept. The config backend is currently implemented to [only](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/DeclarationValidation.scala#L43-L50) support primitive wdl types (WdlInteger, WdlString, WdlFloat, WdlBoolean) and their optional wrappers. While it does not support them, one could update that code to support arrays of values too. Meanwhile, the JES backend already does support arrays for some attributes, e.g. for [`zones`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L127) and [`disks`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L142).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343:1039,update,update,1039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343,1,['update'],['update']
Deployability,"Hi @leepc12 - This is a really good question. Internally our interest in requester pays has been fully under the umbrella of a separate team so we haven't run into this. I suspect that this would require the Pipelines API (i.e. JES) to support it and isn't something we can change directly. We can dig into this a bit. Note that Pipelines API has a new version coming out soon which will open up all kinds of functionality, I suspect this is already on the list. Again, we can dig into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-345524845:208,Pipeline,Pipelines,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-345524845,2,['Pipeline'],['Pipelines']
Deployability,"Hi @multimeric ,. Thank you so much for providing the current location of those links. . I still have a question regarding the solution proposed here. In the current documentation website, I could not find the guidance or CloudFormation for creating a custom AMI, which is described above. And moreover, I also could not find the description saying that ""The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as \cromwell_mount"", which was mentioned above as the solution. . Is it because the whole creation procedure has been changed since then? In the current documentation, I only found [this link](https://docs.opendata.aws/genomics-workflows/core-env/create-custom-compute-resources.html#custom-amis) which only briefly talked about creating a custom AMI but not gave any CloudFormation link. Do you know where I should look for this information? Thanks!. Also, thank you for letting me know about the Genomics CLI. I have to say that the current deployment procedure on AWS is way more complicated than GCP, especially given that my company's cloud team puts more restrictions which complicates the standard procedure that used to work in my personal AWS account. I'm looking forward to hearing from the development of Genomics CLI. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156:997,deploy,deployment,997,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156,2,['deploy'],['deployment']
Deployability,"Hi @rasmuse - a few thoughts. In terms of the submit time keep in mind things like JVM initialization. Calling individual invocations of a java program like that for what's effectively a blink of an eye operation is never going to be ideal from a performance perspective. If you're submitting to a Cromwell server consider using something like `curl` instead. . On the second part, there are a few things potentially going on here. First is that Cromwell doesn't necessarily immediately start a workflow. It scans every `n` seconds for new workflows to start, which defaults to `20`. In a worst case scenario `21` of your `20` seconds could be due to that, although that seems unlikely. You can make that time window shorter by overriding the `system.new-workflow-poll-rate` configuration setting to something smaller, e.g. `1`. Even then, there's a some overhead in there as ultimately we're trying to optimize for a case that's not running single, extremely short tasks. I just ran the moral equivalent of a hello world workflow locally with that config setting set to 1 second. The workflow was picked up for execution at `11:08:44` and registered as complete at `11:08:51` with exactly half of that time spent with the system running the underlying job (i.e. not in Cromwell) so it might be worth revisiting this w/ a combination of using `curl` and speeding up the workflow polling rate. That said while I'd love you to continue to use Cromwell/WDL, it might not wind up being the best tool for your job. If these workflows are purely for yourself & you don't intend on building them up over time and/or distributing them to others, you might want to check out Snakemake which is more intended to be a direct Makefile replacement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936:775,configurat,configuration,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378636936,1,['configurat'],['configuration']
Deployability,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:117,configurat,configuration,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,2,['configurat'],['configuration']
Deployability,Hi @salonishah11 ; I've updated PR and resolved conflicts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281:24,update,updated,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281,1,['update'],['updated']
Deployability,"Hi @seandavi - That does seem like it should work. Thinking back in my past I've definitely encountered tools which expect TMPDIR to exist and aren't smart enough to create it themselves. Also in JES there shouldn't be any issues with permissions, etc. We'd certainly welcome a PR if you're game for it, either (or both) against `0.19_hotfix` or `develop`. On that note, I should point out that a new release (currently `develop`) is imminent and for all but one use case (call caching) we beliee it to be more robust/stable that 0.19. I'd personally recommend people who don't need call caching work with the new system, but I understand that some people aren't comfortable working with code which isn't yet released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458:401,release,release,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,2,['release'],"['release', 'released']"
Deployability,"Hi @tseemann - . Thanks, we're aware. We followed the instructions on brew's site and used the revision flag, but eventually worked it out with @ilovezfs to use `28.1` in this casee. We don't, at least for now, release our software to bioconda. We've since decided to switch to an x.y system as our x versions are meaningful internally and shouldn't be changing for things like critical bug fixes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254:211,release,release,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480#issuecomment-317223254,1,['release'],['release']
Deployability,"Hi @vsoch - here's what I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possibl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:291,install,install,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,2,"['configurat', 'install']","['configuration', 'install']"
Deployability,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:153,configurat,configuration,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720,1,['configurat'],['configuration']
Deployability,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:21,update,updated,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921,2,"['integrat', 'update']","['integration', 'updated']"
Deployability,"Hi @wleepang . Yes, you are right. The problem I have is particularly when using a slurm backend, as I don't find an easy way to load environment modules except to actually modify the individual command section of each task definition in my workflow. This is inconvenient because when I'm running the same pipeline on AWS batch, there are no environment modules, so my tasks fail unless I explicitly remove all the `module load <module name>` from the command part of each task. I would like to switch back and forth between cloud and cluster without having to touch the pipeline script (i.e. individual tasks) itself. Put differently, can I specify a runtime attribute called `module` much like the `docker` attribute, and then somehow modify the backend configuration settings to have cromwell load this module? . Did I explain myself better this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782:306,pipeline,pipeline,306,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502902782,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,Hi @wleepang @cjllanwarne @aednichols !; Could you please take a look at updated changes?; Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432:73,update,updated,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432,1,['update'],['updated']
Deployability,"Hi @zfrenchee, you can find the docker images for Cromwell 36.1 in our Releases tab [here](https://github.com/broadinstitute/cromwell/releases/tag/36.1). Is this the information you were looking for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467576363:71,Release,Releases,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-467576363,2,"['Release', 'release']","['Releases', 'releases']"
Deployability,Hi Brad. In addition to the `http` entry in the backend filesystems config the `http` filesystem also needs to be defined system-wide. Cromwell's `reference.conf` [defines this already](https://github.com/broadinstitute/cromwell/blob/35_hotfix/core/src/main/resources/reference.conf#L290) so as long as that's being pulled into your configuration and you're not overriding the filesystems you should be set.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644:333,configurat,configuration,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426052644,1,['configurat'],['configuration']
Deployability,"Hi Brad. To revert to the external cwltool process you can specify this in your configuration: . ```hocon; cwltool-runner {; class = ""cwl.CwltoolProcess""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816:80,configurat,configuration,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4183#issuecomment-426012816,1,['configurat'],['configuration']
Deployability,"Hi ChenYong,. I'm closing this issue here on GitHub as Cromwell [30-16f3632 / 30.2](https://github.com/broadinstitute/cromwell/releases/tag/30.2) seems to run fine against a basic MariaDB 5.5.56. I also tried changing the database initialization script to run with-and-without setting `SET GLOBAL sql_mode = 'ANSI_QUOTES';`. If you're still running into problems, can you please create a post over in the [Ask the WDL team](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) forum? There please provide as many logs and configuration files as possible (without passwords) so that your issue may be reproduced. ---. To test Cromwell with MariaDB I combined the following files and ran them from an `issues_3346/` directory with `docker-compose up`. - `issues_3346/compose/cromwell/app-config/application.conf`; - `issues_3346/compose/cromwell/Dockerfile`; - `issues_3346/compose/mysql/init/init_user.sql`; - `issues_3346/docker-compose.yml`. The files are in this archive: [issues_3346.tar.gz](https://github.com/broadinstitute/cromwell/files/2190721/issues_3346.tar.gz). Cromwell started and connected to the db. I was able to browse to `http://localhost:80` and submit a workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457:127,release,releases,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346#issuecomment-404688457,2,"['configurat', 'release']","['configuration', 'releases']"
Deployability,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:405,pipeline,pipeline,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402,1,['pipeline'],['pipeline']
Deployability,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:784,Pipeline,Pipeline,784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['Pipeline'],['Pipeline']
Deployability,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:419,Pipeline,Pipeline,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,4,"['Pipeline', 'update']","['Pipeline', 'updated']"
Deployability,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:376,Pipeline,Pipeline,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605,12,"['Pipeline', 'deploy', 'pipeline']","['Pipeline', 'Pipelines', 'deployment-manager', 'pipeline', 'pipeline-data', 'pipelines']"
Deployability,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:799,pipeline,pipeline,799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,4,['pipeline'],['pipeline']
Deployability,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:761,pipeline,pipelines,761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipelines']"
Deployability,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:73,update,updated,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['update'],['updated']
Deployability,"Hi thanks for reporting this issue, could you post your Cromwell configuration ? Specifically the [call caching part of the backend configuration](https://github.com/broadinstitute/cromwell/blob/cea07d69919a609362d2e374888f9ed8c4220564/cromwell.examples.conf#L332) (if any)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431:65,configurat,configuration,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393883431,2,['configurat'],['configuration']
Deployability,"Hi, . I downloaded the jar file from the GitHub release page:. > wget https://github.com/broadinstitute/cromwell/releases/download/34/cromwell-34.jar. sha256sum results in the hash mentioned by @Horneth. Actually I have two java versions on my system:. ```; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```; and. ```; openjdk version ""1.8.0_141""; OpenJDK Runtime Environment (build 1.8.0_141-b16); OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode); ```. It fails with the first one, however I can launch the server with the openjdk version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300:48,release,release,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300,2,['release'],"['release', 'releases']"
Deployability,"Hi, I see you've indeed created a service account and gotten a json file, but I'm not seeing how you're passing it to Cromwell.; Your configuration uses `application_default` as authentication mode, and you are logged in using your personal gmail it seems.; Did you use this json in any way ?. To use the service account in Cromwell you'd want to either; 1 - Recommended) Change your configuration to use the service account instead of application default; You can see how to do that [here](https://cromwell.readthedocs.io/en/develop/backends/Google/); It is slightly outdated, instead of `pem-file` use `json-file` and the path to your json.; 2) You can keep application default and use [`gcloud auth activate-service-account`](https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account) to authenticate as the service account on your machine. Also could you print the result of `gcloud auth list` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451:134,configurat,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392038451,2,['configurat'],['configuration']
Deployability,"Hi, I updated this patch with suggestions made by @mcovarr and also updated README.md documenting the new SGE backend optional parameters. Let me know what you guys think!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-222772287:6,update,updated,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-222772287,3,"['patch', 'update']","['patch', 'updated']"
Deployability,"Hi,. I am wondering if there was progress made on that issue? . Running GATK pipelines uses a lot of disk space for intermediate (bam) files, which is problematic for large cohorts. It seems that removing those files before the pipeline complete would break the Cromwell cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532:77,pipeline,pipelines,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881#issuecomment-620049532,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > [â€¦](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:671,install,install,671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410,1,['install'],['install']
Deployability,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:23,update,updated,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,4,"['Install', 'configurat', 'release', 'update']","['Installing', 'configuration', 'release', 'updated']"
Deployability,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) â€” You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:445,update,updated,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491,4,"['Install', 'configurat', 'release', 'update']","['Installing', 'configuration', 'release', 'updated']"
Deployability,"Hi,. any update here? I will be really interested in these answers, as well. Thanks a lot!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232:9,update,update,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232,1,['update'],['update']
Deployability,"Hi,; ; I'm experiencing the same problem on AWS Batch. My workflow has 2 subworkflows. Even I don't change any part of my workflow/subworkflow, the caching only works for the first task in subworkflows. The subsequent tasks cannot be recognized by hashing. I guess this is because the subworkflow id is also involved in the task inputs, so it change hash. . Does anyone have any update or workaround for this problem? Thank you in advance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581:379,update,update,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4699#issuecomment-716072581,1,['update'],['update']
Deployability,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"Hi. Call caching is turned off by default, did you turn it on in your configuration? Also call caching will require the use of a MySQL-like database to preserve caching information between runs. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031:70,configurat,configuration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395410031,1,['configurat'],['configuration']
Deployability,Highly related to the [summarizer and worker Cromwell](https://github.com/broadinstitute/cromwell/issues/4781) issue and probably a better place to start since this is our existing configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741:181,configurat,configuration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4780#issuecomment-478984741,1,['configurat'],['configuration']
Deployability,"Hiï¼; I am glad to see this issue, and I have also tried using PBS as the backend to run it. But I'm not very good at it.; Can you show me how the configuration file for cromwell is defined when using PBS as the backend?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521:146,configurat,configuration,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967#issuecomment-2105600521,2,['configurat'],['configuration']
Deployability,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:149,Configurat,ConfigurationFiles,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,3,"['Configurat', 'configurat']","['ConfigurationFiles', 'configuration', 'configuration-file']"
Deployability,Hmm yeah you're right. Scrolling up it looks like this ticket was referencing hotfix and not develop. And yeah we're not even trying to load this key on develop so Cromwell certainly shouldn't fail for its absence.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653:78,hotfix,hotfix,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224691653,1,['hotfix'],['hotfix']
Deployability,"Hmm, ok, sounds like the documentation might need an update - otherwise, can you double check that you can connect to the mysql instance from outside of its docker container (ie running `mysql -u cromwell -p` from where Cromwell will run instead of where `mysql` is running?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373069972:53,update,update,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373069972,1,['update'],['update']
Deployability,"Hmmm, still stuck on this - any updates from your guys' end? I tried cloning and resubmitting, still getting the same error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050:32,update,updates,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050,1,['update'],['updates']
Deployability,Hotfix version,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401064686:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401064686,1,['Hotfix'],['Hotfix']
Deployability,Hotfix version: #4157,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4156#issuecomment-424930302:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4156#issuecomment-424930302,1,['Hotfix'],['Hotfix']
Deployability,Hotfix worthy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477:0,Hotfix,Hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477,1,['Hotfix'],['Hotfix']
Deployability,How about rolling your two new fields up with @Horneth's new field idea and making a composite object that has all the caching stuff in one logical grouping that only increases arity by 1 instead of 3? :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198528803:10,rolling,rolling,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/585#issuecomment-198528803,1,['rolling'],['rolling']
Deployability,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,configurat,configuration,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885,10,['configurat'],['configuration']
Deployability,"I actually only ever got this issue once, and that's it. I am currently re-running the same WDL pipeline and it is working fine. But if you send me a new JAR, I am more than happy to run that from now on and report back if I encounter the same issue again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444:96,pipeline,pipeline,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444,1,['pipeline'],['pipeline']
Deployability,I added a commit to this branch with some updates to the `README.md` which gives a scatter example,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137723052:42,update,updates,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137723052,1,['update'],['updates']
Deployability,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:75,configurat,configuration,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253,1,['configurat'],['configuration']
Deployability,"I added the `concurrent-job-limit` to the `reference.conf` and I will add it to the [Configuration draft on the WDL website](http://gatkforums.broadinstitute.org/dsde/discussion/8687/how-to-configure-cromwell), tracked in [DSDE-docs #1524](https://github.com/broadinstitute/dsde-docs/issues/1524).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191:85,Configurat,Configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1841#issuecomment-276804191,1,['Configurat'],['Configuration']
Deployability,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:73,rolling,rolling,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178,2,"['configurat', 'rolling']","['configurations', 'rolling']"
Deployability,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:90,integrat,integration,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052,1,['integrat'],['integration']
Deployability,I also hit this while experimenting with using singularity with cromwell. Just replicating https://github.com/kundajelab/atac-seq-pipeline/blob/master/docs/tutorial_local_singularity.md locally. Get the error with cromwell-37 and with a fresh build of develop branch. Works with cromwell-36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989:130,pipeline,pipeline,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989,1,['pipeline'],['pipeline']
Deployability,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:850,pipeline,pipelines,850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:211,install,installed,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450,2,"['install', 'patch']","['installed', 'patch']"
Deployability,"I am experiencing an issue that may also be related to this, using WDL draft-2 spec and cromwell-39.; Here is a dummy example I created off a real error I received, it is minimal but hopefully descriptive enough:. ```WDL; task example {; Map[String, File] sample_files; Array[Array[String]]? tax_id_and_name; String? summary_report_name. String default_summary_report = select_first([summary_report_name, 'summary_report.txt']). command <<<; set -ex; example_command \; -o ${default_summary_report} \; -i ${write_json(sample_files)} \; ${ if defined(tax_id_and_name) then '-t ' + write_tsv(tax_id_and_name) else '' }; >>>; runtime {; docker: ""<local or private image name with the custom `example_command` installed>""; }; output {; File summary_report = ""${default_summary_report}""; }; }; ```; The run fails and the offending log output from Cromwell says:. ```commandline; example_command -o summary_report.txt -i /cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_json_b428b2ef25b3a99656256ecf58545736.tmp -t /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp; E Unable to open file /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp for reading (No such file or directory). Stopped at /usr/bin/example_command line 192.; ```. `write_json()` has no issue creating a path within the container, while `write_tsv()` returns a host path which is not found within the container.; I am able to workaround this at the moment by using `basename(write_tsv())` since the file is still in the execution directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411:706,install,installed,706,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411,1,['install'],['installed']
Deployability,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:126,release,release-,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700,1,['release'],['release-']
Deployability,"I am having the same problem. [I think the issue may be here.](https://github.com/broadinstitute/cromwell/blob/b4ec53e0f038c3e27a7a3a8b483066c962cc164d/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L135). This seems to be where we check which outputs are type File or Directory, I think it's perhaps missing File-typed outputs within structs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196:177,pipeline,pipelines,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"I am of two minds on this. I would be happy to chat. Nonetheless, for now, let's not create impediments within the sprint by having outside work being a burden. Next sprint you can create a task that has only a few points relating to its integration not the actual coding and testing. . Thumb typed for added typos. > On Jun 19, 2015, at 10:19 AM, Scott Frazer notifications@github.com wrote:; > ; > Heh yeah.. though that velocity will be artificially high because a lot of the work I do for Cromwell (including this PR and the next PR) I do off hours.; > ; > â€”; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678:238,integrat,integration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678,1,['integrat'],['integration']
Deployability,I am running into an issue whereby this feature would be helpful. I am developing workflows using CWL but one of the command line tools being used in CWL requires the runtime attribute 'bootDiskSizeGb' to be set to 100 instead of 10 for this particular task. As CWL doesn't have an equivalent attribute the only way I can set this is in the 'default-runtime-attributes' section of my configuration file but now all tasks for the workflow will use 100GB for their boot disk size instead of the single task that actually requires it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941:384,configurat,configuration,384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511067941,1,['configurat'],['configuration']
Deployability,"I am using exactly the wdl and json offered by gatk GitHub page for gatk4-germline-snps-indels, locally, I got this error, intervals-hg38.even.handcurated.20k.intervals is larger than 128000 Bytes. Maximum read limits can be adjusted in the configuration under system.input-read-limits.; I tried to change it via type this in command line: java -Dsystem.input-read-limits=500000 -jar /cromwell-34.jar ; Didn't work.; Who can tell me how to fix it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960:241,configurat,configuration,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960,1,['configurat'],['configuration']
Deployability,"I am using this code and trying various configurations to write workflow logs, but I find myself confused a lot... Currently the README has [three environment variables](https://github.com/broadinstitute/cromwell/blob/ks_copy_logs/README.md#logging) listed (`LOG_ROOT`, `LOG_MODE`, `LOG_LEVEL`). It appears that `logback.xml` expects `LOG_MODE` to be `pretty` or `standard` but `WorkflowDescriptor` is expecting it to be `server`. I also couldn't figure out how to both output logs to stdout and also write to a workflow log (a feature I'd be particularly interested in!). Also it appears that we only honor `LOG_ROOT` when `LOG_MODE` is `server`. Can we maybe have a tech talk whenever you get a chance to understand how all these options play together?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329:40,configurat,configurations,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188390329,1,['configurat'],['configurations']
Deployability,"I am using v2alpha1, and WDL. If I understand you correctly, does that mean that if I upgrade to 36.1, it should stop adding that action to the pipelines request?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466548089:86,upgrade,upgrade,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466548089,2,"['pipeline', 'upgrade']","['pipelines', 'upgrade']"
Deployability,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:130,upgrade,upgrade,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['upgrade'],['upgrade']
Deployability,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:14,patch,patch,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111,1,['patch'],['patch']
Deployability,I believe this has been done already: https://github.com/broadinstitute/cromwell/blob/develop/release/release_workflow.wdl#L24-L33. @Horneth please re-open if I misunderstood.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-424931955,1,['release'],['release']
Deployability,"I believe this is complete with the release of Cromwell 25, is this ready to be closed @ruchim?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513:36,release,release,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1986#issuecomment-286130513,1,['release'],['release']
Deployability,"I believe this might be a consequence of the fact that the `WorkflowExecutionActor` is responsible for sending status updates to the metadata, and under the current load it accumulates those updates in its mailbox. Those updates then get processed and it's possible that 2 status updates close to each other in the mailbox end up generating the same timestamp. Those timestamps reflect the `WEA` view of the world, which might be delayed compared to reality if it's very busy. If that's not the desired behavior we could maybe have each job (EJEA) independently send status updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559:118,update,updates,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2173#issuecomment-295318559,5,['update'],['updates']
Deployability,I believe we auto release womtool now. @Horneth please re-open if I missed something.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416:18,release,release,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2400#issuecomment-424933416,1,['release'],['release']
Deployability,"I believe we want to patch this off of the cromwell hash we are currently; using - 3eb1623; https://github.com/broadinstitute/cromwell/commit/3eb1623d9a5ffdf0fc3626820eab84ae6560b2cd. On Fri, Mar 18, 2016 at 12:01 PM, mcovarr notifications@github.com wrote:. > Sounds good; > ; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198426842",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262:21,patch,patch,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198459262,1,['patch'],['patch']
Deployability,I came here to say what I apparently said a few months ago already :). We don't have access to a SLURM cluster so anything we put together would be a guess on our part. I know folks have been able to pretty easily get LSF & PBS working based on our example of an SGE configuration so my assumption is that it's not hard but I have no way of knowing. If someone were to get it working and submit docs we'd happily accept them but we have no way of handling that ourselves.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190:267,configurat,configuration,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-281771190,1,['configurat'],['configuration']
Deployability,"I came out with a custom and dirty way of going around this issue. In my configuration file, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker containe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:73,configurat,configuration,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,6,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,I can add a test.; I'm all for making PRs to CWL conformance tests but it's going to increase the merge time of our PRs if we want to wait for it to be in the CWL repo. Also we'd need to unpin the hash for conformance test or update it every time..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225:226,update,update,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374939225,1,['update'],['update']
Deployability,"I can also offer to help, in whatever form is useful! If you just need to use / pull, then Singularity image support via installing it should fit the bill. Users can use Github to host images via Singularity Hub. If you want to host your own registry, then Singularity Registry is the way to go! Let me know if I can help, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650:121,install,installing,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330348650,1,['install'],['installing']
Deployability,I can reproduce this problem on Cromwell 29 but happily this runs fine on the forthcoming Cromwell 30. We're hoping to release this **very** soon. ðŸ™‚,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972#issuecomment-349014226:119,release,release,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972#issuecomment-349014226,1,['release'],['release']
Deployability,"I can't seem to reproduce this - I have a workflow running, stop cromwell, restart it, and no matter how many times I do it I don't see anything wrong with the sequence of events:. ```. 2016-07-27 16:48:47,144 INFO - Slf4jLogger started; 2016-07-27 16:48:48,456 cromwell-system-akka.actor.default-dispatcher-3 INFO - Bound to /0.0.0.0:8009; 2016-07-27 16:48:48,460 ForkJoinPool-2-worker-15 INFO - Cromwell service started...; 2016-07-27 16:48:48,703 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell_new; 2016-07-27 16:48:50,251 INFO - Reading from cromwell_new.DATABASECHANGELOG; 2016-07-27 16:48:50,337 INFO - Successfully acquired change log lock; 2016-07-27 16:48:50,400 INFO - Successfully released change log lock; 2016-07-27 16:48:50,689 cromwell-system-akka.actor.default-dispatcher-3 INFO - 1 new workflows fetched; 2016-07-27 16:48:50,689 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Restarting workflow UUID(c6eb4949-cb81-4a56-b3de-11b1cde3e13e); 2016-07-27 16:48:50,693 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - WorkflowManagerActor Successfully started WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e; 2016-07-27 16:48:50,773 cromwell-system-akka.dispatchers.engine-dispatcher-15 INFO - WorkflowActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-07-27 16:48:51,258 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: Call-to-Backend assignments: hello.hello -> JES; 2016-07-27 16:48:51,284 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - MaterializeWorkflowDescriptorActor-c6eb4949-cb81-4a56-b3de-11b1cde3e13e [UUID(c6eb4949)]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; 2016-07-27 16:48:51,291 cromwell-system-akka.dispatchers.engine-dispatcher-15 INF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544:715,release,released,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196#issuecomment-235716544,1,['release'],['released']
Deployability,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:284,configurat,configuration,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288,1,['configurat'],['configuration']
Deployability,"I did a configuration using a local MySQL server without docker. I guess that the problem was that my docker machine does not connect the `localhost` address from the VM to the host machine. Thus, the `localhost` port was not providing the connection to the MySQL server. This might be a common problem in MacOS computers, which are running `boot2docker`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603:8,configurat,configuration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-373318603,1,['configurat'],['configuration']
Deployability,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:209,configurat,configuration,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027,2,['configurat'],['configuration']
Deployability,"I did that and began encountering the following error:; ```; 2019-02-25 18:17:52,693 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:145,configurat,configuration,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['configurat'],['configuration']
Deployability,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,integrat,integrations,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633,2,['integrat'],['integrations']
Deployability,I don't know why Github conversation screen is not being updated with new commits.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424#issuecomment-247664605,1,['update'],['updated']
Deployability,"I don't know. I usually mentally bundle that into the general ""metadata updates get missed"" issue but that doesn't really answer the ""root cause"" question.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478634137:72,update,updates,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478634137,1,['update'],['updates']
Deployability,I don't think it closes anything no. It should be enough for the release though ? We'll see if/where we need more retries in the logs if this error pops up again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850:65,release,release,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850,1,['release'],['release']
Deployability,"I don't think the pull/build should be built in to the command that is run at scale - the image should be pulled /built once, and then the direct path passed into the script / pipeline that is run at scale.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463887263:176,pipeline,pipeline,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463887263,1,['pipeline'],['pipeline']
Deployability,"I don't think this needs reviews, instead direct comments to the centaur PR for the 29 hotfix",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2848#issuecomment-343969150:87,hotfix,hotfix,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2848#issuecomment-343969150,1,['hotfix'],['hotfix']
Deployability,"I dropped a [question into the Google Genomics google group](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE) about the ""stuck"" Google Genomics Pipeline operations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269:172,Pipeline,Pipeline,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260142269,1,['Pipeline'],['Pipeline']
Deployability,"I dug up the source for my statement about autocommit:. >If autocommit mode is enabled, each SQL statement forms a single transaction on its own. https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467:211,rollback,rollback,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467,1,['rollback'],['rollback']
Deployability,"I exposed this as `monitoring_image` workflow option, and included the Dockerfile & script in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`. Should we also add a CI script that builds the image?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217:120,pipeline,pipelines,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217,1,['pipeline'],['pipelines']
Deployability,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:48,update,update,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381,4,"['Pipeline', 'pipeline', 'update']","['PipelinesApiDockerCacheMappingOperations', 'pipelines', 'update']"
Deployability,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; â€¦java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:84,update,updates,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546,2,['update'],"['update', 'updates']"
Deployability,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:180,configurat,configuration,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,1,['configurat'],['configuration']
Deployability,I got the same issue when update to version 46. Any idea? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608,1,['update'],['update']
Deployability,"I guess I'm still confused... is there anything for me to do with this ticket? On both hotfix and develop, I can leave out the `system.workflow-restart` and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224696780:87,hotfix,hotfix,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224696780,1,['hotfix'],['hotfix']
Deployability,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:78,release,release,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440,1,['release'],['release']
Deployability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:91,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,3,"['Pipeline', 'configurat']","['PipelinesApiAsyncBackendJobExecutionActor', 'configuration']"
Deployability,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:101,release,release,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,1,['release'],['release']
Deployability,"I have checked in a new version. Will make a pull request for it soon,. https://github.com/broadinstitute/cromwell/blob/mjs-AWS-config-example-fix/cromwell.example.backends/AWS.conf. On Wed, Sep 16, 2020 at 6:14 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > would you guys update ""cromwell/cromwell.example.backends/AWS.conf""; >; > it seams this file for old version .; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5857>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EK6P5Z7C4RDBY2BIVDSGCFZTANCNFSM4ROS34OQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576:314,update,update,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576,1,['update'],['update']
Deployability,"I have confirmed and worked-around by changing the configuration -o and -e parameters to ; ```; -o ${out}.cromwell; -e ${err}.cromwell; ```; identical duplicated files are now written to the work directory. (Identical except in the case of error, which is when I need these files anyway). My request is to remove the >(tee) lines, but I understand they probably exist to serve some other backend. The ability to turn them off would be appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752:51,configurat,configuration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752,1,['configurat'],['configuration']
Deployability,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:332,update,updated,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782,1,['update'],['updated']
Deployability,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:84,pipeline,pipelines,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,2,"['Pipeline', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'pipelines']"
Deployability,"I have no idea why the ""codecov/patch"" test is failing, or how to fix it - the ""details"" link isn't very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488093257:32,patch,patch,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488093257,1,['patch'],['patch']
Deployability,"I have no reason to believe develop is any less stable today than it was a few weeks ago, but if anyone feels otherwise please speak up! . Branching GotC releases from a branch based at this hash means we'll need to put fixes on both the GotC branch and develop going forward. That will become increasingly difficult as these branches diverge. And eventually PBE Cromwell would be released with the full bolus of ported fixes that were never previously tested at scale.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018:154,release,releases,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198489018,2,['release'],"['released', 'releases']"
Deployability,"I have not seen it in a while. And I think your suggestion would work.; Feel free to close this ticket. On Sun, Feb 26, 2017 at 1:35 PM, kshakir <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Any more updates on this ticket?; >; > In general we were wondering if a gcloud logout and then gcloud login; > helped.; >; > If this is no longer an issue, mind closing this one, and open another in; > the future with current wdl / details / version-info?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282576487>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk8Bmz3FbZVh7tyQAJ63aCZyquTaoks5rgcYPgaJpZM4KnP3t>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570:233,update,updates,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644#issuecomment-282735570,1,['update'],['updates']
Deployability,"I have not used this configuration in some time. On Aug 6, 2017 14:39, ""Geraldine Van der Auwera"" <notifications@github.com>; wrote:. @LeeTL1220 <https://github.com/leetl1220> Do you have a reproducible test; case? Otherwise we probably need to close this. â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320524441>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk2gj4A8fOPuWbRAQvNF1k1H9Ct9Aks5sVgh-gaJpZM4LrbMZ>; .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147:21,configurat,configuration,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895#issuecomment-320647147,1,['configurat'],['configuration']
Deployability,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:537,configurat,configurations,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438,1,['configurat'],['configurations']
Deployability,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:53,Configurat,Configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,2,"['Configurat', 'configurat']","['Configuration', 'configuration-examples']"
Deployability,"I have to note that I didn't make this configuration (@rhpvorderman will know more about this); This is in the backend section:; ```; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; ```; This is on the top level:; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848:39,configurat,configuration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-393886848,1,['configurat'],['configuration']
Deployability,"I have to update my ""workflow inputs"" branch to update that YAML file... :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/183#issuecomment-140139777:10,update,update,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/183#issuecomment-140139777,2,['update'],['update']
Deployability,"I haven't run the workflow manually, but I don't see any commits in the last few releases which would have helped this. I think we should mark this as a bug (even if the ""fix"" just ends up being a test case to prove that it's fixed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670#issuecomment-493539374:81,release,releases,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670#issuecomment-493539374,1,['release'],['releases']
Deployability,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:46,continuous,continuous,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506,6,"['continuous', 'release', 'upgrade']","['continuous', 'releases', 'upgrade']"
Deployability,I heard chatter about a 30.2 release ...is there any chance this change can make it in that release? It's mostly for FC users as the current failure logs are sent to the server logs and basically the user never sees call caching fail even though the job succeeds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283:29,release,release,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283,2,['release'],['release']
Deployability,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:158,release,release,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476,4,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:86,release,releases,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136,1,['release'],['releases']
Deployability,"I just found out that Cromwell-Singularity integration will be on the agenda on Winter Codefest 2018, starting tomorrow! See https://docs.google.com/document/d/1RlDUWRFqMcy4V2vvkA1_ENsVo6TXge2wIO_Nf73Itk0/edit#heading=h.xg79ql4rt605. You can join in (also remotely) by checking this file: https://docs.google.com/spreadsheets/d/1o4xDUgl2iu_CgFuDpB1swtG8XVZK3aifvKlhh5qagyI/edit#gid=0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186:43,integrat,integration,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186,1,['integrat'],['integration']
Deployability,I just updated the description,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/441#issuecomment-182541538:7,update,updated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/441#issuecomment-182541538,1,['update'],['updated']
Deployability,"I just want to point out that this used to work in Cromwell 29, so some sort of regression has happened such that sub workflows aren't working anymore. I'm not sure what kind of sub workflow integration tests you guys have, but it looks like they aren't comprehensive enough. Feel free to add this one to your test suite (it's actually not a super complicated sub workflow). . This is pretty important to some of the work we're doing with Gaddy to get the somatic genome pipeline ready (we can't run the samples for him). And the ultimate goal of this project is to bring more users to FireCloud...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823:191,integrat,integration,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:207,patch,patching,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['patch'],['patching']
Deployability,I merged the DRS stability PR https://github.com/broadinstitute/cromwell/pull/7179 to `develop` and then updated this branch from it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515:105,update,updated,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515,1,['update'],['updated']
Deployability,"I might have missed it -- can you put together the files so we can; reproduce this (cromwell configuration, WDL and json)? We might need some; permissions, but I want to run this sort of thing continually. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Nov 8, 2016 at 12:32 PM, Lee Lichtenstein notifications@github.com; wrote:. > N=4/4 on my workflow...; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259203566,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4gwHvsAAXHshNNM4GFWqWhx1Cvrsgks5q8LI_gaJpZM4Ko1_r; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344:93,configurat,configuration,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-259207344,1,['configurat'],['configuration']
Deployability,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:227,pipeline,pipeline,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513,1,['pipeline'],['pipeline']
Deployability,I need to make an update to changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/479#issuecomment-190455236:18,update,update,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/479#issuecomment-190455236,1,['update'],['update']
Deployability,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:172,configurat,configuration-reference,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687,1,['configurat'],['configuration-reference']
Deployability,"I only updated test code, so I'm ignoring codecov's complaint as I have no idea what it's talking about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670:7,update,updated,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670,1,['update'],['updated']
Deployability,I placed the configuration option into backend/abortJobsOnTerminate. If you guys want me to move or rename it to something else I'm happy to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183:13,configurat,configuration,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175077183,1,['configurat'],['configuration']
Deployability,I posted a heads up in the WDL blog; will update WDL user docs when v29 is released. Any update on its ETA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487:42,update,update,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-320520487,3,"['release', 'update']","['released', 'update']"
Deployability,I pushed the updates that I think might fix the JES and bad label test cases. Still working on getting the sbt tests set up locally,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497390203:13,update,updates,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497390203,1,['update'],['updates']
Deployability,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:56,pipeline,pipeline,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,1,['pipeline'],['pipeline']
Deployability,I realized that downstream libs won't automagically get the updated cats library so no need to relax this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698:60,update,updated,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698,1,['update'],['updated']
Deployability,"I saw that, but since `File?` isn't a compound type, I'm under the impression that example was for comparison to the actual compound types. That's the inconsistency I'm not groking -- since `File?` isn't an example of a compound type, it seems that example's existence implies that something that accepts a `File` should also accept a `File?`, which is indeed the case with Cromwell's integration for size() but not basename() or sub(). . If we relied entirely on what the spec's headings and examples said as being the only acceptable inputs, then basename() wouldn't work on `File` at all because the spec says it actually takes in a `String`, not a `File`, and has no `File` examples. Since basename() works on `File` it seems Cromwell is already going beyond what the 1.0 spec explicitly says.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450:385,integrat,integration,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450,1,['integrat'],['integration']
Deployability,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:81,upgrade,upgrades,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760,1,['upgrade'],['upgrades']
Deployability,I snuck in a quick 31.1 GitHub release that includes the fix for this. Thanks again for reporting it!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390436881:31,release,release,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390436881,1,['release'],['release']
Deployability,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:157,configurat,configuration,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141,1,['configurat'],['configuration']
Deployability,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:328,integrat,integration,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['integrat'],['integration']
Deployability,I suspect that making outputs and inputs a toggle-able option will kick this particular can significantly down the road.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067:43,toggle,toggle-able,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067,1,['toggle'],['toggle-able']
Deployability,"I think I can do better! It's all on the branch [here](https://github.com/vsoch/wgbs-pipeline/tree/add/singularity). These would normally render into a nice site given being served from Github pages, but a markdown file will hopefully do for now :). https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md. Let me know if you have questions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416300815:85,pipeline,pipeline,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416300815,2,['pipeline'],['pipeline']
Deployability,"I think I found the one you are talking about. ![screen shot 2018-10-15 at 11 28 47 am](https://user-images.githubusercontent.com/2978948/46960909-92efc580-d06d-11e8-97fe-d81ef63da81a.png). The failure reason is . ```; Task requester_pays_engine_functions.functions:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```. which is not related to the requester pays feature but rather yet another dockerhub flaky response that we should ask google to retry IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811:553,configurat,configuration,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811,1,['configurat'],['configuration']
Deployability,"I think I misunderstood how to use `flock` in a bash script, so I've updated this with a tentative improvement:. ```bash; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419:69,update,updated,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419,1,['update'],['updated']
Deployability,"I think in April we chose Option 1 with a sprinkling of Option 3 in https://docs.google.com/a/broadinstitute.com/document/d/1feRDusWXQQ2pJ03sNHTNmrrnnwL3y-vtyF1fv_RdogU/edit?usp=sharing whereas this is clearly Option 2... (I'm not saying it's wrong, this seems to address all of the ""cons"" as I saw them with gusto... just pointing it out :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340:92,a/b,a/broadinstitute,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340,1,['a/b'],['a/broadinstitute']
Deployability,"I think it's extremely unlikely we will be attempting to support SELinux even on an infinite timescale. The vast majority of our install base runs in containers, either on Kubernetes or Docker. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213:129,install,install,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213,1,['install'],['install']
Deployability,"I think so, if you and @kshakir or someone else don't mind giving it a second look that'd be nice though because I updated it a bit last week after realizing it was broken in some case.; I should also probably update the changelog to advertise the introduction of `WaitingForQueueSpace` status",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373188385:115,update,updated,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373188385,2,['update'],"['update', 'updated']"
Deployability,"I think the ""CaaS"" comment at the end should be put into the headline (ie something like `CaaS wraps 404 into 500 on releaseHold requests`). And we should probably check this works in other cases where 404s might be returned, not just `releaseHold`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863:117,release,releaseHold,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863,2,['release'],['releaseHold']
Deployability,I think the PAPI Centaur `/bin/bash` dependency is purely an artifact of having a job shell effectively hardcoded to `/bin/bash` for the previous 31 releases of Cromwell so that unintentionally `/bin/bash` dependent WDLs were written into the test suite. ðŸ™‚,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761:149,release,releases,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761,1,['release'],['releases']
Deployability,"I think the minimum requirements are:; 1) Project-wide `Genomics Pipelines Runner` and `Compute Instance Admin (v1)` _roles_ for the service account used by Cromwell itself.; 2) `Service Account User` _permission_ on `Compute Engine default service account` for the Cromwell service account.; 3) `Storage Object Admin` _permission_ on the Cromwell execution and data buckets, as well as `Storage Object Viewer` on the GCR bucket (if used).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-439653262:65,Pipeline,Pipelines,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-439653262,1,['Pipeline'],['Pipelines']
Deployability,I think this should be fixed in 25 hotfix or develop. I entered #1945 a while back for what sounds like the same issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654:35,hotfix,hotfix,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2162#issuecomment-293007654,1,['hotfix'],['hotfix']
Deployability,"I think what what we should do is once the hotfix is in place to replace the jar. Can have a note here that makes a mention of it, but honestly no one is going to come back here to read the changelog after they've already pulled the release, and as @mcovarr points out it's not even possible if they're using a MySQL based database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103:43,hotfix,hotfix,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2456#issuecomment-315820103,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,"I think what you're running into is that the `endpoint-url` refers to where the Life Sciences / Genomics application _itself_ runs, which is different than where the compute VMs spin up. . You can try putting `us-west2` in the WDL runtime section, or in the workflow options JSON. Both of those should be readily Googleable in terms of documentation, if you get stuck definitely comment. > The location you specify is only used to store metadata about the pipeline operation. [Source.](https://cloud.google.com/life-sciences/docs/concepts/locations#available_locations)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365:456,pipeline,pipeline,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365,1,['pipeline'],['pipeline']
Deployability,"I think you're right, in order to allow for private IPs, the noAddress field needs to be added both in the pipeline and the run resources.; It's possible that the `pipelineArgs`resources don't contain what they're supposed to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406:107,pipeline,pipeline,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501#issuecomment-250503406,2,['pipeline'],"['pipeline', 'pipelineArgs']"
Deployability,I updated README in my PR #2058,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661:2,update,updated,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-285116661,1,['update'],['updated']
Deployability,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:2,upgrade,upgraded,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,2,"['release', 'upgrade']","['release', 'upgraded']"
Deployability,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:122,pipeline,pipeline,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,3,['pipeline'],"['pipeline', 'pipeline-gilad']"
Deployability,"I wanted to second this, as we're setting the same issue submitting to PBSPro clusters; with the latest Cromwell development version. Looking through the code, it appears to; originate from https://github.com/broadinstitute/cromwell/blob/33c58ef22b6a8edc4c1912c1416225c79d298f76/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala; which was tweaked since the last release in a change from @cjllanwarne (https://github.com/broadinstitute/cromwell/commit/33c58ef22b6a8edc4c1912c1416225c79d298f76#diff-39fe7186c2383fc1135f29a9c05e4e57) but I don't; grasp the scope of the change enough to know if this triggers it. In our CWL run, the jobs get submitted to the cluster and run okay based on the; work directories in `cromwell-execution` but the polling dies with:; ```; [2019-01-17 12:34:15,18] [info] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Status change from - to Running; [2019-01-17 12:34:16,27] [ESC[38;5;220mwarnESC[0m] DispatchedConfigAsyncJobExecutionActor [ESC[38;5;2mf2e0c573ESC[0malignment_to_rec:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); at scala.concurrent.Promise$.fromTry(Promise.scala:138); at scala.concurrent.Future$.fromTry(Future.scala:635); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:691); at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:691); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:418,release,release,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['release'],['release']
Deployability,I was encountering the call caching issue with the latest released version 87. The issue is resolved with the develop branch 88-90ca58d.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915:58,release,released,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915,1,['release'],['released']
Deployability,"I was ultimately able to reproduce this on 36 by putting the value for `y` in the inputs JSON instead of as a default. The same scenario works fine on latest `develop`, so I am going to close this issue with the advice that you upgrade to 37.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474470766:228,upgrade,upgrade,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474470766,1,['upgrade'],['upgrade']
Deployability,"I would still love to see precomputed metadata for completed workflows (would need to be updated on label PATCHes, but ....). But I think that's a larger project",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429107279:89,update,updated,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429107279,2,"['PATCH', 'update']","['PATCHes', 'updated']"
Deployability,I'd also like to cherry pick this onto 32 hotfix if the shorebirds allow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3789#issuecomment-397692617:42,hotfix,hotfix,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3789#issuecomment-397692617,1,['hotfix'],['hotfix']
Deployability,"I'd be surprised if these changes can't be migrated fairly easily to 0.19-hotfix. Metadata is an area under heavy construction on develop, and I suspect you're right this will need to be completely reimplemented once the dust settles. ðŸ˜¦ . I'd certainly thumbs up a 0.19-hotfix version of this with updates to the README.md. ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219124036:74,hotfix,hotfix,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219124036,3,"['hotfix', 'update']","['hotfix', 'updates']"
Deployability,I'd buy that logic assuming the hotfix had been thumbed. ðŸ˜›,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050:32,hotfix,hotfix,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315835050,1,['hotfix'],['hotfix']
Deployability,I'd double check what I say with @kshakir as he always corrects me but I'd vote for putting this in `src/bin/release`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947:109,release,release,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1780#issuecomment-267122947,1,['release'],['release']
Deployability,"I'd just mark the original PR as ""Hotfix Candidate"" tbh, then we can do everything all in one commit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3076#issuecomment-352557525:34,Hotfix,Hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3076#issuecomment-352557525,1,['Hotfix'],['Hotfix']
Deployability,"I'll start working with development, then. We don't have anything in; ""production"" that we cannot break. Thanks for the direction. On Aug 14, 2016 14:05, ""Jeff Gentry"" notifications@github.com wrote:. > Hi @seandavi https://github.com/seandavi - That does seem like it; > should work. Thinking back in my past I've definitely encountered tools; > which expect TMPDIR to exist and aren't smart enough to create it; > themselves. Also in JES there shouldn't be any issues with permissions, etc.; > ; > We'd certainly welcome a PR if you're game for it, either (or both); > against 0.19_hotfix or develop. On that note, I should point out that a; > new release (currently develop) is imminent and for all but one use case; > (call caching) we beliee it to be more robust/stable that 0.19. I'd; > personally recommend people who don't need call caching work with the new; > system, but I understand that some people aren't comfortable working with; > code which isn't yet released.; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AAFpE17tmdKE5bY42PWUnqMW6YV6rifAks5qf1jugaJpZM4INzbb; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994:650,release,release,650,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994,2,['release'],"['release', 'released']"
Deployability,"I'll update the issue title... this isn't about SGE per-se, but rather about running local + some other back end in a single workflow. Google JES is fine for that. Just to further clarify, this doesn't require being able to schlep files between the two (although it would be great to know if that works!) but as @delocalizer mentioned, a use case of running something locally that generates a non-file type (e.g. string) that can be passed as input to a task on another backend would be the thing to verify or get working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986:5,update,update,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-253986986,1,['update'],['update']
Deployability,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:120,integrat,integration,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778,1,['integrat'],['integration']
Deployability,"I'm a :-1: on this idea. Seems like it is overly demanding as compilation is less of an ask than committing code. For instance, now our build servers must have git secrets installed where it is irrelevant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418:172,install,installed,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418,1,['install'],['installed']
Deployability,I'm a bit confused by this... it's labelled with 0.20 milestone but this is for a feature that's only in 0.19? This should be fixed on the 0.19 hotfix branch right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224686926:144,hotfix,hotfix,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/845#issuecomment-224686926,1,['hotfix'],['hotfix']
Deployability,"I'm a little wary of introducing 2 different AWS sdk's into the project. Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?. Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165:128,update,updates,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165,1,['update'],['updates']
Deployability,I'm caught in a release cycle but believe the relevant code belongs here for others watching:. https://github.com/broadinstitute/cromwell/blob/fc43c6954eb47912930e4c28f73781a112fdfbf8/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-459409119:16,release,release,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-459409119,1,['release'],['release']
Deployability,I'm checking out WDL/Cromwell at the moment and this feature would make Cromwell definitely more interesting. It would make it much easier to run reproducible pipelines without relying on docker. (Docker is a no go on our cluster because it gives users root access.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888:159,pipeline,pipelines,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-352784888,1,['pipeline'],['pipelines']
Deployability,"I'm closing this ticket because the issue is addressed in our next release (and our develop branch, in case you're feeling brave). I've also added and started working on https://github.com/broadinstitute/cromwell/issues/1940 for the `File?` inputs bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216:67,release,release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276804216,1,['release'],['release']
Deployability,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:217,pipeline,pipelines,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152,2,['pipeline'],['pipelines']
Deployability,"I'm confused. Sandbox mode doesn't/shouldn't require sudo. We included it because it requires the *least* permissions to use it. . As for `pull`, it seems to force rebuild the image every time, exactly the same as `build --force`. What we want ideally is a way to build the image if it doesn't exist or needs to be updated, but if neither is the case, then do nothing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252:315,update,updated,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463866252,1,['update'],['updated']
Deployability,"I'm definitely having this problem with AWS Backend. Not sure how newest, but I believe I had this problem during the HCA Pipeline Surges as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953:122,Pipeline,Pipeline,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-505704953,1,['Pipeline'],['Pipeline']
Deployability,"I'm experiencing the issue, running the Broad ""gatk4-data-processing"" pipeline on their sample data, on Google Cloud. Repository with their code: https://github.com/gatk-workflows/gatk4-data-processing. The only change I made to the .wdl was setting Pre-emption to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:70,pipeline,pipeline,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipeline']"
Deployability,"I'm fine with pushing this in without integration tests, assuming @tovanadler has manually tested. :+1: . Nominating @Horneth as second reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938:38,integrat,integration,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938,1,['integrat'],['integration']
Deployability,I'm going to close this MR until it is ready to be updated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838:51,update,updated,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838,1,['update'],['updated']
Deployability,I'm going to overrule `codecov/project` here since it appears to be a hiccup. The patch coverage is at around 77% (which is higher than the usual project overall average) so I'm pretty confident this doesn't represent an actual overall coverage reduction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-470343985:82,patch,patch,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-470343985,1,['patch'],['patch']
Deployability,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:1373,install,install,1373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888,1,['install'],['install']
Deployability,"I'm not fully up to date on original issue, but this patch verifies that all hsqldb connections use `mvcc`, whether liquibase or slick created. Assigning @mcovarr as first reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/122#issuecomment-125431277:53,patch,patch,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/122#issuecomment-125431277,1,['patch'],['patch']
Deployability,"I'm not great / experienced with Cromwell, and to be honest I'm not sure what native support would mean. What I was trying is to just treat a singularity container like an executable, and add it as a Local backend, sort of like this --> https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. That works to run the analysis step (in a singularity container) just using singularity like any executable. I don't totally understand the job_id so there is a bug, but my colleague @bek is going to take a look! The container is run to produce the output, so that's a good start at least (and probably I'm missing something huge here). So to answer your question... in my wdl at least, I'm just using the same local commands. It looks the same as it would running any Local backend configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375:267,pipeline,pipeline,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411571375,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"I'm not sure I understand what you mean by ""putting globs in arguments is still not supported"" ?. In the meantime you can override what the glob command is using this field in your backend configuration: `glob-link-command`; For instance: `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`. This is not well documented, I'll fix that.; (`GLOB_PATTERN` and `GLOB_DIRECTORY` are placeholders that will be replaced at runtime with the right values)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644:189,configurat,configuration,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4395#issuecomment-439384644,1,['configurat'],['configuration']
Deployability,"I'm not sure about the specifics your issue, but the latest [Cromwell release (version 86) ](https://github.com/broadinstitute/cromwell/releases) has some improvements related to pulling docker images.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572:70,release,release,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572,2,['release'],"['release', 'releases']"
Deployability,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:97,release,releases,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750,1,['release'],['releases']
Deployability,"I'm wondering if it'd make more sense to add it to the [release wdl](https://github.com/broadinstitute/cromwell/blob/3e577223845ee8d20cab38590579b65fa73fe64e/release/release_workflow.wdl). @tomkinsc what do you think, yo'ure just looking for this to be applied to official releases, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532:56,release,release,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301188532,3,['release'],"['release', 'releases']"
Deployability,"I'm wondering if this is related to cromwell creating new job definitions for **every** new call, versus using parameter substitution to modify the inputs for a single job definition? There may be some sort of backend issue with the integration to the AWS APIs that and old job definition is being called incorrectly instead of yet another new definition being created with the correct inputs? . This would track with the workflow log saying that the job definition already exists and then re-using a job that has inputs for a completely different sample.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451:233,integrat,integration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451,1,['integrat'],['integration']
Deployability,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:431,update,update,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505,2,['update'],['update']
Deployability,"I've gone through and updated the dependency graph of updating sttp, you can view diff at https://github.com/delagoya/cromwell/tree/update-depversions . Still one more set of errors to fix: ; ```; root(update-depversions)> | 31>; [info] Compiling 4 Scala sources to $HOME/src/cromwell/womtool/target/scala-2.12/classes...; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:46: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val thisLevelNodesAndLinks: NodesAndLinks = callsAndDeclarations foldMap { graphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:56: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val subGraphNodesAndLinks: NodesAndLinks = subGraphs foldMap { wdlGraphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:26: private val clusterCount in object GraphPrint is never used; [error] private val clusterCount: AtomicInteger = new AtomicInteger(0); [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:40: local default argument in method listAllGraphNodes is never used; [error] def upstreamLinks(wdlGraphNode: WdlGraphNode, graphNodeName: String, suffix: String = """"): Set[String] = wdlGraphNode.upstream collect {; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:54: value foldMap is not a member of Set[wom.graph.GraphNode]; [error] graph.nodes foldMap nodesAndLinks _; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:89: private method nodesAndLinks in class WomGraph is never used; [error] private def nodesAndLinks(graphNode: GraphNode): NodesAndLinks = {; [error] ^; [error] 6 errors found; [error] (womtool/compile:compileIncremental) Compilation failed; [error] Total time: 4 s, completed Apr 16, 2018 9:00:54 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626:22,update,updated,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626,3,['update'],"['update-depversions', 'updated']"
Deployability,"I've had a request from @geoffjentry and @mcovarr to convert the ""validation response aggregation"" into a separate actor - see https://docs.google.com/a/broadinstitute.com/document/d/1qTXiPtiJcmfmWghLC_uBeWlL2SuCE2Ek1xBiT0j8-iQ/edit?usp=sharing - I'll rework this PR then re-open",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678:151,a/b,a/broadinstitute,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-207589678,1,['a/b'],['a/broadinstitute']
Deployability,"I've made the builds of both toolsets, updated the dockerfile (see dsde-pipelines branch kc_jg_turbocharge), built and pushed the docker image for our testing (kcibul/tiledb-with-gcloud:2.2.5-1492828987) in the JG WDL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337:39,update,updated,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296491337,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,I've merged this as-is to make sure it gets into the release! ðŸŽ‰,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153:53,release,release,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2049#issuecomment-284017153,1,['release'],['release']
Deployability,"I've tested both WDLs [in the forum post](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) on firecloud-dev, which is running Cromwell 27. Neither of them work. Reopening this issue, but there isn't anything for us over in FC to do here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031:117,configurat,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-307478031,1,['configurat'],['configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file']
Deployability,"IIRC (which is really not guaranteed), as far as the localization of `cwl.inputs.json` is concerned in PAPI 2, [this bit](https://github.com/broadinstitute/cromwell/blob/aeca54929b5d85e7961ac01a784c08a129cfc265/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L686) is what maps the cloud path of ""ad hoc files"" to the localized path (which the AWS equivalent [doesn't have](https://github.com/broadinstitute/cromwell/blob/aeca54929b5d85e7961ac01a784c08a129cfc265/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L420)).; So it's possible the `cwl.inputs.json` is actually localized but not to the right place and so the tool can't find it.; Again my memory is fading quickly so don't take this as ðŸ’¯ :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459299224:236,pipeline,pipelines,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-459299224,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,IMO anything being updated in a shared environment should be protected but i also loathe shared mutable state so YMMV,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-437031892:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344#issuecomment-437031892,1,['update'],['updated']
Deployability,"IMO if we put any security recommendations in our README we should be very careful to disclaimer it. **Heavily**. Maybe with something along the lines of:. ```; Warning! ; - Only YOU are responsible for your own security! ; - Cromwell is NOT a security appliance! ; - What follows are ideas and starting points and not necessarily a secure system configuration in your situation""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538:347,configurat,configuration,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259192538,1,['configurat'],['configuration']
Deployability,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:457,release,releases,457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092,1,['release'],['releases']
Deployability,"If it is intended that the `qsub` files also capture the `command` output, then I think the only 'bug' is the default configuration and docs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416:118,configurat,configuration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393106416,1,['configurat'],['configuration']
Deployability,If it's a recurring issue whould it not make sense to have it configurable?; 1. Patch it on every cromwell release; 1. Hard-code a chmod reset in every WDL command,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690:80,Patch,Patch,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394406690,2,"['Patch', 'release']","['Patch', 'release']"
Deployability,If that test is chronically failing due to Docker Hub flakiness we should tag it as Integration so it doesn't break our builds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762:84,Integrat,Integration,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762,1,['Integrat'],['Integration']
Deployability,If the AWS backend uses ioActor this may already be covered in configuration?; ```; system {; io {; # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # the quota availble on the GCS API; #number-of-requests = 100000; #per = 100 seconds. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }; }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778:63,configurat,configuration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778,1,['configurat'],['configuration']
Deployability,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:39,integrat,integration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034,1,['integrat'],['integration']
Deployability,"If you back Cromwell with a database (see [persisting data between restarts](https://cromwell.readthedocs.io/en/stable/tutorials/PersistentServer/)), you are able to stop Cromwell and restart without job information loss. After setting up this database, you can enable [call-caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) which if you modify your workflow, tools or inputs, will use previously computed results where:. - The command line used to generate the files is exactly the same; - The computed files still exist; - Other [call-caching configurations](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/#call-caching-options) are valid. Alternatively, you could always stop your workflow, and modify it to run only from the point you've executed from with your already computed files. Straight forward, but could be a little tedious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640:578,configurat,configurations,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640,1,['configurat'],['configurations']
Deployability,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:125,configurat,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007,1,['configurat'],['configuration']
Deployability,Implementer - please check out https://www.trivento.io/creating-settingsactor-configuration-properties-using-akka-extensions/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505:78,configurat,configuration-properties-using-akka-extensions,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/796#issuecomment-231463505,1,['configurat'],['configuration-properties-using-akka-extensions']
Deployability,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:60,configurat,configuration,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843,1,['configurat'],['configuration']
Deployability,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:341,update,updates,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,3,['update'],"['update', 'updates']"
Deployability,"In case @kshakir isn't the one managing the master patch today, reminder that lenthall needs to be deal with in that situation as well",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213449018:51,patch,patch,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213449018,1,['patch'],['patch']
Deployability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:233,configurat,configuration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,3,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:79,configurat,configuration,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,1,['configurat'],['configuration']
Deployability,"In general, it seems like the swagger spec, which can be very useful, has not been updated to reflect recent changes to the cromwell API.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-369956933:83,update,updated,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-369956933,1,['update'],['updated']
Deployability,"In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989:118,pipeline,pipelines,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989,1,['pipeline'],['pipelines']
Deployability,"In other words, I'm looking for the *default* configuration of Cromwell to *never* run `isAlive` (except on server restarts, like today). A user should have to deliberately change their configuration to make it happen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365:46,configurat,configuration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424373365,2,['configurat'],['configuration']
Deployability,In that case is it a won't-fix situation? We can keep patching if necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266:54,patch,patching,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336166266,1,['patch'],['patching']
Deployability,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > â€¦; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,configurat,configurations,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090,4,['configurat'],"['configuration', 'configurations']"
Deployability,Initial implementation complete. Not yet closing this issue as integration tests are not yet operable and there are still several TODOs in the code. See commit f788704.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466,1,['integrat'],['integration']
Deployability,"Integration test added in #4488, and FYI a patch added in #4508.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472:0,Integrat,Integration,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472,2,"['Integrat', 'patch']","['Integration', 'patch']"
Deployability,"Interesting - I thought it would only push as far as ""can I execute it"" if an inputs file is provided, but that appears to not be true. Thanks for raising this - we should be able to get a fix in for the Cromwell 33 release which should be sometime next week.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396950006:216,release,release,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396950006,1,['release'],['release']
Deployability,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:209,toggle,toggles,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781,1,['toggle'],['toggles']
Deployability,"Investigation update; ---. This [branch](https://github.com/broadinstitute/cromwell/tree/tj-metadata-stream-experiment-2) contains code attempting to stream metadata events from the database and build the json as events arrive. It does **not** stream the json itself back to the endpoint. The whole json is still built in memory and then returned (see the end for thoughts on that). Results:. ### The good; - Streaming the data from the database has a positive effect on memory (left CPU, right heap); The below graphs represent Cromwell's activity when it's building a metadata of around 2.8M metadata events. Building metadata without streaming:; ![screen shot 2018-10-17 at 11 16 32 am](https://user-images.githubusercontent.com/2978948/47925639-134b5e80-de95-11e8-8ce3-43f52c4a4067.png). We can see that memory builds up throughout the process of generating the JSON, with a larger burst towards the end. CPU activity is inexistent until the very end where a lot of CPU resources are needed to go through all the events and build the json. Building metadata with streaming:; ![screen shot 2018-10-17 at 10 08 08 am](https://user-images.githubusercontent.com/2978948/47925627-0d557d80-de95-11e8-8ad0-14444456c05a.png). In contrast, here there is moderate CPU activity throughout the process, as well as lots of a much more sawtooth-looking heap graph, indicating that objects are getting GCed a lot. The max memory used is also smaller than for the non streaming version. - Using a streaming approach allows the stream to be stopped at any point in time (say if we ran over the endpoint timeout).; Note that even without streaming data from the database, we can still build the json from the strict set of events using an fs2 stream and stop that if/when needed. Another graph where Cromwell was asked to build several large metadata jsons:. ![screen shot 2018-10-19 at 1 17 28 pm](https://user-images.githubusercontent.com/2978948/47926437-ee57eb00-de96-11e8-89b4-a7df8db9e164.png); Red is non str",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-435955806,1,['update'],['update']
Deployability,"Is it possible for you to inform me or Beri when this is released, so that we can update the folks on this thread: https://gatkforums.broadinstitute.org/firecloud/discussion/comment/52504#Comment_52504",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425232775:57,release,released,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425232775,2,"['release', 'update']","['released', 'update']"
Deployability,Is it still the case that call caching is not implemented for S3? Any update on this issue @delagoya?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-452039660:70,update,update,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-452039660,1,['update'],['update']
Deployability,"Is the cromwell.examples.conf biased to not include full examples for its clients? Having lots of experience reading documentation vs. getting code, my general preference is to find a complete config, copy paste, and customize it. Pointing the user to the docs is okay as long as the full configuration is there (not broken into sections.). On the other hand, I agree it's bad to confuse the user with too many options. My preference would be for consistency. If you have the examples file, you should provide all examples there, as this would be the expectation. If you decide to link to docs, this should be standard for other types, as the two could get disconnected (i.e. different versions across locations).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014:289,configurat,configuration,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832014,1,['configurat'],['configuration']
Deployability,Is there an equivalent for JES runtime attributes validation that could need an update as well ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068:80,update,update,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1738#issuecomment-264853068,1,['update'],['update']
Deployability,Is there any update to this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-453221742:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-453221742,1,['update'],['update']
Deployability,"Is there anything you can suggest with regards to ""looking at your DB""?; Are there queries I could issue myself?. If I understand correctly, when I run `docker-compose ... build`, the Dockerfile is just pulling `broadinstitute/cromwell:develop`. When I look at https://hub.docker.com/r/broadinstitute/cromwell/tags/, I don't see any 32 snapshots explicitly pushed, but I do see that `broadinstitute/cromwell:develop` has been updated. ```; $ docker run --rm -ti broadinstitute/cromwell:develop --version; cromwell 32-d30d9f0-SNAP; ```. I am planning on waiting for my current batch of workflows (that I newly submitted) to complete.; Then I will want to pull the more recent snapshot. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391895624:426,update,updated,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391895624,1,['update'],['updated']
Deployability,"Is this a question, bug, or feature request? As far as I know this does already work (as of release 23). Maybe it missed the changelog though?. Check out the wdlDependencies field of the batch endpoint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710:92,release,release,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265537710,1,['release'],['release']
Deployability,Is this the SFS version of invalidate cache results? Can we also update the centaur test `invalidate_bad_caches` to include whatever causes this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752:65,update,update,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1999#issuecomment-280664752,1,['update'],['update']
Deployability,Is this the sort of scheme you mean? ie considering the operation as idempotent if you ask to release twice? ; * `releaseHold` on a held workflow => 201; * `releaseHold` on a running workflow => 200,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494025632:94,release,release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983#issuecomment-494025632,3,['release'],"['release', 'releaseHold']"
Deployability,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:105,install,installed,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,2,"['deploy', 'install']","['deploy', 'installed']"
Deployability,"It is a bit unfortunate cromwell 50 was released before this was merged. It will break all testing everywhere since there is no way of running cromwell and knowing beforehand where the outputs will end up.; In biowdl all testing is already pinned to cromwell-48 to ensure continued operation, we were hoping we could unpin this with 50, but it seems we have to wait a little longer. EDIT: I do understand though with COVID-19 raging across the world that some other stuff deservedly gets priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902:40,release,released,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902,1,['release'],['released']
Deployability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:118,integrat,integration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['integrat'],['integration']
Deployability,It seems fixed with release 70,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169:20,release,release,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169,1,['release'],['release']
Deployability,"It should be in-place (e.g. the source and destination tables are in the same database). . It would be nice to be automagic (e.g. liquibase), but just having a script in the repository to do the upgrade would be fine as well. It's really just for existing production customers who need to bridge the gap (GOTC/FC)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031:195,upgrade,upgrade,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031,1,['upgrade'],['upgrade']
Deployability,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:61,configurat,configuration,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730,1,['configurat'],['configuration']
Deployability,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:197,update,updated,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178,1,['update'],['updated']
Deployability,It was suggested that we create both a develop PR as well as for a hotfix. Had I been aware of the differences between the two branches I probably would not have created this PR... but we had already finished. Similar work will be done for a PR on the hotfix branch. I'm happy to do whatever your team prefers. I suspect that this work will need to be duplicated in either case. :(,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219122627:67,hotfix,hotfix,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219122627,2,['hotfix'],['hotfix']
Deployability,"It works! . Now everyone back away slowly, don't breathe, and never update develop ever again",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188503022:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/473#issuecomment-188503022,1,['update'],['update']
Deployability,"It would be very nice to support Mesos, as it is a very nice framework for in house cloud computing systems. Not everyone is able to launch their jobs into the cloud. I see you already have support for Yarn here: . http://cromwell.readthedocs.io/en/develop/backends/Spark/. And we also have this:; ```; A not so widely known fact is that Spark has its root in Mesos: it was ; initially developed at the AMPLab as a proof-of-concept Mesos ; framework to demonstrate how easy and fast ; it is to develop a distributed platform on top of Mesos; ```; taken from here : ; * https://mesosphere.com/blog/spark-mesos-shared-history-and-future-mesosphere-hackweek/. Spark and Mesos was really closely integrated, though I see that Spark has created their own scheduler, Mesos is still a very good way of running Spark jobs. It would be a very nice addition to the Chromwell framework! . Mesos is used in many other Big Data cloud environments outside of the Bioinformatics pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576:692,integrat,integrated,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576,2,"['integrat', 'pipeline']","['integrated', 'pipelines']"
Deployability,It's a release improvement ticket. Currently the release WDL creates a github release even if the build fails on the master branch right after merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405:7,release,release,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-333241405,3,['release'],['release']
Deployability,It's expected that `wdltool 0.4` will not validate this as the `String main_output = hello_and_goodbye.hello_output` syntax in workflow outputs was introduced specifically for sub workflows which `wdltool 0.4` pre-dates.; Try to update to the latest version of wdltool and it should validate.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261:229,update,update,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271702261,1,['update'],['update']
Deployability,"It's happened for the 11k run, but the changes to the joint genotyping workflow haven't been merged into dsde-pipelines/master yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722:110,pipeline,pipelines,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-313503722,2,['pipeline'],['pipelines']
Deployability,It's happening again. Last time this was the result of our LB blocking new pingdom servers. @hjfbynara could you please run update script?. I don't see any added in december here: https://help.pingdom.com/hc/en-us/community/posts/208953545-Pingdom-Probe-Servers-Pingdom-IPs.; But it seems too coincidental that the symptoms are exactly the same as they were last time. ![image](https://user-images.githubusercontent.com/165320/50653143-8f762700-0f56-11e9-9b2d-76da416c47f2.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835:124,update,update,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835,1,['update'],['update']
Deployability,"It's low effort, low reward. If someone does it in their ""spare"" time, awesome. Otherwise, imho, the cromwell.yaml can be manually patch in a commit post 30, and someone can also fix the migration script at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2930#issuecomment-347984022:131,patch,patch,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2930#issuecomment-347984022,1,['patch'],['patch']
Deployability,"It's not really a dupe -- one ticket is about the README/swagger and one is; specifically about 0.19->0.20+ upgrade instructions (which could go in the; readme as one option!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Wed, Jul 13, 2016 at 11:46 AM, Ruchi notifications@github.com wrote:. > @kcibul https://github.com/kcibul since this is a duplicate, is it okay; > if we just close this issue?; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232397729,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4g2tMjutZYDtV_3n_DEN05wi6rjllks5qVQhjgaJpZM4I69py; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232402414:108,upgrade,upgrade,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1043#issuecomment-232402414,1,['upgrade'],['upgrade']
Deployability,"It's really a question of time and cost efficiency, but since we've got the; actual GATK team and joint calling authors working on the pipeline we'll; definitely take advantage of all the features there are (and they'll write; the ones we need!). ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jun 23, 2016 at 11:55 AM, Paul Grosu notifications@github.com; wrote:. > @kcibul https://github.com/kcibul I believe GATK can perform; > incremental joint calling, so then you should be able to use a collection; > of Cromwells submissions to build it up. Would that work?; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228096103,; > or mute the thread; > https://github.com/notifications/unsubscribe/ABW4gxuO55o58LyKHTfSEVvS97Z1-7Nxks5qOqxWgaJpZM4I8rmu; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757:135,pipeline,pipeline,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228109757,1,['pipeline'],['pipeline']
Deployability,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" â€¦`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:82,pipeline,pipelines,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026,1,['pipeline'],['pipelines']
Deployability,"JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3085,Update,UpdateVisitor,3085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['Update'],['UpdateVisitor']
Deployability,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (Ã¸)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (Ã¸)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4363,update,update,4363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,2,['update'],['update']
Deployability,Jira issue: https://broadworkbench.atlassian.net/browse/BA-6006. Please reference that ticket for updates! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396:98,update,updates,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396,1,['update'],['updates']
Deployability,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4312,configurat,configuration,4312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['configurat'],['configuration']
Deployability,"Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885,1,['update'],['update']
Deployability,"Just a friendly reminder that the Changelog could use an update, possibly the Readme ðŸ¤“",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924:57,update,update,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-277081924,1,['update'],['update']
Deployability,"Just before I merge this PR, I will coordinate disabling the existing automatic database updates with @coreone",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-170711740:89,update,updates,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-170711740,1,['update'],['updates']
Deployability,Just did a 0.8 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647:15,release,release,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837#issuecomment-271884647,1,['release'],['release']
Deployability,Just thinking out loud - there seems to be a lot of duplication between this backend and the configurable shared-filesystem backend. ; I don't know whether you've tried expressing this as a configuration file backend?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358:190,configurat,configuration,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246796358,1,['configurat'],['configuration']
Deployability,"Just to check, this is not yet addressed in the recently release Release 36?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-431895566:57,release,release,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-431895566,2,"['Release', 'release']","['Release', 'release']"
Deployability,"Just to clarify -- this isn't currently not possible on the JES backend, only Local. However, Pipelines APIv2 would allow for more flexible delocalization.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597:94,Pipeline,Pipelines,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3065#issuecomment-353106597,1,['Pipeline'],['Pipelines']
Deployability,"Just wanted to re touch base on this quickly, since the :+1: were given I fixed a bug that prevented the ""filepassing.wdl"" from working on JES and updated the config template / README. Is updating the template enough to make any configuration change work when cromwell will redeploy ? Because several field names / location have been changed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180:147,update,updated,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162623180,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"Just wonder why the runtime block has to be key-value pairs? `spark-submit` has tons of attributes and new attributes may be added in the future. Can the runtime block just be wrapped as string and passed to `spark-submit`?. If it has to be key-value pair, can the key be something like ""additionalArgs"" and the value be a string of containing attributes the user wants to add? for example:; `""additionalArgs"": ""--conf 'xx -Dxx' --name xx""`; In this way, if `spark-submit` has new attributes in the future, cromwell doesn't need to updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537:532,update,updated,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537,1,['update'],['updated']
Deployability,LGTM! I updated [this doc](https://docs.google.com/document/d/1X1mpcUtsukeWez82UpXAXkX7SJpcSB-pZk1lFwd6LUA/edit) to reflect the change here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668,1,['update'],['updated']
Deployability,"Learned that a change in Pipelines API omitted the ""preemptible"" key from the operations metadata, and that change introduced a null pointer in the Cromwell code. . AC: As a way to address this, it would be great if we could modify the Cromwell code so that when its parsing operation metadata, that if certain keys are missing (such as Preemptible) -- we use the defaults where possible, else fail gracefully with an error that states which information couldn't be parsed, and that caused the workflow to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804:25,Pipeline,Pipelines,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804,1,['Pipeline'],['Pipelines']
Deployability,"Let's chat IRL later ~~today~~. To be clear, I'm totally fine mothballing this PR. In the context of PBE, conceivably this ticket was introduced so one could implement other final calls that run on a backend?. EDIT: No rush on this ticket. Making some patches, but will chat about next steps some other time when we can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861:252,patch,patches,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861,2,['patch'],['patches']
Deployability,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:213,PATCH,PATCH,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228,2,"['PATCH', 'update']","['PATCH', 'updated']"
Deployability,"Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1867,configurat,configuration,1867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['configurat'],['configuration']
Deployability,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:187,update,updates,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,6,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:146,update,updates,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,6,"['pipeline', 'update']","['pipeline', 'updates']"
Deployability,"Looking at this with Cromwell 24, the user can be specified with a config like:. ```; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit-docker = """"""docker run --rm ${ ""--user "" + docker_user } -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash ${docker_cwd}/execution/script""""""; .; .; .; ```. The WDL can pass in `docker_user` as a runtime attribute, which could be an expression involving an input or just a hardcoded value. But even with a container path not under `/root`, there are currently permissions problems that prevent this from working due to the different users inside and outside the Docker container. It may be possible to `chmod` and `umask` our way past these problems, but I need to think through the security implications of doing so. Maybe making this more liberal behavior an opt-in configuration value in the backend config would be okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535:963,configurat,configuration,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-271364535,1,['configurat'],['configuration']
Deployability,"Looking into it more closely, there's a good chance we fixed this issue already, we just haven't released a 30.3 jar yet. Meanwhile if you're able to build from the `30_hotfix` branch you might want to try that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-365310137:97,release,released,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-365310137,1,['release'],['released']
Deployability,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:79,pipeline,pipelines,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339,4,"['pipeline', 'rolling']","['pipelines', 'pipelines-tools', 'rolling']"
Deployability,"Looks like it's here ""https://github.com/broadinstitute/firecloud-develop/blob/dev/run-context/live/configs/cromwell/docker-compose.yaml.ctmpl"" so it means it'll get promoted automatically on next release.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106:197,release,release,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394768106,1,['release'],['release']
Deployability,"Looks like read_json in current trunk still has some issues.; When I give a json like this:; ```json; {; ""Homo sapiens"": {; ""transcriptome"" : ""/pipelines/indexes/HUMAN/27/gencode.v27.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/HUMAN/27/"",; ""salmon"": ""/pipelines/indexes/HUMAN/27/salmon""; },; ""Mus musculus"": {; ""transcriptome"" : ""/pipelines/indexes/MOUSE/M16/gencode.vM16.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/MOUSE/M16/gencode.vM16.annotation.gtf"",; ""salmon"": ""/pipelines/indexes/MOUSE/M16/salmon""; }; }; ```; with wdl like this; ```; Map[String, Map[String, String]] indexes = read_json(references) ; ```; I get:; ```; Workflow input processing failed; WorkflowFailure(ERROR: indexes is declared as a Map[String, Map[String, String]] but the expression evaluates to a Object: Map[String, Map[String, String]] indexes = read_json(references) ^ ,List()); ```; I do not get what is wrong there, I've tried different type combinations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127:144,pipeline,pipelines,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127,6,['pipeline'],['pipelines']
Deployability,Looks like some tests need to be updated in response to these changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343:33,update,updated,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343,1,['update'],['updated']
Deployability,Looks like this error was specific to my runtime+configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282:49,configurat,configuration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282,1,['configurat'],['configuration']
Deployability,"Looks like this is still an issue, correct? Iâ€™ve just stumbled upon it in one of our pipelines that uses PAPIv2 (on Cromwell 36). The pipeline step uses a Docker image based on `openjdk:13-alpine`, which does have its own entrypoint that weâ€™d like Cromwell to ignore. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517:85,pipeline,pipelines,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-478755517,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:85,configurat,configuration,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['configurat'],['configuration']
Deployability,Looks like this was partially addressed in another merge and needs merge conflict updates: https://github.com/broadinstitute/cromwell/pull/6994,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457:82,update,updates,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457,1,['update'],['updates']
Deployability,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:14,patch,patch,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078,3,['patch'],['patch']
Deployability,Mailbox(Mailbox.scala:258); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3745,update,update,3745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['update']
Deployability,"Makes sense. This can wait for the official announcement of which way the feature is going. In the meantime, our users are gradually migrating from on-prem to Terra. Our Cromwell instance allows users to run workflows on GCP or GridEngine. We want to ensure our instance has feature parity with launching workflows in Terra, so we needed something like this commit. After the announcement, I'll update the PR to copy these auth config lines over to the Batch backend. Otherwise, Terra will have removed the checkbox for reference disks and we can close the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127:395,update,update,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127,1,['update'],['update']
Deployability,"Maybe add an entry in the changelog, even just; ```; ## 33.1 Release Notes. ### Bug fixes; ```. So that something shows up in the release in github (if you do make a release)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401099308:61,Release,Release,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3828#issuecomment-401099308,3,"['Release', 'release']","['Release', 'release']"
Deployability,"Meh. On Thu, Jun 27, 2019, 4:33 PM Chris Llanwarne <notifications@github.com>; wrote:. > Hotfix worthy?; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5050?email_source=notifications&email_token=AABILSEPI46IZV46NXKJVRLP4UP2FA5CNFSM4H363QWKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYJK7I#issuecomment-506500477>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AABILSD3E6NIWBT63BF3Y3DP4UP2FANCNFSM4H363QWA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118:89,Hotfix,Hotfix,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118,1,['Hotfix'],['Hotfix']
Deployability,"Merge `develop` and re-push, you may be unlucky or your branch may be missing test updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944:83,update,updates,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944,1,['update'],['updates']
Deployability,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs ðŸ˜…. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Meâ„¢.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:313,release,released,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698,3,['release'],"['release', 'released', 'releases']"
Deployability,Merge to develop gated on a BT-219 release of Martha that supports access urls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854:35,release,release,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854,1,['release'],['release']
Deployability,Merged an updated version of this missing documentation (See #6800),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470:10,update,updated,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470,1,['update'],['updated']
Deployability,Merging as the failing test is that same stupid integration test w/ dockerhub failing on develop,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014:48,integrat,integration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014,1,['integrat'],['integration']
Deployability,Merging despite `codecov/patch` in this case. No tests necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381:25,patch,patch,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381,1,['patch'],['patch']
Deployability,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:65,hotfix,hotfix,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120,1,['hotfix'],['hotfix']
Deployability,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:131,deploy,deployed,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141,2,"['deploy', 'hotfix']","['deployed', 'hotfix']"
Deployability,Might or might not block release. We'll await Adam's verdict on Monday morning,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4940#issuecomment-491383867:25,release,release,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4940#issuecomment-491383867,1,['release'],['release']
Deployability,"Minor infix patches, and then unless anyone else chimes in, :+1: for merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/117#issuecomment-125171839:12,patch,patches,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/117#issuecomment-125171839,1,['patch'],['patches']
Deployability,Minor updates and then :+1: from me for merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/177#issuecomment-140108585:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/177#issuecomment-140108585,1,['update'],['updates']
Deployability,"Minor updates, plus (yet another) rebase. ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/893/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222332578:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-222332578,1,['update'],['updates']
Deployability,"Minor updates, then :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/599/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/599#issuecomment-200356197:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/599#issuecomment-200356197,1,['update'],['updates']
Deployability,"Minor updates, then LGTM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170287369:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/365#issuecomment-170287369,1,['update'],['updates']
Deployability,"Most of the remaining backend dependencies on engine look straightforward to move to backend or core, though there are a few prizes like the direct DB updates from JES and Local backends. No heroics required, but do what's possible to remove unnecessary dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844:151,update,updates,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844,1,['update'],['updates']
Deployability,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:178,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"My task are not using docker. Also I see no attempts at all to copy or softlink the files. Not in the log, and not in the cromwell-executions folder.; Also hard-linking seems to persist using the `SGE` backend. Even though the localization has the same configuration as above. So the error is not backend specific. Fortunately, all the other values in the config are used. Which makes me think that either my configuration file has some error (keys in wrong place). But I have checked this over and over again already with the example files and it seems to be correct (though I am not infallible of course).; Or the backend just ignores the values due to a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440:253,configurat,configuration,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440,2,['configurat'],['configuration']
Deployability,NB this will also require a fixup of https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/joint-discovery-gatk/joint-discovery-gatk4.wdl#L341,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102:120,integrat,integrationTestCases,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102,1,['integrat'],['integrationTestCases']
Deployability,NOTE: Centaur tests require https://github.com/broadinstitute/martha/pull/186 merged-and-deployed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881:89,deploy,deployed,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881,1,['deploy'],['deployed']
Deployability,"NOTE: If anyone wants to follow up on #1499 and put in [less aggressive polling](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482), here is the [heart of the batch requesting](https://github.com/broadinstitute/cromwell/blob/de95456159c27a5145612b6acc95d4e713a5e9f8/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiBatchHandler.scala) wired into the Google/PAPI backend that could be generalized for all backends.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426347827:319,pipeline,pipelines,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426347827,3,"['Pipeline', 'pipeline']","['PipelinesApiBatchHandler', 'pipelines']"
Deployability,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:109,update,updated,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867,1,['update'],['updated']
Deployability,"NOTE: While I need to describe these WDLs using our Cromwell instance, there is a CROM-4572 ticket already created that was previously closed, and [this comment](https://broadworkbench.atlassian.net/browse/CROM-4648?focusedCommentId=17048) mentions eliminating zip imports. While that's being figured out, I'm contributing this patch JIC someone else wants to cherry-pick this for their instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229:328,patch,patch,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229,1,['patch'],['patch']
Deployability,Native install,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480346756:7,install,install,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480346756,1,['install'],['install']
Deployability,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:124,install,installing,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['install'],['installing']
Deployability,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:269,configurat,configuration,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130,1,['configurat'],['configuration']
Deployability,"No longer blocked. It appears that the service account used by centaur tests for requester-pays testing is not compatible with the bucket used in `arrays` centaur-integration-test. Options:; - Use a workflow option to override-the-override, the service account back to the ""original"" service account specified in the JSON; - Reconfigure the centaur-integration-tests to not use an override of the requester-pays service account",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696:163,integrat,integration-test,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696,2,['integrat'],"['integration-test', 'integration-tests']"
Deployability,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134,1,['integrat'],['integration']
Deployability,"No worries, but thank you for the update! We're on our current sprint and assuming we have some time leftover, we'll come back to it, otherwise it's been allotted towards the upcoming sprint. Thanks! @gemmalam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-501727926:34,update,update,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-501727926,1,['update'],['update']
Deployability,"No worries, thanks for the update, I'll skip this workflow for now then :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436234476:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436234476,1,['update'],['update']
Deployability,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user 'â€¦'@'â€¦' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:426,configurat,configuration,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,3,"['Update', 'configurat']","['Updated', 'configuration']"
Deployability,"Not it isn't, I believe @mcovarr is working on something that should make this ""go away"". ; In the meantime you can try to increase `database.db.queueSize` in the configuration. Results are not guaranteed though it's just giving slick more room but it might still fail. The default is 1000.; Also how big is your workflow ? Must be large to hit this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093:163,configurat,configuration,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298055093,1,['configurat'],['configuration']
Deployability,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:635,pipeline,pipeline-project,635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['pipeline'],['pipeline-project']
Deployability,Not sure about switching from the canonical repo to an unknown source for something like this. Also it might be nice to upgrade from jq 1.5 to the 1.6 that was released a couple of months ago if we're going to change this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4578#issuecomment-457335175:120,upgrade,upgrade,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4578#issuecomment-457335175,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"Not sure how hard it would be to do this, but another useful metric would be network usage (as represented by bytes going in/out at a moment in time). I haven't looked at this in cromwell workflow but at my last job we had a number of workflows that were bandwidth constrained (mainly due to network drive mounts) until we moved them to to SSD. Pipelines issue: https://broadinstitute.atlassian.net/browse/DSDEGP-1360",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573:345,Pipeline,Pipelines,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-319378573,1,['Pipeline'],['Pipelines']
Deployability,"Not sure what I should be doing. I have tried the following command:; ```; gcloud logging read 'timestamp>=""2020-09-01T00:00:00Z""' > logs; ```; And then:; ```; $ cat logs | grep 30148356615-compute@developer.gserviceaccount.com -A10 | grep -i permission | cut -d: -f2 | sort | uniq -c; 14 lifesciences.operations.cancel; 425 lifesciences.workflows.run; 12 storage.buckets.get; 30629 storage.objects.create; 30985 storage.objects.delete; 12819 storage.objects.get; 157 storage.objects.getIamPolicy; 6859 storage.objects.list; ```; It does seem to be the case that `storage.objects.delete` is requested many times, so that is definitely an issue when you only have roles `storage.objectCreator` and `storage.objectViewer` but not `storage.objectAdmin`. I did not observe any permission from role `iam.serviceAccountUser` but that role is indeed needed. And I observe some requests for permission `storage.buckets.get` that do end in ERROR, but it does not seem to affect the pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970:973,pipeline,pipeline,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970,1,['pipeline'],['pipeline']
Deployability,Not sure why codecov/patch is reporting a low number. I have added unit tests for the modified code as possible but it doesn't seem to be recognizing it ðŸ¤”,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592:21,patch,patch,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592,1,['patch'],['patch']
Deployability,Not sure why the build is showing no code coverage on the patch. I tagged the new test the same way as the existing tests (as AwsTest). It ran successfully in my local env when I enable that tag.; Does the CI not run any of the AwsTests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068:58,patch,patch,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068,1,['patch'],['patch']
Deployability,Note before merging: Change WDL in DSDE-Pipelines w.r.t. commas in workflow outputs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/309#issuecomment-161101612:40,Pipeline,Pipelines,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/309#issuecomment-161101612,1,['Pipeline'],['Pipelines']
Deployability,"Note for reviewers verifying testing changes: Travis, `sbt test`, and `sbt 'test-only cromwell.engine.db.slick.SlickDataAccessSpec'` no longer run the database integration tests. One must use either:. ``` bash; sbt 'integration:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. or:. ``` bash; sbt 'alltests:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. @cjllanwarne First reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505,2,['integrat'],['integration']
Deployability,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:260,pipeline,pipeline,260,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757,2,['pipeline'],['pipeline']
Deployability,"Note: the upgraded 1.0 version of this workflow passed validation, so it's something specific to the draft-2 WDL implementation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951:10,upgrade,upgraded,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3885#issuecomment-404899951,1,['upgrade'],['upgraded']
Deployability,"Note:. As it stands, this PR could return false positives for a workflow that has been archived from metadata but still has a summary. I think this is already true for the function converted in https://github.com/broadinstitute/cromwell/pull/4617. I'm not sure whether it's a problem per se, but certainly notable. **Update**: based on the analysis in https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053 the old function `validateWorkflowIdInMetadata` already returns `true` for archived workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930:317,Update,Update,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930,1,['Update'],['Update']
Deployability,"OK I think I got it working. . It turns out the custom AMI I created was incorrect. When making a custom AMI using the cloud formation stacks as described [here](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/). The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as `\cromwell_mount`. This information is stated [elsewhere](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/), but perhaps this [page](https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/) should be updated to include this information?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435134332:583,update,updated,583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435134332,1,['update'],['updated']
Deployability,OK will keep you updated. Looping in @abaumann as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879:17,update,updated,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609#issuecomment-256070879,1,['update'],['updated']
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:40,release,release,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172,10,"['release', 'update']","['release', 'updates']"
Deployability,"OK, after discussion with @Horneth and @cjllanwarne:. ``` hocon; google {; application-name = ""cromwell"". auths = [; {; name = ""cromwell-service-account""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; ]; }. backend {; default = ""JES""; providers = [; {; name = ""JES""; class = ""cromwell.engine.backend.jes.JesBackend""; config {; ...; genomics {; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // Use the Cromwell service account for creating the Pipeline and manipulating auth JSONs.; auth = ""cromwell-service-account""; }; filesystems = [; {; gcs {; auth = ""user-via-refresh""; }; }; ]; }; },; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798:624,Pipeline,Pipeline,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203502798,1,['Pipeline'],['Pipeline']
Deployability,Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572:102,update,updates,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572,1,['update'],['updates']
Deployability,"Oh funny, they just released 2.9.2 yesterday. I can give that a shot, this should wait until the release goes out anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011:20,release,released,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011,2,['release'],"['release', 'released']"
Deployability,"Oh great! Is the root cause of that behavior known, why sometimes workflow store updates but not workflow metadata? Or is it just that the workflow metadata update fails, and we know why?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478633097:81,update,updates,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4767#issuecomment-478633097,2,['update'],"['update', 'updates']"
Deployability,Oh looks like the API client needs to be updated too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912:41,update,updated,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2357#issuecomment-310082912,1,['update'],['updated']
Deployability,"Oh wow, thank you so much for this tip. I knew you could export to BigQuery, but I didn't realise that:; > if you use regular file export, you should be aware that regular file export captures a smaller dataset than export to BigQuery. (from [this page](https://cloud.google.com/billing/docs/how-to/export-data-bigquery)). In the end, I enabled BigQuery export, then ran this query:. ```sql; SELECT SUM(cost); FROM `PipelineBilling.gcp_billing_export_v1_BILLING_ACCOUNT_ID`, UNNEST(labels) as l; WHERE l.key = 'cromwell-workflow-id' AND l.value = 'cromwell-MY-WORKFLOW-ID'; ```. It works perfectly, and gives me a single cost for each pipeline! Thanks so much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4394#issuecomment-440863484:416,Pipeline,PipelineBilling,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4394#issuecomment-440863484,2,"['Pipeline', 'pipeline']","['PipelineBilling', 'pipeline']"
Deployability,"Oh, and `sbt assembly` already had _all_ of its tests disabled by build.sbt, so @geoffjentry's request that ""when one checks out the code, sbt assembly not fail on an integration test"" holds true without any other modifications. :wink:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288:167,integrat,integration,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288,1,['integrat'],['integration']
Deployability,"Oh, and during rebase I switched from singularity-ce to apptainer, only because the latter's fork is easier to install in 2024.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075:111,install,install,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075,1,['install'],['install']
Deployability,Ok -- after discussion it seems worth it to enable compression on this field on hotfix so that they can scale to ~5000 samples,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224691438:80,hotfix,hotfix,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224691438,1,['hotfix'],['hotfix']
Deployability,Ok I've updated the PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221913147:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221913147,1,['update'],['updated']
Deployability,"Ok, I see! Now as you mentioned, I think the problem here is to update the doc, to distinguish between execution statuses and workflow statuses, and elaborate execution statuses. Glad to know more about the internal mechanism of Cromwell and thanks for the explanation here!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-372387957:64,update,update,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3328#issuecomment-372387957,1,['update'],['update']
Deployability,"Ok, running the Docker daemon as root is normal (the docs says it's [required](https://docs.docker.com/engine/security/security/), actually). The issues with non-root default users should be fixed in [this PR](https://github.com/broadinstitute/cromwell/pull/1865), but that code is currently only on develop (the forthcoming 25 release). non-root default users should Just Work with the code from that PR, you shouldn't have to make any changes to your config. The `master` branch corresponds to the 24 release; do you mean you're running the 23 release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835:328,release,release,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283009835,3,['release'],['release']
Deployability,"Ok. I'll revert [this](https://github.com/broadinstitute/cromwell/commit/2e3f45bbedeaa4c522751e9ff6f5594c57b88b35#diff-facc2160a82442932c41026c9a1e4b2bL28) change in behavior from a while ago, and update the code to reset logging type based on standard command line arguments. This goes against what the docs currently say, so I'll update those too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165228352:197,update,update,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/330#issuecomment-165228352,2,['update'],['update']
Deployability,Okay updated again...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170125680:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170125680,1,['update'],['updated']
Deployability,"Okay, I'll update and see if it works. Regardless, you can close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2527#issuecomment-321121155,1,['update'],['update']
Deployability,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:593,pipeline,pipelines,593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,2,['pipeline'],['pipelines']
Deployability,"Okay, thanks. I look forward to hearing any updates from you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509815272:44,update,updates,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509815272,1,['update'],['updates']
Deployability,"On the servers where this will be used, . > Did you install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files?; > http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835:52,install,install,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835,1,['install'],['install']
Deployability,"Once this review is complete, I'll back port the patch to the actual hotfix branches `0.21_hotfix` and `0.22_hotfix`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983:49,patch,patch,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-260833983,2,"['hotfix', 'patch']","['hotfix', 'patch']"
Deployability,"One could make the argument that it'd hose people who don't want to update their options files, but I know that's not really the answer :'(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1167#issuecomment-253881280,1,['update'],['update']
Deployability,"One last question, @geoffjentry --is this a change just for hotfix branch or develop as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/755#issuecomment-218190886:60,hotfix,hotfix,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/755#issuecomment-218190886,1,['hotfix'],['hotfix']
Deployability,One option: we're currently using conditions by `files`. We could replace-or-update those conditions using `labels`: http://docs.pullapprove.com/groups/conditions/#labels,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392192566:77,update,update,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392192566,1,['update'],['update']
Deployability,"Options to fix it:; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure`.; - Migration that changes all `executionStatus: Preempted` metadata entries into `executionStatus: RetryableFailure` and also add `backendStatus: Preempted`.; - Unlike option 1, I think this will require a custom scala migration so might be a fair bit slower; - Change the CRDT to just be fine with `Preempted` as an `ExecutionStatus` again.; - Not intellectually honest, but easy to add as a hotfix and no migration necessary",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152:530,hotfix,hotfix,530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438#issuecomment-315120152,1,['hotfix'],['hotfix']
Deployability,Our team has a similar Singularity backend for SGE and SLURM.; https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/backends/backend.conf,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438702335:102,pipeline,pipeline,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438702335,1,['pipeline'],['pipeline']
Deployability,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:102,integrat,integrated,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562,1,['integrat'],['integrated']
Deployability,"PR comments addressed, and tests re-passing. Changes include:. - Primitive and non-primitive file types.; - Primitive 'Dir' now 'Directory'.; - Split up and fixed test descriptions.; - Exception patches.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3079#issuecomment-355723031:195,patch,patches,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3079#issuecomment-355723031,1,['patch'],['patches']
Deployability,"PR now in for review #3772 - I can add another workaround in the meantime, since apparently this was only affecting the draft-2 language factory:. * upgrade to WDL 1.0. However, since an upgrade script is also coming in as part of Cromwell 33, maybe one of your other workarounds is less effort in the short term?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396973479:149,upgrade,upgrade,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-396973479,2,['upgrade'],['upgrade']
Deployability,PS Basically I'd like to close these ugly gaps in my pipeline ; ![image](https://github.com/broadinstitute/cromwell/assets/57629300/ebe89fb6-8420-486d-b52f-653f29fc73c5). The gap between `rc` generation and the `Status change from Running to Done` message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820,1,['pipeline'],['pipeline']
Deployability,"Per @vivster7 the bug report was against 0.19 hotfix, not develop. Not that this shouldn't be fixed here too. ðŸ˜„ . @kcibul It seems #794 is also labeled 0.21 but actually applies to 0.19 hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975:46,hotfix,hotfix,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231123975,2,['hotfix'],['hotfix']
Deployability,Per discussion with PO merging this as is and possibly remove altogether for the release after next.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3985#issuecomment-411089609:81,release,release,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3985#issuecomment-411089609,1,['release'],['release']
Deployability,Per standup 2020-11-05 we're not going to bother with the 53 hotfix version of #6007 since 54 is imminent-ish.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506:61,hotfix,hotfix,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506,1,['hotfix'],['hotfix']
Deployability,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:1447,update,updated,1447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['update'],['updated']
Deployability,"Per the direction of the conversation on your integration testing doc, it sounds like there's a sentiment to disable integration (and maybe Docker) tests by default. . The wheel has chosen @scottfrazer but I suspect @geoffjentry may have some opinions here as well. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477,2,['integrat'],['integration']
Deployability,"Perfect, thanks for the update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645:24,update,update,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-285912645,1,['update'],['update']
Deployability,Pic updated. FYI merging if/when tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136:4,update,updated,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136,1,['update'],['updated']
Deployability,"PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3813,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,Please investigate and see what the root cause is. Then let's decide if would still be a problem on 0.20+ and if we need to hotfix it in 0.19,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-229635762:124,hotfix,hotfix,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737#issuecomment-229635762,1,['hotfix'],['hotfix']
Deployability,Please mention the change (and the update to metadata format) in the CHANGELOG.MD,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-181466777:35,update,update,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/412#issuecomment-181466777,1,['update'],['update']
Deployability,"Pretty sure this was fixed by @delocalizer way back in #4109. However during my debugging of `globbingBehavior` for #4854, it seemed something was rotten in the state of `GenomicsHighPriorityQue-c1ed17c72de5fcb`. I still don't 100% know the setup for the AWS queues, but I think a) perhaps we just never updated ecs-proxy over in quay?, and/or b) maybe the ARN ""fixes"" in #4896/#4902 pulled in Conrad's fixes?. Either way #4958 stops excluding the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782:304,update,updated,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4855#issuecomment-491122782,1,['update'],['updated']
Deployability,Quick update. I tweaked the config to be:; ```; system {; job-rate-control {; jobs = 1; per = 2 second; }; }; ```. and ran the test workflow above. I saw maximum concurrency - i.e. Batch requested the full number of vCPUs set in my compute environment (100). About 500 jobs succeeded before Cromwell threw an OOM exception. No Batch API Request Limit exceptions were encountered.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567:6,update,update,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443927567,1,['update'],['update']
Deployability,README.md should be updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219088738:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219088738,1,['update'],['updated']
Deployability,"Ran it just now, there were updates.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-453140060:28,update,updates,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-453140060,1,['update'],['updates']
Deployability,"Re ""centaurable""-- I'm confident by updating the local backend to use `${script}` that centaur is actually exercising a third of the variables updated in patch. We could also change centaur to completely run on a new backend that also uses the `${out}` and `${err}` paths, as outlined in issue #1126. But because of that issue, this new centaur test would require a separate, new backend definition in `local_centaur.conf`. FYI: I still don't have a full fix for #1126, but this is a step in that direction. When I looked at the reference.conf, I noticed the local backend wasn't using `${script}`, wanted to know why, and discovered this small issue. This patch will also make the workaround in that ticket work as expected, instead of pointing to paths outside the docker container like `/Users/kshakir/<path>` now pointing paths inside like `/cromwell_root/<path>`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954:143,update,updated,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2028#issuecomment-282807954,3,"['patch', 'update']","['patch', 'updated']"
Deployability,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:467,install,install,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,1,['install'],['install']
Deployability,"Re:. > what of this?. Where I think this was [this](https://github.com/broadinstitute/cromwell/blob/develop/project/Settings.scala#L50) (same line but in 59). Via the scala compiler options linked below:. ```; -target:TARGET or --target:TARGET; Target platform for object files. ([8],9,10,11,12). Default: 8; ```. I'm guessing `1.8` worked for backwards compatibility just like the `1.x` synonyms mentioned in the `javac` docs later below. For consistency like the original rawls PR I removed the explicit `--target` setting. But according to the `scalac` docs that just means Rawls, Cromwell, etc. are still just emitting Java 8 bytecode from `.scala` files. Rawls also explicitly sets the `javac` options. Cromwell doesn't. The latest version of this PR did not make the `javac` options explicitly consistent between the two projects. Instead, Cromwell is consistent in that it does NOT specify the target bytecode for `scalac` nor `javac`. I did review the SBT docs, plus the java docs for the last community LTS (11) and the current (16) `javac`. Unlike `scalac` using `8`, from my understanding of the docs, not specifying `--source` / `--target` / `--release` implicitly means use ""the current Java SE release"". So Rawls, Cromwell, and others should be emitting Java 11 bytecode from `.java` files. - https://github.com/broadinstitute/rawls/pull/1372/files#diff-b0608ed4fcebc8b5aa969f0c92dc9809e860d963b04e73affd55bb51e4fd10a1L18-L27; - https://docs.scala-lang.org/overviews/compiler-options/index.html; - https://www.scala-sbt.org/1.x/docs/Java-Sources.html; - https://docs.oracle.com/en/java/javase/16/docs/specs/man/javac.html; - https://docs.oracle.com/en/java/javase/11/tools/javac.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591:1157,release,release,1157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591,2,['release'],['release']
Deployability,Reached out to Pipelines API for a potential retry on their end.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-395994748:15,Pipeline,Pipelines,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-395994748,1,['Pipeline'],['Pipelines']
Deployability,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a â€œnot beforeâ€ time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:238,configurat,configuration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524,1,['configurat'],['configuration']
Deployability,"Recently for an operation task that updating labels for ~2500 workflows, we have to write a loop to end ~2500 PATCH /label requests to the Cromwell, which took more than 3 hrs. (In Cromwell IAM, this is even worse since a single token will expire in 60mins, so you have to also deal with the token refreshment) . We tried to use multi-threading to speed it up but ended up getting transient 500 errors when using a thread pool with a size of >=4 threads. . So having this batch feature will make a lot of things much easier!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631:110,PATCH,PATCH,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631,1,['PATCH'],['PATCH']
Deployability,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685,2,['update'],['update']
Deployability,"Reminder: could you also make a non-hotfix version of this PR (leaving out the metrics change for now, since that's going to be covered separately/properly in https://broadworkbench.atlassian.net/browse/BA-6307)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266:36,hotfix,hotfix,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266,1,['hotfix'],['hotfix']
Deployability,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:139,update,updated,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943,1,['update'],['updated']
Deployability,Resolved by installing Java with a more mainstream and supported github action.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533:12,install,installing,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533,1,['install'],['installing']
Deployability,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:44,configurat,configuration,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441,2,['configurat'],['configuration']
Deployability,RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3039,pipeline,pipeline,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"Review complete. ðŸ‘ after one new ""PBE"" comment in the code, and a string interpolation patch. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/893/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223408968:87,patch,patch,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/893#issuecomment-223408968,1,['patch'],['patch']
Deployability,"Right I just meant that in the tests the ratio (DB access / executedCode) may be higher compared to ""normal execution"" where we spend a lot of time waiting for calls to end. But yes production will definitely not be an easier environment than tests :); I kinda like the DataAccess actor option, although I think slick already manages its own pool of threads and everything, so maybe just by tweaking some configuration we could improve performance before going full Super Saiyan Actor Scaling mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444:405,configurat,configuration,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143037444,1,['configurat'],['configuration']
Deployability,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:43,pipeline,pipeline,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,2,['pipeline'],['pipeline']
Deployability,"Right, what I was suggesting is that maybe this is ""fixed"" already. Or at least we could wait and see if it still happens once a Cromwell with your change gets deployed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417859698:160,deploy,deployed,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-417859698,1,['deploy'],['deployed']
Deployability,"Right. So it sounds like this is an issue with disk space on the PAPI worker. Since I'm using the official Broad pipelines, the way to configure this is with the `flowcell_medium_disk` input field to the `PreProcessingForVariantDiscovery_GATK4` workflow",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496:113,pipeline,pipelines,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337#issuecomment-434525496,1,['pipeline'],['pipelines']
Deployability,"Running into this problem again today. Having true modularity would be nice. So I can make a workflow, that imports a workflow, that imports a workflow. Imports need to be evaluated at the file level to handle this. ; This could be set as a config option to not break backwards compatibility.; I have some experience in scala myself as most of the tools in our institute are programmed in scala as well. If given some pointers I could maybe help in implementing this?. Am I correct in thinking that imports are evaluated [here](https://github.com/broadinstitute/cromwell/blob/develop/wdl/model/draft2/src/main/scala/wdl/draft2/model/Import.scala)?; Default configuration is in [here](https://github.com/broadinstitute/cromwell/blob/2cd38db0eb818b07ad38463183fe7a8af4706899/core/src/main/resources/reference.conf) right? Can I just add a new key,value pair which I can call from `import.scala` or are some changes to a code file required?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601:657,configurat,configuration,657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366954601,1,['configurat'],['configuration']
Deployability,"STAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1621,release,released,1621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['release'],['released']
Deployability,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2116,update,updated,2116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['update'],['updated']
Deployability,"Same here.; This effects running picard intervallisttools with the scatter command for example. Might be os depent or cromwell version depent. affects cromwell 56 running with java 11 on CentOS Linux release 7.9.2009. lazy workaround is something like this:. ```bash; FILEINDEX=0; for FILE in ./scatter_list/*/*.interval_list; do; mv ""$FILE"" ""$(dirname $FILE)""""/""""$FILEINDEX""""_""""$(basename $FILE)""; FILEINDEX=$((FILEINDEX+1)); done; ```; ----. update also affects cromwell 79 running with java(openjdk) 11 on CentOS Linux release 7.9.2009",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479:200,release,release,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479,3,"['release', 'update']","['release', 'update']"
Deployability,"Sample of a possible new log message thread:; ```; [INFO] [...] [.../TestJesApiQueryManager-1262117937] Running with 1 PAPI request workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 has been removed and replaced by statusPoller2 in the pool of 1 workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 has been removed and replaced by statusPoller3 in the pool of 1 workers. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147:472,pipeline,pipelines,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147,6,"['Pipeline', 'pipeline']","['PipelinesApiRequestHandler', 'PipelinesApiRequestManager', 'pipelines']"
Deployability,"Saw DSDEEPB-1549 was already filed. Minor time scaling issue for jenkins, then :+1: as far as I'm concerned. Btw, anyone have any idea what's up with travis-to-coveralls integration's SSL errors? We don't have coverage results anymore? :crying_cat_face:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086:170,integrat,integration,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086,1,['integrat'],['integration']
Deployability,"Sayeth @mcovarr: ; >The engine upgrade already has a horicromtal test, with some very recent bugfixes :wink:; so I think #4800 can be closed too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4800#issuecomment-484262549:31,upgrade,upgrade,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4800#issuecomment-484262549,1,['upgrade'],['upgrade']
Deployability,"Scala is very much not in my wheelhouse, so unfortunately this is not a task I'd be comfortable taking on. But I really do appreciate the status update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478681472:145,update,update,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478681472,1,['update'],['update']
Deployability,Should be fixed once https://github.com/broadinstitute/wdl4s/pull/52 is merged and Cromwell brings in that WDL4S update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1513#issuecomment-263652829:113,update,update,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513#issuecomment-263652829,1,['update'],['update']
Deployability,Should we refrain from making liquibase updates until this is merged ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173391410:40,update,updates,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173391410,1,['update'],['updates']
Deployability,Simple workaround - add the following block into the configuration file:. ```; system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Drastically improved the situation for me,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442274340:53,configurat,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442274340,1,['configurat'],['configuration']
Deployability,"Since I always build `womtool` myself instead of downloading, I did notice this problem but assumed there was some magic that made the version set correctly for release builds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044:161,release,release,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044,1,['release'],['release']
Deployability,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:198,integrat,integration,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434,1,['integrat'],['integration']
Deployability,"Since already had logging set up, I went ahead and checked out your branch:; ```; SET autocommit=1; SELECT @@session.transaction_isolation; SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '0a0318d2-5eb8-429b-99e0-e5dab7918666'; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'a3c93521-b3d7-4abf-afa6-747906c0e127'; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '40d9547a-2dda-46e7-938f-dd1e99e3431f'; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'a862cafe-94e9-422e-bbcb-bf7187c09f91'; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '34737343-fd77-4b33-bd21-22a3c772720d'; update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-12-03 10:47:39.882' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '1ccf56b3-f8d4-4a9e-b92f-a20cbc1de931'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4444#issuecomment-443758946:197,update,update,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4444#issuecomment-443758946,6,['update'],['update']
Deployability,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:46,update,update,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,2,['update'],['update']
Deployability,"Since this is blocking the release and comments are ToL I'm going to merge as is and address the comments in a later PR.; @mcovarr Yeah we could also do that, I just never know how much coercion logic from third-party libraries to jam into WOM (e.g now we coerce from spray.json but not circe.json)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3812#issuecomment-400002150:27,release,release,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3812#issuecomment-400002150,1,['release'],['release']
Deployability,"Since this is the same as the hotfix, I think this is pre-thumbed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315833698:30,hotfix,hotfix,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2458#issuecomment-315833698,1,['hotfix'],['hotfix']
Deployability,"Singularity would require changes to the cromwell configuration file, correct? Since the docker command would change. Probably not hard, but would need documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922:50,configurat,configuration,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423179922,1,['configurat'],['configuration']
Deployability,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so youâ€™ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:568,update,updates,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763,1,['update'],['updates']
Deployability,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:577,configurat,configuration,577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068,2,['configurat'],['configuration']
Deployability,"So can i post wdl and other inputs, start pipeline and after get any status about created any files - stop process, get some data, edit files that is cromwell contains and then rerun it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-559094884:42,pipeline,pipeline,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-559094884,1,['pipeline'],['pipeline']
Deployability,"So here is a final update. I have tried running Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:19,update,update,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['update'],['update']
Deployability,"So it appears that what I did (creating a new AMI with the Scratch Mount Point set to `/cromwell_root` instead of the default `/scratch`) cleared up this particular issue. However, I don't think we should close the issue yet because it doesn't appear to be documented anywhere that this is what you need to do. Until the documentation is updated I'd like to see the issue remain open. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262:338,update,updated,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262,2,['update'],['updated']
Deployability,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:275,update,update,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,1,['update'],['update']
Deployability,"So the gist of how udocker caches things is that it uses a directory similar to Docker, which defaults to [`~/.udocker`](https://github.com/indigo-dc/udocker/blob/master/udocker.py#L137), but you can override that with a config file [described here](https://github.com/indigo-dc/udocker/blob/master/doc/installation_manual.md#9-configuration), to set a custom location, e.g. `reposdir = ""/path/to/cache""`. Within that directory it caches the different layers and images in different subdirectories. I'll write that up into the document.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267:328,configurat,configuration,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464521267,1,['configurat'],['configuration']
Deployability,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:201,release,release,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067,2,['release'],['release']
Deployability,"So two things are happening here:; * We can't create a new WDL type (without modifying the spec) but we can update the backend implementation in Cromwell to widen the supported values. My comment in the previous PR was making that case.; * We can only widen the supported values as far as the WDL spec allows.; * As of WDL draft-2 and 1.0, the supported width is unspecified; * As of WDL 2.0 it's defined thus: ; * `Float` is a finite 64-bit IEEE-754 floating point number.; * `Int` can be used to hold a signed Integer in the range `[-2^63, 2^63]`. . So I think the best thing we could do is support the data types specified in WDL 2.0. Even though we'd be technically compliant with WDL 1.0 by going wider, I think it's just going to cause us headaches later. I'd suggest starting by defining what we will and won't support by adding these test cases and trying to satisfy them:; * Support for the max value of a finite 64-bit IEEE-754 floating point number; * Disallowing the max value of a finite 64-bit IEEE-754 floating point number + 1; * Support for the min value of a finite 64-bit IEEE-754 floating point number; * Disallowing the min value of a finite 64-bit IEEE-754 floating point number - 1; * Support for the max value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the max value of a signed Integer in the range `[-2^63, 2^63]` + 1; * Support for the min value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the min value of a signed Integer in the range `[-2^63, 2^63]` - 1. By the way - I think if we do achieve this, there's a PR that's been sitting in the WDL spec for a while that could then merge as ""implemented"" so it'd be really cool to be able to achieve that!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405:108,update,update,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405,1,['update'],['update']
Deployability,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,configurat,configuration,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259,2,['configurat'],['configuration']
Deployability,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:14,upgrade,upgrade,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961,6,"['release', 'update', 'upgrade']","['release', 'update', 'upgrade']"
Deployability,"Someone will need to document the new `defaultBackend` and `backendsAllowed` configuration in the CHANGELOG, and the workflow option `backend` before this can be released. Should I do that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/609#issuecomment-203068046:77,configurat,configuration,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/609#issuecomment-203068046,2,"['configurat', 'release']","['configuration', 'released']"
Deployability,"Sorry @katevoss I didn't answer this before - it's the issue that the workflow fails because we are in fail fast state, but the calls inside that workflow don't get updated - so you can get a workflow that says ""Failed"" with a bunch of tasks that say ""Running"", which is confusing to users and they often ask if they are still running or not (the answer is that they are until they get to a final state, but no subsequent parts of the workflow continue after that point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308771792:165,update,updated,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-308771792,1,['update'],['updated']
Deployability,Sorry about this. That's a bug in the qualimap parsing in bcbio that we've fixed (https://github.com/bcbio/bcbio-nextgen/commit/e15f787f984da3e5d727733f2a1d7c58c50c6be0) but hasn't yet been rolled into the Docker container. We're planning a release tomorrow so I can push a new Docker container as well which should fix the problem. So I don't think this is a Cromwell issue but a bug on the bcbio side and if other workflows are good I'd skip it for now. Thanks again for all this testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436101342:241,release,release,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436101342,1,['release'],['release']
Deployability,"Sorry for the noise. Closing (again) till I can confirm what's going on here.; UPDATE:; For the record, it turned out I was using an old cloudformation stack. With the most recent version of `quay.io/broadinstitute/cromwell-aws-proxy:latest` globbing is working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434288736:79,UPDATE,UPDATE,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4335#issuecomment-434288736,1,['UPDATE'],['UPDATE']
Deployability,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:28,configurat,configuration,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938,1,['configurat'],['configuration']
Deployability,Sounds good thanks ! I'll update here once I have more info.; In similar news I was able to run [gvcf_joint](https://github.com/bcbio/test_bcbio_cwl/tree/master/gvcf_joint/gvcf-joint-workflow) to completion using the same inputs as in the gcp/somatic workflow (in the `gs://bcbiodata/test_bcbio_cwl` bucket),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435958655:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435958655,1,['update'],['update']
Deployability,"Sounds good to me, thanks. I'll make a ticket for us to update our wdl user docs with a note about the upcoming change; y'all go ahead with this as you like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313241466:56,update,update,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313241466,1,['update'],['update']
Deployability,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:282,update,updated,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159,1,['update'],['updated']
Deployability,Still :+1: for me. . On the issue of whether or not to use the library my general preference is to use well vetted 3rd party libraries for things instead of rolling our own. That said I've not heard of this library before so can't say much (although I did see it endorsed by some people I trust). I don't really care one way or the other on this particular one.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/434#issuecomment-182474297:157,rolling,rolling,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/434#issuecomment-182474297,1,['rolling'],['rolling']
Deployability,"Strangely, this non-input workflow and non-AWS configuration, lead to the same error, so seems to be something about my build, not likely the AWS Batch specific part:. [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081156/my-cromwell.conf.txt); [myWorkflow.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081161/myWorkflow.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255:47,configurat,configuration,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255,1,['configurat'],['configuration']
Deployability,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:905,update,update,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,4,['update'],['update']
Deployability,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:60,release,release,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228,2,['release'],['release']
Deployability,"Sure, I forgot about the the other issue. And this was not in a branch, but in the release.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250459311:83,release,release,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497#issuecomment-250459311,1,['release'],['release']
Deployability,"Sure, but generally I would advocate to leave open PRs that you might want other contributors to see (that they have not been fully addressed / might be integrated in the future). There isn't any harm in leaving it open, until it's irrelevant and has no chance of being looked over again!. This is of course my 0.02 and just an opinion, so if you feel strongly about closing it, that is up to you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416321838:153,integrat,integrated,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416321838,1,['integrat'],['integrated']
Deployability,"Sure. Otherwise, seems fine. On May 17, 2017 10:42, ""Jeff Gentry"" <notifications@github.com> wrote:. > Could you hold off a bit? There's an update in flight to the plugin.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302112778>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk59bJp_C1jcwAeUotUZ-mtu2w9_3ks5r6wdqgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476:140,update,update,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-302113476,1,['update'],['update']
Deployability,"TB in testing so I donâ€™t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > [â€¦](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf â€¦ <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1043,update,updated,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['update'],['updated']
Deployability,"TL;DR ðŸ‘ post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:197,update,updated,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854,1,['update'],['updated']
Deployability,"TOL / pipe dream: inspired by the lovely system we have for CWL conformance testing, it would be nice to give Centaur distinct concepts for `-e` (exclude a test that is conceptually inappropriate for this configuration) and `-s` (really should work on this configuration but right now doesn't). Centaur could try to run ""shoulda"" tests with the sense of pass and fail reversed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484251925:205,configurat,configuration,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4840#issuecomment-484251925,2,['configurat'],['configuration']
Deployability,"TOL and assuming the investigation requested above confirms it would be helpful:. The current blacklisting implementation asks ""did a read from this source bucket result in a 403?"" If the answer is no, Cromwell tries to copy from that source bucket at full blast. However if Cromwell's first attempts to read from that source bucket do in fact result in 403s, there can be a large number of failed copies before blacklisting kicks in. One possibility would be modifying the question to ""what is the status of this source bucket with respect to 403s?"" with valid responses of ""known good"", ""known bad"" and ""I don't know"". . For ""I don't know"" Cromwell could be more cautious in its handling of that source bucket and launch a single ""canary"" copy attempt. Based on the result of that canary attempt Cromwell could choose ""known good"" or ""known bad"" and blacklisting could proceed the same as it does today with fewer copy failures.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497:733,canary,canary,733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497,2,['canary'],['canary']
Deployability,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:211,update,update,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934,1,['update'],['update']
Deployability,"TOL: You might want to add the `src/ci/bin/testMetadataComparisonPython.sh` file to the ""scripts only"" filter so that it only needs to run scripts tests if that file is updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121:169,update,updated,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121,1,['update'],['updated']
Deployability,"TTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3279,update,update,3279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['update'],['update']
Deployability,"Talked to @jsotobroad in person.; - Kamon was removed in develop, but no hotfix needed because; - Green team changed their `java ...` script to not invoke the kamon code. No longer seems to be an issue. Closing and removing my branch that, in parallel, tested the kamon removal against a stale version of 0.19_hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661:73,hotfix,hotfix,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661,1,['hotfix'],['hotfix']
Deployability,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:81,pipeline,pipelines,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533,2,"['integrat', 'pipeline']","['integration', 'pipelines']"
Deployability,Tests will fail until review/merge/release of https://github.com/broadinstitute/martha/pull/190,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597:35,release,release,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597,1,['release'],['release']
Deployability,Text updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/428#issuecomment-181484804:5,update,updated,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/428#issuecomment-181484804,1,['update'],['updated']
Deployability,"Thank you @aednichols . 1: I understand that it's free, and I'm only owed what I've paid for. :-). 2: A Unix tool running in the `WDLTesting/src/wdl`directory would report that `WDLTesting/src/wdl/Child/ChildWF.wdl` does not exist. But because it's being called from the main workflow, that still works. It seems to be that you can say ""all file references based off the launching directory work"", or ""no file references based off the launching directory work, you have to evaluate the reference based on the location of the file that made the reference"". Saying ""sometimes it will work, and sometimes it won't. We know you've been depending on this behavior, but we're nuking it anyway""? That, I would say, is rather user unfriendly. 3: All that said, that change, making a called workflow behave significantly differently from the starting workflow, was at least easily dealt with. (I wrote a python script to update the import statements.) But the change that called workflows no longer get passed anything from the JSON file. Given a choice between adding 10 - 50 parameters to each sub-workflow call, and just sticking with draft-2, we're sticking with draft-2. So, thank you for your time, and good luck going forward",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013:912,update,update,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013,1,['update'],['update']
Deployability,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations â†’ docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:800,release,release,800,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981,2,"['release', 'upgrade']","['release', 'upgrade']"
Deployability,"Thank you for looking into this! Since you encountered this problem in real life, is there any chance you have a minimal WDL / options that reproduces the issue? If so we can take care of turning that into a Centaur integration test so this doesn't regress.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986:216,integrat,integration,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986,1,['integrat'],['integration']
Deployability,"Thank you. Looking forward to kicking the tires on this release.; What would be the default value of the driverVersion? It doesn't look like its a required attribute (??), so why not let GCP pick the stable one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-490067240:56,release,release,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4942#issuecomment-490067240,1,['release'],['release']
Deployability,"Thank you. When (time frame) will it be released after the merge?. On Fri, Apr 3, 2020 at 12:48 PM Chris Llanwarne <notifications@github.com>; wrote:. > I've made a clone of this PR (#5475; > <https://github.com/broadinstitute/cromwell/pull/5475>) to allow our CI; > to run against these changes. Assuming all looks good I think we'll be able; > to merge!; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608626878>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABBCBVPS2R3XQKUFVN5NFV3RKY4RJANCNFSM4KUEM3YQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608971741:40,release,released,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608971741,1,['release'],['released']
Deployability,"Thanks @Horneth , for the update and explanation and workaround fix!. ________________________________; From: Thib <notifications@github.com>; Sent: Friday, 24 February 2017 12:44 AM; To: broadinstitute/cromwell; Cc: Conrad; Mention; Subject: Re: [broadinstitute/cromwell] Using Local backend, localization of relative symbolic links doesn't work (#1950). @delocalizer<https://github.com/delocalizer> Hey sorry I forgot to give you an update on that and I don't know if you've seen it. You were right and the bug should be fixed now. It was due to a bug in the better files library we use and I filed a ticket on their github pathikrit/better-files#115<https://github.com/pathikrit/better-files/issues/115>.; In the meantime I added a workaround. â€”; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-281994706>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AAx6ka7RU3i7jBQp7dQaZGPB1z6WtzlRks5rfY05gaJpZM4L1_PC>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993:26,update,update,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950#issuecomment-282145993,2,['update'],['update']
Deployability,"Thanks @Horneth - any estimate on v30 target release date? Or, any other way you can think of to work around the issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337729403:45,release,release,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337729403,1,['release'],['release']
Deployability,"Thanks @Selonka! That looks like a nifty workaround. Does this work with the Pipelines API backend, or just with a local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477972220:77,Pipeline,Pipelines,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-477972220,1,['Pipeline'],['Pipelines']
Deployability,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:374,configurat,configuration,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,1,['configurat'],['configuration']
Deployability,Thanks @evantheb Iâ€™ll update woth the aigogenerated version,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389411:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389411,1,['update'],['update']
Deployability,"Thanks @ffinfo - looks good to me: ðŸ‘ . ps don't worry too much about missing Cromwell 35, we're hoping to get better at releasing more regularly (even if we don't have world-changing features in every release) - so it should only be a few weeks before Cromwell 36 gets released, including this change. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4112/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426064906:201,release,release,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-426064906,2,['release'],"['release', 'released']"
Deployability,"Thanks @gemmalam... will let you know when I run into that user joy... all I've found thus far are flying pigs :joy: . @leepc12 and I had discussion yesterday, and I've updated the recipe to be more general, meaning that it would allow the pipeline creator to set (general) strings of singularity commands (e.g., `--debug --vvv`) or the command itself (`exec` vs `run`) and then the command options (`--bind /host:/container --cleanenv`) etc. This will work to run a docker:// uri for a singularity container, or a file path, which I understand to be a use case that is wanted? I think providing this example is a good solution (for the time being) until Singularity can be added as a backend proper.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-417084078:169,update,updated,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-417084078,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,Thanks @gsaksena I'll update this code to use `-terse`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/181#issuecomment-140150656:22,update,update,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/181#issuecomment-140150656,1,['update'],['update']
Deployability,"Thanks @illusional, I've come to a very similar configuration, albeit for singularity 3.X. I ended up settling on this:. ```; submit-docker = """"""; export SINGULARITY_CACHEDIR=/data/cephfs/punim0751/singularity_cache; module load Singularity/3.0.3-spartan_gcc-6.2.0; IMAGE=/data/cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of runni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:48,configurat,configuration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,1,['configurat'],['configuration']
Deployability,"Thanks @mcovarr, I've moved this back to draft state, as it's something I'll need to put behind a configuration option, so requires more work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451:98,configurat,configuration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451,1,['configurat'],['configuration']
Deployability,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:531,configurat,configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000,1,['configurat'],['configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file']
Deployability,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:1129,release,release,1129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,1,['release'],['release']
Deployability,"Thanks @pshapiro4broad, I've updated the issue title. I don't know how much I agree with the behaviour, but to me it sounds strange. It's logged on OpenWDL, so I'll close this for now and reopen if required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243:29,update,updated,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243,1,['update'],['updated']
Deployability,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:63,pipeline,pipelines,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132,1,['pipeline'],['pipelines']
Deployability,"Thanks @rhpvorderman! This does indeed sound like a useful thing to be able to control. Based on your description, it feels it should probably be more of a runtime option (ie different per-task/step) rather than a global configuration option. Would you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420763709:221,configurat,configuration,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420763709,1,['configurat'],['configuration']
Deployability,"Thanks @wleepang for the response. I was using the custom AMI as specified in the link. But I also had a LaunchTemplate that will mount EFS to batch computes. The problem was my userData in the LT had this line ""yum update"" that updated everything including the ECS-agent to the latest version and then it failed with the specified error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953:216,update,update,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953,2,['update'],"['update', 'updated']"
Deployability,Thanks Miguel. Sounds like it makes sense to close this PR. We'll create a hotfix PR with this functionality.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219125700:75,hotfix,hotfix,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219125700,1,['hotfix'],['hotfix']
Deployability,Thanks everyone! FWIW I would need to have this released by mid-Jan probably so that we can have all the right versions etc show up in the relevant book chapters. Will that be possible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567523168:48,release,released,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567523168,1,['release'],['released']
Deployability,Thanks for letting us know about this. We discovered a critical bug in Cromwell 28 yesterday and released a patched version of the jar. I believe @geoffjentry prepared an updated version of the Homebrew formula yesterday as well. The formula from June 30 is certainly referencing the older jar and will not match the current checksum.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656:97,release,released,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316009656,3,"['patch', 'release', 'update']","['patched', 'released', 'updated']"
Deployability,"Thanks for looking into it! Closing this spike as its complete, and will update the respective issue with the proposed solution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4818#issuecomment-481815773:73,update,update,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4818#issuecomment-481815773,1,['update'],['update']
Deployability,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:869,pipeline,pipeline,869,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738,3,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,Thanks for providing code recommendation! Submitted PR to update.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1773193840:58,update,update,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1773193840,1,['update'],['update']
Deployability,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:409,configurat,configuration,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874,1,['configurat'],['configuration']
Deployability,"Thanks for the comment Paul -- we've already started those discussions with the Google folks. We want this built deep into the Pipelines API as true notifications via pubsub which seems a reasonable feature. It's possible to do what you say yourself (using a GCE node(s) to query and then post) but with 100,000s of operations in flight it's much more efficient to be notified than poll... but I'm sure you agree with that ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282:127,Pipeline,Pipelines,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282,1,['Pipeline'],['Pipelines']
Deployability,Thanks for the fix. Whenever updating the swagger you'll also need to run `sbt generateRestApiDocs` then commit & push the updates.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5054#issuecomment-507519998:123,update,updates,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5054#issuecomment-507519998,1,['update'],['updates']
Deployability,"Thanks for the insight Jeff.Let's just leave it then if the PBE stuff is happening so soon, I don't want to waste your time. I'd be very happy to be a guinea pig for making a backend using an early release of the new design. Cromwell rocks b.t.w.; and thanks for making it open!; cheers,C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-230188328:198,release,release,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-230188328,1,['release'],['release']
Deployability,"Thanks for the issue @kottmast, we probably won't be able to patch a fix for this in 29_hotfix but we'll definitely try to make sure it's fixed in 30 :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337605367:61,patch,patch,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337605367,1,['patch'],['patch']
Deployability,"Thanks for the issue @meganshand, we probably won't be able to patch a fix for this in 29_hotfix but we'll definitely try to make sure it's fixed in 30 :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2753#issuecomment-337605187:63,patch,patch,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2753#issuecomment-337605187,1,['patch'],['patch']
Deployability,"Thanks for the report! This should actually be resolved already by https://github.com/broadinstitute/cromwell/pull/1857, so should be fixed in the next Cromwell release. In the meantime (if you're feeling brave), you could try running directly from develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-273824799:161,release,release,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-273824799,1,['release'],['release']
Deployability,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:87,patch,patch,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313,1,['patch'],['patch']
Deployability,Thanks for the update @aednichols!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442207842:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442207842,1,['update'],['update']
Deployability,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313,2,['update'],['update']
Deployability,"Thanks for the update, could you provide the full metadata of the failed workflow if it's not too much trouble ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435690356:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435690356,1,['update'],['update']
Deployability,"Thanks for the updated PR, we will take a look (should we close https://github.com/broadinstitute/cromwell/pull/5523?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5562#issuecomment-655116850:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5562#issuecomment-655116850,1,['update'],['updated']
Deployability,"Thanks for this Ales,. I wonder if it's a slurm thing?; https://slurm.schedmd.com/resource_limits.html. Some key things to look at from here:; https://slurm.schedmd.com/slurm.conf.html. MaxJobCount - Number of jobs in the active database - this includes recently finished jobs, new jobs won't go into pending.; MaxSubmitJobs - Can be set for a per user too. Could Cromwell be continuously trying to submit jobs and waiting until a new position is available?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986:376,continuous,continuously,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986,1,['continuous'],['continuously']
Deployability,"Thanks for this help with the configuration. I'm not intentionally overwriting the global `filesystem`, but I don't have an explicit import of reference.conf. Do I need to have an import like we do for `application`? Do you spot anything else I might be doing wrong?. https://gist.github.com/chapmanb/72c6bf2d8282412b252f6192968b17cf. I appreciate all the help debugging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426279585:30,configurat,configuration,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-426279585,1,['configurat'],['configuration']
Deployability,"Thanks mcovarr and scottfrazer for your helpful suggestions/comments. . I'm going to resubmit the patch with the suggested changes; and go from there, . Thanks,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-221363178:98,patch,patch,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/875#issuecomment-221363178,1,['patch'],['patch']
Deployability,"Thanks so much for the pointer to the hashing and details about adjusting that. You're exactly right that was what was happening. I saw the hash error and saw no new jobs launched, thought it had failed, so must have stopped the tasks before they finished the md5 summing and continued. I ran our analysis both with md5 checksumming and path based hashing, and it does save some time. The md5 based one took 6:43 and path based was 5:55. Doing based on paths works great for us so I'll swap to that as the default in bcbio pipelines. For database storage, are their known deficiencies with file based HSQL over MySQL? Do I have ways to mitigate those through different settings? We're planning to use MySQL in some circumstances but for providing something generally for users the file-based database will be the default and I'd like to make that as good and error free as possible. Is leaving this open for the `null` hash issue worthwhile? I also got some other hash errors on a different run that didn't affect completion but might be influencing re-use and storage that I could raise an issue on, but might be due to file based storage. If there are general things I can tweak and test I can do that first prior to opening additional issues. Thank you again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743:523,pipeline,pipelines,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743,1,['pipeline'],['pipelines']
Deployability,"Thanks so much for walking through this. The tests are develop built on Friday after the merge. This is the CWL workflow I'm running, which isn't much different than the ones we've tested on but has everything in Google Storage:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. and this is the configuration:. https://gist.github.com/chapmanb/93d5468ac691012eaf70e67c17ed2498. It's running good up until this point, which I'm excited about. I figured I triggered the issue by having a slightly longer root location:. https://gist.github.com/chapmanb/93d5468ac691012eaf70e67c17ed2498#file-bcbio-cromwell-gcp-conf-L89. Thanks again for helping look at it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445563569:303,configurat,configuration,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445563569,1,['configurat'],['configuration']
Deployability,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history ðŸ˜¡ so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:68,integrat,integrated,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,2,['integrat'],['integrated']
Deployability,Thanks will update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382390194:12,update,update,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382390194,1,['update'],['update']
Deployability,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:836,configurat,configuration,836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641,2,['configurat'],['configuration']
Deployability,Thanks! I was just about to ask you for the logs from GCP. Looks like the issue is the mount path. Batch only allows mounting to specific locations. I will push an update to the mount path.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221:164,update,update,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221,1,['update'],['update']
Deployability,"Thanks, @vsoch, I will look into the new caching functionality once 3.1.0 is fully released, and I'll open an issue soon. The reason this matters is because if we run a scatter job in Cromwell where we have multiple jobs running the same image, they will all simultaneously try to rebuild the image file, which I imagine will catastrophically fail. But now that I think about it, if the image does need updating this will still cause it to fail. I might have to think about some kind of locking mechanism to use here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463885163:83,release,released,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463885163,1,['release'],['released']
Deployability,"Thanks, this is much better. Given this behavior, it would be good to update the issue summary to match, with something like: `Cromwell File to String conversion uses input name, not localized path name`. Regarding this behavior, the relevant part of the spec that I see says:. > When a WDL author uses a File input in their Command Section, the fully qualified, localized path to the file is substituted into the command string. As I read this, the point is that `File` has special behavior inside a command block. Outside of a command block, it has no special behavior, and therefore when converted to `String` it has the path that was provided as an input. So to me it seems that cromwell's behavior is consistent with the spec.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678343746:70,update,update,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678343746,1,['update'],['update']
Deployability,Thanks. Do you know what email address they updated? I still cannot access the project containing that issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-509710309:44,update,updated,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981#issuecomment-509710309,1,['update'],['updated']
Deployability,That is quite some digging you've done! I hope you were able to reach an acceptable working configuration. I am going to go ahead and close this issue because there is not a clear Cromwell bug here as far as I can see.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945:92,configurat,configuration,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945,2,['configurat'],['configuration']
Deployability,"That plot is the load on the Cromwell server itself? . From what's you've said, and the top output, it looks like your tasks are exeucting on that server as well. You mentioned that you're running SFS, can you tell us more about your configuration? Are you dispatching to PBS but the execution host is the cromwell server itself (where the sync commands are running)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542:234,configurat,configuration,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284736542,1,['configurat'],['configuration']
Deployability,"That sounds great. For the community at large would probably be nice to open a PR to bioconda on each new release with an updated recipe, but it could also push builds to its own channel on anaconda.org (probably easier).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076:106,release,release,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-301189076,2,"['release', 'update']","['release', 'updated']"
Deployability,"That's a cool use of this configuration to work around this issue !. I just want to give some context around it. This rate control was originally put in place to protect Cromwell against excessive load or a very large spike of jobs becoming runnable in a short period of time. Through this mechanism Cromwell can also stop starting new jobs altogether when under too heavy load.; While this achieve the desired effect of rate limiting how many submit requests are being sent to AWS batch in a period of time, I think a medium-term better fix is too implement something similar to the [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) for the AWS backend.; The reason is that it acts as a coarse level of granularity which might have undesired side-effects:. 1) it is a system wide configuration, meaning in a multi backend Cromwell it might be too constraining for some backends and not enough for others; 2) It also rate limits starting jobs that might actually be call cached and incur 0 requests to AWS Batch, making it too conservative; 3) It only helps rate limiting the number of job creation requests to batch. Once a job is started, it issues status requests to monitor the job which aren't throttled. Not to say that this is a bad workaround, I think it's a *good* workaround, but still a workaround :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181,6,"['Pipeline', 'configurat', 'pipeline']","['PipelinesApiRequestManager', 'configuration', 'pipelines']"
Deployability,"That's good to know thanks, looks like the docs need an update :); I'm going to close this ticket, don't hesitate to re-open it if you run into the same issue again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392807644:56,update,update,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-392807644,1,['update'],['update']
Deployability,"That's not currently possible, but IMO it's a good idea. . @kcibul what do you think about having a JES configuration setting for the default zone(s) for jobs? Currently it's just hardwired to us-central1-b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604:104,configurat,configuration,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604,1,['configurat'],['configuration']
Deployability,"Thats good to hear @huangzhibo! Thanks for the help @jeremiahsavage. Based on the last update I'm closing this issue, but please re-open as needed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-440499591:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-440499591,1,['update'],['update']
Deployability,"The Centaur upgrade test uses a WDL temp file as the input of `womtool upgrade`, and imports do not work in this scenario. Attempting to re-engineer this now...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3925#issuecomment-408117654:12,upgrade,upgrade,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3925#issuecomment-408117654,2,['upgrade'],['upgrade']
Deployability,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:201,Configurat,ConfigurationFiles,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005,2,"['Configurat', 'Pipeline']","['ConfigurationFiles', 'Pipelines']"
Deployability,"The E Pam pipeline builder has actually gotten pretty good at making graph visualizations, you should try that yossi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305614974:10,pipeline,pipeline,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305614974,1,['pipeline'],['pipeline']
Deployability,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:135,release,released,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757,2,"['pipeline', 'release']","['pipeline', 'released']"
Deployability,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:392,update,updates,392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480,4,['update'],"['update', 'updates']"
Deployability,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:29,pipeline,pipeline,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722,1,['pipeline'],['pipeline']
Deployability,"The [quick start tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/) uses. ```; java -jar cromwell-XY.jar [ ... ]; ```. so the `-jar` option would be the first thing to try. I also removed the potentially extraneous `cromwell` in `cromwell.jar cromwell run`:. ```; java -jar cromwell.jar -Dconfig.file=../config/LSF.conf run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json; ```. If that doesn't work for you, feel free to reopen the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608:356,pipeline,pipelines,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608,2,['pipeline'],['pipelines']
Deployability,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:189,release,release,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532,1,['release'],['release']
Deployability,"The downloadable jar on Github was intentionally kept the same, and that's the URL used in the Homebrew formula. This whole incident has illustrated the need for better release & version management going forward but that doesn't exist right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639:169,release,release,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639,1,['release'],['release']
Deployability,The entire configuration here.; https://gist.github.com/rhpvorderman/cd91d3356e3fb460df09b0953c31eadb,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-394625591:11,configurat,configuration,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-394625591,1,['configurat'],['configuration']
Deployability,"The existing test doesn't have a sleep method long enough for a restart. The cha-cha and capoeira could be perhaps be merged in the future. The cha-cha in this PR is actually a blend of the capoeira_jes + capoeira_local + capoeira_tes scripts, minus some local docker checks, plus extra sleeps for engine-upgrade restarts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4178#issuecomment-426876798:305,upgrade,upgrade,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4178#issuecomment-426876798,1,['upgrade'],['upgrade']
Deployability,The fix will be in the next release of Cromwell. Thank you for reporting.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4722#issuecomment-474907876:28,release,release,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4722#issuecomment-474907876,1,['release'],['release']
Deployability,"The following could be used as the command body of a Centaur WDL to test this feature (copy-paste-edited from Thibault's [`docker_size_gcr.wdl`](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_size/docker_size_gcr.wdl)):; ```; apt-get install --assume-yes jq > /dev/null; NAME=`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/instance/name`; ZONE=`basename \`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/instance/zone\``; PROJECT=`curl -s -H ""Metadata-Flavor: Google"" http://metadata.google.internal/computeMetadata/v1/project/project-id`; curl -s -H ""Authorization: Bearer `gcloud auth print-access-token`"" ""https://www.googleapis.com/compute/v1/projects/$PROJECT/zones/$ZONE/instances/$NAME?fields=cpuPlatform"" | jq -r '.cpuPlatform'; ```; Run on one of Jeff's legion GCE VMs this produces. ```; Intel Haswell; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460339656:295,install,install,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460339656,1,['install'],['install']
Deployability,"The following gives a 2 GB cache on top of Cromwell, and forwards aborts to a different host. I've manually tested with the following configuration as an excuse to play w/ Kubernetes:. ## Varnish config. points all queries to reader service except aborts:; ```; vcl 4.0;. backend worker {; .host = ""cromwell-worker-service.default"";; .port = ""8000"";; }. backend reader {; .host = ""cromwell-reader-service.default"";; .port = ""8000"";; }. sub vcl_recv {; if (req.url ~ ""abort/$"") {; set req.backend_hint = worker;; } else {; set req.backend_hint = reader;; }; }; ```; Source: https://raw.githubusercontent.com/danbills/ammoniteExample/master/kubernetes/varnish-rw-cromwell-config.vcl. ## Varnish docker . w/ latest 6.1 version:; https://hub.docker.com/r/danbills/varnish/; Source: https://github.com/danbills/ammoniteExample/tree/master/kubernetes/varnish. ## Kubernetes Config(map); Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) to run varnish w/ the config file loaded into a [ConfigMap](https://cloud.google.com/kubernetes-engine/docs/concepts/configmap) named `rw`:; ```; apiVersion: v1; kind: Pod; metadata:; name: varnish-cache; labels:; app: varnish-cache; spec:; containers:; - name: cache; resources:; requests:; # We'll use two gigabytes for each varnish cache; memory: 2Gi; image: danbills/varnish:6_1; imagePullPolicy: Always; args: [""-F"", ""-f"", ""/conf/varnish-rw-cromwell-config.vcl"", ""-a"" , ""0.0.0.0:8080"" , ""-s"" , ""malloc,2G""]; ports:; - containerPort: 8080; volumeMounts:; - name: config-volume; mountPath: /conf; volumes:; - name: config-volume; configMap:; # Provide the name of the ConfigMap containing the files you want; # to add to the container; name: rw; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248:134,configurat,configuration,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4253#issuecomment-437427248,1,['configurat'],['configuration']
Deployability,"The import behavior in `draft-2` is convenient but is essentially a bug. Any Unix tool run in the `WDLTesting/src/wdl/Child` directory would similarly report that `WDLTesting/src/wdl/Child/ChildTask.wdl` does not exist. That's because it's actually located at `../../../../WDLTesting/src/wdl/Child/ChildTask.wdl` relative to that directory. It is acknowledged that many people came to rely on it, which is why fixing it was reserved for a major version. Cromwell does not decide the spec, if you have issues with that please raise them [here](https://github.com/openwdl/wdl). I do not recommend running a production pipeline on version `development` as it is not yet a shipping version and could be subject to change at any time. Career pro tip, calling an open source maintainer's work ""horrible"" is not a winning strategy to get them to help you ðŸ˜",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-882605538:616,pipeline,pipeline,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-882605538,1,['pipeline'],['pipeline']
Deployability,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:53,pipeline,pipeline,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963,1,['pipeline'],['pipeline']
Deployability,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:40,configurat,configuration,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019,1,['configurat'],['configuration']
Deployability,"The motivation for this is that I would like FireCloud/GAWB method configurations to include brief descriptions of declared input and output parameters. It would be great if these description fields could be initialized by metadata contained within a workflow's WDL file. I was thinking we could leverage the parameter_meta sections, but currently that section is only supported in the task definition block, and only for input parameters. I don't, however, want to request a specific solution....the use and expansion of the parameter_meta section may not be the answer. My primary goal is to be able to initialize the input/output parameter descriptions with strings drawn from the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259:67,configurat,configurations,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1131#issuecomment-230915259,1,['configurat'],['configurations']
Deployability,"The new release jar has been renamed and updated, thanks for the catch!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081:8,release,release,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2052#issuecomment-284161081,2,"['release', 'update']","['release', 'updated']"
Deployability,"The next error is:; ```; 2019-01-31 19:38:58,499 INFO - changelog.xml: changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi: ChangeSet changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$Enhanc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:535,UPDATE,UPDATE,535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,2,"['UPDATE', 'release']","['UPDATE', 'released']"
Deployability,The official word from Google is that the OS installed on Pipelines API v1 workers has aged enough that driver support is no longer available. The best I can do on behalf of Cromwell is recommend switching to v2; if you have any further questions feel free to reopen!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959:45,install,installed,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959,2,"['Pipeline', 'install']","['Pipelines', 'installed']"
Deployability,"The only way they appear is similar at all is that they involve the database. FWIW this sort of topic falls under what I see as a third tier of Cromwell which we're not currently considering, but IMO will need to over the next year. There is/will be:. - Cromwell as we know it now. Not intended for horizontal scaling situations. Expected that it can meet the needs of most typical users needs out of the box without a lot of configuration and that it can run anywhere.; - Cromwell with cool implementations of subsystems that are optimized to take advantage of GCP for scaling purposes (i.e. CaaS, but such that a savvy user could set up their own if they wanted to); - An in between phase for people who have scaling needs beyond vanilla cromwell but aren't on GCP, and most likely are on more traditional setups (on prem, hpc, etc). To some extent we might be able to rely on outside contributions here once we've made supporting both the first two (as that'd imply taking advantage of things like the service registry for pluggable implementations) but we'll likely need to prime that pump.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377:426,configurat,configuration,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452#issuecomment-345292377,1,['configurat'],['configuration']
Deployability,The other PR is probably to be considered a temporary patch until your change gets merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/463#issuecomment-187266329:54,patch,patch,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/463#issuecomment-187266329,1,['patch'],['patch']
Deployability,"The output which is produced by STAR might be named differently depending on the settings used. Currently this isn't supported yet in this task, but it might be in the future. Because of this I didn't want to make any assumptions on the naming of these files, in order to maintain flexibility in the use of these settings. After some consideration, though, for this specific pipeline I suppose these settings won't (or shouldn't) be changed. So, for now: Yes, the path to the BAI can be calculated in a more stable way. It still seems a bit awkward, though, that a rerun might try to produce a different file from the original run, but I'll close this for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395677185:375,pipeline,pipeline,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395677185,1,['pipeline'],['pipeline']
Deployability,"The removed `-Xms2g` was saying ""Never run sbt, ever, without less than 2g of memory"". Meanwhile, Intellij has its own ""[Maximum Heap Size](https://www.jetbrains.com/help/idea/sbt.html#82b10b37)"" configuration value for the amount of memory required to import an sbt project. The IDE doesn't use this for running tests, so it does not need to be as large as the sbt options one uses for `sbt test` from the command line. The net effect of having the Intellij maximum less than 2g and the sbt opts _minimum_ at 2g caused a cryptic error of: . ```; Error while importing sbt project:. Error occurred during initialization of VM; Initial heap size set to a larger value than the maximum heap size; ```. This PR still leaves .sbtopts maximum amount of memory for running `sbt test` at 4g. It just no longer states that the JVM should start at 2g of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767:196,configurat,configuration,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767,1,['configurat'],['configuration']
Deployability,"The workflow options are not stored in metadata directly, but there is a record of how Cromwell behaved based on their values. For call caching, this is `allowResultReuse` and `effectiveCallCachingMode`. I _think_ we still record call caching info if the default is used. See https://github.com/broadinstitute/cromwell/releases/tag/25",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-561689371:319,release,releases,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-561689371,1,['release'],['releases']
Deployability,"Then; ```; 2019-01-31 20:10:51,323 INFO - changelog.xml: changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne: ChangeSet changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:498,UPDATE,UPDATE,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,2,"['UPDATE', 'release']","['UPDATE', 'released']"
Deployability,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:755,configurat,configuration,755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952,1,['configurat'],['configuration']
Deployability,"There are some real-looking test failures that might be due to the removal of the execution status setting logic from the backend info update method. Probably the way to fix these is to change the test expectations, but some investigation seems warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209:135,update,update,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209,1,['update'],['update']
Deployability,There was a [34 hotfix](https://github.com/broadinstitute/cromwell/pull/4149) for this a while back. This should also be fixed in Cromwells 35 and greater.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4140#issuecomment-429463944:16,hotfix,hotfix,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4140#issuecomment-429463944,1,['hotfix'],['hotfix']
Deployability,There was indeed a case where a workflow could get stuck. I added a commit to this PR that fixed it.; I think we should hotfix this to 30 (at least the last commit for sure),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3072#issuecomment-352212169:120,hotfix,hotfix,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3072#issuecomment-352212169,1,['hotfix'],['hotfix']
Deployability,"There were indeed a few regressions around optionals introduced when we implemented the conditionals. They should be fixed in C25 (EDIT: I should probably say, not yet released!), so if your WDL has a pattern like anything in the following test case, it should be good: https://github.com/broadinstitute/centaur/blob/develop/src/main/resources/standardTestCases/optional_parameter/optional_parameter.wdl. I'll add another test for the optional file input since it's not a case I was aware of. I believe @kshakir has been fixing a lot of file path manipulations recently so it might also be good in C25.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340:168,release,released,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276758340,1,['release'],['released']
Deployability,"They'll look like the two tags currently in https://hub.docker.com/r/broadinstitute/cromwell/tags/ that are:; - `23-0c25192`; - `23`. Note to future viewers: I'll be deleting the git hashed tag, but 23 should re-appear when 23 is actually released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017:239,release,released,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1668#issuecomment-261033017,1,['release'],['released']
Deployability,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I donâ€™t necessarily need Cromwell to do the delete for me in my use case. Iâ€™m happy to do it if I have all the files at hand that I want to delete. This is important because I donâ€™t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:434,pipeline,pipeline,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283,2,['pipeline'],['pipeline']
Deployability,"This Cromwell PR accompanies https://github.com/broadinstitute/martha/pull/197 but can be deployed independently. Since DRS is no-longer W3C URI compatible, this PR no longer tries to use `java.net.URI` to check for valid values and instead just forwards on whatever opaque `dos://` or `drs://` strings to `martha_v3`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6020#issuecomment-722554011:90,deploy,deployed,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6020#issuecomment-722554011,1,['deploy'],['deployed']
Deployability,This PR contains the result of the work done on CromIAM load testing https://github.com/broadinstitute/mcnulty/pull/15; There's an CromIAM + Cromwell + SAM + OpenDJ running in broad-dsde-cromwell-dev. The gatling test in the PR above can be run against that CromIAM through a [jenkins job](https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4265#issuecomment-445345005:385,Pipeline,Pipeline,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4265#issuecomment-445345005,1,['Pipeline'],['Pipeline']
Deployability,"This PR has been merged. There is a [commit](https://github.com/broadinstitute/cromwell/commit/0f549a9fef716d8988c3269c8f42ec830e2f625c) in develop as well. But since this was merged when Github was having issues, it has been left in a weird state. Even after being merged it shows it is Open. I had filled a support ticket with GitHub with no response. As there is an option to close the PR, I am going to close it. Update: Well it's weird. As soon as I closed it, it changed the status to Merged ðŸ¤·â€â™€ï¸",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5458#issuecomment-611659610:417,Update,Update,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5458#issuecomment-611659610,1,['Update'],['Update']
Deployability,This also happens to me with scattered tasks. Can a hotfix be enabling RetryStrategy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-733360486:52,hotfix,hotfix,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-733360486,1,['hotfix'],['hotfix']
Deployability,"This appears to be an issue with Pipelines API and its container-optimized OS (""COS"") not having the right drivers for the GPU. You could potentially try [changing the Nvidia driver version](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#gpucount-gputype-and-nvidiadriverversion) but I think your best bet is asking Google support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182:33,Pipeline,Pipelines,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182,1,['Pipeline'],['Pipelines']
Deployability,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:69,integrat,integration,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307,1,['integrat'],['integration']
Deployability,This happened because we haven't published a wdltool with an updated wdl4s for a long time. Closing this issue but we should watch this next time we publish.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2294#issuecomment-303737006:61,update,updated,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2294#issuecomment-303737006,1,['update'],['updated']
Deployability,"This happens because the EJEAs are not aborted directly. Because they're waiting for tokens the cycle looks like this:; - WEA is waiting for all EJEAs to finish; - All running BJEAs get aborted; - This releases tokens so the next group of EJEAs can begin; - They immediately get aborted; - this releases tokens so the next group of EJEAs can begin; - Ad nauseam, until all queued EJEAs start and get aborted",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043:202,release,releases,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-255799043,2,['release'],['releases']
Deployability,This has been fixed today in [`develop`](https://github.com/broadinstitute/cromwell/pull/3260) as well as hotfixed on the latest release branch ([`30_hotfix`](https://github.com/broadinstitute/cromwell/pull/3261)); Thanks again for spotting it !; Do not hesitate to re-open if you still encounter the issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-365100944:106,hotfix,hotfixed,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-365100944,2,"['hotfix', 'release']","['hotfixed', 'release']"
Deployability,This has been tested in production now for quite some time and it seems that the update catches all folders. (At least on the sfsBackend.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782:81,update,update,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782,1,['update'],['update']
Deployability,"This is GREAT news!. Sorry I meant ""cross cloud platform"", not ""cross cloud"" compatibility. It has a lot to do with that - if my pipelines are in WDL, I can't use them e.g. on the SevenBridges platform; if they are in CWL, I can't use them in FireCloud (currently). Anyways, thanks for the positive reply and keep up the great work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2237#issuecomment-298782585:129,pipeline,pipelines,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2237#issuecomment-298782585,1,['pipeline'],['pipelines']
Deployability,"This is a problem we have ran into as well. Our solution for now was to patch cromwell to trap SIGUSR1/2 signals to modify the rc file. These are a pre-signal fired by SGE to let the job know it's about to be killed because of requested-resource limits (https://github.com/princessmaximacenter/cromwell/commit/fe5dab7505f089f99d56b1a7eefac9eb108f5898). However, patching Cromwell every release is a bit of a bother and your solution looks a lot more promissing. Looking at the different recurring discussions on this topic I would strongly urge you to to make the timeout a recurring poll instead of a one-time-fire; I am not well versed in scala so it might be that this is already what you do =).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349:72,patch,patch,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424700349,3,"['patch', 'release']","['patch', 'patching', 'release']"
Deployability,"This is a small patch on the patch of #2067. Yes, labels and the sql converters both need better unit and or centaur tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021:16,patch,patch,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2083#issuecomment-288469021,2,['patch'],['patch']
Deployability,"This is a thing which theoretically shouldn't even be possible in 0.20+, another reason I didn't want to tackle the ticket. . If you're fine on not patching 0.19, I'd like to move to close this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229824845:148,patch,patching,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/800#issuecomment-229824845,1,['patch'],['patching']
Deployability,"This is a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:232,Update,Updated,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,1,['Update'],['Updated']
Deployability,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:230,configurat,configuration,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002,2,['configurat'],['configuration']
Deployability,"This is an amazing contribution!. I wonder whether it would be possible to make a single â€œcreate all the tables at once, in their latest configurationâ€ migration for Postgres, given that no Cromwell instantiated prior to Postgres support could possibly have a database that needs migrating.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-487949926:137,configurat,configuration,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-487949926,1,['configurat'],['configuration']
Deployability,"This is an exceptionally annoying error, any more thoughts on how to potentially fix this? Should we go through the pipelines team?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094:116,pipeline,pipelines,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094,1,['pipeline'],['pipelines']
Deployability,"This is fixed. On Sep 20, 2017 5:12 PM, ""Kate Voss"" <notifications@github.com> wrote:. > @cjllanwarne <https://github.com/cjllanwarne> I know you just made an; > update to the IntelliJ plugin, did you fix this too?; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982393>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAKFyC05-lHHzULkbRCfoBisrrWy8cMvks5skX-jgaJpZM4Nd63a>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664:162,update,update,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276#issuecomment-330982664,1,['update'],['update']
Deployability,This is going to be part of the .21 release.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1347#issuecomment-247151430:36,release,release,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347#issuecomment-247151430,1,['release'],['release']
Deployability,This is great! We should wait for https://github.com/broadinstitute/centaur/pull/193 (fixing our integration tests) to merge but otherwise looking good to me!; ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2280/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444:97,integrat,integration,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444,1,['integrat'],['integration']
Deployability,"This is intended to be a very narrow change to move the ""does this workflow exist"" check from metadata to metadata summaries so the release can go forward. This is deliberately intended to follow existing naming conventions, coding patterns and API structure as much as possible. If anyone feels strongly that these things should be improved there can certainly be enhancement requests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4617#issuecomment-461526777:132,release,release,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4617#issuecomment-461526777,1,['release'],['release']
Deployability,"This is not a Cromwell problem, but rather a known issue affecting all users of Google Pipelines API. Fortunately our contacts at Google report it will be fixed today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714#issuecomment-669212811:87,Pipeline,Pipelines,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714#issuecomment-669212811,1,['Pipeline'],['Pipelines']
Deployability,"This is potentially still an issue but the version of Cromwell has updated significantly. If you do still notice a problem, feel free to file a new issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-288137272:67,update,updated,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-288137272,1,['update'],['updated']
Deployability,This is related to CI Updates PR #4169?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425680548:22,Update,Updates,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425680548,1,['Update'],['Updates']
Deployability,"This is related to Custom labels (being able to apply and update them at different times in the workflow) and custom retry strategies, #1847 & #2253",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1677#issuecomment-304958057:58,update,update,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1677#issuecomment-304958057,1,['update'],['update']
Deployability,This is the hotfix PR:; https://github.com/broadinstitute/cromwell/pull/842,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219757976:12,hotfix,hotfix,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219757976,1,['hotfix'],['hotfix']
Deployability,"This is the hotfix edition, I didn't title it appropriately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637817161:12,hotfix,hotfix,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637817161,1,['hotfix'],['hotfix']
Deployability,"This is the standard way to configure cromwell, to provide your own .conf file. Best to start [here](http://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076:152,Configurat,ConfigurationFiles,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413194076,1,['Configurat'],['ConfigurationFiles']
Deployability,"This is unusual, I have successfully call cached files of 1 TB in testing; so I donâ€™t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:390,release,release,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['release'],['release']
Deployability,"This is with the GCP Batch backend correct? Machine Type is a parameter in Batch, but not a parameter in Cromwell. If a machine type is not defined Batch selects the machine type based on the CPU and Memory request. Setting `cpuPlatform` is the way to not get an e series machine. Did you get an e series machine with ""Intel Cascade Lake"" in the configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190:346,configurat,configuration,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190,1,['configurat'],['configuration']
Deployability,This pull-request may be a little outdated (see conflict). But we're running this at my work. . The solution may need to discussed/documented further as we're essentially introducing another a separate project-id into the configurations.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369:222,configurat,configurations,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369,1,['configurat'],['configurations']
Deployability,"This remains ready for review, but I haven't been able to do the scale testing I planned because there's a change needed to get the auth to GCP working in Terra deployments. Planning to take care of that and do the scale testing before merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259:161,deploy,deployments,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259,1,['deploy'],['deployments']
Deployability,This seems like an excellent idea -- do you have a suggestion for how big this should be? I'd vote for several-x the size of anything the joint calling pipeline does (which might be our biggest user of this?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266805146:152,pipeline,pipeline,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266805146,1,['pipeline'],['pipeline']
Deployability,This seems to be fixed with the latest release wdl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-312347678:39,release,release,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2225#issuecomment-312347678,1,['release'],['release']
Deployability,This should be in the next release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/360#issuecomment-172064962:27,release,release,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/360#issuecomment-172064962,1,['release'],['release']
Deployability,"This should go green soon and be good to merge. I'll submit the master patch tomorrow, unless someone else gets to it first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862:71,patch,patch,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862,1,['patch'],['patch']
Deployability,"This task is covered by the DSDE-Docs Epic ""[Update Cromwell Documentation](https://github.com/broadinstitute/dsde-docs/issues/1514)"", which includes moving most of the content to the WDL website and [slimming down the README](https://github.com/broadinstitute/dsde-docs/issues/1515). Closing this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330:45,Update,Update,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330,1,['Update'],['Update']
Deployability,"This tests as working for me at least with release 24; I can specify ""Local"" or my custom ""PBS"" backend (essentially a modified SGE configured backend) in a task `runtime` block and it behaves as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494:43,release,release,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-276574494,1,['release'],['release']
Deployability,This was already released in Cromwell 32,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-402844677:17,release,released,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-402844677,1,['release'],['released']
Deployability,This would be useful to us as well. We have a similar case where we have a large reference collection that we don't want to have to fetch every time. For our now AWS deploys we can override the submit-docker parameter to add the mount. But this isn't exposed for this backend. It would be nice if we could fully customize the docker command similar to what is possible with ConfigBackendLifecycleActorFactory.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786:166,deploy,deploys,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-754945786,1,['deploy'],['deploys']
Deployability,This'll be brought in by https://github.com/broadinstitute/cromwell/pull/1676 and required some (minor) Cromwell updates. Closing this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1709#issuecomment-263679390:113,update,updates,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1709#issuecomment-263679390,1,['update'],['updates']
Deployability,"Timing-based fix is suboptimal, but discussed w/ the group and patching this for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2975#issuecomment-349000690:63,patch,patching,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2975#issuecomment-349000690,1,['patch'],['patching']
Deployability,"To be totally honest, [this file](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf) is a total config from my darkest of nightmares. If a user opens that and needs to figure out what to do? I'd guess they would be overwhelmed. What about a folder of example configuration files instead? Eg. ```; examples/; cromwell.singularity.conf; cromwell.docker.conf; cromwell.slurm.singularity.conf; cromwell.slurm.conf; ....; ```; At least that way it's not config overload. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832876:288,configurat,configuration,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468832876,1,['configurat'],['configuration']
Deployability,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:250,integrat,integration,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712,1,['integrat'],['integration']
Deployability,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:181,update,update-git-secrets,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599,3,"['install', 'update']","['install', 'update-git-secrets']"
Deployability,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,configurat,configuration,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,2,['configurat'],['configuration']
Deployability,"ToL:. It'd probably be best to slim down and refactor the old engine `cromwell.CromwellTestkitSpec` to a `cromwell.core.CromwellTestKitSpec` and `cromwell.engine.WorkflowTestKitSpec`. Also, the actor system created in the current `CromwellTestkitSpec` uses a custom configuration. As it doesn't fall back to `ConfigFactory.load()`, it doesn't seem to be support modifying [`akka.test.timefactor`](https://github.com/akka/akka/blob/v2.3.12/akka-testkit/src/main/scala/akka/testkit/TestKit.scala#L712-L714) on the sbt command line.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926:266,configurat,configuration,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926,1,['configurat'],['configuration']
Deployability,"ToL:. Looking at this, I wonder whether an easier route to the upgrade script is to make another implementation of this `WdlWriter` typeclass for draft 2 `WdlNamespace`. It leaves us further away from funneling draft 2 and draft 3 through the same object mode (and the massive code deletion we'd get from that)l, but it might be a much more expedient (and maybe safer?) way of achieving the upgrade script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833:63,upgrade,upgrade,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833,2,['upgrade'],['upgrade']
Deployability,Tracked as Google Pipelines API Feature Request #28858407 and is in progress,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/843#issuecomment-230882499:18,Pipeline,Pipelines,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/843#issuecomment-230882499,1,['Pipeline'],['Pipelines']
Deployability,Travis thinks you forgot to update some hash expectations,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201246839:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/616#issuecomment-201246839,1,['update'],['update']
Deployability,"True about `sbt assembly`, but do we want to run integration tests by default with `sbt test`? Currently these are on by default and the `.travis.yml` is set up not to run them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441877:49,integrat,integration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441877,1,['integrat'],['integration']
Deployability,"Typically, I do that through the Cloud Logging Console, instead of fetching the entire log (which could be huge, and expensive) ;); There, you can set up filters to narrow down on particular log entries. `iam.serviceAccountUser` is mostly about granting one `iam.serviceAccounts.actAs` permission on a service account. Not sure why it doesn't show up here, but this permission is required for the Cromwell server to be able to run a pipeline with a Compute SA. BTW `iam.serviceAccountUser` **should** be granted on a per-service-account level, not at the project level (not sure if you've set it up this way, just wanted to confirm). First make sure you don't have that permission granted at the project level, and then if you remove it from the service-account level, it should be able to be seen in the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109:433,pipeline,pipeline,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109,1,['pipeline'],['pipeline']
Deployability,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:0,UPDATE,UPDATE,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310,1,['UPDATE'],['UPDATE']
Deployability,"UPDATE:. Refactored to use the standard backend. Based on my manual crude testing, basic functionality is there. Globs definitely don't work. . More to come soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912:0,UPDATE,UPDATE,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-275251912,1,['UPDATE'],['UPDATE']
Deployability,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:74,release,release,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572,1,['release'],['release']
Deployability,Unit tests are passing. Centaur provides the integration tests we should be using - no need for separate tests. Centaur script was added in rev 3bd9b6a. Closing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3286#issuecomment-394931718:45,integrat,integration,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3286#issuecomment-394931718,1,['integrat'],['integration']
Deployability,Unit-tested functionality in this PR is better integration-tested by #4488 and #4508.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4415#issuecomment-453374229:47,integrat,integration-tested,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4415#issuecomment-453374229,1,['integrat'],['integration-tested']
Deployability,Unsurprising update: Cromwell is definitely leaking workflow log file handles.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-396673480:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-396673480,1,['update'],['update']
Deployability,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:368,release,release,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397,2,['release'],['release']
Deployability,Update README too?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143852430:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/205#issuecomment-143852430,1,['Update'],['Update']
Deployability,"Update README, including documenting what the default behavior is if unspecified.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254606765:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1592#issuecomment-254606765,1,['Update'],['Update']
Deployability,Update README.md with the new API format?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137502827:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137502827,1,['Update'],['Update']
Deployability,Update WDL spec too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200498163:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200498163,1,['Update'],['Update']
Deployability,Update is based on this analysis:. ![workflow_duration_by_status](https://user-images.githubusercontent.com/791985/117333982-7d2e8f80-ae67-11eb-95eb-3cf8f76fa77b.png). See BT-272 for the R script. Edit: Filtered out the workflows that run the individual tests as workflows. Those wrapper workflows were the ones failing (as expected) after ~90 minutes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214,1,['Update'],['Update']
Deployability,"Update, during the most recent scaling tests we ran, 37/4000 and 26/2544 workflows failed due to this issue respectively. It's on CaaS-prod. Some of the workflows:; - 7541e4e2-f74c-43f7-82af-8df891a27520; - 0a18083f-367d-441b-8df5-174817ff7d31; - 84e96b82-d407-4d0f-b03a-cdf307da0739; - 4a18804a-dabe-4b68-ac38-74c2c10fa906; - 31a54cef-7cfd-4fc7-acea-71c178908b2e; - 189747ec-0782-4d40-9f9e-f13d7a9d7d59",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-436639422:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-436639422,1,['Update'],['Update']
Deployability,"Update. I did extensive testing recently to determine whether we might be submitting an empty zip file accidentally, which would produce the same symptoms. That seems unlikely now. I used Lira, the same service we use to submit to CaaS, I sent many post requests to a dummy service that I stood up purely to check the contents of the zip file. The zip was not truncated in any of the 25,000 post requests I sent. When submitting to CaaS recently, we saw this 8 times out of 2,500 submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-428251575:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-428251575,1,['Update'],['Update']
Deployability,"Update: I think this is caused by the `.toTry.get` here: https://github.com/broadinstitute/cromwell/blob/f9cfd25fcf6f29ce50ea532334f30a38a247a7dd/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L257-L260. When the evaluation fails, the actor crashes, and the supervisor must be restarting it indefinitely to try again... EDIT: Switched floating link to permalink. -kshakir",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3088#issuecomment-353467537:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3088#issuecomment-353467537,1,['Update'],['Update']
Deployability,"Update: Nothing is happening (according to `top`). But the workflow seems to be running... ```; {; ""status"": ""Running"",; ""id"": ""7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220094:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220094,1,['Update'],['Update']
Deployability,Update: Still cannot get a timing diagram:; http://104.198.41.229:8080/api/workflows/v2/7b3cdd40-2c3c-4533-be62-06b7d9135546/timing. Just gives me a `)` in my browser window.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220474:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220474,1,['Update'],['Update']
Deployability,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254,1,['Update'],['Update']
Deployability,Update: now pointed to Slick 3.2.2 that was released last week. Not sure how I missed this yesterday especially when I deliberately went looking for it... ðŸ˜¦,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3414#issuecomment-373351153:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3414#issuecomment-373351153,2,"['Update', 'release']","['Update', 'released']"
Deployability,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309,3,"['Update', 'update']","['Update', 'update']"
Deployability,"Update:. ```; $ curl http://localhost:8080/api/engine/v1/stats; {; ""workflows"": 0,; ""jobs"": 0; }; ```. Except, when using `top` I see several jobs running with the workflow ID: 7b3cdd40-2c3c-4533-be62-06b7d9135546. ```; java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --reference /root/case_gatk_acnv_workflow/**7b3cdd40-2c3c-4533**-b+; ```. Status gives me: . ```; {; ""status"": ""fail"",; ""message"": ""Failed lookup attempt for workflow ID 7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460:0,Update,Update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460,1,['Update'],['Update']
Deployability,"Updated everything that I had a clear fix for. Open to other suggestions, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118435565:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118435565,1,['Update'],['Updated']
Deployability,Updated for code review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1718#issuecomment-264297263:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1718#issuecomment-264297263,1,['Update'],['Updated']
Deployability,"Updated per @kshakir's AC above. :smile: . The ""produces"" content type is not 100% right in the Swagger annotations here and probably in other APIs. This API is documented as producing application/json which it will for successful requests, but for failed requests text/plain is produced instead. Given the way the ""Try it out!""s are built, they will fail with undocumented 406s if the underlying request fails with a documented status.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-126404896:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-126404896,1,['Update'],['Updated']
Deployability,Updated the file in the issue description. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395539408:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395539408,1,['Update'],['Updated']
Deployability,"Updated to be way more specific in targeting Denis-reported errors. I've found this area can get difficult to reason about when we throw in ranges and very broad regexes, so I chose to be incredibly specific at the cost of more code & slightly less functionality (but Denis's functionality is there).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999,1,['Update'],['Updated']
Deployability,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:0,Update,Updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497,2,"['Update', 'configurat']","['Updated', 'configuration']"
Deployability,"Upon closer inspection, I think Travis is doing the right thing by not currently running `centaurLocal` for the ""push"", but only running it for the ""pr"". The `.travis.yml` in the current ""push""ed commit doesn't specify to run `centaurLocal`. Meanwhile the develop branch has already been updated, meaning that if-this-""pr""-were-merged `centaurLocal` will run. Rebase onto develop, picking up the `.travis.yml` changes, and the ""push"" build should also run `centaurLocal`, probably?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242275943:288,update,updated,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334#issuecomment-242275943,1,['update'],['updated']
Deployability,"Usually when config and code are in disagreement I'd lean towards correcting the config to match the code rather than the other way round, especially if the code has already been deployed... but in this case if nobody is using the ""wrong"" code value then fixing the code to be consistent with all the other options seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973:179,deploy,deployed,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973,1,['deploy'],['deployed']
Deployability,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:230,install,install,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939,1,['install'],['install']
Deployability,Waiting for release 53. Will not hotfix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968:12,release,release,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,We already use the patched Jar in production and encountered no issues so far.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6672#issuecomment-1108101215:19,patch,patched,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672#issuecomment-1108101215,1,['patch'],['patched']
Deployability,"We are ready to submit PR with fix for this issue and we performed manual testing on several backends (AWS, GCP, Local), but there are some troubles with creation of integration test for that: in particular, we didn't find the way to pass cromwell options to cromwell running in server mode. Is this possible and is integration test required for this issue? @wleepang",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385:166,integrat,integration,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385,2,['integrat'],['integration']
Deployability,We are working on updating the code to fix the bugs describe above and will provide an update when complete.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2302694224:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2302694224,1,['update'],['update']
Deployability,"We can make it happen. We should schedule the PR update. I suspect if one were to touch the `changelog.xml` in develop, it would cause a merge conflict here in this PR, preventing the two changes from being _accidentally_ merged. I would need to rebase this branch on top of any other changes. Specifically, the new liquibase file would need to be moved, along with updating the moved `changelog.xml`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400:49,update,update,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400,1,['update'],['update']
Deployability,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:19,patch,patch,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414,3,"['patch', 'release']","['patch', 'releases']"
Deployability,"We have RNA-Seq and de-novo assembly pipelines with many sub-workflows, forgetting to include a file or two is a common mistake that people make in the lab. Another inconvenience is that we have sub workflows in subfolders, and we do not know how to both keep references (incl. subfolders) to sub-workflows in the main WDL script (so IntelliJ idea WDL plugin can check that they are correct) but at the same time - send them as a lot of files via REST. I think being able to pack everything into one zip archive(preserving subfolder structure) and send it to REST API will allow tracking relative paths properly.; And the last argument is that sending only one file (instead of 5-15) to REST API is way more convenient for users!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726:37,pipeline,pipelines,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2410#issuecomment-333265726,1,['pipeline'],['pipelines']
Deployability,"We have a dazzling new ""monorepo"" setup and release process which builds and publishes a version of womtool matched to the currently released version of Cromwell. Though we may have some more work to do here since:. * womtool won't actually report [its version](https://github.com/broadinstitute/cromwell/issues/2868); * We publish jfrog ""executable"" artifacts for cromwell, womtool, and centaur-cwl-runner, but only the cromwell artifact is published to Github.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-344358518:44,release,release,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2872#issuecomment-344358518,2,['release'],"['release', 'released']"
Deployability,"We have ran into this with nearly every single pipeline run. It has become quite irritating. Mainly because a side-effect is that due to the incorrect CSS `height` the bottom of the time diagram might be hiding with an hidden scrollbar. <img width=""1128"" alt=""Screenshot 2020-02-21 at 10 29 59"" src=""https://user-images.githubusercontent.com/7871310/75022381-48b6f980-5496-11ea-84df-014dcedae9fd.png"">. I concur with @cjllanwarne that this is because of a timing overlap (two events, same start time). For instance, here the `Pending` and `RequestingExecutionToken` startTime overlap for the `GermlineSingleSample.Picard` call at `2020-02-17T13:54:23.127Z`:. ```; ""executionEvents"": [; {; ""startTime"": ""2020-02-17T13:54:23.127Z"",; ""description"": ""Pending"",; ""endTime"": ""2020-02-17T13:54:23.127Z""; },; {; ""startTime"": ""2020-02-17T13:54:23.351Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2020-02-17T13:54:54.304Z""; },; {; ""startTime"": ""2020-02-17T13:54:23.323Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2020-02-17T13:54:23.349Z""; },; {; ""startTime"": ""2020-02-17T13:54:23.349Z"",; ""description"": ""CallCacheReading"",; ""endTime"": ""2020-02-17T13:54:23.351Z""; },; {; ""startTime"": ""2020-02-17T13:54:23.323Z"",; ""description"": ""WaitingForValueStore"",; ""endTime"": ""2020-02-17T13:54:23.323Z""; },; {; ""startTime"": ""2020-02-17T13:54:54.304Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2020-02-17T13:54:55.253Z""; },; {; ""startTime"": ""2020-02-17T13:54:23.127Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2020-02-17T13:54:23.323Z""; }; ],; ""start"": ""2020-02-17T13:54:23.127Z""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3484#issuecomment-589577125:47,pipeline,pipeline,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3484#issuecomment-589577125,1,['pipeline'],['pipeline']
Deployability,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:1177,pipeline,pipelines,1177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,2,['pipeline'],['pipelines']
Deployability,"We lose this feature for local backend yes, but it's not the only one (we also lose support for the ability to use files in calls that were not explicitly set as inputs), which was the point of the ticket because that is what JES is doing.; In the end this was a downgrade of Local/SGE Backends to match JES. Now we can re-upgrade but it should be done on all backends at the same time IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098:323,upgrade,upgrade,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098,1,['upgrade'],['upgrade']
Deployability,We made changes to avoid changing the model that was returned (opting for metadata instead) those changes aren't reflected here. I will update this PR shortly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480:136,update,update,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-221861480,1,['update'],['update']
Deployability,We noticed something similar where it would re-use previous job definitions and do the tasks but fail to write back to the RC file (b/c it was looking for the wrong location which didn't exist). If you can get any info from AWS about what job definition is being used here that might give you another clue. We were using non-release branches though (aws hackathon and then now the develop branch from about a week ago after the polling pull request was merged) so I can't say much else. Maybe test on 42??,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-498498798:325,release,release,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-498498798,1,['release'],['release']
Deployability,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:103,configurat,configuration,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,We recommend `broadinstitute/cromwell:latest` which updates regularly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956772844:52,update,updates,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956772844,1,['update'],['updates']
Deployability,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:119,deploy,deployment,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802,1,['deploy'],['deployment']
Deployability,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:49,pipeline,pipelines,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705,5,"['Continuous', 'integrat', 'pipeline']","['Continuous', 'integration', 'pipeline', 'pipelines']"
Deployability,"We should probably also update the ""look like default"" commented out values in `examples.conf`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4830#issuecomment-482718058:24,update,update,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4830#issuecomment-482718058,1,['update'],['update']
Deployability,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:46,hotfix,hotfix,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363,2,['hotfix'],['hotfix']
Deployability,"We will gladly accept a doc PR for this, docs are one of the more popular updates by the community.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7486#issuecomment-2271513747:74,update,updates,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7486#issuecomment-2271513747,1,['update'],['updates']
Deployability,We will likely need to upgrade liquibase at some point but for the moment PR #4619 does a temporary downgrade. Issue #4618 tracks that MariaDB needs to be CI tested to make sure this doesn't re-occur. Thanks for the report!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197:23,upgrade,upgrade,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197,1,['upgrade'],['upgrade']
Deployability,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:242,pipeline,pipeline,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132,2,['pipeline'],['pipeline']
Deployability,We're pausing this investigation for now so it doesn't make sense to deploy an enhanced-logging version of Cromwell if we're not going to look at the logs for a while,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4407#issuecomment-442196385:69,deploy,deploy,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4407#issuecomment-442196385,1,['deploy'],['deploy']
Deployability,"We've updated gatk-protected to remove the JSON requirement for reading from GenomicsDB in `GenotypeGVCFs`, and also made it so that you can use the same jar to run both the `GenomicsDBImport` tool and `GenotypeGVCFs`. The commit you want is `8812780144561904e6e529c4673c3770076f543b` in `https://github.com/broadinstitute/gatk-protected`. Updated clone/build commands:. ```; git clone git@github.com:broadinstitute/gatk-protected.git gatk-protected; cd gatk-protected; git checkout 8812780144561904e6e529c4673c3770076f543b; ./gradlew clean localJar; Jar will be in build/libs with a name like gatk-protected-package-*-SNAPSHOT-local.jar; You can use this jar to run either GenomicsDBImport or GenotypeGVCFs (no need for a second jar); ```. We're still waiting on a PR from Intel to add the --batchSize argument to the GenomicsDBImport tool, which will be necessary to control memory usage with many samples, but otherwise we should be good to go.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669:6,update,updated,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-297770669,2,"['Update', 'update']","['Updated', 'updated']"
Deployability,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:125,configurat,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667,2,['configurat'],['configuration']
Deployability,What version of cromwell will include this feature/update?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-884467881:51,update,update,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-884467881,1,['update'],['update']
Deployability,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:184,update,update,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210,2,['update'],['update']
Deployability,"When I upgraded to Akka http I found and verified that note from @kshakir that out paging ""exists"" but is massively broken . Generally it is viewed as good practice to provide paging for either all or all potentially large results, this is definitely the latter and obviously the former :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1793#issuecomment-328529130:7,upgrade,upgraded,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1793#issuecomment-328529130,1,['upgrade'],['upgraded']
Deployability,"When deploying Cromwell on our HPC which uses slurm as a scheduler, I use a wckey unique to a workflow and identical across tasks in that workflow - very handy for searching for failed jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095:5,deploy,deploying,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095,1,['deploy'],['deploying']
Deployability,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:848,install,installed,848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182,1,['install'],['installed']
Deployability,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:372,install,install,372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328,1,['install'],['install']
Deployability,"When using the `PAPIv2` backend, I have noticed that the same previous set of roles is not sufficient to be able to run the pipelines. Instead, after a long and tedious amount of work, I have figured that the following set of roles:; 1) [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2) [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3) [Firebase Develop](https://firebase.google.com/docs/projects/iam/roles-predefined-category#analytics_roles) Admin (`firebase.developAdmin`). are sufficient to run a pipelne on Google Cloud through a service account. I suppose that `lifesciences.workflowsRunner` is a replacement for `genomics.pipelinesRunner`, but I have no idea why `firebase.developAdmin` is required (or what else should be required in its place). To save my life, I could not find this information anywhere in the Cromwell documentation nor evince it from the Cromwell error messages themselves (nor understand what the `firebase.developAdmin` roles actually allows).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059:124,pipeline,pipelines,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680282059,2,['pipeline'],"['pipelines', 'pipelinesRunner']"
Deployability,"When we get back, please also make sure to update the documentation as well. This is a breaking change (which is essential for consistency) but we should be super clear about that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134:43,update,update,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134,2,['update'],['update']
Deployability,"When you ran into this locally, was the DB a native install or running in Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480345417:52,install,install,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480345417,1,['install'],['install']
Deployability,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So itâ€™s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:91,deploy,deploy,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419,2,['deploy'],"['deploy', 'deployed']"
Deployability,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:302,pipeline,pipelines,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,4,['pipeline'],['pipelines']
Deployability,"While this ticket is being resolved for the womtool-on-the-releases-page, for docker users the latest WOMTool is available @ https://hub.docker.com/r/broadinstitute/womtool/tags/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3031#issuecomment-350516520:59,release,releases-page,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3031#issuecomment-350516520,1,['release'],['releases-page']
Deployability,"Whoops, yeah, I thought I was writing in an issue that was exclusively about the I/O stuff, but neglected to scroll up. Never mind. Side issue solved (pending doc update), main issue not yet solved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468973849:163,update,update,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468973849,1,['update'],['update']
Deployability,Why not just reference the appropriate doc in the wdl spec (which also needs to be updated)?. I don't think of a changelog as a place for in depth documentation but rather a thing that tells me what happened that I might be interested in,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1805#issuecomment-268824680:83,update,updated,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1805#issuecomment-268824680,1,['update'],['updated']
Deployability,Will back-port to 26-hotfix once the develop version gets the seal of approval.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297183874:21,hotfix,hotfix,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2203#issuecomment-297183874,1,['hotfix'],['hotfix']
Deployability,Will be much neater if a new release is published (requested at https://github.com/scala/scala-collection-compat/issues/466),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-869833225:29,release,release,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-869833225,1,['release'],['release']
Deployability,Will need to patch a lack-of-jdbc-`convert()` breaking call caching in MariaDB.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-509633947:13,patch,patch,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-509633947,1,['patch'],['patch']
Deployability,Will push a patch updating the lenthall and wdl4s dependencies-- once those PRs get one more thumb.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795:12,patch,patch,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245486795,1,['patch'],['patch']
Deployability,"Will update wdl4s to a published version once https://github.com/broadinstitute/wdl4s/pull/38 is reviewed, merged, and auto-published. https://github.com/broadinstitute/lenthall/pull/22 needs to be reviewed also, but the artifact update isn't required as lenthall's cats dependencies are listed as Provided.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325:5,update,update,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254399325,2,['update'],['update']
Deployability,"Would having that information (filesystems configured for a backend) in the existing backend endpoint work for you ? I don't think we can consider a generic ""get a config value"" endpoint as it would be a massive security hole because the configuration contains secrets and passwords.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430:238,configurat,configuration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4317#issuecomment-433236430,1,['configurat'],['configuration']
Deployability,Would like to know the expectations regrading unit tests. codecov/patch is not successful. Hence the query.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922:66,patch,patch,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922,1,['patch'],['patch']
Deployability,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:111,pipeline,pipelines,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039,1,['pipeline'],['pipelines']
Deployability,"YAML claims to be a superset of JSON, but it is not (as this proves). Every time I have encountered YAML in a project it has been a headache, just my 2c. It is a strange syntax, with the potential for infinite recursion and bombs. It makes for a hard parser implementation, and there have been security and perfomance issues in the past. Fine for local configuration files, but probably not a good idea for any public facing APIs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556:353,configurat,configuration,353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487#issuecomment-379600556,1,['configurat'],['configuration']
Deployability,"Yeah - if it's a bug in the hermes grammar that can be updated w/o voting. If the spec is wrong, that requires a vote",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-478249212:55,update,updated,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-478249212,1,['update'],['updated']
Deployability,Yeah I would agree that's a bug. If you've patched this locally we would certainly welcome a PR! ðŸ˜„,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282703575:43,patch,patched,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282703575,1,['patch'],['patched']
Deployability,Yeah currently there are 3 non-exponentially-backed-off retries with 500 ms pauses in GcsFileSystemProvider#crc32cHash. Given the comments in the vicinity I don't think there would be strong objections to increasing the number of retries in a hotfix. This should be implemented differently in PBEs so it's not Thread.sleeping.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491:243,hotfix,hotfix,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491,2,['hotfix'],['hotfix']
Deployability,"Yeah that's unfortunate, thanks for reporting it. ðŸ™‚ This was [fixed](https://github.com/broadinstitute/cromwell/issues/3421) on the 31_hotfix branch a couple of months ago. There's a corresponding 31-39223b8 image up on Docker Hub but GitHub doesn't seem to have the 31.1 release. For now you could use the Docker image or build from the 31_hotfix branch, I'll try to update GitHub with 31.1 this week.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390433889:272,release,release,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3656#issuecomment-390433889,2,"['release', 'update']","['release', 'update']"
Deployability,"Yeah, it was probably lost in the back & forth on that thread but IIRC the numbers were minimums which could be increased by configuration",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488:125,configurat,configuration,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2291#issuecomment-303434488,1,['configurat'],['configuration']
Deployability,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:36,rolling,rolling,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037,4,['rolling'],['rolling']
Deployability,"Yeah, that Terra email notification is awesome. For methods developers, we rely on our own Cromwell servers, which will benefit from this requested feature.; In addition, in Cromwell-as-an-app (Azure, AoU workbench, etc), this feature will be handy as well. BTW, if there's some DSP-provided product/service we can deploy together with our Cromwell server (similarly on Azure and AoU), that'd be great too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508872775:315,deploy,deploy,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-1508872775,1,['deploy'],['deploy']
Deployability,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:472,pipeline,pipelines,472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"Yes -- this is an excellent idea! Let's make it so. -------------------------------; Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Fri, Dec 16, 2016 at 10:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > That's not currently possible, but IMO it's a good idea.; >; > @kcibul <https://github.com/kcibul> what do you think about having a JES; > configuration setting for the default zone(s) for jobs? Currently it's just; > hardwired to us-central1-b; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739604>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g0FHRjUSCxRbTRcCYsMvaZwaa1mZks5rI1SAgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748:442,configurat,configuration,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267739748,1,['configurat'],['configuration']
Deployability,"Yes that's correct, sorry I didn't see you are building from a docker uri. . I don't think there exists the exact functionality you want, but we are getting there. The current cache support is for docker layers, which will save you download time, but you still would rebuild from them each time. I think it would be worth the effort to ask for what you need. Take a look at the [latest release](https://github.com/sylabs/singularity/releases) that has some support for caching (for library images). Then I would open an issue and say something about it - you want the same caching but for docker pulled containers. This particular flow to pull (or build) and honor a cached image if it exists is very common and reasonable, and should be supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477:386,release,release,386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463867477,2,['release'],"['release', 'releases']"
Deployability,"Yes the I/O actor really is a ""Filesystem I/O Actor"" and does not handle backend API calls. The PAPI backend has a custom actor for that (PipelinesApiManager I think) but it's not (yet) available to all backends unfortunately",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-437003042:138,Pipeline,PipelinesApiManager,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-437003042,1,['Pipeline'],['PipelinesApiManager']
Deployability,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:347,update,update,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931,1,['update'],['update']
Deployability,"Yes we can update the database manually but I hesitate to do that unless it's really serious. Since this specific ticket is not about aborts, should this be moved to a ticket about aborts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334867019,1,['update'],['update']
Deployability,"Yes, I also think dead letters message is no issue. . root@d0ef87b8b6b8:/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution# **cat stderr**; > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; > [M::bwa_idx_load_from_disk] read 0 ALT contigs; > [W::main_mem] when '-p' is in use, the second query file is ignored.; > . stdout is 0 byte. I'm running on local machine. ; Just I used 2 bam file only.; $ /BiO/Project/brandon-genome-analysis/data/NA12878_24RG_small.txt; /BiO/Project/brandon-genome-analysis/data/HJYFJ.4.NA12878.downsampled.query.sorted.unmapped.bam; /BiO/Project/brandon-genome-analysis/data/HJYFJ.5.NA12878.downsampled.query.sorted.unmapped.bam. I found some issue in stderr file. > Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/7ffcdf28-2324-4c07-8e87-926a150334d9/call-SamToFastqAndBwaMem/shard-0/execution/tmp.rmIqEe; > ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Could you suggest any comment for this ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315:456,configurat,configuration,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367256315,4,['configurat'],['configuration']
Deployability,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:106,configurat,configuration,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,2,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"Yes, as far as I'm concerned it's :+1: for the release. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/625/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202592548:47,release,release,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202592548,1,['release'],['release']
Deployability,"Yes, it would also be nice. Right now, most of the wasted time in my pipelines is due to the things like this: when something in cromwell crashes and I spend a lot of time figuring out what exactly.; P.S.; To be honest, I do not understand why you started to create a language with a lot of problems to solve ( like this one) instead of just providing better declarative scala DSL (I think it is even possible to make it look almost like current wdl syntax) where tooling and errors are solved by scala ecosystem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365:69,pipeline,pipelines,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298098365,1,['pipeline'],['pipelines']
Deployability,"Yes, now that tests have IDs we will only append and not insert new tests. Thanks for the update!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411200852:90,update,update,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411200852,1,['update'],['update']
Deployability,"Yes, that's a valid alternative approach @notestaff. The tradeoff here is that by using the environment variables, no additional permissions are required for the process running Cromwell. By using an S3 bucket, we gain the ability of unlimited output size, but also increase the amount of configuration and permissions required to successfully run Cromwell. I think a good enhancement would be to add the ability to pass these things through S3 and let the user decide which mechanism and tradeoffs they would like. This said, removing the need for AWS_CROMWELL_OUTPUTS_GZ doesn't actually solve the 8k limit issue. In the testing for haplotype caller, some of the commands **themselves** are larger than 8k, even without considering environment variables. So, regardless of the environment variable compression, some solution is still needed for command text compression. I plan to file a couple PRs on the ECS Agent repo to get the conversation started with ECS and AWS Batch service teams on the limit and the mechanism being used to pass data between the services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427590325:289,configurat,configuration,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-427590325,1,['configurat'],['configuration']
Deployability,"Yes, the Cromwell 51 release completed the metadata archiving implementation and it is now operating in production at Broad Institute. Archiving is targeted at our internal production instance only so it's not an officially documented or supported feature, but an enterprising Cromwell admin could surely get it working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-642789149:21,release,release,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-642789149,1,['release'],['release']
Deployability,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isnâ€™t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > â€¦ <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. â€” You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1006,pipeline,pipeline,1006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843,2,['pipeline'],['pipeline']
Deployability,"Yes, we ran the 50 the same way for a previous release and did not notice any slowness. I'm not sure we checked on it as much during the first couple of hours last time though. About 3 hours in now, it seems back to normal, no slowness. The submissions are as close to simultaneous as we can get, meaning as fast as Cromwell will accept submissions from our script. Cromwell doesn't respond right away for each submission, so it takes about 5 minutes to submit 50.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228411072:47,release,release,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228411072,1,['release'],['release']
Deployability,"Yes, we'd still like to be able to get the total results count in the json response, which is a little more convenient and gives us a little more info than a total page count. Thanks for putting that in the 30 hotfix https://github.com/broadinstitute/cromwell/pull/3145. We would definitely like to continue to have that functionality in future versions of Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3083#issuecomment-359793700:210,hotfix,hotfix,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3083#issuecomment-359793700,1,['hotfix'],['hotfix']
Deployability,"You will likely need two `v37` confs. During the upgrade tests, new confs are needed to run against the cromwell-37 jar. Something like these should work:. #### local_37_application.conf; ```hocon; include ""local_application.conf""; database.db.driver = ""com.mysql.jdbc.Driver""; ```. #### papi_v2_37_application.conf; ```hocon; include ""papi_v2_application.conf""; database.db.driver = ""com.mysql.jdbc.Driver""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4706#issuecomment-470144771:49,upgrade,upgrade,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4706#issuecomment-470144771,1,['upgrade'],['upgrade']
Deployability,"You're right, it shouldn't. But because Cromwell only gives us one single hook to customise running containers, `submit-docker`, we have to ensure it can run at scale. Ideally Cromwell would have a `pull-docker` hook that's run each time a new image needs to be pulled or updated. But since that's not an option, I have to build the image separately for each job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367:272,update,updated,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367,1,['update'],['updated']
Deployability,Your config most likely needs an update to remove usage of the `languages.cwl.CwlV1_0LanguageFactory` class referenced in the exception.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538:33,update,update,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538,1,['update'],['update']
Deployability,Yup the release WDL has most definitely been broken. ðŸ¤•,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2679#issuecomment-338281475:8,release,release,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2679#issuecomment-338281475,1,['release'],['release']
Deployability,"Yup, thanks. I never circled back to update this. ðŸ˜„ It actually was being set but not quite correctly and also some environment variables aren't quite right, that's still WIP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3208#issuecomment-361771938:37,update,update,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3208#issuecomment-361771938,1,['update'],['update']
Deployability,"Yup... just started updated workflow where spaces are removed. On Wed, Dec 7, 2016 at 12:53 PM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm taking a look at this, in; > the meantime if you can remove the space from the file (maybe in your; > command line) it should work around it; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265520297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk3re5Xwc7pha3xBCkVSb5IRuqdkHks5rFvK0gaJpZM4LGJFu>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265532682:20,update,updated,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754#issuecomment-265532682,1,['update'],['updated']
Deployability,"[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3365,update,updateSchema,3365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['update'],['updateSchema']
Deployability,[sorry for resurrecting this old thread]. Is Cromwell now stable enough for this kind of contributions? I see that the big backend changes were included in the 0.21 release.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254153089:165,release,release,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-254153089,1,['release'],['release']
Deployability,"^ I would add that its not terribly important that it get fixed in hotfix, if thats what you're suggesting. Just that it gets fixed in some future version of cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231140664:67,hotfix,hotfix,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231140664,1,['hotfix'],['hotfix']
Deployability,_Side note: Hoping our PO prioritizes testing soon. We're about to cross the 70% threshold._. Minor updates and then :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172017111:100,update,updates,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/386#issuecomment-172017111,1,['update'],['updates']
Deployability,"_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3462,update,updateSchema,3462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['update'],['updateSchema']
Deployability,"_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8161,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"_state_widening.xml::workflow-store-state-widening::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: Columns RESTARTED(BOOLEAN) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: Columns WORKFLOW_ROOT(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Column WORKFLOW_STORE_ENTRY.RESTARTED dropped; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns CROMWELL_ID(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,993 INFO - chan",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:89326,update,update-restartable,89326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['update'],['update-restartable']
Deployability,"`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it performed exactly the same process~~ (although still relevant, see my edit below - needed 30 minutes). ---. **Edit**: _I left the Cromwell server running for 30 minutes and it just randomly started the next job. I don't know if this is related or not, but obviously needed to update my comment._",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:2332,update,update,2332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['update'],['update']
Deployability,"`README.md` updated, for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144242465:12,update,updated,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144242465,1,['update'],['updated']
Deployability,"`ServiceRegistryActorSpec` needs to be patched also, but otherwise LGTM ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1772/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1772#issuecomment-266470980:39,patch,patched,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1772#issuecomment-266470980,1,['patch'],['patched']
Deployability,`SprayDockerRegistryApiClientSpec` is finicky. Should probably me marked as an `CromwellSpec.IntegrationTest`. It was already using `org.scalatest.concurrent.IntegrationPatience`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841:93,Integrat,IntegrationTest,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168885841,2,['Integrat'],"['IntegrationPatience', 'IntegrationTest']"
Deployability,"```. 2017/02/07 15:40:50 I: Switching to status: pulling-image; 2017/02/07 15:40:50 I: Calling SetOperationStatus(pulling-image); 2017/02/07 15:40:50 I: SetOperationStatus(pulling-image) succeeded; 2017/02/07 15:40:50 I: Writing new Docker configuration file; 2017/02/07 15:40:50 I: Pulling image ""leetl1220/m1:bs""; 2017/02/07 15:41:48 I: Pulled image ""leetl1220/m1:bs"" successfully.; 2017/02/07 15:41:48 I: Switching to status: localizing-files; 2017/02/07 15:41:48 I: Calling SetOperationStatus(localizing-files); 2017/02/07 15:41:48 I: SetOperationStatus(localizing-files) succeeded; 2017/02/07 15:41:48 I: Docker file /cromwell_root/exec.sh maps to host; location /mnt/local-disk/exec.sh.; 2017/02/07 15:41:48 I: Copying; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; to /mnt/local-disk/exec.sh; 2017/02/07 15:41:48 I: Running command: sudo gsutil -q -m cp; gs://broad-dsde-methods/lichtens/cromwell-executions-test_dl_oxoq/bamsurgeon_workflow/f6f01c57-23e2-4d29-854a-0d39c11ff950/call-generate_true_positives/exec.sh; /mnt/local-disk/exec.sh; 2017/02/07 15:41:49 I: Docker file; /cromwell_root/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; maps to host location; /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list.; 2017/02/07 15:41:49 I: Copying; gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; to /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list; 2017/02/07 15:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974:240,configurat,configuration,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1961#issuecomment-278115974,1,['configurat'],['configuration']
Deployability,"```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud al",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4227,Pipeline,Pipelines,4227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,```; [info] - should check that we classify error code 10 as a preemption *** FAILED *** (183 milliseconds); [info] Preempted was not equal to Preempted (GetRequestHandlerSpec.scala:207); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at cromwell.backend.google.pipelines.v2alpha1.api.request.GetRequestHandlerSpec.$anonfun$new$2(GetRequestHandlerSpec.scala:207); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265:288,pipeline,pipelines,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265,1,['pipeline'],['pipelines']
Deployability,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:131,continuous,continuously,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497,1,['continuous'],['continuously']
Deployability,"`womgraph` produced vastly too much output for my (apparently) complex pipeline. `dot` couldn't render it in png at all and produced a corrupted pdf file. I think that an option that produces ""simple"" output that non-developers can look at and mostly comprehend, like the old `graph` option, is still very desirable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295:71,pipeline,pipeline,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295,2,['pipeline'],['pipeline']
Deployability,"a) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the mo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1512,release,releases,1512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['release'],['releases']
Deployability,"a818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz /tmp/scratch/s3:/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz; >; > {code}; >; > (sorry for the long log); > As you can see, in the last line, the output path of s3 copy is malformed,; > there is an 's3:/' lost there. This causes the whole pipeline to fail. I; > already tried several times and sometimes it happens, sometimes don't. Also; > when it happens, it's not always in the same shard. Do you have any ideia; > why this is happening?; > Thanks in advance; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6106>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELH2MI44ZN2D3LYJZTSSOGSVANCNFSM4UHQIFCA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:15462,pipeline,pipeline,15462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857,1,['pipeline'],['pipeline']
Deployability,"a9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.Cal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1792,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"abase was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome_build;genome_resources__rnaseq__transcripts;config__algorithm__tools_off;config__algorithm__qc;analysis;config__algorithm__tools_on;align_bam' 'sentinel_inputs=qc_rec:record' 'run_number=",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5513,pipeline,pipeline,5513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1985,pipeline,pipeline,1985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,2,['pipeline'],['pipeline']
Deployability,"ackends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2084,pipeline,pipelinesRunner,2084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['pipeline'],['pipelinesRunner']
Deployability,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,configurat,configuration,9784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['configurat'],['configuration']
Deployability,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2712,configurat,configuration,2712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['configurat'],['configuration']
Deployability,"also, once it's looking good, we'll want a hotfix edition of this changeset",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718:43,hotfix,hotfix,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718,1,['hotfix'],['hotfix']
Deployability,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > [â€¦](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf â€¦ <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1268,release,release,1268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,any update on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-583565273:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-583565273,1,['update'],['update']
Deployability,any updates?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564,1,['update'],['updates']
Deployability,"arne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1438,Update,UpdateVisitor,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Update'],['UpdateVisitor']
Deployability,"ationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1522,configurat,configuration,1522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['configurat'],['configuration']
Deployability,"atus change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$Ab",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1873,pipeline,pipelines,1873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"ausedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1556,update,update,1556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['update']
Deployability,"aven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue to prototype to ensure 4 and 5 work as expected, then we can make a final call either on this thread or in a meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:3113,configurat,configuration,3113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068,2,['configurat'],['configuration']
Deployability,awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1643,pipeline,pipeline,1643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"b was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecuto",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2204,pipeline,pipelines,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"bExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": 18.0,; ""drs_usa_jdr.hash1"": ""faf12e94c25bef7df62e4a5eb62573f5"",; ""drs_usa_jdr.cloud1"": ""gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc""; - should successfully run drs_usa_jdr (7 minutes, 24 seconds); </pre>; </details>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8823,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"bio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome_build;genome_resources__rnaseq__transcripts;config__algorithm__tools_off;config__algorithm__qc;analysis;config__algorithm__tools_on;align_bam' 'sentinel_inputs=qc_rec:record' 'run_number=0'; ```. And the `cwl.inputs.json`:. ```; {; ""qc_rec"": {; ""genome_build"": ""hg19"",; ""config__algorithm__tools_on"": [],; ""align_bam"": {; ""nameext"": "".bam"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/call-process_alignment/shard-0/align/Test1/Test1-sort.bam"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5893,update,update,5893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['update'],['update']
Deployability,"ble/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2225,Pipeline,Pipelines,2225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['Pipeline'],['Pipelines']
Deployability,"can I close this ticket and we can make a separate one for Hotfix? I'm also unsure of why the ""closes #issue"" doesn't seem to be working properly for most tickets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-221252219:59,Hotfix,Hotfix,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-221252219,1,['Hotfix'],['Hotfix']
Deployability,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2341,release,release,2341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"cho ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3060,pipeline,pipelines,3060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['pipeline'],['pipelines']
Deployability,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3531,configurat,configuration,3531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['configurat'],['configuration']
Deployability,closing -- sounds like this is a documentation update (which is being handled). Please re-open if there is still a bug here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259,1,['update'],['update']
Deployability,"cluded in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the probl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1416,release,releases,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['release'],['releases']
Deployability,"column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Column WORKFLOW_STORE_ENTRY.RESTARTED dropped; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns CROMWELL_ID(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,993 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns HEARTBEAT_TIMESTAMP(TIMESTAMP) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,994 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: ChangeSet changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr ran successfully in 3ms; 2018-06-07 12:16:10,997 INFO - Successfully released change log lock; 2018-06-07 12:16:11,007 INFO - Running with database db.url = jdbc:hsqldb:mem:78e2c868-f948-49e1-b7ba-840a9b54f3aa;shutdown=false;hsqldb.tx=mvcc; 2018-06-07 12:16:11,051 INFO - Successfully acquired change log lock; 2018-06-07 12:16:11,069 INFO - Creating database history table with name: PUBLIC.SQLMETADATADATABASECHANGELOG; 2018-06-07 12:16:11,071 INFO - Reading from PUBLIC.SQLMETADATADATABASECHANGELOG; 2018-06-07 12:16:11,080 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table CUSTOM_LABEL_ENTRY created; 2018-06-07 12:16:11,081 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table METADATA_ENTRY created; 2018-06-07 12:16:11,081 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table SUMMARY_STATUS_ENTRY created; 2018-06-07 12",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:90822,release,released,90822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['release'],['released']
Deployability,"com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks like the configuration of the local backend in the docs is still under development (http://cromwell.readthedocs.io/en/develop/tutorials/LocalBackendIntro/). I think that this kind of things can be part of the docs if not included as default in the source code - let me know if I can do something to help documenting the local end, which I am using as my default one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:2364,configurat,configuration,2364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['configurat'],['configuration']
Deployability,core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1770,pipeline,pipeline,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1059,Update,Updated,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,1,['Update'],['Updated']
Deployability,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:6210,configurat,configuration,6210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,['configurat'],['configuration']
Deployability,"d change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1739,update,updateSchema,1739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['updateSchema']
Deployability,"de 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1191,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,dk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awss,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2617,pipeline,pipeline,2617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,doc updates forthcoming,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118,1,['update'],['updates']
Deployability,"docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above tha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1877,deploy,deploy,1877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,['deploy'],['deploy']
Deployability,"e there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1256,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,configurat,configuration,12174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['configurat'],['configuration']
Deployability,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4537,update,update,4537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,2,['update'],['update']
Deployability,"e, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1080,configurat,configuration,1080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['configurat'],['configuration']
Deployability,"e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2040,pipeline,pipelines,2040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,efreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withB,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1475,Pipeline,PipelinesApiRunCreationClient,1475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"ell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2689,configurat,configuration,2689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['configurat'],['configuration']
Deployability,ent-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-http-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[google-api-client-1.20.0.jar:1.20.0]; at cromwell.engine.backend.jes.Run$.runPipeline$1(Run.scala:59) ~[classes/:na]; at cromwell.engine.backend.jes.Run$.apply(Run.scala:67) ~[classes/:na]; at cromwell.engine.backend.jes.Pipeline.run(Pipeline.scala:75) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.cromwell$engine$backend$jes$JesBackend$$anonfun$$attemptToCreateJesRun$1(JesBackend.scala:409) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:2720,Pipeline,Pipeline,2720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['Pipeline'],['Pipeline']
Deployability,equestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1756,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809,8,['update'],['update']
Deployability,er-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1114,configurat,configuration,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['configurat'],['configuration']
Deployability,"erformed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2189,configurat,configuration,2189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['configurat'],['configuration']
Deployability,ermissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2292,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"es before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVaria",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2497,Install,Installing,2497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['Install'],['Installing']
Deployability,esh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1573,Pipeline,PipelinesApiRunCreationClient,1573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"ete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionToken",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96258,configurat,configuration,96258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['configurat'],['configuration']
Deployability,fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecuto,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1338,pipeline,pipelines,1338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2221,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,fixed in https://github.com/broadinstitute/cromwell/releases/tag/34,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3878#issuecomment-407144286:52,release,releases,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3878#issuecomment-407144286,1,['release'],['releases']
Deployability,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:8,integrat,integration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839,1,['integrat'],['integration']
Deployability,"from Aaron Kemp of Google pipelines, emphasis mine:. ""In the short term, I think the original option we discussed is the best: try without, if it fails, try with. **You will not be billed for the failed request**.""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347492:26,pipeline,pipelines,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347492,1,['pipeline'],['pipelines']
Deployability,fwiw - green team currently run their own cromwell instances - so they are not currently impacted by anything we do on WB prod for their pipelines. Plus they generally are not as aggressive at taking newer cromwell versions - so even if you disable (remove) that endpoint they would likely not see the results for quite a bit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351:137,pipeline,pipelines,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351,1,['pipeline'],['pipelines']
Deployability,"g image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(A",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2286,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,configurat,configuration,2567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,4,['configurat'],['configuration']
Deployability,gStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:4428,update,updateStatuses,4428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateStatuses']
Deployability,gh there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1355,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,gle-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-http-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[google-api-client-1.20.0.jar:1.20.0]; at cromwell.engine.backend.jes.Run$.runPipeline$1(Run.scala:59) ~[classes/:na]; at cromwell.engine.backend.jes.Run$.apply(Run.scala:67) ~[classes/:na]; at cromwell.engine.backend.jes.Pipeline.run(Pipeline.scala:75) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.cromwell$engine$backend$jes$JesBackend$$anonfun$$attemptToCreateJesRun$1(JesBackend.scala:409) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:2707,Pipeline,Pipeline,2707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['Pipeline'],['Pipeline']
Deployability,"gotcha! I'll dig into this and try for a first shot, will send back update when I break I mean, dip in my toe a big more.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139,1,['update'],['update']
Deployability,h.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1433,Pipeline,PipelinesApiRunCreationClient,1433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"he Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupport"" that Cromwell feature with this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:1840,rolling,rolling,1840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,1,['rolling'],['rolling']
Deployability,"he script is generated by AwsBatchJob.scala; > [â€¦](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦ <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1134,pipeline,pipeline,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,2,['pipeline'],['pipeline']
Deployability,"hers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1714,pipeline,pipelines,1714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,hey @dspeck1 or @jgainerdewar just wondering if there is a plan for a patch release to fix the current cromwell release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150:70,patch,patch,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150,3,"['patch', 'release']","['patch', 'release']"
Deployability,"hey @kshakir ! I'd be happy to make that change for you (the first one). For the second one, I've linked the example in the folder to the online docs, and I'd suggest that the update of the docs themselves be a separate PR. I've noticed with Cromwell (and other software, generally) that it's much cleaner / better to have smaller PRs that have scoped changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470279447:176,update,update,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470279447,1,['update'],['update']
Deployability,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:537,deploy,deploy,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,4,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,hey friends! Just wanted to poke here again that this is still badly wanted / needed / desired / dreamed of / prayed for / sacrificial lambs... (you get the idea :P _) Any updates? Can I help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369:172,update,updates,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369,1,['update'],['updates']
Deployability,hotfix is in the past... 0.20+ is the future ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/967#issuecomment-230882303:0,hotfix,hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/967#issuecomment-230882303,1,['hotfix'],['hotfix']
Deployability,"https://broadinstitute.atlassian.net/wiki/display/DSDEEPB/Cromwell+Release. One week prior to release cut develop to a release candidate branch (e.g. release-0.18); On develop branch, update build.sbt with the next version number. Unless we want to wait until the actual release day to switch ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175210044:67,Release,Release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175210044,6,"['Release', 'release', 'update']","['Release', 'release', 'release-', 'update']"
Deployability,"https://broadworkbench.atlassian.net/browse/BA-2919; All future updates to this issue will be posted in JIRA. Sorry for the inconvenience, but you will need to create a free account to access it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209:64,update,updates,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209,1,['update'],['updates']
Deployability,https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=desc) into [44_hotfix](https://codecov.io/gh/broadinstitute/cromwell/commit/8055dad79afe29bbfb6b4b558f997a03d38dabd4?src=pr&el=desc) will **increase** coverage by `16.84%`.; > The diff coverage is `28.57%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## 44_hotfix #5076 +/- ##; =============================================; + Coverage 62.75% 79.6% +16.84% ; =============================================; Files 1031 1031 ; Lines 26424 26433 +9 ; Branches 869 819 -50 ; =============================================; + Hits 16582 21041 +4459 ; + Misses 9842 5392 -4450; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [...lines/v2alpha1/PipelinesParameterConversions.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzUGFyYW1ldGVyQ29udmVyc2lvbnMuc2NhbGE=) | `82.85% <100%> (+54.91%)` | :arrow_up: |; | [.../main/scala/cromwell/filesystems/drs/DrsPath.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoLnNjYWxh) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...cala/cromwell/filesystems/drs/DrsPathBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoQnVpbGRlci5zY2FsYQ==) | `92.85% <100%> (+7.14%)` | :arrow_up: |; | [...omwell/filesystems/drs/DrsPathBuilderFactory.scala](https://codecov.io/gh/broadinstitute/cromwell,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:1101,Pipeline,PipelinesParameterConversions,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,1,['Pipeline'],['PipelinesParameterConversions']
Deployability,"i.e. something like:. ```; auths {; googleauth1: {; class: ""path.to.googleauth""; otherstuff: ""blah""; }; }. filesystems {; fs1 {; class: ""path.to.gcs.filesystem""; auth: googleauth1; }; }. backends {; googlepipelineservice {; class: ""path.to.pipelineservicejar""; gcsfilesystem: fs1; auth: googleauth1; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203488655:240,pipeline,pipelineservicejar,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203488655,1,['pipeline'],['pipelineservicejar']
Deployability,"ia engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3968,Pipeline,Pipelines,3968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,ialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1854,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"if #4865 has already merged before this one, update that table",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-484928363:45,update,update,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-484928363,1,['update'],['update']
Deployability,if it's as intended you should update the swagger then :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4139#issuecomment-424070734:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4139#issuecomment-424070734,1,['update'],['update']
Deployability,if you had to update the Cromwell server repo template for the `CROMWELL_BUILD_CENTAUR_256_BITS_KEY` variable could you please include those changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739,1,['update'],['update']
Deployability,"ing ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2057,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ing [#4947](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=desc) into [develop](https://codecov.io/gh/broadinstitute/cromwell/commit/26085f5833cee3c9091d391102d5d0885a2b7d71?src=pr&el=desc) will **increase** coverage by `2.22%`.; > The diff coverage is `84.61%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #4947 +/- ##; ===========================================; + Coverage 62.61% 64.84% +2.22% ; ===========================================; Files 1015 1015 ; Lines 25904 25914 +10 ; Branches 811 829 +18 ; ===========================================; + Hits 16221 16804 +583 ; + Misses 9683 9110 -573; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [.../google/pipelines/v2alpha1/api/ActionBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:1082,pipeline,pipelines,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,2,['pipeline'],['pipelines']
Deployability,"instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1174,pipeline,pipelines,1174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,configurat,configuration,2791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['configurat'],['configuration']
Deployability,is there a similar ticket for not-hotfix?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225913678:34,hotfix,hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225913678,1,['hotfix'],['hotfix']
Deployability,"ispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1063,pipeline,pipeline,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,1,['pipeline'],['pipeline']
Deployability,issue has not recurred since update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-454452174:29,update,update,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-454452174,1,['update'],['update']
Deployability,"k PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2122,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"la:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures[%]%:failure' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(D",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2805,UPDATE,UPDATE,2805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['UPDATE'],['UPDATE']
Deployability,leLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$an,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5635,update,updateForStatusNames,5635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateForStatusNames']
Deployability,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:215,release,release,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902,1,['release'],['release']
Deployability,"lignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); ```; I was trying to lift off how things were done with the Google/gcp resolution so added it in there to fix this issue. Is there a different configuration approach I should be using?. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:2812,configurat,configuration,2812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['configurat'],['configuration']
Deployability,ll-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.conc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3937,update,updateSchema,3937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['updateSchema']
Deployability,looks like the jenkins script needed to forcibly install newer versions of all dependencies. Looks to be fixed now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507:49,install,install,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507,1,['install'],['install']
Deployability,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,configurat,configuration,2446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['configurat'],['configuration']
Deployability,"main while cromwell 35 is released, that's to bad that this change is not in there yet ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425875557:26,release,released,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425875557,1,['release'],['released']
Deployability,make sure to document the configuration in readme and add a sentence that this exists and the changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313,1,['configurat'],['configuration']
Deployability,mergeable now that we've (you've) released?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373187082:34,release,released,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373187082,1,['release'],['released']
Deployability,"metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1124,UPDATE,UPDATE,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['UPDATE'],['UPDATE']
Deployability,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1237,Update,Update,1237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['Update'],['Update']
Deployability,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3079,Pipeline,PipelinesApiLifecycleActorFactory,3079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"munshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1186,UPDATE,UPDATE,1186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['UPDATE'],['UPDATE']
Deployability,"n/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2210,pipeline,pipelines,2210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"ndJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5431,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2676,Pipeline,Pipelines,2676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,"nfusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1262,configurat,configuration,1262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['configurat'],['configuration']
Deployability,"ngularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1074,pipeline,pipeline,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['pipeline'],['pipeline']
Deployability,"no worries, thanks for the update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-170765478:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-170765478,1,['update'],['update']
Deployability,nternal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseS,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2753,pipeline,pipeline,2753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,configurat,configuration,8398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,4,"['configurat', 'integrat']","['configuration', 'integrate']"
Deployability,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1038,update,updates,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['update'],['updates']
Deployability,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2029,install,install,2029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,4,"['deploy', 'install']","['deploy', 'install']"
Deployability,"od -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFun",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2063,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does support [adding/dropping a unique index](https://www.sqlite.org/lang_crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:2743,update,update,2743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,2,['update'],['update']
Deployability,ogle.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1544,pipeline,pipelines,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,oh i see that's great -- i never liked positional args. thanks! we'll plan to update docs accordingly -- when do you expect to release this/29?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779:78,update,update,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779,2,"['release', 'update']","['release', 'update']"
Deployability,okay I've just updated the circle badge to be cromwell on broadinstitute (which doesn't technically exist yet) but the purpose was to add all the commits where I failed like a struggling panda to get the testing setup :) All is working now!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413851686:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413851686,1,['update'],['updated']
Deployability,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,configurat,configuration,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958,8,"['configurat', 'integrat', 'pipeline']","['configuration', 'integrated', 'pipeline']"
Deployability,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:18,update,updated,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694,4,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,configurat,configuration,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['configurat'],['configuration']
Deployability,oogle.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaFor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1702,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"oot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5210,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ould not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2618,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ow_store_state_widening.xml::workflow-store-state-widening::tjeandet: WORKFLOW_STORE_ENTRY.WORKFLOW_STATE datatype was changed to varchar(20); 2018-06-07 12:16:10,983 INFO - changelog.xml: changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet: ChangeSet changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: Columns RESTARTED(BOOLEAN) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: Columns WORKFLOW_ROOT(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:89002,update,update-restartable,89002,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,2,['update'],"['update-restartable', 'updated']"
Deployability,"patcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5657,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecuto",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1922,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,pelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3189,pipeline,pipeline,3189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"pelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1583,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,pes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1404,pipeline,pipelines,1404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,"pt-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > [â€¦](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦ <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1064,pipeline,pipeline,1064,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,2,['pipeline'],['pipeline']
Deployability,"r: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1458,Update,UpdateVisitor,1458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Update'],['UpdateVisitor']
Deployability,"rce file the same bucket as the workflow bucket? If not, are they in the same region?; > [â€¦](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf â€¦ <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcess",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1424,Install,Installing,1424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['Install'],['Installing']
Deployability,"rds spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$B",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1092,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"re the build breakage, I need to have the wdl4s snapshot published by the other PR's merge to develop and then update the build.sbt to reference that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193330854:111,update,update,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193330854,1,['update'],['update']
Deployability,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:78,release,release,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,3,['release'],"['release', 'released']"
Deployability,"rkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Prom",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1829,pipeline,pipelines,1829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"rminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > [â€¦](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sync`. This could be set by adding `script-epilogue = ""sync; ls | grep -v 'rc.txt' | xargs rm -rf` in the cromwell.conf, referring to https://github.com/broadinstitute/cromwell/blob/8e5ca8791d2ba685965a655f2404972f2c80299d/cromwell.example.backends/LocalExample.conf#L43 I'm not sure if this works for AWS batch backend. **UPDATE: `SCRIPT_EPILOGUE` is not valid for AWS backend. This workaround doesn't work ... :(**. Thanks,; Luyu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:2322,UPDATE,UPDATE,2322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,1,['UPDATE'],['UPDATE']
Deployability,"rs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8375,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"rt images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4695,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"s problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$Abst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1009,pipeline,pipelines,1009,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"s/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1718,update,updateSchema,1718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['update'],['updateSchema']
Deployability,"sApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2833,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"s_again.xml::standardize_column_names_again::kshakir: Unique constraint added to WORKFLOW_STORE_ENTRY(WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:10,858 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_JOB_STORE_ENTRY_WEU created; 2018-06-07 12:16:10,858 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WN created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_STORE_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,860 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: ChangeSet changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir ran successfully in 54ms; 2018-06-07 12:16:10,880 INFO - Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; 2018-06-07 12:16:10,896 INFO - [RenameWorkflowOptionsInMetadata] 100%; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: RenameWorkflowOptionsInMetadata complete.; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:68611,update,updated,68611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['update'],['updated']
Deployability,"sample_list.data,; sex_mismatch_sample_list = sex_mismatch_sample_list.data,; low_genotyping_quality_sample_list = low_genotyping_quality_sample_list.data; }; }. call tasks.load_shared_covars { input:; script_dir = script_dir,; fam_file = fam_file,; sc_pcs = sc_pcs,; sc_assessment_ages = sc_assessment_ages; }. if (!is_binary) {; call tasks.load_continuous_phenotype { input :; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; assessment_ages_npy = load_shared_covars.assessment_ages,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariate_scs; }; }; if (is_binary) {; call tasks.load_binary_phenotype { input:; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; sc_year_of_birth = sc_year_of_birth,; sc_month_of_birth = sc_month_of_birth,; sc_date_of_death = sc_date_of_death,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,; is_zero_one_neg_nan = is_zero_one_neg_nan; }; }; # regardless of continuous or binary, get the outputs and move on; File pheno_data = select_first([load_continuous_phenotype.data, load_binary_phenotype.data]); File covar_names = select_first([load_continuous_phenotype.covar_names, load_binary_phenotype.covar_names]); File pheno_readme = select_first([load_continuous_phenotype.README, load_binary_phenotype.covar_names]). output {; Array[File] out_sample_lists = all_qced_sample_lists.data; File assessment_ages = load_shared_covars.assessment_ages; File shared_covars = load_shared_covars.shared_covars; File shared_covar_names = load_shared_covars.covar_names; File pheno_data_out = pheno_data; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:7128,continuous,continuous,7128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['continuous'],['continuous']
Deployability,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,configurat,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892,10,"['configurat', 'update']","['configuration', 'updated']"
Deployability,should really go against hotfix though,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219845066:25,hotfix,hotfix,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219845066,1,['hotfix'],['hotfix']
Deployability,should there be a hotfix version too?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-220454280:18,hotfix,hotfix,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-220454280,1,['hotfix'],['hotfix']
Deployability,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1078,update,updated,1078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['update'],['updated']
Deployability,slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (Ã¸)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (Ã¸)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInter,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2635,pipeline,pipelines,2635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,3,"['Pipeline', 'pipeline']","['PipelinesApiJobPaths', 'pipelines']"
Deployability,"spatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1104,pipeline,pipeline,1104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['pipeline'],['pipeline']
Deployability,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:3157,configurat,configuration-reference,3157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,['configurat'],['configuration-reference']
Deployability,"ss; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1890,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"stem-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7646,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,7646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"stions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > â€¦ <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isnâ€™t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. â€” You are; > receiving this because you are subscribed to this thread. Reply to this; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1467,pipeline,pipeline,1467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,2,['pipeline'],['pipeline']
Deployability,"t this database has not had the ANALYZE sqlite3 command run on it. Doing so can dramatically speed up queries, and is done by default for databases created with gffutils >0.8.7.1 (this database was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5325,pipeline,pipeline,5325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,configurat,configurations,2912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configurations']
Deployability,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files â€¦ Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1837,install,installs,1837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,4,['install'],['installs']
Deployability,"taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10872,pipeline,pipeline,10872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['pipeline'],['pipeline']
Deployability,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1312,release,release,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"task (vs passing through an optional value from its caller), you could use a default value:. Although this would prevent me from using select_first() as often, in the actual workflow (which I didn't post, as it's monstrous compared to the toy example), I also have to use defined() to build the path of an optional TSV file which is either going to be in the zipped directory, or passed in directory, or not used at all. Setting an default value means I now have to check for equality with an empty string instead of using defined. In the end it'd be just as verbose and probably a little harder to debug then select_first(). > Regarding the difference in behavior of cromwell vs miniwdl, I don't think miniwdl's support for this form is backed by the WDL spec. As I see it, the spec is explicit with regard to optional parameters to Standard Library functions. For example, the signature for `size()` is `Float size(File?|Array[File?], [String])`. Since the signature for `basename()` is `String basename(String|File, [String])`, it doesn't look like it should support `String?` as an input. I'm a little confused, where are you seeing this? [sub()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-substring-string-string) is titled `String sub(String, String, String)` and [size()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#float-sizefile-string) is titled `Float size(File, [String])` in the 1.0 spec. I get that sub() [has examples with compound types](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#acceptable-compound-input-types) but I took that to mean ""here's some examples with arrays plus an optional for comparison"" since File? isn't a compound type (if I am understanding [this](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#compound-types) correctly). If size() accepts optionals in spite of the spec continuously saying `File` instead of `File?` except in one example, I don't see why basename() and sub() cannot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358:1964,continuous,continuously,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358,1,['continuous'],['continuously']
Deployability,tch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.Threa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4061,update,updateSchema,4061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['updateSchema']
Deployability,"tchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1956,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"te keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 20",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2392,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"te/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1505,pipeline,pipelines,1505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"ted right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3EDFTTMFP7HANCNFSM44FGDSRQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1207,pipeline,pipeline,1207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,2,['pipeline'],['pipeline']
Deployability,"thanks for the docker image! just wondering, when should we expect the next cromwell release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946:85,release,release,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946,1,['release'],['release']
Deployability,"that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutori",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1395,pipeline,pipelinesRunner,1395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['pipeline'],['pipelinesRunner']
Deployability,"the other errors I get are all from the workflow itself due to not preserving the original file names. The numerical hashes for files get passed directly into the downstream tools, stripping off any extensions or other identifying information. This results in tool confusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesys",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1160,pipeline,pipeline,1160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['pipeline'],['pipeline']
Deployability,they're `grep` and `wc` commands. I'll update the documentation accordingly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836:39,update,update,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836,1,['update'],['update']
Deployability,this has been fixed on the `30_hotfix` branch but not released. The next release will have the fix.; In the meantime you could either build the jar yourself either from `develop` or from `30_hotffix`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-368998293:54,release,released,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-368998293,2,['release'],"['release', 'released']"
Deployability,"this is a critical issue of cromwell/wdltool, how did Broad avoid this issue in its internal pipeline using cromwell/wdl? this feature of WDL seems to be so commonly used in pipeline development.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672:93,pipeline,pipeline,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672,2,['pipeline'],['pipeline']
Deployability,this is fixed and the README appears to have been be updated too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951:53,update,updated,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951,1,['update'],['updated']
Deployability,"tible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1026,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"tils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Delegating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:2728,UPDATE,UPDATE,2728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['UPDATE'],['UPDATE']
Deployability,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4534,update,update,4534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,2,['update'],['update']
Deployability,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1557,release,releases,1557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,2,"['release', 'update']","['releases', 'updates']"
Deployability,"torage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serviceAccounts.list; roles/iam.serviceAccountUser resourcemanager.projects.get; roles/iam.serviceAccountUser resourcemanager.projects.list; roles/lifesciences.workflowsRunner lifesciences.operations.cancel; roles/lifesciences.workflowsRunner lifesciences.operations.get; roles/lifesciences.workflowsRunner lifesciences.operations.list; roles/lifesciences.workflowsRunner lifesciences.workflows.run; roles/storage.objectAdmin resourcemanager.projects.get; roles/storage.objectAdmin resourcemanager.projects.list; roles/storage.objectAdmin storage.objects.create; roles/storage.objectAdmin storage.objects.delete; roles/storage.objectAdmin storage.objects.get; roles/storage.objectAdmin storage.objects.getIamPolicy; roles/storage.objectAdmin storage.objects.list; roles/storage.objectAdmin storage.objects.setIamPolicy; roles/storage.objectAdmin storage.objects.update; roles/storage.objectCreator resourcemanager.projects.get; roles/storage.objectCreator resourcemanager.projects.list; roles/storage.objectCreator storage.objects.create; roles/storage.objectViewer resourcemanager.projects.get; roles/storage.objectViewer resourcemanager.projects.list; roles/storage.objectViewer storage.objects.get; roles/storage.objectViewer storage.objects.list; ```; Somehow the [tutorial](https://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/) suggests to add roles `storage.objectCreator` and `storage.objectViewer` but these do not include one of the four permissions `storage.objects.delete`, `storage.objects.getIamPolicy`, `storage.objects.setIamPolicy`, or `storage.objects.update` that are further added when adding also role `storage.objectAdmin` and at least one of these must be further needed by Cromwell. Either than by trial and error, I still do not understand how users are supposed to understand this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:3531,update,update,3531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,2,['update'],['update']
Deployability,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,configurat,configuration,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['configurat'],['configuration']
Deployability,tware.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTracki,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1503,pipeline,pipeline,1503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2464,configurat,configuration,2464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['configurat'],['configuration']
Deployability,update title to include triage. Just because someone put in a PBE TODO doesn't mean it absolutely has to be done. Use good judgement ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656,1,['update'],['update']
Deployability,updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4756#issuecomment-474512847:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756#issuecomment-474512847,1,['update'],['updated']
Deployability,"ure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1362,Update,UpdateVisitor,1362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Update'],['UpdateVisitor']
Deployability,ute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPip,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2114,pipeline,pipeline,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,va:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.ama,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2004,pipeline,pipeline,2004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423,1,['update'],['updated']
Deployability,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,configurat,configuration,3890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['configurat'],['configuration']
Deployability,what about the stats endpoint? . https://github.com/broadinstitute/cromwell#get-apiworkflowsversionstats. Note: If you're not on the release you probably don't have this endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630:133,release,release,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630,1,['release'],['release']
Deployability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,configurat,configurations,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configurations']
Deployability,why not fix this more narrowly so the update of input expressions isn't broken for non-shards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170081954:38,update,update,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170081954,1,['update'],['update']
Deployability,"wo, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Wo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2328,Pipeline,Pipelines,2328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,x.run(Mailbox.scala:225); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3824,update,update,3824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['update']
Deployability,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1254,pipeline,pipeline,1254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['pipeline'],['pipeline']
Deployability,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1213,pipeline,pipeline,1213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,1,['pipeline'],['pipeline']
Deployability,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4346,update,update,4346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758,2,['update'],['update']
Deployability,"ye sorry about that, did a rollback and saved the other changes to this PR: https://github.com/broadinstitute/cromwell/pull/4205",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126:27,rollback,rollback,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126,1,['rollback'],['rollback']
Deployability,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,configurat,configuration,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624,1,['configurat'],['configuration']
Deployability,"yep, it's required to run the fastq->bam pipeline. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org; 617-714-7637. On Mon, Aug 24, 2015 at 9:31 AM, Jeff Gentry notifications@github.com; wrote:. > @kcibul https://github.com/kcibul we'll need this on staging for demo?; > ; > â€”; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134201403; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134204617:41,pipeline,pipeline,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134204617,1,['pipeline'],['pipeline']
Deployability,yes this is in the 0.22 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769:24,release,release,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769,1,['release'],['release']
Deployability,"~~Also, this is going to be 0.17 so the original version was correct.. maybe we should update the doc to update the SBT version on the first Develop commit AFTER branching?~~. I didn't realise this was for us to continue with after the Develop branch was separated. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175233017:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175233017,2,['update'],['update']
Deployability,"~~possibly yes~~; probably not, however I will update the doc to point to what will (help).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3462#issuecomment-377018271:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3462#issuecomment-377018271,1,['update'],['update']
Deployability,ðŸ‘ . Does this need to be on hotfix as well? @jsotobroad @kcibul . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1213/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480:28,hotfix,hotfix,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480,1,['hotfix'],['hotfix']
Deployability,"ðŸ‘ ; I feel that we should hotfix this in to the next Firecloud release, given the improvement we saw in Alpha. The problem was probably exacerbated recently with the increase to the metadata batch size (bigger batches == more slowdown in List.append).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694:26,hotfix,hotfix,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,ðŸ‘ I thought Kristian mentioned this is important for Firecloud... does this also need to go into hotfix?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218793047:97,hotfix,hotfix,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218793047,1,['hotfix'],['hotfix']
Deployability,ðŸ‘ swagger yaml to be updated. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1293/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679:21,update,updated,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679,1,['update'],['updated']
Deployability,ðŸ‘ when above issues are addressed w/ comments/updates. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3682#issuecomment-391699492:46,update,updates,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682#issuecomment-391699492,1,['update'],['updates']
Deployability,ðŸ‘ ðŸ‘ ðŸ‘ ; Thanks so much! I've just updated my system with the latest Cromwell and this is perfect,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-495405207:34,update,updated,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-495405207,1,['update'],['updated']
Energy Efficiency, 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at w,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2649,adapt,adapted,2649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Energy Efficiency," - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1918,schedul,scheduleOnce,1918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['schedul'],['scheduleOnce']
Energy Efficiency," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesnâ€™t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesnâ€™t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1197,reduce,reduce,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,2,['reduce'],['reduce']
Energy Efficiency," at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 dae",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5405,Schedul,ScheduledThreadPoolExecutor,5405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (Ã¸)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1499,Power,Powered,1499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119,1,['Power'],['Powered']
Energy Efficiency,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:1479,monitor,monitor,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['monitor'],['monitor']
Energy Efficiency,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:296,efficient,efficient,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997,2,['efficient'],['efficient']
Energy Efficiency,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2270,schedul,scheduler,2270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['schedul'],['scheduler']
Energy Efficiency,.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.St,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3799,Schedul,Scheduler,3799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Schedul'],['Scheduler']
Energy Efficiency,"/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool.Main.main(Main.scala); ```. It would also be nice if the documentation included the fact that [`cwltool`](https://github.com/common-workflow-language/cwltool) needs to be installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:1798,adapt,adapted,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['adapt'],['adapted']
Energy Efficiency,"1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3686,adapt,adapted,3686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['adapt'],['adapted']
Energy Efficiency,":+1: It seems there's a potential to reduce some of the boilerplate around instances dealing with `ExpressionElement`. It seems like those instances exist to refine the type down to the ""leaf"" level where the more specific type can do its thing. I would try to eliminate one of these and see if you can parameterize the callers with a `[T]` or `[T <: ExpressionElement]` to save you from the trouble of specializing/refining/narrowing/casting (not sure the right word) the type yourself. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911:37,reduce,reduce,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911,1,['reduce'],['reduce']
Energy Efficiency,:+1: feel free to merge once it goes green if I don't get to it first,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109#issuecomment-123709570:37,green,green,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109#issuecomment-123709570,1,['green'],['green']
Energy Efficiency,:+1: reaffirmed though it would be really nice to see green builds before merge ðŸ˜„,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604:54,green,green,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604,1,['green'],['green']
Energy Efficiency,:+1: you'll need to rebase on develop to get your builds to go green (zone issue). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1403/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747:63,green,green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747,1,['green'],['green']
Energy Efficiency,; 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:19); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:17); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$3(WdlDraft2LanguageFactory.scala:120); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$1(WdlDraft2LanguageFactory.scala:119); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.getWomBundle(WdlDraft2LanguageFactory.scala:118); 	at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); 	at scala.util.Either.flatMap(Either.scala:338); 	at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); 	at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); 	at womtool.validate.Validate$.validate(Validate.scala:14); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:47); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:134); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:139); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:21); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:21); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:9545,adapt,adapted,9545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['adapt'],['adapted']
Energy Efficiency,"; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files â€¦ Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1406,green,greener,1406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,2,['green'],['greener']
Energy Efficiency,"=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.lang.Class for java.lang.Shutdown); at java.lang.Runtime.exit(Runtime.java:109); at java.lang.System.exit(System.java:971); at scala.sys.package$.exit(package.scala:40); at cromwell.Main$.waitAndExit(Main.scala:92); at cromwell.Main$.runWorkflow(Main.scala:77); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:25); at cromwell.Main$delayedInit$body.apply(Main.scala:15); at scala.Function0$class.apply$mcV$sp(Function0.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9530,monitor,monitor,9530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:679,efficient,efficient,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,2,['efficient'],['efficient']
Energy Efficiency,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... ðŸ˜¬ Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:198,efficient,efficient,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033,1,['efficient'],['efficient']
Energy Efficiency,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,adapt,adapter,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['adapt'],['adapter']
Energy Efficiency,"> @pshapiro4broad just want to be sure you'll be adding the Centaur tests?. It turned out that we (green) didn't need this feature after all, and I've been a bit busy to run the required test. Maybe a red person could take this over, as I think it may be generally useful for other users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227:99,green,green,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227,1,['green'],['green']
Energy Efficiency,"> Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check. Cool, so pending one more approval and then merge? (also finger cross on other tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650:114,green,green,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650,1,['green'],['green']
Energy Efficiency,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:674,schedul,scheduler,674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984,1,['schedul'],['scheduler']
Energy Efficiency,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:328,schedul,schedule,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['schedul'],['schedule']
Energy Efficiency,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:269,allocate,allocated,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,1,['allocate'],['allocated']
Energy Efficiency,"> Futures are fine, just not in the code which the actor itself directly controls. Hmm pretty much all our relevant code lives in actors so if the only place where we can use `Future`s is outside of actors, then I'd say `Future`'s days are counted xD. The _burden_ of verifying that new code doesn't break anything is part of the review process anyway IMHO, and adding N more actors instead of `Future`s with twice as many more messages, states, transitions, and classes doesn't reduce the burden of checking that everything is wired correctly, as far as I'm concerned. . Also, about futures being dangerous in actors because they can mutate state, this can only happen in a true `Actor` where your state pretty much has to be `var`s if you want to be able to mutate it, or in an `FSM` where you would also store some state as mutable `var`s inside the actor, instead of using the FSM data.; If you're using an FSM and all your mutable data is contained in the FSM data, then I don't see how creating futures would ever lead to mutating your state (and by state I mean mutable data, not FSM state) asynchronously. The FSM data is only updatable when the actor receives a message and decides to change state or stay in the same. A future `onComplete` could never force the FSM to mutate its data, which is why all we do is always send a message to someone when the future completes. And I don't see what difference it makes, from an actor perspective, if a message comes from a `Future` or another actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592:479,reduce,reduce,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592,1,['reduce'],['reduce']
Energy Efficiency,"> Have you thought about where you'd like to place the conversion from ""quota timestamp is recent"" to Boolean ""quota is exhausted""?. @aednichols I was thinking probably in the `/database/slick/GroupMetricsSlickDatabase.scala` file or maybe 1 level higher. But yeah that will be included in the next PR which will actually use the values from the table to decide where to allocate new tokens to this group or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927:371,allocate,allocate,371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927,1,['allocate'],['allocate']
Energy Efficiency,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:333,adapt,adapt,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,1,['adapt'],['adapt']
Energy Efficiency,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:776,reduce,reduce,776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,1,['reduce'],['reduce']
Energy Efficiency,"> I think they are true - I didn't write any tests. I was hoping there were already some... ðŸ˜¬ Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:190,efficient,efficient,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400,1,['efficient'],['efficient']
Energy Efficiency,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:1584,adapt,adapter,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['adapt'],['adapter']
Energy Efficiency,"> Is it a huge overhead/burden to also turn on the draft-3 versions?. Whenever the originals get updated, these should (in theory) be kept in sync. The point of the CRON tests is to run as close as possible what the real world workflows are running. As many of the originals run in FC, I believe they should be draft-2 for now. If one wanted to additionally clone draft-3/1.0 versions I think that would be fine. ToL: A better version of the CRON tests would just point-to/reference the originals from the source with smaller inputs, instead of having clones in this git repo. EDIT: More specifically re: burden-- this PR is just trying to get the tests green and then move on. I personally don't know enough about ""what's an input, what's an input-with-defaults, what's a non-input-but-calculated-from-an-input"" to go through the hundreds of lines for a ""quick"" convert.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458:654,green,green,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458,1,['green'],['green']
Energy Efficiency,"> Is this run on all jobs? If so Is it potentially something a user would want to turn off?. Yes, it runs on all jobs by default. I'm not sure if users would explicitly want to turn it off, since it doesn't interfere with anything as far as I can tell, and the pricing issue is virtually non-existent. > Also did you produce that graph manually? Is there a way to generate it easily for a workflow?. Yep, the graph can be easily produced through Stackdriver monitoring console with a few clicks, or a link to it can be constructed programmatically and exposed to the user. The graph is interactive, so there's no need to ""pre-render"" it - it is constructed dynamically by the monitoring console, based on user inputs and/or the link. > Can you include your monitor python/image code in this PR? Would be easier to maintain that way. Sure! Is there a folder path you'd prefer to keep it at? Perhaps I could put it under `supportedBackends/google/pipelines/v2alpha1/src/monitoring`?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862:458,monitor,monitoring,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862,4,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"> Looks good to me, once @cjllanwarne's comments are addressed. Thank you!; > Out of curiosity, what kind of cluster/tooling/data processing work are you using at your site?. Actually, we're trying to run gene sequencing tasks using cromwell, meanwhile adopting Volcano as scheduler for our kubernetes clusters.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382:273,schedul,scheduler,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382,1,['schedul'],['scheduler']
Energy Efficiency,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:143,schedul,scheduler,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511,2,['schedul'],['scheduler']
Energy Efficiency,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,adapt,adapter,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,1,['adapt'],['adapter']
Energy Efficiency,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:322,green,green,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,1,['green'],['green']
Energy Efficiency,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:87,monitor,monitoring,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395,2,['monitor'],['monitoring']
Energy Efficiency,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:267,schedul,scheduler,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929,3,['schedul'],['scheduler']
Energy Efficiency,> not sure why this was showing all green with only one review ðŸ¤”. PullApprove audits are always available via the `code-review/pullapprove` [Details](https://pullapprove.com/broadinstitute/cromwell/pull-request/3691/) links. In this case the change fell into `groups.one_reviewer` because of `groups.two_reviewers.conditions.files.exclude: centaur/*`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770:36,green,green,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770,1,['green'],['green']
Energy Efficiency,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:236,schedul,scheduler,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['schedul'],['scheduler']
Energy Efficiency,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:193,schedul,scheduler,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['schedul'],['scheduler']
Energy Efficiency,@Ghost-in-a-Jar Also take a look at the green `alpha` label at [https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100](https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577:40,green,green,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577,1,['green'],['green']
Energy Efficiency,"@Horneth oh right. Yeah, lets do it as in person unless no one is interested in which case disregard. If it's too much of a hassle to schedule around me feel free to not bother and I'll attend if I can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638:134,schedul,schedule,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638,1,['schedul'],['schedule']
Energy Efficiency,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:1189,monitor,monitor,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['monitor'],['monitor']
Energy Efficiency,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:636,sustainab,sustainable,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,2,['sustainab'],['sustainable']
Energy Efficiency,"@aednichols this PR is currently blocked, as I have a few questions Iâ€™m hoping to resolve with your teamâ€™s help, once you have time. The PR Jeff merged is very welcome, but it only solves a tiny portion of this one. As such, Iâ€™ve reverted the scope commit from it, so its only concern now is being able to pass additional information to the monitoring task. If youâ€™d prefer the questions to be raised here instead, please let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-505654233:341,monitor,monitoring,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-505654233,1,['monitor'],['monitoring']
Energy Efficiency,"@alexfrieden - I just ran this successfully. With your first error, the fact that you are getting an HTTP response indicates that it's not necessarily a networking issue on the AWS side, and as @Horneth stated something going on with DockerHub. For debugging in that scenario, I would try launching a t2.micro based on the CustomAMI you created with the same Batch instance profile in the VPC used by the Batch compute environment and try `docker pull <image>`. As for your second issue, did your tasks eventually transition from RUNNABLE? I've noticed that sometimes it takes about 5-10min for the Batch scheduler to ""re-warm"" after scaling down instances that were used for previous tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443:605,schedul,scheduler,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443,1,['schedul'],['scheduler']
Energy Efficiency,@alexwaldrop NB that I don't work there anymore and sadly haven't had the energy to actively contribute. Perhaps @aednichols can chime in,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-582136461:74,energy,energy,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-582136461,1,['energy'],['energy']
Energy Efficiency,"@antonkulaga I am reviewing the backlog systematically over the next few months so I am using the PO Cleanup label to keep track of Github issues that I am planning to review next or have already reviewed. It's not a reflection of it being fixed or scheduled, it's just for my tracking purposes. . I haven't reviewed this issue yet, good to know that it is still not functioning as intended. . @geoffjentry Is there a workaround to using Docker-Compose?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084:249,schedul,scheduled,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084,1,['schedul'],['scheduled']
Energy Efficiency,"@cjllanwarne ; I'm not sure I fully understand what you mean here. `checkalive` does run/schedule when `isAlive` is called.; Al tough I think what I use now is almost as you did explain. Only difference is that I first do isAlive instead of checking the exitcode. In f30c2be I did switch this. (first exitcode, then isAlive). Before I did had a default timeout of 120 seconds but I did remove that because of earlier comments. I can bring that back in if you want?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018:89,schedul,schedule,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018,1,['schedul'],['schedule']
Energy Efficiency,@cjllanwarne I asked @Horneth if we could use this during one of the upcoming lunch & learns which i need to schedule,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115:109,schedul,schedule,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115,2,['schedul'],['schedule']
Energy Efficiency,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:513,efficient,efficient,513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066,2,['efficient'],['efficient']
Energy Efficiency,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1077,power,power,1077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176,2,['power'],['power']
Energy Efficiency,"@cjllanwarne Thanks for the explanation! Exactly right, this is what the script says: ; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp; cd /cromwell_root. echo ""Hello foobar!"" && exit 1; echo $? > job.rc.txt. @pgrosu The exit 1 was the purpose here - I was modifying the basic hello.wdl test, trying to do a lightweight test simulating the failure of the binary being called. Turns out, as @cjllanwarne explains, that exit 1 is not a good way to simulate that, because it defeats Cromwell's return-code monitoring. Here's a better test, which does work as expected: . task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && head nonexistent; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792:560,monitor,monitoring,560,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792,1,['monitor'],['monitoring']
Energy Efficiency,"@cjllanwarne Things in this backend long existed before sfs backend came into being, and we didn't look into it yet. Good point though, I think we might try to adapt this to the sfs backend some time in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765:160,adapt,adapt,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765,1,['adapt'],['adapt']
Energy Efficiency,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:394,power,powerful,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569,2,['power'],['powerful']
Energy Efficiency,@danbills we do still need pullapprove to go green prior to merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4532#issuecomment-452746834:45,green,green,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4532#issuecomment-452746834,1,['green'],['green']
Energy Efficiency,"@danbills yes IMO the ""powers of 2"" you have here is the usual and expected form of exponential backoff. The other way is technically exponential but doesn't slow down nearly as quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403:23,power,powers,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403,1,['power'],['powers']
Energy Efficiency,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:55,schedul,scheduled,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046,1,['schedul'],['scheduled']
Energy Efficiency,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:24,monitor,monitoring,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082,4,['monitor'],"['monitoring', 'monitoringPidNamespace', 'monitoringTerminationAction', 'monitoringTerminationGraceTime']"
Energy Efficiency,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:103,monitor,monitoring,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,4,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"@ffinfo As I read it:. - If state is `None`, go to `Running` state; - If state is `Running`, the first thing we do is run `isAlive` (which I don't want to ever do!). That means that I have no way to opt-out of ever running `isAlive` (which is the thing I want before approving this PR). ---. What I was suggesting is (but there are many other ways):; - If I set `pollForAliveness: ""1 minute""` in the config file:; - Use `context.system.scheduler.scheduleOnce` to run an `isAlive` 1 minute in the future (completely separate from `pollStatus`).; - If that `isAlive` is true, schedule again another 1 minute in the future; - If not, record the time at which the job was not alive; - `pollStatus` continues on a different schedule:; - If the job is no longer alive, the `pollStatus` switches to `WaitingForReturnCode`; - If a time limit is set for the `WaitingForReturnCode` state, honor it; - If I set `pollForAliveness: false` in the config file:; - Go straight to `WaitingForReturnCode`; - No time limits for `WaitingForReturnCode`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053:436,schedul,scheduler,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053,4,['schedul'],"['schedule', 'scheduleOnce', 'scheduler']"
Energy Efficiency,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:257,green,green,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['green'],['green']
Energy Efficiency,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:684,schedul,schedule,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,2,['schedul'],['schedule']
Energy Efficiency,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:106,Green,Green,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179,1,['Green'],['Green']
Energy Efficiency,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:778,efficient,efficient-whole-exome-data-processing-using-workflows-on-the-cloud,778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,1,['efficient'],['efficient-whole-exome-data-processing-using-workflows-on-the-cloud']
Energy Efficiency,@geoffjentry Did you do anything related to dispatcher tooling for your recent Health monitoring pre-gull PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760:86,monitor,monitoring,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry I removed retry in two spots which hopefully could be added back per this scheme as part of #808. Note that neither of those spots corresponds to GCS auth upload, which should be happening in the initialization actor and is the subject of #806. I actually thought this ticket was meant for the hotfix branch to deal with problems the Greenies had seen, but there doesn't seem to be any more detail or labeling to confirm or refute that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863:347,Green,Greenies,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863,1,['Green'],['Greenies']
Energy Efficiency,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:94,monitor,monitoring,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry I understand re: egress charges. In my use case these aren't an issue, so a flag option would still help. Maybe, make egress cost a config option of a filesystem, and only reuse results if the egress cost would be under some user-specified value? You can also drop the requirement of specifying one engine/filesystem for all tasks. You could then return a cached result from any filesystem where it exists, without needing to copy it to a target filesystem. You could then also let workflow inputs point to files on different filesystems, and automatically choose the engine for each job based on where its inputs are.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286:37,charge,charges,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286,1,['charge'],['charges']
Energy Efficiency,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:165,reduce,reduces,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['reduce'],['reduces']
Energy Efficiency,"@geoffjentry Why do we need to reduce the scope of EJEA? What's the current problem? ; We do have the i/o actor, does that help?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923:31,reduce,reduce,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923,1,['reduce'],['reduce']
Energy Efficiency,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:229,monitor,monitoring,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:537,efficient,efficiently,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656,2,['efficient'],['efficiently']
Energy Efficiency,"@geoffjentry my labmates prefer TSV-s with headers and I had to adapt, so I tried loops as a solution. Going through Array[Array[String]] and turning it into Array[Map[String, String]] in a loop looked like the best solution for me. I tried both while loops and scatters and both of them could not change the array announced before the loop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645:64,adapt,adapt,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645,1,['adapt'],['adapt']
Energy Efficiency,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:146,green,green,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902,1,['green'],['green']
Energy Efficiency,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,adapt,adapter,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['adapt'],['adapter']
Energy Efficiency,"@horneth Talk to miguel as he was the last to look at this and would have the most up to date information. IIRC where this was left off is that there were a handful of known hotspots, all of the low hanging fruit had been picked and at this point we're talking fundamental changes to how things are being stored/tracked. As you can see from the numbers used it'll vary by the oomph your computer has but you'll want to find a number closer to the ""it starts happening here"" point vs some arbitrarily huge number as IIRC some of the initial chokepoints were less bad than deeper ones but at huge numbers were still horrible to wait through. You'll definitely want to become friends with JProfiler if you're not already friends with it. The lightbend monitor could also be helpful here (although I'm less sure of that for these purposes) but it's probably worth waiting until I'm back for that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149:749,monitor,monitor,749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149,1,['monitor'],['monitor']
Energy Efficiency,"@jeremiahsavage thank you again for reporting, and in particular for your excellent repro case. The fix for this bug has merged to develop and is scheduled to go out in Cromwell 40.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-482213725:146,schedul,scheduled,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-482213725,1,['schedul'],['scheduled']
Energy Efficiency,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:59,Green,Green,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['Green'],['Green']
Energy Efficiency,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:446,green,green,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812,1,['green'],['green']
Energy Efficiency,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:428,schedul,scheduler,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,['schedul'],['scheduler']
Energy Efficiency,"@katevoss This is actually really important, not just for @droazen and @lbergelson ... This issue has cost the Broad $$$ and analysts a lot of time. Not just the people on this issue. And putting retry code into the GATK (or any task for that matter) is bit arduous and actually a more expensive solution, especially when some random code path is missed. Also, retry on memory should do a lot for us to be able to reduce costs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748:414,reduce,reduce,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748,1,['reduce'],['reduce']
Energy Efficiency,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:140,adapt,adapter,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['adapt'],['adapter']
Energy Efficiency,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:46,schedul,scheduled,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802,1,['schedul'],['scheduled']
Energy Efficiency,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:50,reduce,reduce,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012,1,['reduce'],['reduce']
Energy Efficiency,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:101,schedul,schedulers,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482,4,['schedul'],"['scheduler', 'schedulers']"
Energy Efficiency,@ktibbett and @jsotobroad does the Green team use this API? Will you need it in order to adopt Cromwell 25?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222:35,Green,Green,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222,1,['Green'],['Green']
Energy Efficiency,@ktibbett is green team using /stats for anything meaningful?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393918710:13,green,green,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393918710,1,['green'],['green']
Energy Efficiency,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, Iâ€™m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:324,monitor,monitoring,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,1,['monitor'],['monitoring']
Energy Efficiency,@mcovarr The ontology change is related to the workflows that need to be run for the demo. It works without but they reduce the parsing time from 10 minutes to less than 30 seconds,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4168#issuecomment-425478951:117,reduce,reduce,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4168#issuecomment-425478951,1,['reduce'],['reduce']
Energy Efficiency,@mcovarr it was ðŸ’¯ green before i pushed the PR so i'm gonna blame our very stable test suite for now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224:18,green,green,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224,1,['green'],['green']
Energy Efficiency,@myazinn hey - the more i look at the rest of my week the more i'm thinking we might as well schedule the call :) Can you email me - `jgentry` with hostname `broadinstitute.org`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511602109:93,schedul,schedule,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511602109,1,['schedul'],['schedule']
Energy Efficiency,@nrockweiler requester pays buckets can be used in Cromwell when the Cromwell 32 is released. It is scheduled for this month,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394:100,schedul,scheduled,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394,1,['schedul'],['scheduled']
Energy Efficiency,@pshapiro4broad you flatter me. But yeah we should probably come up w/ a centaur test that makes sure it is actually working as intended. The good news is that our travis PR tests conduct a whole battery of papi v2 tests and those pass so this didn't break anything :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110:196,battery,battery,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110,1,['battery'],['battery']
Energy Efficiency,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:400,adapt,adapter,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['adapt'],['adapter']
Energy Efficiency,"@ruchim I think @geoffjentry is spot on - adding a scope by itself won't change existing behavior. It's only when the user sets `monitoring_image` to `quay.io/broadinstitute/cromwell-monitor-bigquery`, _then_ it will fail if the SA for the task doesn't have `bigquery.tables.updateData` permission on the monitoring dataset. So existing users on Terra won't be affected, unless we start routinely adding that option to all workflows and don't adjust the IAM permissions on pets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348:183,monitor,monitor-bigquery,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348,2,['monitor'],"['monitor-bigquery', 'monitoring']"
Energy Efficiency,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:50,monitor,monitor,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,8,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:955,battery,battery,955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['battery'],['battery']
Energy Efficiency,"@seandavi - The scratch mount point is passed to a daemon that monitors the disk space and attaches more EBS volumes as needed. `/scratch` is the default value for the scratch mount point when creating the AMI. However, for Cromwell you need to specify that this be `/cromwell_root`. See [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions) for Cromwell specific instructions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434849219:63,monitor,monitors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434849219,1,['monitor'],['monitors']
Energy Efficiency,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1344,adapt,adaptive,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['adapt'],['adaptive']
Energy Efficiency,A big thank you to @cjllanwarne for his Centaur magic that greened the Travis builds on this PR! ðŸ‡¬ðŸ‡§,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033:59,green,greened,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033,1,['green'],['greened']
Energy Efficiency,"A bit more info on this. The job mentioned above ran out of disk space. The monitoring.log is full of ""out of space"" errors. However, the job ran to completion and the output directory has an rc file containing 0, so Cromwell considered it a success. But the output files were truncated to zero bytes, presumably due to the disk space issue. Normally we get a hard failure when we run out of disk space but not in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997:76,monitor,monitoring,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997,1,['monitor'],['monitoring']
Energy Efficiency,"A system event monitoring and interrupt system seems way complex to me than implementing some conventions that leverage a standard task definition. At the point of submitting a process to Batch, you should already have all of the information needed to run a task, and can pass this along in a standard way to the standard task definition. This is the same information that is used to create the current job wrapping script. . FYI - the Funnel worker that is wrapped in `batch-task-runner` is modified to run stand-alone from a input JSON file, as opposed to communicating back to a Funnel server for task scheduling and distribution. The modified worker consumes the return codes passes the result back through it's own process and back to Batch. In this way, the sibling process is not hidden from Batch (or Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490:15,monitor,monitoring,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490,2,"['monitor', 'schedul']","['monitoring', 'scheduling']"
Energy Efficiency,Actually I think @vivster7 was the person to tag from green,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562:54,green,green,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562,1,['green'],['green']
Energy Efficiency,"Actually it seems that the config I was using already had that line in it, set to 16. I've reduced the concurrent-job-limit to 8, and I'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499:91,reduce,reduced,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499,1,['reduce'],['reduced']
Energy Efficiency,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:86,allocate,allocated,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738,1,['allocate'],['allocated']
Energy Efficiency,"Ah, if it's a local sregistry, then the build limits are not the same, and Singularity Hub isn't incurring any charges on Google Cloud :) . Carry on!. Are you running a local sregistry? I put in a PR today to add a keystore, in case you want to test it out :) https://github.com/singularityhub/sregistry/pull/235",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005:111,charge,charges,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005,1,['charge'],['charges']
Energy Efficiency,"Alright, would you prefer to expose it as a workflow (or Cromwell) option? Something like `monitoring_image` and if it's not defined, then the action is skipped. This way, one can also use an alternative image with their custom logic (incl. other monitoring APIs).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789:247,monitor,monitoring,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789,1,['monitor'],['monitoring']
Energy Efficiency,Also :+1: once green. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303,1,['green'],['green']
Energy Efficiency,"Also yellow is kind of a ""warny"" color, maybe a happier green? :frog:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186290162:56,green,green,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186290162,1,['green'],['green']
Energy Efficiency,"Also, from a user perspective, it might be nice to include the monitoring script file path in the workflow metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226287321:63,monitor,monitoring,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226287321,1,['monitor'],['monitoring']
Energy Efficiency,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:221,monitor,monitoring,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['monitor'],['monitoring']
Energy Efficiency,"An addendum to this and #1456 is that a) I should formalize (at least a bit) what I was going a few weeks ago w/ spamming cromwell, monitoring, etc and b) we should go over it as a group such that everyone feels comfortable doing such things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482:132,monitor,monitoring,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482,1,['monitor'],['monitoring']
Energy Efficiency,Any suggestions or plans to reduce the verbosity?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-407124690:28,reduce,reduce,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-407124690,1,['reduce'],['reduce']
Energy Efficiency,"Any thoughts on how a general task retry policy may interact with [#1499](https://github.com/broadinstitute/cromwell/issues/1499) when the job scheduler (or user) kills a job on sge and the server is restarted?. I'm envisioning the ""check-alive"" result showing no job on sge invoking one of the task retries that a user specifies (whereas currently it'd be marked as failed). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515:143,schedul,scheduler,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515,1,['schedul'],['scheduler']
Energy Efficiency,"Applying the ""do not merge"" label for now, just in case turning off the awkward tests makes this go green... ðŸ¤”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505:100,green,green,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505,1,['green'],['green']
Energy Efficiency,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:452,schedul,scheduler,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['schedul'],['scheduler']
Energy Efficiency,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:174,charge,charged,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762,1,['charge'],['charged']
Energy Efficiency,"Based on extensive log-examination, I believe this change has somehow broken the ability of a CWL workflow to survive restarts. Some suites pass some of the time due to a confluence of (1) getting lucky and avoiding a restart and (2) retries. It is very often the case that a test case only succeeds on the second or third try. Because I don't want anything to slip through the cracks due to probability, I'm not personally going to call this green until I see zero `Could not read from gs://cloud-cromwell-dev/cromwell_execution/travis/` messages.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066:443,green,green,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066,1,['green'],['green']
Energy Efficiency,"Based on the documentation it (at least seems like) there are ways to get this working. For example:. > if the docker image uses a hash, all call caching settings apply normally. You could try a full uri WITH a hash, e.g,. ```; library/ubuntu:latest@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68; ```; Then in the case that you provide a tag (e.g., library/ubuntu:14.04):. > Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend. So if you try providing the example above, does it still reduce it to `ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68`?. > If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, it will run the job with the user provided floating tag. So couldn't you just give it something you know will fail, and then have it use the tag you did provide?. If you wanted udocker to work, it seems more like a bug that cromwell is considering an incomplete uri (just ubuntu and the hash) as ""the correct way."" That tells us nothing about the registry, the start of the namespace (you could consider this like the collection, library) or a tag. Minimally the entire namespace should be honored, and the hash can still be looked up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692:635,reduce,reduce,635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692,1,['reduce'],['reduce']
Energy Efficiency,"Below is a timing diagram from a run of this WDL on develop Cromwell. The large turquoise blobs are `BackendIsCopyingCachedOutputs` and the small bright green blobs that immediately follow are `UpdatingCallCache` (the medium sized forest green blobs that follow that are `UpdatingJobStore` and are the subject of a [separate ticket](https://github.com/broadinstitute/cromwell/issues/2085)). For this particular WDL the former consumes slightly over 6 minutes and the latter about 9 seconds, but everything seems to happen in parallel for all shards. . Does this seem acceptable or should this get more attention?. <img width=""1062"" alt=""screen shot 2017-04-21 at 3 11 54 pm"" src=""https://cloud.githubusercontent.com/assets/10790523/25292867/160548a0-26a6-11e7-8573-f6dff2948df3.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-296289162:153,green,green,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-296289162,2,['green'],['green']
Energy Efficiency,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:141,green,green,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['green'],['green']
Energy Efficiency,"Can you rebase and see if centaur on travis-ci/push is happier? I suspect your branch contains stale develop code, since travis-ci/pr is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905:137,green,green,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905,1,['green'],['green']
Energy Efficiency,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1069,adapt,adapted,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499,1,['adapt'],['adapted']
Energy Efficiency,Closing for now because CWL support in WaaS / Womtool is now scheduled to land some time after the essential WaaS functionality for WDL has shipped. See https://github.com/broadinstitute/cromwell/issues/4333,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-442099426:61,schedul,scheduled,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-442099426,1,['schedul'],['scheduled']
Energy Efficiency,Closing this given the fact that the latest QA testing framework in the form of a Jenkins build is now green: https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline/237/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870:103,green,green,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870,1,['green'],['green']
Energy Efficiency,"Closing, since we've implemented Cromwell metadata export via a Cloud Function. For details, see https://github.com/broadinstitute/cromwell-task-monitor-bq#metadata-upload",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-528065027:145,monitor,monitor-bq,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-528065027,1,['monitor'],['monitor-bq']
Energy Efficiency,"Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453:112,green,green,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453,1,['green'],['green']
Energy Efficiency,Compilation included at no extra charge in #2649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2648#issuecomment-331915147:33,charge,charge,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2648#issuecomment-331915147,1,['charge'],['charge']
Energy Efficiency,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:12,schedul,scheduled,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653,1,['schedul'],['scheduled']
Energy Efficiency,"Cromwell is a scheduler at heart and wants to dispatch jobs to a backend that it doesn't manage, like cloud or HPC. You may have better results with [MiniWDL](https://github.com/chanzuckerberg/miniwdl) for the jobs-on-laptop use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360:14,schedul,scheduler,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360,1,['schedul'],['scheduler']
Energy Efficiency,"Deleting some comments due to being interspersed with untrue things. @LeeTL1220 . As per #1762 the intention was to have spec mandated minimums and implementation level maximums. The former never happened so technically it's not part of the spec at all. And as I noted, Cromwell team is no longer in charge of the WDL spec, so ... That said, it's tunable. You can increase it if you want. I wouldn't recommend going all that high unless you're willing to really jam a lot of memory in there. As per your iterator comment, I go back to the Cromwell team doesn't control WDL anymore and there's no WDL construct which would allow that. There's been chatter about things which might help but they're unlikely to arrive until after WDL 1.0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853:300,charge,charge,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853,1,['charge'],['charge']
Energy Efficiency,"Depending on what your monitoring script does and how long your command takes to run it's possible that the task finishes before the monitoring script had time to write / flush anything into the monitoring log.; I ran this and got an empty log . ```; task t {; command {; echo ""hey""; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```. but this gave me a non-empty one . ```; task t {; command {; sleep 5; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034:23,monitor,monitoring,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034,3,['monitor'],['monitoring']
Energy Efficiency,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:939,schedul,scheduled,939,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['schedul'],['scheduled']
Energy Efficiency,Does anyone know what is wrong with codecov? My changes shouldn't have reduced test coverage that much.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096:71,reduce,reduced,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096,1,['reduce'],['reduced']
Energy Efficiency,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:717,schedul,scheduler,717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,6,['schedul'],['scheduler']
Energy Efficiency,"Exactly. When you use Akka's scheduler it's not blocking a thread, which leads to much happiness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929:29,schedul,scheduler,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929,1,['schedul'],['scheduler']
Energy Efficiency,Excellent! The build failures are due to the MySQL quirks addressed by #160; once that's merged to scatter-gather and this is rebased I expect the build will go green.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384:161,green,green,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384,1,['green'],['green']
Energy Efficiency,Feel free to merge as soon as we get 4 green checkboxes. I know I'd love to get this in my other branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027,1,['green'],['green']
Energy Efficiency,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:157,monitor,monitoring,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['monitor'],['monitoring']
Energy Efficiency,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:663,reduce,reduce,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102,1,['reduce'],['reduce']
Energy Efficiency,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, although other jobs are not stopped when a job fails, they're left as is and are being monitored until they reach completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733:150,monitor,monitored,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733,1,['monitor'],['monitored']
Energy Efficiency,"Followed up with @ruchim via email and we poked around the codebase this afternoon and think we have a good idea on how to add this functionality. First though we wanted to run the general plan by you guys and had a couple of questions. . It seems like the place to staple this in would be as an additional `Action` in `GenomicsFactory` (as was alluded to above). We weren't sure if it made sense to staple it in as an additional `monitoringAction` or `userAction` as they are both handled slightly differently than this would be. Neither fits perfectly, but it seems like it could potentially be made to work in either place. . Another alternative (maybe the cleanest one?) would be to just introduce a `remoteAccessAction` or similar helper that builds up an action just for this purpose. We wouldn't want to change the default behavior and/or have this `ssh` server always running, so it seems to make sense to make it configurable. We figured the PAPI section of the conf file would be a reasonable place to add this new option. Does that generally sound in line with how you guys would approach this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050:431,monitor,monitoringAction,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050,1,['monitor'],['monitoringAction']
Energy Efficiency,For JES and Local backends:; - Workflow root; - Call root. JES backend only:; - Google project; - Execution bucket; - Endpoint URL; - GCS path to the monitoring log,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1020#issuecomment-227234967:150,monitor,monitoring,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1020#issuecomment-227234967,1,['monitor'],['monitoring']
Energy Efficiency,"From my investigation, those were due to transient failures of ontology parsing, which this should reduce now: https://github.com/broadinstitute/cromwell/pull/4210",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750:99,reduce,reduce,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750,1,['reduce'],['reduce']
Energy Efficiency,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:534,charge,charge,534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592,1,['charge'],['charge']
Energy Efficiency,"Given that:; - The WDL spec today is un-opinionated on the size of an `Int`; - The WDL `Int` is defined as 64 bit in the upcoming https://github.com/openwdl/wdl/pull/250; - NB: swap the red and green diffs in your head because Jeff merged it too soon and this is a placeholder ""if we for some reason needed to revert"" PR; - And assuming that people are not currently writing workflows specifically relying on an explosion if their Ints are above 2^16. I suggest you just default to making anything WDL produce`WomLong`s in all new situations, since we'll probably sweep through and update everything else at some point soon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372:194,green,green,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372,1,['green'],['green']
Energy Efficiency,"Given the current Tyburn bent toward JES workflows with GCS inputs and Docker images, the standard battery isn't really compatible with stress testing these changes. But I can make some three-step and scatter WDLs and do some custom posthumous execution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147788296:99,battery,battery,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147788296,1,['battery'],['battery']
Energy Efficiency,Good news: I think if you rebase this on develop the builds may go green ðŸ¤ž,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062:67,green,green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062,1,['green'],['green']
Energy Efficiency,Good refactor to reduce future headaches/bugs/effort,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583:17,reduce,reduce,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583,1,['reduce'],['reduce']
Energy Efficiency,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:345,monitor,monitor,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,2,['monitor'],['monitor']
Energy Efficiency,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:105,charge,charge,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789,1,['charge'],['charge']
Energy Efficiency,"Hello @ruchim :-). The Spark job will run just as good in Yarn as in Mesos, I am pretty sure about that. Main difference is that Mesos is much more advanced than Yarn. It is more scalable, both in terms of nr of nodes, nr of jobs, and types of jobs and applications. . In Mesos, you can run both normal applications (web apps etc.), like you do in Kubernetes, and you can run compute / Big Data processing jobs in the same cluster. You can schedule both cpu and memory usage, not only memory usage as in Yarn. Mesos creates a virtual operating system on top of your cluster, kind of. Yarn is not capable of that as I know it. You can even run Yarn and Kubernetes on top of Mesos etc. Choosing Mesos over Yarn, will therefor make sense for many companies, because you get one system to rule them all. It might add more complexity also though ... I am a bit dated on this, Yarn might have evolved since I looked at it. This article is good at explaining the difference:. https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn. Here is a nice summary of the main differences:; https://data-flair.training/blogs/comparison-between-apache-mesos-vs-hadoop-yarn/. Hope this give some answers :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977:440,schedul,schedule,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977,1,['schedul'],['schedule']
Energy Efficiency,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:89,reduce,reduce,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988,1,['reduce'],['reduce']
Energy Efficiency,"Hello Jeffâ€”would you mind testing if this would break Terra or Green team prod? Meaning, if the SAs donâ€™t have BQ scope before being sent to Cromwell, and then Cromwell adds the BQ scopeâ€”does the task fail to run?. <sub>Sent with <a href=""http://githawk.com"">GitHawk</a></sub>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501312553:63,Green,Green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501312553,1,['Green'],['Green']
Energy Efficiency,"Hello and welcome to the Cromwell repo. Three minutes is about what I would expect from personal experience, as a minimum time to run any task. Consider that Life Sciences allocates, starts, and pulls Docker on a dedicated VM just to print ""hello world"". It will never look favorable for small tasks whose execution time is short compared to VM setup time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814:172,allocate,allocates,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814,1,['allocate'],['allocates']
Energy Efficiency,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:319,adapt,adapt,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053,1,['adapt'],['adapt']
Energy Efficiency,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:59,schedul,scheduler,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087,4,"['adapt', 'schedul']","['adapting', 'scheduler']"
Energy Efficiency,"Hey @Shenglai,. Although this isn't well documented, but for each scheduler based backend (such as SLURM), on has to configure the backend with the memory units required for the scheduler. And without this conversion, Cromwell doesn't assume what needs to be sent to the scheduler. I believe we just need to improve the documentation, but I do believe the intended behavior is that Cromwell always converts declared memory into bytes by default, and backends can be configured to use other units. . Let me know if you have any questions, and I'll be closing this issue (and replacing it with an issue to improve documentation) if there are no concerns. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849:66,schedul,scheduler,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849,3,['schedul'],['scheduler']
Energy Efficiency,"Hey @benjamincarlin,. The call caching feature is really designed with the focus on not having to recomputing outputs. In the case of the monitoring script, it really was meant to be treated as a debugging tool and not a true output from a task. Can you explain why you need to the monitoring log for cached jobs, especially as its not new information? Is the motivation to be able to access all monitoring logs under one workflow uuid directory?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805:138,monitor,monitoring,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805,3,['monitor'],['monitoring']
Energy Efficiency,"Hey @bkohrn, just checking up on old threads:. - running a MySQL database will reduce the memory usage of Cromwell for larger workflows.; - ie: memory(mySQL + cromwell) < memory(cromwell w/ NO MySQL); - CPU can be mitigated by turning off call-caching, or at least the ""File"" strategy for call-caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-572796376:79,reduce,reduce,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-572796376,1,['reduce'],['reduce']
Energy Efficiency,"Hey @cjllanwarne yes, I regularly run it with `monitoring_image` and it does terminate the monitoring container gracefully every time, as can be seen via the PAPIv2 operation log. This is the whole reason for its existence - to send SIGTERM to the monitoring container, otherwise PAPIv2 sends a SIGKILL and the last time points don't get reported..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119:91,monitor,monitoring,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119,2,['monitor'],['monitoring']
Energy Efficiency,"Hey @indraniel -- this isn't yet supported --but the plan is to add it eventually. For now, it may be helpful to add something like an rsync at the top of your task, to see what logs are being produced by your task, or if things have gone quiet. It's also possible to add a monitoring script to run in the background that includes info about cpu/memory usage ; https://cromwell.readthedocs.io/en/stable/wf_options/Google/#google-pipelines-api-workflow-options (see ""monitoring_script"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358:274,monitor,monitoring,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358,1,['monitor'],['monitoring']
Energy Efficiency,"Hey @jsotobroad -- if you think you need this fix for sometime in the near future then we should label it GreenFriends so it gets the deserved attention. Otherwise, this sounds like a bug--I would consider it a regression as I believe we expect all WF's to produce metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-212041006:106,Green,GreenFriends,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-212041006,1,['Green'],['GreenFriends']
Energy Efficiency,"Hey Denis -- I'll pose the same question as I did in another open [PR](https://github.com/broadinstitute/cromwell/pull/5023). Would you mind testing if this change is going to break for Terra and/or Green team prod? Meaning, if the user service Accounts donâ€™t have BQ permission before setting those scopes in Cromwell, does the task fail to run?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502094173:199,Green,Green,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502094173,1,['Green'],['Green']
Energy Efficiency,"Hi !; Just to make sure I understand, are you saying that the monitoring log is not copied over when a call is being cached ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512:62,monitor,monitoring,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512,1,['monitor'],['monitoring']
Energy Efficiency,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:76,schedul,scheduling,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,1,['schedul'],['scheduling']
Energy Efficiency,"Hi @cjllanwarne @mcovarr @ruchim !; I've again rebased my branch against the latest develop and Travis jobs are all green. So I want to repeat my last question about failing test: **was it removed, or is everything OK now?**; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031:116,green,green,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031,1,['green'],['green']
Energy Efficiency,"Hi @cjllanwarne @ruchim @mcovarr !; I've finally rebased against the latest develop and resolved conflicts, and CI is all green. So it a good sign? :); I still could try to reproduce a `test with space` scenario, if it was turned off on CI. Should I?; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592:122,green,green,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592,1,['green'],['green']
Energy Efficiency,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:203,SCHEDUL,SCHEDULED,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,2,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:528,schedul,scheduleOnce,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,8,['schedul'],"['scheduleOnce', 'scheduled']"
Energy Efficiency,"Hi @illusional, we put this PR into our schedule to review, could you please make a version of it inside the repo as you did for https://github.com/broadinstitute/cromwell/pull/5573?. Thanks,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359:40,schedul,schedule,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359,1,['schedul'],['schedule']
Energy Efficiency,"Hi @myazinn and @Kirvolque - sorry for the delay here, the conversation last week wound up with Chris & I chatting w/ @wleepang from AWS, this looks like another case where the required bits of information are split across multiple people. I'm going to **try** to figure out what needs figuring out this week, but we'll see. If that winds up not working out, one thought Lee & I had was that we could set up a 3-way call next week. Both he & I will be in Basel next week which seemed like it'd be easier to schedule with you all instead of finding a time which worked for him on the west coast of the US and you all",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511458740:507,schedul,schedule,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511458740,1,['schedul'],['schedule']
Energy Efficiency,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:334,green,green,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720,1,['green'],['green']
Energy Efficiency,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:292,schedul,scheduler,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718,1,['schedul'],['scheduler']
Energy Efficiency,"Hi @vanajasmy and thanks for your contribution. Codecov is a nice-to-have, we report it as a useful indicator but don't mandate that every single PR continue a monotonic march towards 100%. The real measure we care about is a matter of judgment - i.e. does all functionality have reasonable tests, and does critical functionality have exhaustive tests. In order to set expectations, it may be a bit before we have cycles to review this PR. Reviewing does take a substantial team effort and has to be included into the schedule alongside other tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292:518,schedul,schedule,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292,1,['schedul'],['schedule']
Energy Efficiency,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:253,schedul,schedule,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['schedul'],['schedule']
Energy Efficiency,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:718,adapt,adapt,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823,1,['adapt'],['adapt']
Energy Efficiency,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:754,Schedul,Scheduler,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,2,['Schedul'],['Scheduler']
Energy Efficiency,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:814,schedul,schedule,814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852,1,['schedul'],['schedule']
Energy Efficiency,HipChat chooses @scottfrazer and @Horneth because they show as green,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/376#issuecomment-171254307:63,green,green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/376#issuecomment-171254307,1,['green'],['green']
Energy Efficiency,"Hmm, another moment is the Cromwell SA used by the GCE instances will need to have `Monitoring Metric Writer` role to be able to write metrics. I'll verify what happens when they don't. This is not an issue if the SA is Default Compute Engine account though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516304:84,Monitor,Monitoring,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516304,1,['Monitor'],['Monitoring']
Energy Efficiency,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:392,monitor,monitoring,392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,2,['monitor'],['monitoring']
Energy Efficiency,"I can understand. This solution is not ideal. On the upside: it is only activated for those who willingly put ""cached-copy"" in their configs. The rest of the cromwell users are **not** affected by the lock mechanism. By default this does **not** affect anyone. I could adapt this PR and plaster the words: `WARNING: EXPERIMENTAL` all over it if that helps. EDIT: While I mention it ""is not ideal"", the only situation where the locks might not be effective is when using multiple cromwell processes, that do use the same execution folder. Does this happen often in practice? Is this even a supported use case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225:269,adapt,adapt,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225,1,['adapt'],['adapt']
Energy Efficiency,"I don't have the power to invite people, but the signup link is here: https://sylabs.io/singularity/slack/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160:17,power,power,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160,1,['power'],['power']
Energy Efficiency,"I exposed this as `monitoring_image` workflow option, and included the Dockerfile & script in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`. Should we also add a CI script that builds the image?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217:167,monitor,monitor,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217,1,['monitor'],['monitor']
Energy Efficiency,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:229,charge,charges,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,1,['charge'],['charges']
Energy Efficiency,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:227,Adapt,Adapt,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,1,['Adapt'],['Adapt']
Energy Efficiency,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:436,green,green,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['green'],['green']
Energy Efficiency,"I have adapted @delocalizer's script for an LSF backend: https://github.com/wtsi-hgi/olly-maersk/blob/master/zombie-killer.sh It's currently untested, but it shouldn't be far off from what's required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014:7,adapt,adapted,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014,1,['adapt'],['adapted']
Energy Efficiency,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:524,charge,charges,524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,1,['charge'],['charges']
Energy Efficiency,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:424,power,power,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161,1,['power'],['power']
Energy Efficiency,I know green requested this so they'll be happy but should also let blue know as well,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276:7,green,green,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276,1,['green'],['green']
Energy Efficiency,"I recognize the bind to /data, I had discussion with Seth about taking this approach! I think @leepc12 has been actively working and testing and can give quick feedback? If it works, it works, there is enough change coming to singularity wrt services that I wouldnâ€™t put too much energy into iterating over this if itâ€™s working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338:280,energy,energy,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338,2,['energy'],['energy']
Energy Efficiency,"I reproduced the problem locally. The problem is our default health monitor `StandardHealthMonitorServiceActor` which periodically tries to ping Docker Hub. I'm not sure what changed between 36 and 37 that this now manifests when previously it did not. There is a `cromwell.services.healthmonitor.impl.noop.NoopHealthMonitorServiceActor` that one should be able to swap in to turn off health monitoring, but unfortunately there's a bug in the 37 version of this class that causes it to crash during initialization. As you've noticed the workflow still runs successfully albeit with a lot of noise, we'll try to come up with a more satisfactory resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697:68,monitor,monitor,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,I suggest also allow specifying walltime and queue; walltime in particular as it enables most efficient performance of the scheduler and is often a required resource request in traditional HPC cluster setups.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967:94,efficient,efficient,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967,2,"['efficient', 'schedul']","['efficient', 'scheduler']"
Energy Efficiency,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:16,green,greens,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447,2,['green'],['greens']
Energy Efficiency,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,adapt,adapter,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720,2,['adapt'],['adapter']
Energy Efficiency,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:199,schedul,scheduling,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,1,['schedul'],['scheduling']
Energy Efficiency,"I want to comment on the fact that there are hidden dangers in using the function `Float size(Array[File])` as shown in a Terra community forum [post](https://support.terra.bio/hc/en-us/community/posts/360071583412-PreparingJob-state-consumes-most-of-a-task-running-time-how-to-avoid-). For large arrays, this can cause tasks to take forever to start in Google Cloud. Although this is not a WDL specification issue, developers need somehow to be made aware of this, especially in cases where the array contains files that are known in advance to have similar sizes, in which case the following code:; ```; input {; Array[File]+ files; }; Float arr_size = size(files, ""GiB""); ```; Could be replaced by:; ```; input {; Array[File]+ files; }; Float arr_size = length(files) * size(files[0], ""GiB""); ```; And be significantly more efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665:827,efficient,efficient,827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665,1,['efficient'],['efficient']
Energy Efficiency,I was trying to pick something we haven't already used and we already have green. The colors they have are here:; https://github.com/demhydraz/badge-collection/blob/master/licenses/bsd3clause.md. We could try to unify all of the colors but that might not be possible on some badges,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186291377:75,green,green,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186291377,1,['green'],['green']
Energy Efficiency,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:884,charge,charged,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,2,"['allocate', 'charge']","['allocated', 'charged']"
Energy Efficiency,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:1811,schedul,schedulers,1811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,2,['schedul'],"['scheduler', 'schedulers']"
Energy Efficiency,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:825,Adapt,Adapters,825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,2,['Adapt'],['Adapters']
Energy Efficiency,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:264,monitor,monitor,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152,2,['monitor'],['monitor']
Energy Efficiency,"I'm curious - did we need to ensure that SAs had support for `monitoring.write` when you all added that in #4562? My bet is that this would have the same behavior, although perhaps not as that scope seems to be a little hidden",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502204552:62,monitor,monitoring,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502204552,1,['monitor'],['monitoring']
Energy Efficiency,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:982,energy,energy,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['energy'],['energy']
Energy Efficiency,"I'm late to the party on this, but:. > Then chains of tasks could effectively become one task. I don't think merging of tasks works if you have certain resource or software dependencies, eg: inside a docker container. From a software engineering POV, is it easy / possible to detect and facilitate streaming between tasks like this, especially if they're scheduled as completely separate jobs? To me it sounds super difficult, like you'd have like a ""fuzzy"" dependency graph, and you could end up streaming your result data between nodes or tasks (and even worse if you're running on the cloud). (@mr-c, you've talked about this a [few times](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)). > parallel, rather than sequentially. Mostly, but what happens if two of the inputs are technically streamable, or even more complicated how would `stdin` fit into this. The [CWL documentation](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineTool) says that it requires the path (eg: [`$(inputs.stdinRef.path)`](; https://www.biostars.org/p/258614/#290536)) which to me sounds like it isn't exactly streamable, but `stdout` [implicitly is?](https://www.commonwl.org/v1.0/CommandLineTool.html#stdout) WDL in the version [1.0 spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#language-specification) doesn't include any reference to 'stream', so I'm surprised to see the DNAnexus adding a separate tagging mechanism for this optimisation. _Late edit: reformatting for clarity_; Engine support:. - Cromwell (not supported) [[source](https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734)]; - CWLTool (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644)]; - Toil (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)]. But piping (named and anonymous) is super easy in WDL because you have a command line, and in CWL yo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417:355,schedul,scheduled,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417,1,['schedul'],['scheduled']
Energy Efficiency,I'm looking at ways to reduce that,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442:23,reduce,reduce,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442,1,['reduce'],['reduce']
Energy Efficiency,"I'm using this in a fully automated setting, as part of this script:; https://github.com/broadinstitute/dsp-scripts/blob/master/cromwell/methods/cromwell_monitoring_script.sh. So for me it won't work if the combination of google VM / docker / cromwell results in the disk *not* being mounted on /dev/sdb for any reason. This could happen if the user requests disks to be mounted in a specific place (https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/), requests more than one disk but would prefer the second disk be monitored, or if cromwell starts using /dev/sdb for some other resource and the disks get pushed to /dev/sdc. I guess it would be more precise to say, ""I'm not aware of this happening, but lots of people use this script and I have no idea if any of them would complain to me if it didn't work"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529532328:527,monitor,monitored,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529532328,1,['monitor'],['monitored']
Energy Efficiency,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:158,charge,charges,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,8,['charge'],['charges']
Energy Efficiency,"I've worked out what happened, but I don't know if I can resolve this next problem. . I had call-caching turned on for SFS, and this was MD5 hash was being calculated by Cromwell on the login node, however for 2x 100GB BAM files at each step this was (obviously in retrospect) a resource drain. This was only applicable to backends that use the Local Filesystem (GCS and S3 file systems probably use their blob / object id). If you come across this issue, you might have a couple of solutions:; - Turn off call-caching, might not matter to you.; - If you're not using containers, you might be able to get away with the [path+modtime caching strategy](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options), requires you to use the [`soft-link` copying strategy](https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem).; - If you **are** using containers, you're out of luck unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774:288,drain,drain,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774,1,['drain'],['drain']
Energy Efficiency,"If it can be specified by a workflow option, then everyone is happy. And; we don't have to wait for 1.0. On Oct 20, 2017 20:30, ""Jeff Gentry"" <notifications@github.com> wrote:. > Deleting some comments due to being interspersed with untrue things.; >; > @LeeTL1220 <https://github.com/leetl1220>; >; > As per #1762 <https://github.com/broadinstitute/cromwell/issues/1762> the; > intention was to have spec mandated minimums and implementation level; > maximums. The former never happened so technically it's not part of the; > spec at all. And as I noted, Cromwell team is no longer in charge of the; > WDL spec, so ...; >; > That said, it's tunable. You can increase it if you want. I wouldn't; > recommend going all that high unless you're willing to really jam a lot of; > memory in there.; >; > As per your iterator comment, I go back to the Cromwell team doesn't; > control WDL anymore and there's no WDL construct which would allow that.; > There's been chatter about things which might help but they're unlikely to; > arrive until after WDL 1.0; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkyBD9ZS-tfUwjFaEVS_i9Gro7EOUks5suTs2gaJpZM4QBFpH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338398224:586,charge,charge,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338398224,1,['charge'],['charge']
Energy Efficiency,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:404,monitor,monitor,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244,2,['monitor'],['monitor']
Energy Efficiency,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:93,green,green,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['green'],['green']
Energy Efficiency,"In a sense yes, but it's upstream of it.; Currently if you scatter 10 million wide, Cromwell still creates 10 million EJEAs that are going to ask for a token.; This stops it from even creating EJEAs if it knows they won't be able to run anyway (yet).; To be perfectly honest I just wanted to kinda float the idea as I haven't gotten to precisely measure if it indeed reduces cpu / memory load (I'm strongly guessing yes though since it's less work being done but...).; I wanted to get people's opinion and if it's a no go already I won't bother measuring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846:367,reduce,reduces,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846,1,['reduce'],['reduces']
Energy Efficiency,"In theory it shouldn't make a difference.* Cromwell isn't scheduling anything, it's merely seeing what is runnable and launching it. Thus the time delta should be miniscule. However, in the real world there are things such as quotas on the backend and that could make a difference, yes. But keep in mind that backends will themselves process job requests differently and aren't necessarily going to honor the order we send them in anyways. Thus, in my opinion this sort of general optimization is going to be folly as we can never guarantee the underlying behavior anyways. To your primary point, I get what you're saying although my experience has been that every time someone has stated that a behavior should be X as it matches the real world I find someone telling me that the opposite behavior matches the real world. :) This is one of those cases. This one is more complex in that we also hear differing opinions on the whole workflow level (ie should workflows be processed as many at once as possible or optimizing for throughput for any individual workflow). At the end of the day the limiting factor is going to be the backend set up and whatever quotas are in place. * Yes, there is a global job limit but this can be tweaked as high as one would like, so effectively not an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269:58,schedul,scheduling,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269,1,['schedul'],['scheduling']
Energy Efficiency,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:434,energy,energy,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['energy'],['energy']
Energy Efficiency,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:152,monitor,monitor,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,2,['monitor'],['monitor']
Energy Efficiency,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:495,charge,charges,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358,1,['charge'],['charges']
Energy Efficiency,"It would be very nice to support Mesos, as it is a very nice framework for in house cloud computing systems. Not everyone is able to launch their jobs into the cloud. I see you already have support for Yarn here: . http://cromwell.readthedocs.io/en/develop/backends/Spark/. And we also have this:; ```; A not so widely known fact is that Spark has its root in Mesos: it was ; initially developed at the AMPLab as a proof-of-concept Mesos ; framework to demonstrate how easy and fast ; it is to develop a distributed platform on top of Mesos; ```; taken from here : ; * https://mesosphere.com/blog/spark-mesos-shared-history-and-future-mesosphere-hackweek/. Spark and Mesos was really closely integrated, though I see that Spark has created their own scheduler, Mesos is still a very good way of running Spark jobs. It would be a very nice addition to the Chromwell framework! . Mesos is used in many other Big Data cloud environments outside of the Bioinformatics pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576:750,schedul,scheduler,750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576,1,['schedul'],['scheduler']
Energy Efficiency,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:74,schedul,schedule,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228,3,['schedul'],"['schedule', 'scheduling']"
Energy Efficiency,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (Ã¸)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (Ã¸)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4262,Power,Powered,4262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,1,['Power'],['Powered']
Energy Efficiency,"Just a heads up, after talking with @Horneth and @cjllanwarne I'm planning to merge this before Thibault's PR if/when these builds go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423:134,green,green,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423,1,['green'],['green']
Energy Efficiency,LGTM when travis is green ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2775/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2775#issuecomment-338986251:20,green,green,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2775#issuecomment-338986251,1,['green'],['green']
Energy Efficiency,"Looking at it, and chewing my own words, the changes that I'm seeing has a few round-trips to the database (e.g. getting workflows in a particular state). Given that we are trying to reduce DB calls from the engine, do you guys think it to be a good idea?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201018255:183,reduce,reduce,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201018255,1,['reduce'],['reduce']
Energy Efficiency,Merging as this is all green and there were no unresolved comments,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4865#issuecomment-485612460:23,green,green,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4865#issuecomment-485612460,1,['green'],['green']
Energy Efficiency,"Merging this as it's green and has no requested cleanups. @cjllanwarne I'm not deleting the branch, will leave that to you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485502251:21,green,green,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485502251,1,['green'],['green']
Energy Efficiency,"Merging this for @aednichols as he's out, this is green, and there are no outstanding things described to clean up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4858#issuecomment-485501748:50,green,green,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4858#issuecomment-485501748,1,['green'],['green']
Energy Efficiency,"Monitoring.log file is created when monitoring_script WF option is used, but the log file is completely empty. It's possible that the correct outputs aren't copied over to the log file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226281925:0,Monitor,Monitoring,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226281925,1,['Monitor'],['Monitoring']
Energy Efficiency,My main concern here is relying on it as a proof of validity until it's guaranteed that it won't go green if it's not picking up workflows in flight.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068:100,green,green,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068,1,['green'],['green']
Energy Efficiency,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isnâ€™t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:40,monitor,monitors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993,1,['monitor'],['monitors']
Energy Efficiency,"Nope as far as I know, it's turned on by default (and is used by GCP to monitor most resources): https://cloud.google.com/service-usage/docs/enabled-service#default",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451496022:72,monitor,monitor,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451496022,1,['monitor'],['monitor']
Energy Efficiency,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:92,schedul,scheduled,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['schedul'],['scheduled']
Energy Efficiency,"Not really, I have a work-around (if /sys/block/sdb/ is a directory and /dev/sdb is mounted in mtab, use /sys/block/sdb/); ```; function findBlockDevice() {; MOUNT_POINT=$1; FILESYSTEM=$(grep -E ""$MOUNT_POINT\s"" /proc/self/mounts \; | awk '{print $1}'); DEVICE_NAME=$(basename ""$FILESYSTEM""); FS_IN_BLOCK=$(find -L /sys/block/ -mindepth 2 -maxdepth 2 -type d \; -name ""$DEVICE_NAME""); if [ -n ""$FS_IN_BLOCK"" ]; then; # found path to the filesystem in the block devices. get the; # block device as the parent dir; dirname ""$FS_IN_BLOCK""; elif [ -d ""/sys/block/$DEVICE_NAME"" ]; then; # the device is itself a block device; echo ""/sys/block/$DEVICE_NAME""; else; # couldn't find, possibly mounted by mapper.; # look for block device that is just the name of the symlinked; # original file. if not found, echo empty string (no device found); BLOCK_DEVICE=$(ls -l ""$FILESYSTEM"" 2>/dev/null \; | cut -d'>' -f2 \; | xargs basename 2>/dev/null \; || echo); if [[ -z ""$BLOCK_DEVICE"" ]]; then; 1>&2 echo ""Unable to find block device for filesystem $FILESYSTEM.""; if [[ -d /sys/block/sdb ]] && ! grep -qE ""^/dev/sdb"" /etc/mtab; then; 1>&2 echo ""Guessing present but unused sdb is the correct block device.""; echo ""/sys/block/sdb""; else ; 1>&2 echo ""Disk IO will not be monitored.""; fi; fi; fi; }; ```. I am not sure if this is a google VM problem, a docker problem, or a problem with how cromwell specifies volumes to docker; but I took their response to be ""we don't care and won't fix it"". Fortunately for me the work-around nearly always works for cromwell jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322:1257,monitor,monitored,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322,1,['monitor'],['monitored']
Energy Efficiency,Not sure I have the powers to push a dev image to Docker Hub though (let's discuss offline).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343:20,power,powers,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343,1,['power'],['powers']
Energy Efficiency,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:121,Green,Green,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,1,['Green'],['Green']
Energy Efficiency,"OK I think the TDD world is fucking wit me as code cov gave me a red mark with a 0% diff. Everyhing else is green, merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2906#issuecomment-344821296:108,green,green,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2906#issuecomment-344821296,1,['green'],['green']
Energy Efficiency,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3106,MONITOR,MONITORING,3106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,1,['MONITOR'],['MONITORING']
Energy Efficiency,"Ok thanks, would you take charge of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102:26,charge,charge,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102,1,['charge'],['charge']
Energy Efficiency,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:127,schedul,schedule,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,3,['schedul'],['schedule']
Energy Efficiency,"Probably, depending on what you mean exactly. The real impetus is for monitoring tools though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327:70,monitor,monitoring,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327,1,['monitor'],['monitoring']
Energy Efficiency,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After theyâ€™ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:12,Green,Green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,1,['Green'],['Green']
Energy Efficiency,"Re this PR, I'm re-working it to pass all of the task call-specific monitoring info (workflow name/id, task call name/index/attempt, `inputs`, `disks`, and the opaque image-specific `monitoringConfig` string) through a JSON config file on GCS (thanks @kshakir for the idea!). This way, we no longer have to use environmental variables, and can pass much more data than can be held in a variable (looking at you, `inputs`!). That still has a potential problem of running against the API quota for GCS, but since we already access multiple files from that execution bucket inside the task, the assumption is that accessing one extra file won't hurt. However, I'm unsure if we should just get rid of all of the environmental variables in favor of one that points to the GCS URL of the JSON config, or should we keep them (and thus duplicate information) for backwards compatibility. AFAIK, hardly anyone used this feature yet, especially given that the current ""official"" `monitoring_image` has been essentially broken, because Google decided to start failing Stackdriver Monitoring requests if they are submitted more than 1/minute, for a given time series. We should fix the `monitoring_image` separately (or replace it with the BigQuery version), but the question remains whether we still want to keep those env vars around for that previous use case (for anyone who might've been using a custom `monitoring_image` already). Alternatively, we could pass only the `inputs` and `monitoringConfig` through a JSON blob on GCS, and continue with the rest as-is via env vars (but that's messy, IMO).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400:68,monitor,monitoring,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400,4,"['Monitor', 'monitor']","['Monitoring', 'monitoring', 'monitoringConfig']"
Energy Efficiency,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:233,allocate,allocated,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336,2,['allocate'],['allocated']
Energy Efficiency,"Reproduced the assembly failure in Travis on a different branch that was green last night. i.e., the problem isn't here, so I'm merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710:73,green,green,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710,1,['green'],['green']
Energy Efficiency,"Since Jeff is probably busy, this is the ticket where hopefully @Horneth can take his place. I'll switch out the assignments to reduce confusion...sorry @geoffjentry!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217962409:128,reduce,reduce,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217962409,1,['reduce'],['reduce']
Energy Efficiency,Since it told me to merge #80 first I did so. If this flips back green afterwards before I leave I'll merge this too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/79#issuecomment-118390659:65,green,green,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/79#issuecomment-118390659,1,['green'],['green']
Energy Efficiency,"So, quick thoughts on this. +1 on @delagoya's sidecar container concept (with some concerns I'll mention below). When implementing #3835 I had this issue in mind as well, along with some thoughts on implementation. My thought was to have a sidecar always running, using the [system events](https://docs.docker.com/engine/api/v1.37/#operation/SystemEvents) api to monitor for containers that have exited. At that time, it can [inspect the container](https://docs.docker.com/engine/api/v1.37/#operation/ContainerInspect), make sure it's a cromwell container, and use the volume information (in conjunction with the TASK_ID environment variable I'm setting) to find the local files and copy them out to s3. Something to consider that I don't see discussed yet on this thread: A sidecar approach requires the sidecar to have fairly permissive access to S3. On the positive side, this does alleviate the need for S3 permissions on the container running the task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435:363,monitor,monitor,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435,1,['monitor'],['monitor']
Energy Efficiency,"Still working on getting full test suite to pass, but DBMS test is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281:67,green,green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281,1,['green'],['green']
Energy Efficiency,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:399,reduce,reduce,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,2,['reduce'],['reduce']
Energy Efficiency,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:289,charge,charged,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766,1,['charge'],['charged']
Energy Efficiency,"TL;DR the monitoring script actually does work, but Cromwell and Centaur need a bit of tweaking for this to be testable. I added a `sleep` to the Centaur test per @Horneth's suggestion and the monitoring log does get written. However Cromwell never creates a metadata event with the path to the monitoring log. Also the test needs to know where to expect the monitoring log, which is actually a function of the workflow root which Centaur currently doesn't know. I'd like to fix this by publishing the workflow root as a metadata event and then making Centaur wise to it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226891562:10,monitor,monitoring,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226891562,4,['monitor'],['monitoring']
Energy Efficiency,"Talked to @jsotobroad in person.; - Kamon was removed in develop, but no hotfix needed because; - Green team changed their `java ...` script to not invoke the kamon code. No longer seems to be an issue. Closing and removing my branch that, in parallel, tested the kamon removal against a stale version of 0.19_hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661:98,Green,Green,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661,1,['Green'],['Green']
Energy Efficiency,"Tests are in, waiting for green!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394841642:26,green,green,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394841642,1,['green'],['green']
Energy Efficiency,Tests are now green! Had to double the amount of changes from of the original commit to fix some logging issues.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3030#issuecomment-350478429:14,green,green,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3030#issuecomment-350478429,1,['green'],['green']
Energy Efficiency,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:186,schedul,schedulers,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121,4,['schedul'],"['schedule', 'schedulers']"
Energy Efficiency,Thanks ! I squashed the commits and will merge when travis is green if there is nothing else.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137745619:62,green,green,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137745619,1,['green'],['green']
Energy Efficiency,"Thanks @aednichols, green tick against SBT now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426:20,green,green,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426,1,['green'],['green']
Energy Efficiency,Thanks @illusional! I dont think will be an option for us. Given the variable nature of the number of genomes we are going to be dealing with we are eventually going to hit this again even if we did reduce our reliance on the read functions. It feels like it needs an option in the config to increase the timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311:199,reduce,reduce,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311,1,['reduce'],['reduce']
Energy Efficiency,"Thanks @kshakir, that's awesome! Swagger's expressive power didn't seem able to capture the shape of the returned data, so having the example restored will I'm sure be much appreciated by Blue. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-145079262:54,power,power,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-145079262,1,['power'],['power']
Energy Efficiency,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:501,reduce,reduce,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1603,charge,charges,1603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738,1,['charge'],['charges']
Energy Efficiency,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:217,reduce,reduce,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for the comment Paul -- we've already started those discussions with the Google folks. We want this built deep into the Pipelines API as true notifications via pubsub which seems a reasonable feature. It's possible to do what you say yourself (using a GCE node(s) to query and then post) but with 100,000s of operations in flight it's much more efficient to be notified than poll... but I'm sure you agree with that ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282:352,efficient,efficient,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for your contribution!. I realize we didn't give much of an explanation for the ""hold"" label - we describe our process for external contributors here: https://github.com/broadinstitute/cromwell/blob/develop/CONTRIBUTING.md. In a nutshell, reviewing PRs is time-intensive and we have to work it into our team schedule alongside everything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4943#issuecomment-493122445:315,schedul,schedule,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4943#issuecomment-493122445,1,['schedul'],['schedule']
Energy Efficiency,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history ðŸ˜¡ so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:23,Green,Green,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,1,['Green'],['Green']
Energy Efficiency,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:81,schedul,scheduling,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242,1,['schedul'],['scheduling']
Energy Efficiency,"That's a cool use of this configuration to work around this issue !. I just want to give some context around it. This rate control was originally put in place to protect Cromwell against excessive load or a very large spike of jobs becoming runnable in a short period of time. Through this mechanism Cromwell can also stop starting new jobs altogether when under too heavy load.; While this achieve the desired effect of rate limiting how many submit requests are being sent to AWS batch in a period of time, I think a medium-term better fix is too implement something similar to the [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) for the AWS backend.; The reason is that it acts as a coarse level of granularity which might have undesired side-effects:. 1) it is a system wide configuration, meaning in a multi backend Cromwell it might be too constraining for some backends and not enough for others; 2) It also rate limits starting jobs that might actually be call cached and incur 0 requests to AWS Batch, making it too conservative; 3) It only helps rate limiting the number of job creation requests to batch. Once a job is started, it issues status requests to monitor the job which aren't throttled. Not to say that this is a bad workaround, I think it's a *good* workaround, but still a workaround :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181:1341,monitor,monitor,1341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181,1,['monitor'],['monitor']
Energy Efficiency,"The 10K call caching run revealed that we still spend a lot of time doing in this area, even with scheduled time based calls to `runnableCalls`. ![screen shot 2017-04-19 at 2 09 51 pm](https://cloud.githubusercontent.com/assets/2978948/25194952/1bd7ee2e-250a-11e7-9eff-cef3b3f78c4f.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255:98,schedul,scheduled,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255,1,['schedul'],['scheduled']
Energy Efficiency,"The build failures are in unrelated Swagger tests, rebasing on develop should get this green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941:87,green,green,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941,1,['green'],['green']
Energy Efficiency,The point of the setting is to protect the server from users. Putting the power directly in the hands of the users seems unwise,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338406950:74,power,power,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338406950,1,['power'],['power']
Energy Efficiency,The power of laziness compels me! :P,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4002#issuecomment-412943323:4,power,power,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4002#issuecomment-412943323,1,['power'],['power']
Energy Efficiency,"There's another concern here that PAPIv2 doesn't ([currently](https://groups.google.com/forum/#!topic/google-genomics-discuss/newaE3R-cwY)) terminate background containers nicely once all non-background actions terminate. I.e. it uses SIGKILL, which cannot be caught in our monitoring container, and hence so far it has not been able to report the last timepoint. However, to work around that I'll add one more action, which will be _non-background_, and will run after the user action. This 2nd action will be assigned to the same `pidNamespace` as the monitoring action (which possible with PAPIv2), and hence will have access to the PID of the monitoring action and will then kill it ""nicely"" with SIGTERM, and wait for it to terminate - something like this: `killall python && wait`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901:274,monitor,monitoring,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901,3,['monitor'],['monitoring']
Energy Efficiency,"These light green ones don't seem to have any parents:. <img width=""881"" alt=""Screen Shot 2019-11-07 at 4 06 57 PM"" src=""https://user-images.githubusercontent.com/13006282/68427727-bbff5e80-0178-11ea-9f81-b47efeb616e9.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545:12,green,green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545,1,['green'],['green']
Energy Efficiency,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:26,reduce,reduce,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088,1,['reduce'],['reduce']
Energy Efficiency,This is a [Not Acceptable](http://stackoverflow.com/questions/14251851/what-is-406-not-acceptable-response-in-http) error. The content types that Green said it would accept in the response were not compatible with the type of response Cromwell produced. It may be that Cromwell experienced a real error (which was hopefully logged) and returned a text/plain error response which I don't think you'd accept.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028:146,Green,Green,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028,1,['Green'],['Green']
Energy Efficiency,"This is a recent change which broke it if that's the case, which should be impossible as we only push when green",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483295408:107,green,green,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483295408,1,['green'],['green']
Energy Efficiency,"This should go green soon and be good to merge. I'll submit the master patch tomorrow, unless someone else gets to it first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862,1,['green'],['green']
Energy Efficiency,"To add some more context, not having this ability makes it difficult for us to use the On Hold status for queuing in Mint without a lot of overhead. In more detail:. We have a new service called Falcon that periodically queries our workflow collection in CaaS and starts the oldest On Hold workflow. Right now we have no choice but to query for *all* on hold workflows in each cycle, which for scale testing and production will be thousands of workflows -- even though we just want the oldest one or a few of the oldest ones. We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way. It would be much more efficient if we could ask Cromwell to reverse the order and get just the first page of say the 10 oldest On Hold workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522:744,efficient,efficient,744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522,1,['efficient'],['efficient']
Energy Efficiency,"To stick to our launch schedule for the GATK-aaS alpha, I'd really like to see this functionality (enable Cromwell server to treat SIGINT as abort) go in this week. . Is that going to be feasible? Who should David sync with to figure out how best to do it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937:23,schedul,schedule,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937,1,['schedul'],['schedule']
Energy Efficiency,"Travis doesn't have ignored-test-results afaik, nor support for a dynamic yaml test matrix based on internal-vs.-external contributions. If one knows of a way to do what the ticket wants, ignore vs. pass, I'll happily incorporate that fix. Otherwise, this PR reduces red-fatigue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987:259,reduce,reduces,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987,1,['reduce'],['reduces']
Energy Efficiency,"Useful tidbit to note, [this is how you signal failure due to unimplemented functionality](https://github.com/common-workflow-language/common-workflow-language/pull/278/files#diff-ee814a9c027fc9750beb075c283a973cR49) - which I believe means that one can still be ""green"" on CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542:264,green,green,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542,1,['green'],['green']
Energy Efficiency,Waiting for the tests to turn green-- will take off the EarlyFeedbackRequested label at that point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005:30,green,green,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005,1,['green'],['green']
Energy Efficiency,"We can make it happen. We should schedule the PR update. I suspect if one were to touch the `changelog.xml` in develop, it would cause a merge conflict here in this PR, preventing the two changes from being _accidentally_ merged. I would need to rebase this branch on top of any other changes. Specifically, the new liquibase file would need to be moved, along with updating the moved `changelog.xml`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400:33,schedul,schedule,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400,1,['schedul'],['schedule']
Energy Efficiency,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:95,green,green,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232,2,['green'],['green']
Energy Efficiency,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:933,schedul,scheduler,933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,1,['schedul'],['scheduler']
Energy Efficiency,"We just reduced our use of read functions, usually removing globs helped a lot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823928232:8,reduce,reduced,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823928232,1,['reduce'],['reduced']
Energy Efficiency,"We've got an ongoing situation with Green where their Cromwell halts workflow processing with zips after a certain amount of time https://github.com/broadinstitute/cromwell/issues/4117. . Our current hypothesis is that they are running out of disk space for unzipped imports. Their Cromwell already had hundreds of MBs worth within a few hours after restart (which, because of the way they Docker, clears the disk). @ApChagi @tbl3rd @hjfbynara @tlangs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628:36,Green,Green,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628,1,['Green'],['Green']
Energy Efficiency,"We've merged a few improvements based on the investigation (due in Cromwell 37), but we cannot guarantee this specific problem is fixed. The root cause is proving elusive and we find ourselves needing to pause for now due to schedule pressure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218:225,schedul,schedule,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218,2,['schedul'],['schedule']
Energy Efficiency,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:183,efficient,efficient,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385,1,['efficient'],['efficient']
Energy Efficiency,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:102,monitor,monitoring,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608,1,['monitor'],['monitoring']
Energy Efficiency,"What can we do to make sure this gets done? We need this for FC users (specifically) right now -- basically want to be able to capture VPC traffic. So FC and Compliance is the driver. Since this is cross-team, who owns the work? Who can get it on the schedule? FC PAPI nodes should be launched in a Subnet, is the main issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-434326020:251,schedul,schedule,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-434326020,1,['schedul'],['schedule']
Energy Efficiency,What is the affect of the current EJEA versus a reduced EJEA on Cromwell and the user? What do you mean by developer sanity? What would the effort be to reduce EJEA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912:48,reduce,reduced,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912,2,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"What's here looks good, but our tech talk left me wondering if this is actually going to address the problems the ticket is looking to solve. i.e., if we tell people there's this new `sub()` function but when they try to write real-world WDLs that use it they can't because of existing language restrictions that aren't really related to the changes here. I'm thinking we might want to ask Green for samples of their workflows that have been `sub`bed and see if Cromwell can deal with them on this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200489510:390,Green,Green,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200489510,1,['Green'],['Green']
Energy Efficiency,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:56,power,power,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210,2,['power'],['power']
Energy Efficiency,"When deploying Cromwell on our HPC which uses slurm as a scheduler, I use a wckey unique to a workflow and identical across tasks in that workflow - very handy for searching for failed jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095:57,schedul,scheduler,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095,1,['schedul'],['scheduler']
Energy Efficiency,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:130,monitor,monitoring,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,4,"['Monitor', 'monitor']","['MonitoringAction', 'monitoring']"
Energy Efficiency,"While this is investigated, you should be able to work around this by moving the constant string outside the `${}`s, eg ; ```; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048:195,adapt,adapters,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048,2,['adapt'],['adapters']
Energy Efficiency,"Why not â€œUsing containers with Cromwellâ€? Have a section for singularity (and undocker), and have a subsection of that for singularity with job schedulers?. There are other Cromwell github threads that have the user goal as practically: I canâ€™t use docker, what do I do? Thatâ€™s definitely a part of my use case, and reproducibility is a big point as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066:144,schedul,schedulers,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066,2,['schedul'],['schedulers']
Energy Efficiency,Would it be easy to set it for _all_ text outputs? The other one I'm thinking about is `monitoring.log`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368:88,monitor,monitoring,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368,1,['monitor'],['monitoring']
Energy Efficiency,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:586,efficient,efficient,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039,1,['efficient'],['efficient']
Energy Efficiency,Yeah I recall Jeff tried a simpler setup like this first and then had to resort to nesting Cromwells when that didn't work. But clearly Cromwell has become that much more efficient so now it does work hooray!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423:171,efficient,efficient,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423,2,['efficient'],['efficient']
Energy Efficiency,"Yeah weird, I don't have that much trust in coverall honestly, because although it's -0.02 it's also green..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054459:101,green,green,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054459,1,['green'],['green']
Energy Efficiency,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:373,schedul,scheduler,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,2,['schedul'],['scheduler']
Energy Efficiency,"Yes. On Mon, Oct 29, 2018 at 10:45 AM Thib <notifications@github.com> wrote:. > Hi !; > Just to make sure I understand, are you saying that the monitoring log is; > not copied over when a call is being cached ?; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/Ao_T2PcOQlD_TvcUcH4sutd4vJ9OdN8mks5upxSngaJpZM4X_RGI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423:144,monitor,monitoring,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423,1,['monitor'],['monitoring']
Energy Efficiency,"You could set `preemptible` very high to minimize the chance of preemption. I don't think there would be any issue setting it to 10 or even more. That said, it can be a bit of a false economy because failed attempts still cost real money. It may even be the case that falling back to non-preemptible saves money. Let's say preemptibles are $1 an hour and normal VMs are $3. If you run a 12 hour task that gets preempted 6 times at the 6 hour mark, that's 6 x 6 x $1 = $36 down the drain, a day and a half of wall clock time, and no results to show for it. Whereas a single non-preemptible run would be 12 x $3 = $36 and you'd have your results. Obviously this math will vary widely by use case and you will have to observe your preemption rates in practice to come up with the optimal balance. Thanks for an interesting discussion, I had never thought about the ""only preemptible"" use case before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650:481,drain,drain,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650,1,['drain'],['drain']
Energy Efficiency,_Big picture:_ This PR introduces a `CallDescriptor` wrapper around some other stuff. How do you envision this to be of use in PBE? Do you see this as turning into a `TaskDescriptor`? Does it reduce any effort later on?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148:192,reduce,reduce,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148,1,['reduce'],['reduce']
Energy Efficiency,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16256,adapt,adapted,16256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Energy Efficiency,bb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99771,adapt,adapted,99771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['adapt'],['adapted']
Energy Efficiency,"cess by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:62); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:4311,adapt,adapted,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['adapt'],['adapted']
Energy Efficiency,"dArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5291,Schedul,ScheduledThreadPoolExecutor,5291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4436,Power,Powered,4436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,1,['Power'],['Powered']
Energy Efficiency,"ely polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycle",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1948,schedul,schedules,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,2,['schedul'],['schedules']
Energy Efficiency,"from Ruchi. > Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information.; > ; > We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-232424253:405,reduce,reduce,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-232424253,1,['reduce'],['reduce']
Energy Efficiency,fwiw - green team currently run their own cromwell instances - so they are not currently impacted by anything we do on WB prod for their pipelines. Plus they generally are not as aggressive at taking newer cromwell versions - so even if you disable (remove) that endpoint they would likely not see the results for quite a bit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351:7,green,green,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351,1,['green'],['green']
Energy Efficiency,"gotcha on the need for an `optional` type. I see your points here. At a high level, my concern is making the response from this API easily parseable - and the large variety of key names (optionalType, arrayType, valueType, etc.) adds some complexity. I like @aednichols ' suggestion of using e.g. `innerType` where possible to reduce the set of distinct key names.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-444197460:327,reduce,reduce,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-444197460,1,['reduce'],['reduce']
Energy Efficiency,"hat I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1032,schedul,scheduled,1032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['schedul'],['scheduled']
Energy Efficiency,"hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1103,adapt,adapted,1103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,1,['adapt'],['adapted']
Energy Efficiency,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:621,reduce,reduce,621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,2,['reduce'],['reduce']
Energy Efficiency,is this related to long overheads I'm seeing in a 900 way scatter in firecloud (as I see that firecloud is on 24 now...). https://portal.firecloud.org/#workspaces/broad-ccdg-dev%3AFunctionallyEquivalent-CCDG/Monitor/97002137-7006-47a6-90c8-faf471f0b2d1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755:208,Monitor,Monitor,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755,1,['Monitor'],['Monitor']
Energy Efficiency,"it for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cf000 nid=0x9f7 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cd000 nid=0x9f6 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE; ""C1 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cb000 nid=0x9f5 waitin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5973,monitor,monitor,5973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,"k.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5341,Schedul,ScheduledThreadPoolExecutor,5341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1110,schedul,scheduler,1110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['schedul'],['scheduler']
Energy Efficiency,"localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:24",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1133,adapt,adapted,1133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,1,['adapt'],['adapted']
Energy Efficiency,"lure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatemen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2039,adapt,adapted,2039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['adapt'],['adapted']
Energy Efficiency,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2474,schedul,scheduling,2474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,2,['schedul'],['scheduling']
Energy Efficiency,"mwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CommandLineParser$.main(CommandLineParser.scala:8); 	at cromwell.CommandLineParser.main(CommandLineParser.scala); Caused by: java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:2257,adapt,adapted,2257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['adapt'],['adapted']
Energy Efficiency,ncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.bac,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3777,Schedul,Scheduler,3777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Schedul'],['Scheduler']
Energy Efficiency,"ncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8545,adapt,adapted,8545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Energy Efficiency,"oh nvm, that one was exceeding the maximum log length. in any case there are some systemic test issues that we are working on so this PR should go green â€“ or show evidence of a PR-specific problem â€“ in due time",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483:147,green,green,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483,1,['green'],['green']
Energy Efficiency,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:27,reduce,reduce,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,2,"['Green', 'reduce']","['Green', 'reduce']"
Energy Efficiency,rThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGenera,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4339,adapt,adapted,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['adapt'],['adapted']
Energy Efficiency,"re_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1963,adapt,adapted,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['adapt'],['adapted']
Energy Efficiency,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:110,green,green,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,1,['green'],['green']
Energy Efficiency,scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5983,Adapt,AdaptedForkJoinTask,5983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:74,monitor,monitoring,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509,2,['monitor'],['monitoring']
Energy Efficiency,"tion [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b8800 nid=0x9f0 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b6800 nid=0x9ef waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b4800 nid=0x9ee waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Threa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:8615,monitor,monitor,8615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,tion.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$eva,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2077,adapt,adapted,2077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Energy Efficiency,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1239,schedul,scheduling,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,1,['schedul'],['scheduling']
Energy Efficiency,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4433,Power,Powered,4433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,1,['Power'],['Powered']
Energy Efficiency,"two thumbs, green tests, and it's holding up workbench... so merging!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360:12,green,green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360,1,['green'],['green']
Energy Efficiency,un$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905213- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 905214- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:7168,Adapt,AdaptedForkJoinTask,7168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"utor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5455,Schedul,ScheduledThreadPoolExecutor,5455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?â€‚â€‚(per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:604,monitor,monitoring,604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,2,['monitor'],['monitoring']
Energy Efficiency,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:150,monitor,monitor,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010,10,['monitor'],['monitor']
Energy Efficiency,"wth is going on with Coveralls, we get green ticks for -0.2% now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210132769:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210132769,1,['green'],['green']
Energy Efficiency,"x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.la",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9096,monitor,monitor,9096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4245,Power,Powered,4245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758,1,['Power'],['Powered']
Energy Efficiency,ðŸ‘ LGTM pending Travis greenness!. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4523/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4523#issuecomment-452067058:22,green,greenness,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4523#issuecomment-452067058,1,['green'],['greenness']
Energy Efficiency,"ðŸ‘ but it'd be nice to get travis to go green before merging, even if the current failure is unrelated to this change. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2121/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291,1,['green'],['green']
Energy Efficiency,ðŸ–– ðŸ‘ . I thought there were more than these but I guess not. Didn't have the energy to go back and do them again once I abandoned my first branch :). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2433/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253:76,energy,energy,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253,1,['energy'],['energy']
Integrability," CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2384,message,message,2384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,integrat,integrate,6283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['integrat'],['integrate']
Integrability, akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpret,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3350,Rout,RouteConcatenation,3350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability," by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,221 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:24,401 INFO -; > WorkflowExecutionActor-7e3f9b56-790b-481b-a8d9-f24e88883ed5; > [UUID(7e3f9b56)]: Starting pb_assembly_hifi.generate_config; > 2023-03-06 17:17:28,549 INFO -; > 13f1ea7a-4f35-41ea-9afa-f8fc84d083b0-SubWorkflowActor-SubWorkflow-prepare_input; > ðŸ‘Ž1 [UUID(13f1ea7a)]: Starting prepare_input.dataset_filter; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(7e3f9b56)pb_assembly_hifi.generate_config:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(13f1ea7a)prepare_input.dataset_filter:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/7085>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABCR6IDB7UG3LW47K526FULW22MK5ANCNFSM6AAAAAAVR4KY5U>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667:6266,Message,Message,6266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667,1,['Message'],['Message']
Integrability," is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6947,depend,dependency,6947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,['depend'],['dependency']
Integrability," older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") inst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4321,wrap,wrapped,4321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,2,['wrap'],['wrapped']
Integrability," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1831,message,message,1831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,2,['message'],['message']
Integrability,"![19grnh](https://cloud.githubusercontent.com/assets/791985/17954234/7ba0b698-6a47-11e6-873c-c0e60ca163e1.jpg). I'd be game if all `java.nio.Path`s across cromwell, including the engine, all backends, services, etc. were always absolute. A relative path appearing in a web response, a shell script, or even the logs would then be considered a bug. In JES, there are internal private methods such as `JesAsyncBackendJobExecutionActor.relativeLocalizationPath` that create relative paths, but these relative paths should not be externally visible. `JesAsyncBackendJobExecutionActor.jesInputsFromWdlFiles` should be creating absolute paths. Over in the SFS, the `SharedFileSystemAsyncJobExecutionActor` doesn't create relative paths, but still inconsistently calls `Path.toAbsolutePath` in various places. More usage of `better.files` instead of `java.nio.Paths.get()` would help us from omitting calls to `Path.toAbsolutePath`, since [better's `.path` member](https://github.com/pathikrit/better-files#java-interoperability) creates absolute `java.nio.Path`s (for [now](https://github.com/pathikrit/better-files/issues/48#issuecomment-157837460)). **TL;DR This might also be a problem elsewhere, including the SFS backend.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539:1005,interoperab,interoperability,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539,1,['interoperab'],['interoperability']
Integrability,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:87,message,messages,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,2,"['Message', 'message']","['Message', 'messages']"
Integrability,(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.uti,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2158,Rout,RouteConcatenation,2158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"(ToL, of course). I _still_ don't always know where to draw the line between creating a new `akka..Actor` and using a `scala..Future`. Something does ""smell"" funny though about the way we:; - Queue of things-to-do is on the `ec: ExecutionContext`; - A mailbox `message: AnyRef` is received off the `ec` by the dispatcher and passed to our `actor: Actor`.; - Instead of running a `runnable: Runnable` bit of code immediately, the `actor` chooses to throws the `runnable` onto the back of the `ec` queue and then say ""done processing `message`"". That said, [this blog](https://www.chrisstucchio.com/blog/2013/actors_vs_futures.html) seems to say that `Actor` and `Future` can work together, but maybe something is off about how we're composing them in our `BackendLifecycleActor` interfaces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034:261,message,message,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034,3,"['interface', 'message']","['interfaces', 'message']"
Integrability,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:433,depend,depends,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503,1,['depend'],['depends']
Integrability,"(ðŸ‘ assuming ""force-pushed the mlc_scala_steward_5640_5649 branch from aeaa1f3 to 7f28d1a yesterday"" represents rebasing onto the previously merged dependency bumps)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870:147,depend,dependency,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870,1,['depend'],['dependency']
Integrability,") = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1854,depend,dependencies,1854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['depend'],['dependencies']
Integrability,"* The ""no zip bundle"" set of import resolvers is probably the right one to use when trying to ""resolve"" the original file from String (unless a dependencies zip is *also* supplied?).; * Side comment: should we therefore make the set of import resolvers to use *Cromwell's* responsibility? (right now every language factory comes up with its own set ðŸ˜±); * We should try end up as similar to the WES equivalent as possible when adding this; * To find out: is this just `submit` with `workflowRoot` + no `workflowSource`, or is it a separate endpoint?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3849#issuecomment-403963315:144,depend,dependencies,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3849#issuecomment-403963315,1,['depend'],['dependencies']
Integrability,"**TL;DR Discussed in person with @ruchim. Going to ðŸ‘ , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:123,rout,route,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342,2,['rout'],['route']
Integrability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = â€¦ ++ assemblySettings ++ â€¦`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:1033,depend,depending,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,1,['depend'],['depending']
Integrability,+1 to not wrapping successes. Also +1 to not disturbing the return values without telling blues ahead of time.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171075925:10,wrap,wrapping,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171075925,1,['wrap'],['wrapping']
Integrability,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:23,message,message,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016,2,['message'],['message']
Integrability,",035 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowManagerActor Restarting workflow IDs: 0bbd4dc6-bfa5-4ccb-bcdf-bb3a62bbc24b, 81182fb4-22b5-460b-b78e-72f7750aa598, abafd1cc-977a-47d6-acce-c1b2907829a8, ea0272fc-42ef-4852-8143-8b14d34bfd8a. 2016-04-26 18:26:08,700 cromwell-system-akka.actor.default-dispatcher-11 INFO - Invoking restartableWorkflow on ea0272fc; 2016-04-26 18:26:08,700 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowManagerActor submitWorkflow input id = Some(ea0272fc-42ef-4852-8143-8b14d34bfd8a), effective id = ea0272fc-42ef-4852-8143-8b14d34bfd8a; 2016-04-26 18:26:08,772 cromwell-system-akka.actor.default-dispatcher-10 INFO - Updating WorkflowManager state. New Data: (ea0272fc-42ef-4852-8143-8b14d34bfd8a,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-ea0272fc-42ef-4852-8143-8b14d34bfd8a#787056469]); 2016-04-26 18:26:08,773 cromwell-system-akka.actor.default-dispatcher-3 INFO - WorkflowActor [UUID(ea0272fc)]: Restart message received; 2016-04-26 18:26:09,112 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial symbols:. 2016-04-26 18:26:09,129 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial executions:. 2016-04-26 18:26:09,156 cromwell-system-akka.actor.default-dispatcher-2 INFO - WorkflowActor [UUID(ea0272fc)]: ExecutionStoreCreated(Restart) message received; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Beginning transition from Submitted to Running.; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: transitioning from Submitted to Running.; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: starting calls: GenomeStripBamWorkflow.ComputeMetadata, GenomeStripBamWorkflow.ComputeStatistics; 2016-04-26 18:26:09,646 cromwell-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:1159,message,message,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['message'],['message']
Integrability,- what version of cwltool is used and versions of transitive dependencies if known,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-394008620:61,depend,dependencies,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-394008620,1,['depend'],['dependencies']
Integrability,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:6302,message,messages,6302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['message'],['messages']
Integrability,".jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4828,message,message,4828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,".scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5943,message,messages,5943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['message'],['messages']
Integrability,".services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquiba",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4634,message,message,4634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,"11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2447,message,message,2447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['message'],['message']
Integrability,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11273,message,message,11273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2098,message,message,2098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['message'],['message']
Integrability,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:190,message,message,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['message'],['message']
Integrability,36. I also edited the original message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466543684:31,message,message,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466543684,1,['message'],['message']
Integrability,"4-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658]))) message received; [2016-01-31 16:37:28,789] [info] SingleWorkflowRunnerActor: workflow ID 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,798] [info] WorkflowActor [2a89a995]: Beginning transition from Submitted to Running.; [2016-01-31 16:37:28,800] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-2a89a995-aa89-4172-a5e1-1054cbccd9e0#2034772397],Submitted); [2016-01-31 16:37:28,800] [info] WorkflowActor [2a89a995]: transitioning from Submitted to Running.; [2016-01-31 16:37:28,801] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-31 16:37:28,804] [info] WorkflowActor [2a89a995]: starting calls: w.hello; [2016-01-31 16:37:28,805] [info] WorkflowActor [2a89a995]: persisting status of hello to Starting.; [2016-01-31 16:37:28,959] [info] WorkflowActor [2a89a995]: inputs for call 'hello':; addressee -> WdlString(String); [2016-01-31 16:37:28,962] [info] WorkflowActor [2a89a995]: created call actor for hello.; [2016-01-31 16:37:28,970] [info] WorkflowActor [2a89a995]: persisting status of hello to Running.; [2016-01-31 16:37:28,996] [info] LocalBackend [2a89a995:hello]: echo ""Hello String!"" && kill -SIGINT $BASHPID; [2016-01-31 16:37:29,19] [info] LocalBackend [2a89a995:hello]: command: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:2346,message,message,2346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['message'],['message']
Integrability,42b47 | callCaching:hashes:runtime attribute:docker | test.hello | NULL | 1 | 66E19F14150E71B0E42CA8557A69C5F9 | 2018-11-21 15:09:37.710000 | string |; | 4775 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hashes:runtime attribute:failOnStderr | test.hello | NULL | 1 | 68934A3E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-val,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1082,message,message,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['message'],['message']
Integrability,45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:588); 	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:472); 	at ak,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3530,Rout,RouteConcatenation,3530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,6); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2983,Rout,RouteConcatenation,2983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.uti,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2098,Rout,RouteConcatenation,2098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,8.0_72]; 905154- at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; 905155- at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; 905156- at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; 905157- at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; 905158- at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; 905159- at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; 905160- at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; 905161- at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; 905162- at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; 905163- at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; 905164- at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; 905165- at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; 905166- at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; 905167- at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; 905168- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; 905169- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; 905170- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; 905171- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(Gcs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:1673,protocol,protocol,1673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['protocol'],['protocol']
Integrability,":+1: . Yeah this is going to change a lot in the shadow world, but these changes do eliminate a lot of db stuff from the backend, integrate retries on the upserts, and test the heck out of restarts. ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650:130,integrat,integrate,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650,1,['integrat'],['integrate']
Integrability,":+1: Although there is already another warning being issued a few lines below (`failMessage foreach { m => logger.warn(s""$m. $retryMessage"") }`), maybe the message could be incorporated in this one to try and limit the number of logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093:156,message,message,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093,1,['message'],['message']
Integrability,:+1: a bunch. It didn't take long to make me question why we tried to resolve a problem which involved doing too much stuff up front by adding even more stuff up front. . I was talking about something a lot like this to one of our two main internal customers (I believe FC) and they were good with the idea - however I'd say that we should make sure to have official buy in from our internal policy makers just to be sure. The dept itself seems to be learning to wrap its collective head around models where not everything is immediate & 100% consistent fashion so it should be an easy sell now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959:463,wrap,wrap,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959,2,['wrap'],['wrap']
Integrability,":59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). #",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1703,Message,Message,1703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['Message'],['Message']
Integrability,:frowning: Another argument for why we should integration test. Do we need to do a release?. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940,1,['integrat'],['integration']
Integrability,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8616,message,messages,8616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,6,['message'],['messages']
Integrability,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:160,synchroniz,synchronize,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350,1,['synchroniz'],['synchronize']
Integrability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,adapter,adapter,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['adapter'],['adapter']
Integrability,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:499,rout,routes,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096,1,['rout'],['routes']
Integrability,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:37,integrat,integration,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198,1,['integrat'],['integration']
Integrability,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:74,integrat,integrating,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460,1,['integrat'],['integrating']
Integrability,"> Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory"". Also have this error. Anyone figure out what the issue is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269:42,message,message,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269,1,['message'],['message']
Integrability,"> Futures are fine, just not in the code which the actor itself directly controls. Hmm pretty much all our relevant code lives in actors so if the only place where we can use `Future`s is outside of actors, then I'd say `Future`'s days are counted xD. The _burden_ of verifying that new code doesn't break anything is part of the review process anyway IMHO, and adding N more actors instead of `Future`s with twice as many more messages, states, transitions, and classes doesn't reduce the burden of checking that everything is wired correctly, as far as I'm concerned. . Also, about futures being dangerous in actors because they can mutate state, this can only happen in a true `Actor` where your state pretty much has to be `var`s if you want to be able to mutate it, or in an `FSM` where you would also store some state as mutable `var`s inside the actor, instead of using the FSM data.; If you're using an FSM and all your mutable data is contained in the FSM data, then I don't see how creating futures would ever lead to mutating your state (and by state I mean mutable data, not FSM state) asynchronously. The FSM data is only updatable when the actor receives a message and decides to change state or stay in the same. A future `onComplete` could never force the FSM to mutate its data, which is why all we do is always send a message to someone when the future completes. And I don't see what difference it makes, from an actor perspective, if a message comes from a `Future` or another actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592:428,message,messages,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592,4,['message'],"['message', 'messages']"
Integrability,"> Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code.; Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry. @cjllanwarne I found the problem. I did not have a `final_workflow_outputs_dir` set in the options.json files for the centaur tests. If this path is not set `use_relative_output_paths` is of course not used... :man_facepalming: That is fixed now and it works as expected. Colliding outputs will return as a workflow failure. Since I got the local testing working I was able to add more advanced tests and make sure these are correct as well. The error message is tested when the outputs are colliding. In order for that test to work I had to make the output order of colliding paths in the error message deterministic. (Otherwise it would fail randomly). Also my colleague @DavyCats showed me some centaur tests were file outputs are tested. I used these as an example to also test for the outputs. ; All the behavior that this PR affects is now properly tested, which means that these tests should be able to discover regressions in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876:687,message,message,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876,2,['message'],['message']
Integrability,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:313,message,message,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,1,['message'],['message']
Integrability,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:163,wrap,wrap,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746,1,['wrap'],['wrap']
Integrability,"> I haven't fully thought it through yet, but I'll keep something sorta like this in mind when we're dealing with tracking the WomFile.value to (cloud, vm, container) path split. I think we could use a mini brainstorm session on this. I'm finding other areas where it would be very nice to clarify this cloud / vm /container separation.; Namely `JobPaths` has become a huge messy melting pot over time and it would be awesome to have a clean `cloudCallRoot`, `vmCallRoot`, `containerCallRoot` for example (some of those that could be the same depending on the backend, w/ docker or not etc..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3629#issuecomment-389577205:543,depend,depending,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3629#issuecomment-389577205,1,['depend'],['depending']
Integrability,"> I think I've been viewing Singularity & Docker as more of an ""either/or"" in that perhaps a task would require a singularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:730,depend,dependency,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['depend'],['dependency']
Integrability,"> I'm almost wondering if they should be wrapped in distinct ""root"" and ""possibly not root"" types. Done, plus a squash and rebase. Will merge after tests pass unless the new types aren't what you were imagining.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048:41,wrap,wrapped,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048,1,['wrap'],['wrapped']
Integrability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:1584,adapter,adapter,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['adapter'],['adapter']
Integrability,"> If you are calling Cromwell in run mode, can you wrap it in a script followed by an AWS CLI command to copy the workflow log?. Yes -- I could do that. I was just thinking that workflow log should be automatically copied over since this is a cromwell workflow level log? But if it's too much to implement, the workaround will be just manually copying over it at the end of the workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617:51,wrap,wrap,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617,1,['wrap'],['wrap']
Integrability,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:476,message,message,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901,1,['message'],['message']
Integrability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:159,message,message,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['message'],['message']
Integrability,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:451,contract,contract,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823,1,['contract'],['contract']
Integrability,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:233,message,messages,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488,1,['message'],['messages']
Integrability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:107,depend,depending,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,1,['depend'],['depending']
Integrability,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,adapter,adapter,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,1,['adapter'],['adapter']
Integrability,"> So if I fix that in my conf, the messages should go away,; right?. Yes. > Can I specify docker.hash-lookup.method in the workflow_options?. No, if that's something you'd like feel free to create a github issue :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895:35,message,messages,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895,1,['message'],['messages']
Integrability,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:223,message,message,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655,1,['message'],['message']
Integrability,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:88,message,messages,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,1,['message'],['messages']
Integrability,"> TOL: To me this feels like itâ€™d be way neater if the EGIN had a field or def called inputFileName nameInInputSet, to encapsulate all this into the egin itself rather than having to add it later externally?. Good point, I was just reticent to the idea of jamming yet another attribute injected by the language that will only ever be used once during the lifetime of the workflow but I agree it's still neater. > FWIW in my ideal world weâ€™d have pluggable languages which should only need to define one function like â€œreadWorkflowIntoWom(content: String, l: Set[ImportResolver]): WomExecutableâ€ and everything else would be included/encapsulated in that result. Yes but there would be some non-DRYness by having each language implement entirely how they ingest inputs (most of the logic is the same), plus having it in WOM guarantees that all EGIN are handled the same way w.r.t coercion, validation etc.. It could use some refactoring though I agree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402:286,inject,injected,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402,1,['inject'],['injected']
Integrability,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:376,message,messages,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,1,['message'],['messages']
Integrability,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:6,message,messages,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139,1,['message'],['messages']
Integrability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:162,message,messages,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,['message'],"['message', 'messages']"
Integrability,"> This can happen if the job fails meaning that an rc.txt file isnâ€™t created. It would be worth looking at the CloudWatch log for the batch job.; > [â€¦](#); > On Tue, Jul 21, 2020 at 4:07 PM Sri Paladugu ***@***.***> wrote: Is there any progress on this issue? I am the getting the following exception: IOException: Could not read from s3:///results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt Caused by: java.nio.file.NoSuchFileException: s3:// s3.amazonaws.com/s3bucketname/results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#4687 (comment)](https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662079379)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMJZ66Z5PIAEUX3IBLR4XYPZANCNFSM4G23FFUQ> . Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952:965,message,message,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952,1,['message'],['message']
Integrability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:450,depend,dependency,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,2,['depend'],['dependency']
Integrability,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:640,depend,dependent,640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['depend'],['dependent']
Integrability,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:139,wrap,wrapping,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978,1,['wrap'],['wrapping']
Integrability,"> Why not use something like miniwdl?; > ; > run mode was originally created for **cromwell** development purposes, although for most of time there wasn't really an alternative for workflow development. Hi Geoff thanks for your suggestion. I have checked miniwdl but it has a docker dependency which does not fit my need for a sudo-less installation. The devs will certainly benefit from a quick REPL for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808:283,depend,dependency,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808,1,['depend'],['dependency']
Integrability,">If you're ok with waiting until the next release (likely 31, potentially 30.3), it'll also be fixed for you. Everything depends on how soon you are agoing to publish 30.3 If it is a week, I am ok to wait, if it will take longer - I will build from source",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273:121,depend,depends,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273,1,['depend'],['depends']
Integrability,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:72,depend,dependencies,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595,2,['depend'],['dependencies']
Integrability,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:189,message,message,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['message'],['message']
Integrability,@Horneth Could you update w/ a description of what's being fixed/improved/etc here and/or link to an issue? I can see the code but need help wrapping my head around what it's all doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222:141,wrap,wrapping,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222,1,['wrap'],['wrapping']
Integrability,"@Horneth I believe the transformation is more like `StateData + DB Calls` to `DB Calls + more DB Calls`. Currently, the AbortAll route just sends a msg to all the WorkflowActorRefs in the state data, and then waits's until the all the actors have responded with a terminal state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582:129,rout,route,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582,1,['rout'],['route']
Integrability,@Horneth I totally missed your message here. It was on the methods cromwell 30 instance. I'm not sure how to find logs. My guess is that if they're produced by default they're still available somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854:31,message,message,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854,1,['message'],['message']
Integrability,@Horneth No problem. I *might* scale up to 200k jobs in this workflow. Depends on how many samples I need.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547:71,Depend,Depends,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547,1,['Depend'],['Depends']
Integrability,"@Horneth The more I'm thinking about this I'm transitioning from ""throwing something out there"" to ""advocating"" :). What bothers me about this is that eventually we're going to want workflow submission also go through the same validation actor instead of doing it in multiple ways. Typically you want actor ownership to be hierarchical in nature but this way you'd have multiple parent types. Something to consider - what happens if a VA throws an exception under this model? You'll now need to have supervision code in multiple places. More abstractly what will be said is that validation is A Thing, but needs to exist independently in multiple places - if that's the case it should be pulled out into its own block. Furthermore the validation actor should be the sort of actor which is perfect for being its own concept - it's a completely idempotent, stateless operation. Work gets sent to it, it process the work and responds to the querier. My point about supervision is that by Right Now defining A Validation Actor (presumably owned by the kernel) what you're effectively doing is defining a validation interface. You can change how things are implemented in the future (e.g. it's really a bunch of VAs, it's firing up ephemeral VAs, whatever) and not need to change any code throughout the rest of the system as everything is still talking to the same actorRef that they were before. Alternatively if validation actors are being spun up on demand in multiple places and we decide that we somehow need to handle VAs in a special manner, it'll be a larger refactor. All that said, at the moment only the validation webservice is talking to the VA. I'm happy to shelve this until when something else is talking to VAs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168:1111,interface,interface,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168,1,['interface'],['interface']
Integrability,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:269,rout,router,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,1,['rout'],['router']
Integrability,"@Horneth check out DSDEEPB-2876. Having thought about this and having seen that new issue in our backlog (Add â€œreason(s) for failureâ€ to database, metadata) I think this would be better reported as a stringly-typed ""reason for failure"" error message rather than an explicit ""backend return code"" field. It would then be the job of the backend to convert its ""backend error code"" into a useable and sensible message to report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747:242,message,message,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747,2,['message'],['message']
Integrability,"@Horneth depends when the `Any` coercions happen. If they only happen reading the inputs files then it's actually already separated out I think (and ""coercion"" and ""input parsing"" could easily be separate functions IMO)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3430#issuecomment-373823553:9,depend,depends,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430#issuecomment-373823553,1,['depend'],['depends']
Integrability,"@Horneth my concern on `endpoint` is its overloaded with the REST API endpoints. Realizing that it's also not an **akka** router, it seems to be serving a similar role. Proxy also seems viable (granted you're using that term in your description, but not in the code)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-354340319:122,rout,router,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-354340319,1,['rout'],['router']
Integrability,"@Horneth my suggestion for this PR (happy to be overruled - @geoffjentry @kcibul) would be to get the return code being ONLY the return code. And forget the ""backend return code"" entirely for now until that upcoming ticket (maybe enhance that other ticket to add ""JES return codes appear in failure message"" as another AC?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682:299,message,message,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682,1,['message'],['message']
Integrability,"@Horneth what do you think the effort would be to add retries to the WDL functions? Does it depend on the function? This might get prioritized as a part of Joint Calling, but for now I'll leave it out of the retries improvement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294:92,depend,depend,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294,1,['depend'],['depend']
Integrability,"@IsanEmory thanks for the update, that's good to know. ; @ruchim can you tell me a bit more about what's going on to cause the message to show up?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786:127,message,message,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786,1,['message'],['message']
Integrability,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:123,message,message,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,1,['message'],['message']
Integrability,@MartonKN This is almost certainly not a real error but rather some annoying/alarming yet harmless Cromwell messages. Other than this does it appear that your workflow successfully completed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613:108,message,messages,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613,1,['message'],['messages']
Integrability,"@TMiguelT . I separated out the part that is failing into a separate WDL and tried running just that WDL with different inputs, it all failed with the same error message. The WDL is attached so that you can just run to see the error. [cromwell_4356.zip](https://github.com/broadinstitute/cromwell/files/2555124/cromwell_4356.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756:162,message,message,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756,1,['message'],['message']
Integrability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:346,inject,injected,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,1,['inject'],['injected']
Integrability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:311,message,message,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['message'],['message']
Integrability,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1954,wrap,wrap,1954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['wrap'],['wrap']
Integrability,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case â€¦ => { â€¦ }` could be `case â€¦ => â€¦`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:338,bridg,bridge,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,4,"['bridg', 'message']","['bridge', 'messages']"
Integrability,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:708,synchroniz,synchronized,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703,2,['synchroniz'],['synchronized']
Integrability,"@aednichols . Thanks for your response. We're in a University and the sys admins are worried that with users submitting thousands of jobs, depending on what cromwell actually sends MySQL there may be quite a bit of overhead. Do you have a link to what Cromwell stores in the MySQL database? That may assuage some of their concerns. Using SQLite would just be easier, users can create a local instance and be on with it. @rhpvorderman . That sounds like a workable option. That sounds exactly like our situation, it would be great if you could keep us updated! It would definitely be very useful for us!. Thanks,; Bobbie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678:139,depend,depending,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678,1,['depend'],['depending']
Integrability,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:82,integrat,integrate,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921,1,['integrat'],['integrate']
Integrability,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:55,message,message,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404,1,['message'],['message']
Integrability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:139,message,messages,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['message'],['messages']
Integrability,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:32,depend,dependency,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946,2,"['depend', 'inject']","['dependency', 'injection']"
Integrability,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:202,integrat,integration,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868,1,['integrat'],['integration']
Integrability,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:182,interface,interface,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,1,['interface'],['interface']
Integrability,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:62,message,message,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999,1,['message'],['message']
Integrability,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it ðŸ‘",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:96,message,message,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674,3,['message'],"['message', 'messages']"
Integrability,"@ccarrizo A call wraps a single command line and thus can only be run on a single backend, correct? Composed backends wouldn't apply for a call. We have a need to be able to report back certain backend specific values and from an operational/regulatory perspective we need to track other things for provenance and such. That information needs to be persisted somewhere - should the backends know about the DB (let's ignore for them oment that one backend does indeed talk directly to the DB)? If not that implies now need to know about the internal persistence. What if we decide to bulkhead all DB access behind a specialized actor/router? You can point to separating out the persistence but that's far beyond the scope of this work and is a separate block entirely. I viewed this as a transitional PR. There'd been a request from team members on our side for a while now that things be done as bit by bit as possible. To do that implies not having full & sweeping changes as the only way to do _that_ is one monolithic PR which causes a ton of problems. The whole process would be a bit smoother if you all could find other portions which could be folded directly into develop _now_ which started to move the ball in the right direction instead of requiring that everything be perfect in one fell swoop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303:17,wrap,wraps,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303,2,"['rout', 'wrap']","['router', 'wraps']"
Integrability,"@cjllanwarne :. > Everything has access to the ServiceRegistry. This premise still holds. Why do you think otherwise?. > Anyone (including the Engine) can send a MetadataPut message to the ServiceRegistry, which will forward it automagically to the MetadataService. This is how the magic is currently happening as well. The engine _is_ sending MetadataPut messages to the ServiceRegistry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124:174,message,message,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124,2,['message'],"['message', 'messages']"
Integrability,"@cjllanwarne @ruchim I've been going back and forth as to whether or not this retry count should replace or add on to the ""we know that's flakey so retried it for ya"" situation vs. adding on top of it. Chris, you seemed pretty sure it should be the latter, why do you view it that way? Ruchi, I know this was spec'd out to go the former route, but I think the person speccing it out didn't know that we already did some stuff - do you actually have an opinion here or is this just how you did it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379372182:337,rout,route,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379372182,1,['rout'],['route']
Integrability,"@cjllanwarne As you mentioned ""order is defined in WDL and DAG gets created based on inter-dependencies"" is true during execution of the task but not during initialization phase, hence that order is required during initialization as well. . @geoffjentry If i understand correctly there is no graph being received by the backend during initialization it is `Map[Call, String]` to identifies all calls @ backend and then ultimately `Seq[Call]`, what is expected from engine if user specifies calls as @cjllanwarne mentioned in above wdl example (a then b) that order is preserved and pass through. . RP backend in CCC depends upon order of calls during initialization as well therefore that change is required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600:91,depend,dependencies,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600,2,['depend'],"['dependencies', 'depends']"
Integrability,"@cjllanwarne Do you need a red review on this ? If so could you elaborate just a little on what ; > Allows reading of WDL 1.0 and 1.1 Asts through a shared set of CheckedAtoB functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. means for me poor mortal and / or point to relevant code that I should look at ? ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081:210,inject,inject,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081,1,['inject'],['inject']
Integrability,"@cjllanwarne GCS is not backend specific, which is actually what makes it possible to use GCS on local backend.; We could extract the ""File system logic"" from the WorkflowDescriptor but anything related to GCS will depend on a workflow descriptor because of all the auth stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299:215,depend,depend,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299,1,['depend'],['depend']
Integrability,"@cjllanwarne Hey Chris..in reference to your question from the previous PR.. checkout the Backend contract defined [here](https://github.com/broadinstitute/cromwell/blob/pluggable_backends/cromwell-backend/src/main/scala/cromwell/backend/BackendActor.scala). We'll need the individual backends to implement that contract, for instance how it's been done to [LocalBackend](https://github.com/broadinstitute/cromwell/blob/pluggable_backends/cromwell-backend/src/main/scala/cromwell/backend/provider/local/LocalBackend.scala). The older methods, such as adjustInputPaths() etc. can be clubbed together in, say, prepare() step. If you think we'll need more methods that can be defined in the BackendActor which are generic enough, please let us know.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174084987:98,contract,contract,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174084987,2,['contract'],['contract']
Integrability,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:139,depend,dependency,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063,1,['depend'],['dependency']
Integrability,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:177,integrat,integration,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,1,['integrat'],['integration']
Integrability,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:43,message,message,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584,1,['message'],['message']
Integrability,"@cjllanwarne I pointed to the wrong line of code, sorry for that. I have identified the bug. The refactoring produced **better** code. The code written before the refactoring created a rc file with exit code `9`(Which was probably a mistake as the comments above the code said that 137 was chosen, for kill -9). `137` for SIGKILL would have been the better value. The current refactored code uses SIGTERM (`143`). This looks nicer, but unfortunately the functionality of the code depended on the choice for `9`. . If cromwell gets SIGINT (`130`) , SIGKILL (`137`) or SIGTERM(`143`) as exit codes for a job, it assumes that cromwell was the one that aborted them and the jobs should NOT be retried. This makes perfect sense. . The refactored code now returns a return code(`143`) that makes cromwell believe that the job should not be retried. My solution would be to write a non-sensical return code in the case exit-timeout-seconds is used. I am working on a pr now. EDIT: This change indeed fixes the problem. PR coming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589:480,depend,depended,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589,1,['depend'],['depended']
Integrability,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:632,rout,router,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,1,['rout'],['router']
Integrability,"@cjllanwarne Indeed different containers might have different requirements, as it will depend on the container what mount points are available. I would like to point out, however, that you can already define this per task using a custom runtime attribute. For example, in my config I could put something like:; ```; runtime-attributes= """"""; String? docker; String? dockerMountPoint = ""/data""; """"""; dockerRoot = ""${dockerMountPoint}""; ```. EDIT: Hmm, nevermind, looks like that wouldn't work. In the submission command, it would, but you'll just end up with `${dockerMountPoint}` in the execution script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684:87,depend,depend,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684,1,['depend'],['depend']
Integrability,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:83,message,messages,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744,1,['message'],['messages']
Integrability,"@cjllanwarne On a high level, you can use Intellij to create a new scala project, add the dependencies (wdl4s and cromwell-backend) in the build.sbt from Broad's artefactory repository, copy the JES code folder from develop to this new project, modify the JesBackend to extend from BackendActor, honor the subsequent intellij warnings to implement the abstract methods, and done. The CallActor will control the flow of the backend, going from prepare() -> execute() -> cleanUp() -> stop() etc..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760:90,depend,dependencies,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760,1,['depend'],['dependencies']
Integrability,"@cjllanwarne Really it's just that they were developed simultaneously, and it's probably my fault for not going back and cleaning the other one up. In retrospect I knew it existed, but probably just lost track of it. I certainly wouldn't disapprove of converting future-based logic in actors into a more actor-y solution but that's because IMO it's easier to reason about multiple actors (and their messages) than composed Futures. My stance isn't one which is universally held, however",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998:399,message,messages,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998,1,['message'],['messages']
Integrability,"@cjllanwarne So if a runtime attribute is not supported, there is no warning message? What happens instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331:77,message,message,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331,1,['message'],['message']
Integrability,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:123,depend,depend,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509,4,['depend'],['depend']
Integrability,"@cjllanwarne Sort does not solve the purpose here because it is not what backend expecting and want,However it is expecting the right dependencies as specified in the wdl. . Because wdl writer may not right alphabetically or numeric order for call name it could be something logical name or else. I still don't get how sorting would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348:134,depend,dependencies,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348,1,['depend'],['dependencies']
Integrability,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,integrat,integration,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186,1,['integrat'],['integration']
Integrability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:120,message,message,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,8,"['Message', 'message']","['Message', 'message']"
Integrability,"@cjllanwarne The problem with not injecting, and creating separate tasks, is that you either have to clone the entire repo again for each of the tasks to extract the version information etc.., or clone it once and pass around the execution dir of the corresponding task which is even more horrible IMO.; Unless there's another way I'm missing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793:34,inject,injecting,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793,1,['inject'],['injecting']
Integrability,"@cjllanwarne There's nothing stopping you from submitting a future PR proposing the changes you describe. @kshakir Don't forget about akka streams, which sit in between futures and actors on the generalized concurrency spectrum. . I recognized the URL of that blog post, I'll say that it has surprisingly useful comments at the end as well. Personally I find it a lot easier to reason about actors & messages than futures but that's not true for everyone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404:400,message,messages,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404,1,['message'],['messages']
Integrability,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:55,depend,dependency,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563,2,"['depend', 'interface']","['dependency', 'interface']"
Integrability,"@cjllanwarne new lines adjusted, so are my intellij settings thanks to Jose. I'm thinking the integrationTestCases should run weekly instead of nightly even. @jsotobroad as the original creator of the integrationTestCases -- does that sound okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523:94,integrat,integrationTestCases,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523,2,['integrat'],['integrationTestCases']
Integrability,@cjllanwarne the goal here is to move all backend-specific logic off the `BackendCall` and into the `Backend`. This is an evolutionary step that maintains the same `BackendCall` interface but delegates all meaningful work directly or indirectly to the backend. When this process is complete the `BackendCall` will become useless and methods can become parameterized by `JobDescriptor` instead. The combination of `JobDescriptor` and `Backend` should be able to accomplish any call-level work in the system.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423:178,interface,interface,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423,1,['interface'],['interface']
Integrability,"@cjllanwarne yes, comments were hidden due to file name change.; Ans 1. Adding hidden comment I did: Caching functionality is missing here. Shouldn't each backend implement caching and when engine ask for a jobExecutor, backend may return BackendCachedJobExecutor?; Doing that we can get rid of the engine responsibility to deal with cached data...; IMO, Cache should be encapsulated in each backend. The only thing I'm not sure if we should expose a standard message to force not to use cached data. So with that you tell to each backend to not use cached data but instead to process data again.; Ans 2. I'm not seeing any new msg for WorkflowBackendActor right now. That will depend on the UCs... for CCC backend those msgs are OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495:460,message,message,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495,2,"['depend', 'message']","['depend', 'message']"
Integrability,"@cjllanwarne you're good, thanks ðŸ™‚ The wording on the GitHub message made it sound like two reviews were missing but it was actually only one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469849765:61,message,message,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469849765,1,['message'],['message']
Integrability,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:34,message,messages,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926,1,['message'],['messages']
Integrability,"@cpavanrun Your right here, but this can be improved. What can be done here is lower the number of parallel jobs submitted by cromwell. This depends really on the cluster. In our case I did a stress test with 10000 parallel jobs and it still acts fine. Only downside is that the log is getting spammed a bit but it still works like it should. Still in the past (on older hardware) the headnode could not deal with this number of jobs. If this is the case limiting the parallel jobs could be a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773:141,depend,depends,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773,1,['depend'],['depends']
Integrability,@danbills Could you give more information about the test cases you see for this issue? I thought about using [gatling-akka](https://github.com/chatwork/gatling-akka) in order to send a lot of messages to `WriteMetadataActor` and `MetadataSummaryRefreshActor`.; Do you thing this approach is suitable for this problem?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560:192,message,messages,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560,1,['message'],['messages']
Integrability,@danbills Sure. the point is that there's a built in way to handle it and we should be doing that instead of our ad hoc method of having some catch all on every `receive` method throughout the system that are at best logging a message and potentially slightly changing the stacktrace. . We should remove those catch alls and use the built in capabilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303:227,message,message,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303,1,['message'],['message']
Integrability,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:616,inject,injected,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,1,['inject'],['injected']
Integrability,"@danxmoran I'm trying to recreate this - could you list the `womtool` version that you're using and a minimal WDL that reproduces the issue?. Note: I tried to recreate on `develop` using this WDL:; ```wdl; version 1.0. import ""not/a/file.wdl"" as oops. workflow foo {; call oops.not_a_thing; }; ```. And received an error message and an exit code of 1:; ```; Failed to import 'not/a/file.wdl' (reason 1 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/cromwell (without escaping Some([...]/cromwell))' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 2 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/bad_import (without escaping None)' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 3 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'not/a/file.wdl' relative to nothing; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176:321,message,message,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176,1,['message'],['message']
Integrability,"@davidbenjamin has an interesting proposal for user-defined / explicit sets of params for WDL: https://github.com/broadinstitute/wdl/issues/102. Depending on how ""[CWL support](https://github.com/broadinstitute/cromwell/milestone/20)"" addresses the [secondaryFiles](http://www.commonwl.org/v1.0/Workflow.html#File) mentioned above, it's supposed that similar WDL features will follow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668:145,Depend,Depending,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668,1,['Depend'],['Depending']
Integrability,"@delagoya your dependencies update might conflict with a round I just did in our `develop` branch. Namely we are using cats 1.0.1, and I'm not 100% sure 1.1 will work. So if you pull/rebase you will get most of what you posted minus the sttp update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268:15,depend,dependencies,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268,1,['depend'],['dependencies']
Integrability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,integrat,integration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,1,['integrat'],['integration']
Integrability,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,integrat,integration,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482,1,['integrat'],['integration']
Integrability,"@dgtester I wouldn't avoid the scala shutdown hook if it makes sense (making this up as I go, but e.g. sending a message to WorkflowActor which then propagates outward), there've been a handful of things that I've been meaning to add to a hook anyways so there'll be one sooner or later anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348:113,message,message,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348,1,['message'],['message']
Integrability,@dianekaplan could you run `gcloud alpha genomics operations describe operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU` and add in anything that looks like an error message or error code?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050:183,message,message,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050,1,['message'],['message']
Integrability,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:709,message,message,709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,2,['message'],['message']
Integrability,"@francares Ah! I see. Yeah, I asked because I wasn't sure why we'd need another command type, we could either make a different type of ExecutorActor or have a branch in the existing ExecutorActor for caches. So we agree there. As for another message type, I believe this is all configured in (a) global config file and (b) workflow options so the actors should already have everything they need to decide whether to allow caching or not. So IMO there's no special case at this layer to handle caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492:242,message,message,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492,1,['message'],['message']
Integrability,@francares Why do you view this as a bug? WDL explicitly states that there is no natural order outside of dependencies. Tagging our illustrious PO @kcibul just for record keeping,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544:106,depend,dependencies,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544,1,['depend'],['dependencies']
Integrability,"@freeseek `firebase.developAdmin` is a pretty wide role (with 204 permissions), so it's not surprising that it gives some permissions that are needed here. What would be helpful is if Google showed the exact permissions in their error messages, though from what it seems, that's not always the case. Then if you have a list of permissions, you can find minimal role(s) that encompass those permissions, rather than through a blind hunt (please correct me if it wasn't entirely blind here..). Btw @freeseek, from my limited experience, GitHub issues here are not often-looked-through, it might be better to create an internal JIRA ticket instead ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141:235,message,messages,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141,1,['message'],['messages']
Integrability,"@gauravs90 I would say this maybe could be done in smaller chunks. For this first step, maybe just implement enough for a single step workflow to pass through the WorkflowExecutionActor? I think that should be fairly small but will involve ironing out a lot of the issues with the interface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/652#issuecomment-211918816:281,interface,interface,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/652#issuecomment-211918816,1,['interface'],['interface']
Integrability,@gauravs90 I'm happy to handle the tidy-up of this PR. Could you though review it (including my integration of your changes) and thumbs-up if you're happy? Cheers!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226,1,['integrat'],['integration']
Integrability,"@gauravs90 I'm slightly confused here!. The model I thought we agreed on was:; - Everything has access to the `ServiceRegistry`; - Anyone (including the Engine) can send a `MetadataPut` message to the `ServiceRegistry`, which will forward it automagically to the `MetadataService`.; - ... That's it!. So, just send a bunch of `MetadataPut` messages to `ServiceRegistry` from the engine, and you're done, no need for all the extra stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782:186,message,message,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782,2,['message'],"['message', 'messages']"
Integrability,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:189,inject,injection,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972,2,['inject'],['injection']
Integrability,"@gemlam3 No they're different. This ticket is for the REST API, #2345 is for the command line interface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428:94,interface,interface,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428,1,['interface'],['interface']
Integrability,"@gemmalam - I am trying to access the JIRA tickets for cromwell. I followed the link in the README and created an account, but I'm getting a message ""<my_email_address> doesn't have access to Jira on broadworkbench.atlassian.net.""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028:141,message,message,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028,1,['message'],['message']
Integrability,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:138,message,message,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,1,['message'],['message']
Integrability,"@geoffjentry ; I trying to write some kind of integration test for my fix of this task and me with @TimurKustov came to idea of executing two workflows sequential in order to get outputs, results and call logs copied after execution of the first workflow and assure that they are exist and correctly placed by running second workflow, which would check these files locations and existence.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075,1,['integrat'],['integration']
Integrability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:895,integrat,integration,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,1,['integrat'],['integration']
Integrability,@geoffjentry Closing this issue. I think I figured out what happened and it is okay. Very hard to decipher from log messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765:116,message,messages,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765,1,['message'],['messages']
Integrability,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:240,message,message,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849,1,['message'],['message']
Integrability,@geoffjentry I agree. I think we could easily replace the `promise.complete`s in `AwsSdkAsyncHandler` with appropriate messages to appropriate actors (NB we probably don't _need_ the `AwsSdkAsyncHandler` wrapper class at all),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416:119,message,messages,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416,2,"['message', 'wrap']","['messages', 'wrapper']"
Integrability,@geoffjentry I had trouble building this pr:. ```; [error] /work/engine/src/main/scala/cromwell/webservice/SwaggerService.scala:3:35: imported `CromwellApiService' is permanently hidden by definition of object CromwellApiService in package webservice; [error] import cromwell.webservice.routes.CromwellApiService; [error] ^; ```. Any ideas?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866:287,rout,routes,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866,1,['rout'],['routes']
Integrability,@geoffjentry I removed lib dependencies in Cromwell-backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365563:27,depend,dependencies,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365563,1,['depend'],['dependencies']
Integrability,"@geoffjentry I think I understand what you mean. From my perspective it's a matter of where to stop.; We could ban entirely `Future`s from the codebase and decide that every asynchronous task deserves its own actor. That seems a bit extreme to me though.; I see your point about future being dangerous inside of actors. However I believe that small actors with 2 states and 3 internal messages are a small enough unit to be understood well enough to avoid the kind of problems we encountered before. This actor doesn't even have state data, there's no state to be shared or mutated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385:385,message,messages,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385,1,['message'],['messages']
Integrability,"@geoffjentry I think what I was looking for is defined in the Scala interfaces for backends. I.e. I could create a custom backend to support AWS by implementing those (so it wouldn't be a web API, just an application PI).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256:68,interface,interfaces,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256,1,['interface'],['interfaces']
Integrability,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:342,interface,interface,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368,2,['interface'],['interface']
Integrability,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:250,wrap,wrapper,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290,2,"['depend', 'wrap']","['dependent', 'wrapper']"
Integrability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:111,depend,dependent,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,2,['depend'],['dependent']
Integrability,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:109,message,message,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770,3,['message'],"['message', 'messages']"
Integrability,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:1085,protocol,protocol,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503,1,['protocol'],['protocol']
Integrability,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,integrat,integration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433,1,['integrat'],['integration']
Integrability,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:104,message,messages,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910,1,['message'],['messages']
Integrability,"@geoffjentry my understanding is that the logic here is what we want but the interface could be a little better, and a little more fully implemented. Correct me if I'm wrong, but I _think_ we want all `DataAccess` methods to automatically (and transparently) have deadlock retries. If we were to do that for all methods in `DataAccess` using the scheme laid out in this PR, then we'd be duplicating all of the methods in `DataAccess`. Perhaps we could construct `DataAccess` with an `ActorSystem`... though I can't remember if @kshakir objected to this for a reason that is currently escaping me. We could also just make the whole thing an actor, or put an actor in front of `DataAccess`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703:77,interface,interface,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703,1,['interface'],['interface']
Integrability,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:271,message,message,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['message'],['message']
Integrability,@geoffjentry this also feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401:50,depend,dependency,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401,1,['depend'],['dependency']
Integrability,@geoffjentry this feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328:45,depend,dependency,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328,1,['depend'],['dependency']
Integrability,"@geoffjentry what are unhandled messages, and why do we explicitly log them?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838:32,message,messages,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838,1,['message'],['messages']
Integrability,"@geoffjentry yes, this is turning the knob higher and hoping for the best. The downstream tests succeeded except for the one that depended on cross-talking with the failed test...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458:130,depend,depended,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458,1,['depend'],['depended']
Integrability,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:417,integrat,integration,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,1,['integrat'],['integration']
Integrability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,adapter,adapter,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['adapter'],['adapter']
Integrability,"@grsterin perhaps arrows showing which actors send messages to others? perhaps ""sends messages to""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920:51,message,messages,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920,2,['message'],['messages']
Integrability,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1111,message,message,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,4,['message'],"['message', 'messages']"
Integrability,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:188,message,message,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,2,['message'],['message']
Integrability,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:174,wrap,wrapper,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284,2,['wrap'],['wrapper']
Integrability,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512,1,['integrat'],['integration']
Integrability,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:147,depend,dependency,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992,1,['depend'],['dependency']
Integrability,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,integrat,integration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['integrat'],['integration']
Integrability,@katevoss Assuming @Horneth thinks it'd be easy to add this I think we should. It's pretty irritating when it happens as it tends to come in storms and the message sounds a lot scarier than it really is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496:156,message,message,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496,1,['message'],['message']
Integrability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,integrat,integrate,618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,1,['integrat'],['integrate']
Integrability,"@katevoss iirc we can't do anything with it directly (as i said, pubsub itself has been around for ages), but would need papi to be smart about it. I think there's a ticket, and sine you said you saw it in the tracker that's likely true. also note that what @Horneth said is true. i've resisted this request for quite some time because i don't like the idea of wiring in email sending into cromwell. I wouldn't be opposed to adding a service to the service registry for workflow notifications where we provide two out of the box implementations - one which is just a Noop and one which will push a message to a pubsub topic and then users can do whatever they want with it. Similarly if a user *really* wants to have email notifications they can write their own implementation and plug it in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606:598,message,message,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606,1,['message'],['message']
Integrability,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:81,message,messages,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133,1,['message'],['messages']
Integrability,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:140,adapter,adapter,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['adapter'],['adapter']
Integrability,@kcibul @geoffjentry I actually have a python cli for running Workflows (and a single task) against the cromwell rest interface which I can open source when I get a chance. The cli lets you run one or more workflow. Its missing some parameters but you guys can feel free to expand it. Ive run 1000's of workflows with this little tool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845:118,interface,interface,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845,1,['interface'],['interface']
Integrability,"@kcibul Right now there are two problems:. - We're overloading the DB (largely metadata) in FC; - Nothing good happens when the DB is overloaded. The first one is solvable by a new metadata impl and will be the sort of route we go in CaaS. The second one should be fixed no matter what one is doing, so I'm specifically talking about all slick interaction which includes the stock metadata impl. The solution shouldn't be ""turn your buffers up"", but rather a more appropriate scheme instead of just dropping stuff to the floor. Related - as part of the CaaS milestone there are tickets (#1349 in particular) to make sure the standard impl isn't too tightly coupled with the resto f our DB stuff.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484:219,rout,route,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484,1,['rout'],['route']
Integrability,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:205,message,message,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340,1,['message'],['message']
Integrability,@kshakir I _think_ this covers your points although the test refactor isn't quite what we were talking about on hipchat. I think to go the route I think you were describing would be tough.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530:139,rout,route,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530,1,['rout'],['route']
Integrability,"@kshakir I just made the following changes:. - SBT is now run on pushes, to make sure that the artifact publishing still happens. Since it's ~30 minutes, not dependent on external services, and less flaky than the other tests I still think this is an improvement over today; - I switched the syntax to `[force ci]`. See the most recent commit message and it triggering the tests to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757:158,depend,dependent,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757,2,"['depend', 'message']","['dependent', 'message']"
Integrability,@lbergelson I just messaged our system administrators. I believe someone keeps changing a setting. It shouldnâ€™t matter if you are on the internal WiFi. Iâ€™m sorry for the inconvenience this is causing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-501879498:19,message,messaged,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-501879498,1,['message'],['messaged']
Integrability,"@lij41 - do you have any more error information, like a specific failure message? Also, were you creating the AMI from the CloudFormation templates? If so, which version - with or without VPC creation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888:73,message,message,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888,1,['message'],['message']
Integrability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['integrat'],['integration']
Integrability,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:229,depend,dependency,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522,1,['depend'],['dependency']
Integrability,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:202,inject,injection,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,1,['inject'],['injection']
Integrability,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:294,message,messages,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073,2,['message'],['messages']
Integrability,"@mcovarr I agree re testing, however IMO showing that there are outbound messages to start a CallActor at the same time is sufficient to demonstrate exactly what you just stated. At that point you know you have multiple CallActors running independently of each other which represent async computations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232:73,message,messages,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232,1,['message'],['messages']
Integrability,"@mcovarr I believe all issues are addressed. I'll let you cherry-pick / merge / squash or even reject this PR at your will. Pre-tech talk with @geoffjentry (including a TLA extravaganza!):. **TL;DR The database/slick is for CRUD, not for the T in ETL.**. IMHO, the database code started to evolve well beyond CRUD. Very often in slick specific code, one saw multiple lines of code like: ""before inserting rows in slick, quickly (E)xtract some other rows, (T)ransform them into core objects, filter, re-transform them into new core objects, then (L)oad the new objects into slick."" That ETL embedded in slick could never be used DRY-ly, and to me smelled as violating separation of concerns. With the current SoC, the slick code is _mostly_ concerned with marshaling data to and from the database via slick. If I wanted to, I could very trivially create a different layer that marshaled data using hibernate, a thin layer of prepared statements, mocks, etc., _without_ duplicating a lot of the ETL code. Another way of visualizing the issue: Below is the current project dependency diagram. The services need to access data from the database. Currently that's implemented as the services depend on engine that depends on the database. The database used to have a similar same circular dependency. Gun-shy of folks (including myself) re-introducing a similar dependency loop, I've kept core as far away as possible from the database/slick, because the slick specific code _should not_ need core for basic CRUD. As for the rest of the system, I see core as a base of objects for backends and the engine to communicate. ![cromwell project dependency diagram](https://cloud.githubusercontent.com/assets/791985/15779136/92db94a6-2968-11e6-90f8-c0b40d162a56.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591:1070,depend,dependency,1070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223577591,6,['depend'],"['depend', 'dependency', 'depends']"
Integrability,"@mcovarr So there seems to be some Liquibase issues in terms of synchronizing the column names. If we're going to do this as a part of 0.21, I might have @kshakir refine this ticket in terms of what steps we have to take to successfully change column names without disrupting users too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237553061:64,synchroniz,synchronizing,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1219#issuecomment-237553061,1,['synchroniz'],['synchronizing']
Integrability,"@mcovarr Well okay then. Let this remain a singleton as we discussed in the meeting. Later on, if we feel it's a bottleneck, we can make it a kinda router or something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539:148,rout,router,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-196540539,1,['rout'],['router']
Integrability,"@mcovarr as first reviewer. Travis already seems configured to ignore integration tests, so no other changes appeared necessary for this temporary fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511:70,integrat,integration,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169440511,1,['integrat'],['integration']
Integrability,"@mcovarr re concurrency testing I still think that's _way_ too overkill. There should be no forced timings, scraping of logs, etc. We know that Calls are wrapped by independent CallActors. As long as multiple CallActors are informed of their startable status by the same event, that's all we need to demonstrate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851:154,wrap,wrapped,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103264851,1,['wrap'],['wrapped']
Integrability,@mcovarr would you be content if I made a mock `PipelinesApiRequestWorker` that always crashes and check that the manager handles it?. I'm also thinking about introducing error types at this interface in the stack so that explosions in Google code don't percolate into Cromwell; ```; at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137:191,interface,interface,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917#issuecomment-492863137,1,['interface'],['interface']
Integrability,"@mcovarr, @cjllanwarne agree on your comments, I implemented a PoC on graph topology creation using topological sort algorithm based on DFS calculation at the beginning of this year. The idea was to propose the integration of that concept to Cromwell after PBE.; Coming back to this issue, If we need to recreate a DAG in the backend side it means there is something wrong with that backend (we know it). I explained why we try to do so in the Waffle.io ticket but I think we can explain better in the next meeting we may have. Anyways @jainh will not proceed with this change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218:211,integrat,integration,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236381218,1,['integrat'],['integration']
Integrability,"@natechols - so it works well for failed jobs. However, there seems to be some transient errors on our HPC that occur randomly and qsub/qstat go down temporarily and result in `failed (during ExecutingWorkflowState): java.lang.RuntimeException: Unable to start job.`. I was hoping this would retry failed submissions. . This is my current config:. ```; include required(classpath(""application"")). webservice {; port = 8000; interface = 127.0.0.1; }. #call-caching {; # enabled = true; # invalidate-bad-cache-results = true; #}. system {; job-rate-control {; jobs = 20; per = 1 second; }; }. backend {; default = SGE. providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 10; root = ""cromwell-executions""; run-in-background = true. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; String ? docker; String ? docker_user; """""". submit = ""/bin/bash ${script}"". submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """""". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""file""; check-sibling-md5: false; }; }; }; }; }. SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; root = ""cromwell-executions""; exit-code-timeout-seconds = 600; concurrent-job-limit = 100. default-runtime-attributes {; maxRetries: 3; }. runtime-attributes = """"""; Int cpu = 1; Float ? memory_gb; String sge_queue = ""dgdcloud.q""; String ? sge_project; """""". submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l h_vmem="" + memory_gb / cpu + ""g""} \; ${""-l mem_free="" + memory_gb / cpu + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; """""". kill = ""qdel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362:424,interface,interface,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-511529362,1,['interface'],['interface']
Integrability,"@natechols -- Thanks again for this amazing contribution! We're at a point of trying to wrap up reviews for some urgent changes. Our plan is to come back to this PR in 2 weeks to do a more thorough review. Sorry about the delay, but hoping to get back to this soon.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488300367:88,wrap,wrap,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-488300367,1,['wrap'],['wrap']
Integrability,"@natechols Do you mind making the last change requested above, then git-squashing everything into a one commit with a single commit message? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505530181:132,message,message,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505530181,1,['message'],['message']
Integrability,"@natechols IIRC it has to do with Docker (evidenced by me asking that question above). When you're mounting a volume into a Docker container there are a handful of ways to handle synchronizing permissions from the container and the host system - all of them have serious flaws. This was the flawed approach we chose to go with here. You're correct that if one's not using Docker it's not at all necessary, branching that behavior was just never prioritized.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562686264:179,synchroniz,synchronizing,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562686264,1,['synchroniz'],['synchronizing']
Integrability,"@olsonanl I think it depends on the version - I think(?) in newer versions a singularity hub image is cached, but (from actual experience) in _some_ version it's definitely not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320:21,depend,depends,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320,1,['depend'],['depends']
Integrability,"@patmagee Cool, no rush. If you decide you still want the meta stuff just let me know, or if you want to go the label route just close the issue once you're confident it's the right path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313570149:118,rout,route,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421#issuecomment-313570149,1,['rout'],['route']
Integrability,"@prihoda . miniwdl has a [little CLI wrapper](https://github.com/chanzuckerberg/miniwdl#miniwdl-cromwell) to make it nicer to launch cromwell locally. It doesn't do the the shebang script which is a neat idea, however, it does implement versions of (i) parsing the task/workflow inputs to expose them as command-line arguments, and (ii) parsing the outputs to organize them more nicely after they come out. [Here is a link](https://github.com/chanzuckerberg/miniwdl/blob/b7f399b56ad2f01ed9867e6105c036a251c4ae73/WDL/CLI.py#L289) to the CLI entrypoint for this where you can see how all this happens. I'd be happy to work with you on merging & fleshing out the ideas.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501874278:37,wrap,wrapper,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5029#issuecomment-501874278,1,['wrap'],['wrapper']
Integrability,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:1780,depend,depending,1780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086,1,['depend'],['depending']
Integrability,"@rsasch that's exactly what i'm talking about. we do that everywhere, but akka can be configured to log those automatically. if an unhandled message catcher is doing something useful other than logging, that's different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351:141,message,message,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-430029351,1,['message'],['message']
Integrability,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:400,adapter,adapter,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['adapter'],['adapter']
Integrability,"@ruchim @geoffjentry . Hi, ; Sorry, it looks like I copied the wrong history. I added the correct history at https://gist.github.com/denis-yuen/b3aa8b0e882dee1fe8cb6cab82286e46. The error message is pretty similar, is it possible #4308 affects both scenarios?. Equivalent excerpts below:; ```; dyuen@odl-dyuen2:~/test$ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; Cloning into 'dockstore-workflow-md5sum-unified'...; remote: Enumerating objects: 113, done.; remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 113; Receiving objects: 100% (113/113), 24.79 KiB | 1.24 MiB/s, done.; Resolving deltas: 100% (50/50), done.; dyuen@odl-dyuen2:~/test$ cd dockstore-workflow-md5sum-unified; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; Resolved 'checker_workflow_wrapping_workflow.cwl' to 'file:///home/dyuen/test/dockstore-workflow-md5sum-unified/checker_workflow_wrapping_workflow.cwl'; <snip>; Final process status is success; dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; --2018-11-09 10:24:06-- https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; <snip>; 2018-11-09 10:24:25 (9.05 MB/s) - â€˜cromwell-36.jarâ€™ saved [175930401/175930401]. dyuen@odl-dyuen2:~/test/dockstore-workflow-md5sum-unified$ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; [2018-11-09 10:25:13,02] [info] Running with database db.url = jdbc:hsqldb:mem:563ca6aa-5d9b-4e8f-b0c6-f3901066317d;shutdown=false;hsqldb.tx=mvcc; [2018-11-09 10:25:18,31] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-09 10:25:18,32] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-11-09 10:25:18,39] [info] Running with database d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477:188,message,message,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-437395477,1,['message'],['message']
Integrability,"@ruchim I think @geoffjentry is spot on - adding a scope by itself won't change existing behavior. It's only when the user sets `monitoring_image` to `quay.io/broadinstitute/cromwell-monitor-bigquery`, _then_ it will fail if the SA for the task doesn't have `bigquery.tables.updateData` permission on the monitoring dataset. So existing users on Terra won't be affected, unless we start routinely adding that option to all workflows and don't adjust the IAM permissions on pets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348:387,rout,routinely,387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348,1,['rout'],['routinely']
Integrability,"@salonishah11 for example, I'm running a cromwell container in server mode, bound to port 8000: ; ```; docker run -p 8000:8000 cromwell server; ```; but when I try to ; ```; docker run cromwell submit --host 0.0.0.0:8000 ...; ```; I get: ; ```; Error: Option --host failed when given '0.0.0.0:8000'. no protocol: 0.0.0.0:8000; ```. Some simple docs would help here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124:303,protocol,protocol,303,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124,2,['protocol'],['protocol']
Integrability,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:244,rout,routes,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053,2,"['message', 'rout']","['message', 'routes']"
Integrability,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lotâ€¦ but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:206,message,message,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150,2,['message'],['message']
Integrability,"@salonishah11 with a goal of getting this code back in `develop`, I'm gonna let CI run a couple times and check logs for `Cromwell failed to progress` messages. It has been a mystery exactly what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797:151,message,messages,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797,1,['message'],['messages']
Integrability,@samanehsan @geoffjentry -- is there a good message you'd like to see in replacement of the existing error?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959:44,message,message,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-456487959,1,['message'],['message']
Integrability,@scala-steward - there may be two small bugs happening in this PR:. 1. I didn't expect scala-steward to continue posting updates to this dependency given that there's a `scala-steward:off` line in the file.; 2. This PR is not actually updating the version of sbt - it's just whitespace tidying?. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924:137,depend,dependency,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924,1,['depend'],['dependency']
Integrability,"@scottfrazer @geoffjentry They are going to be used in the next 3 tasks based on priority (Excel sheet), so I think It does not hurt anyone to have them as a baseline right know (they just define interfaces and not concrete implementations) since we took the work of defining them in the past.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191372252:196,interface,interfaces,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-191372252,1,['interface'],['interfaces']
Integrability,"@scottfrazer FWIW my vision of the world was a lot closer to the diagram you drew up but I don't have strong feelings on that. In terms of distributed jars what I'd like to see distributed to the world would be:; - cromwell.jar: full fat jar like we have today which also includes all of the supported backends built in; - cromwell-backend.jar: a jar providing the interface stuff which someone can use to build their own backend jar. I'd be totally okay with (and could see value in):; - cromwell-lite.jar (or something like that): a stripped down fat jar w/o any supported backends or maybe local; - foo-backend.jar for each supported backend. My main concern though is that we always make the most obvious download for a naive user of cromwell to be the one with all of the supported (or perhaps a 'very common supported' subset) backends built in so as to minimize the work someone needs to do to get rolling. The other jars are really an artifact of the multi-project model and can be ignored. The side discussion about `core` is exactly why I was picturing the hierarchy stemming from `backend` all along. Despite the name of `supportedBackends` being my request I'll admit I was just looking at the name when I said that, not thinking the whole thing through critically :). IMO `core` should be code which is shared between all components, I'd call the filesystem concept a component, not something in core itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235:365,interface,interface,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209656235,1,['interface'],['interface']
Integrability,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:938,integrat,integration,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['integrat'],['integration']
Integrability,"@seandavi Gotcha, this might be another case of ""frightening log message"" if you were detecting it via cromwell, although IIRC we squelched those specifically as they were too spammy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342:65,message,message,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645342,1,['message'],['message']
Integrability,"@sleongmgi this is very cool! I believe based off of the SGE backend?. Have you had a chance to see what's happening on our Develop branch? We'll soon have a new engine-to-backend interface and it'd be awesome if we could also bring this backend forward with us!. In any case, thanks for the contribution! :-D",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/980#issuecomment-225589938:180,interface,interface,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/980#issuecomment-225589938,1,['interface'],['interface']
Integrability,@slnovak It'll depend on how soon upcoming is for you :) Over the next few weeks there are a couple of broad products sitting on top of Cromwell going live so I expect the bug tickets to be coming in fast and furious. We've been discussing our direction after the dust settles there and one of the priorities will be community requests. We'd certainly welcome a PR but if you have the time to wait a month or so it should hopefully be something we could get to.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-171414380:15,depend,depend,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375#issuecomment-171414380,1,['depend'],['depend']
Integrability,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:402,depend,depending,402,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597,2,"['depend', 'integrat']","['depending', 'integration']"
Integrability,"@tmdefreitas I observed/experienced a similar issue. I had a WDL with an optional input. It was optional because its type was ""File?"". I was passing in the input when issuing a submission on FireCloud which is currently using v0.24 of Cromwell according to the launch config dialog box. Using the developer tab I saw the error . ```; ""failures"": [{; ""message"": ""Couldn't resolve all inputs for CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task at index None.: Input evaluation for Call CallingGroup_Workflow.CallSomaticMutations_131_Prepare_Task failed.:\n\tnormalPanelSize:\n\tFile not found fc-2edc2716-272a-438a-b458-25dbee1e253d/eb1f9669-ce6c-462d-950d-630b321ddc1f/CallingGroup_Workflow/096768d6-9e90-4d1d-81c7-f909559a1a55/call-CallSomaticMutations_131_Prepare_Task/\""gs:/firecloud-tcga-open-access/tutorial/reference/refseq_exome_10bp_hg19_300_1kg_normal_panel.vcf\""""; }],; ```. I note two things. First, I note as I mentioned that I was passing in the file and so the error ""File Not found"" does not make sense. Second, I note that the gsURL has only one ""/"" after the ""gs"" ; in contrast the file IS where it is and in the workspace attribute (where it is pulled from) it is there and the file preview worked. Also the gsURL in the workspace had two ""//"" as it should. To be able to successfully use the WDL I removed the ""?"" so that it's a plain ""non-optional"" input. After removing the ""?"" I was able to successfully run the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241:351,message,message,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937#issuecomment-276756241,1,['message'],['message']
Integrability,@tom-dyar Based on the logs it appears cromwell is expecting to find the outputs locally. Since Funnel is running against AWS Batch those files are either being moved around on the AWS VM or being uploaded to S3. I think to get this working you would need to setup cromwell's `root` storage in the config to point to an S3 bucket. . @mcovarr I am not quite sure how cromwell can protect against this sort of config issue. One idea would be to inspect the OutputFileLog in the TES Task message and check the URL's of all of the output files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505:485,message,message,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395576505,1,['message'],['message']
Integrability,"@vsoch Sorry, our devops team has asked us to be especially thorough with PRs affecting our CI environments/dependencies. I've been bouncing between reading up on the [CircleCI docs](https://circleci.com/docs/2.0/configuration-reference/) and checking on several ðŸ”¥events this week. If you have time, perhaps we can setup a remote session next week where you can give me a tour of yml and everything that's going on? If you propose three times that work for you I'll pick one. Feel free to msg me here or email if that's easier. Otherwise I'll continue looking through the docs and get back to you once the flames die down. ðŸ¤ž",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742:108,depend,dependencies,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415767742,1,['depend'],['dependencies']
Integrability,@vsoch if in fact this scheme is going to work you have it right. It might **not** be feasible but we can cross that bridge if/when we get there,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413943140:117,bridg,bridge,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413943140,1,['bridg'],['bridge']
Integrability,"@wleepang , We're scattering by chromosome for any step using a scatter, so 25 concurrent jobs/sample. The number of simultaneous samples run at a time varies from 1-100+ depending on needs, and we're using multiple workflows, but some sub-wdls are shared between the larger workflows. And I'm not sure I really understand why a new job definition must be made for every call. I presume you're submitting jobs using a submit_job() API call where you can specify container overrides and set unique mount points versus having them pre-established in a job definition and calling that without any modification? . Scala is not my language so how those calls are being made and how the determination of whether or not a job definition already exists (regardless of it being correct in light of this bug) is not something I can easily determine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934:171,depend,depending,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-504128934,1,['depend'],['depending']
Integrability,@wresch Looks like you are having DNS resolution issues. \. Can you resolve the endpoint per the error message?; `nslookup https://auth.docker.io/token`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891:103,message,message,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462380891,1,['message'],['message']
Integrability,"A current version of Womtool still validates these tasks as OK, so the problem continues to exist in draft-2. Upgrading the tasks to WDL 1.0 causes Womtool validation to fail with messages of varying helpfulness:. ```; version 1.0. task myTask {. input {; File f; }. command {; touch ${f.bam.bai}; }; }; ```; ```; Failed to process task definition 'myTask' (reason 1 of 1):; Failed to process expression 'f.bam.bai' (reason 1 of 1):; No such field 'bam' on type File; ```; ---; ```; version 1.0. task myTask {. input {; File f; }. command {; touch ${f%%.bam.bai}; }; }; ```; ```; Failed to read task definition at line 3 column 6 (reason 1 of 2):; Failed to convert AST node to ExpressionElement (reason 1 of 2):; No attribute 'rhs' found on Ast 'Remainder'. Did you mean: lhs; Failed to read task definition at line 3 column 6 (reason 2 of 2):; Failed to convert AST node to ExpressionElement (reason 2 of 2):; No attribute 'value' found on Ast 'MemberAccess'. Did you mean: member; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2873#issuecomment-439426356:180,message,messages,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2873#issuecomment-439426356,1,['message'],['messages']
Integrability,"A system event monitoring and interrupt system seems way complex to me than implementing some conventions that leverage a standard task definition. At the point of submitting a process to Batch, you should already have all of the information needed to run a task, and can pass this along in a standard way to the standard task definition. This is the same information that is used to create the current job wrapping script. . FYI - the Funnel worker that is wrapped in `batch-task-runner` is modified to run stand-alone from a input JSON file, as opposed to communicating back to a Funnel server for task scheduling and distribution. The modified worker consumes the return codes passes the result back through it's own process and back to Batch. In this way, the sibling process is not hidden from Batch (or Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490:407,wrap,wrapping,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490,2,['wrap'],"['wrapped', 'wrapping']"
Integrability,"A user is reporting the failure message ""the job was aborted from outside Cromwell"". Looking at the Operation details:; ```; ""error"": {; ""code"": 1,; ""details"": [],; ""message"": ""Operation canceled at 2018-08-08T21:05:00-07:00 because it is older than 6 days""; },; ```. Can Cromwell inspect the Operation for this condition and produce a friendlier error message?. Operation `operations/EK3uv-_PLBjok8Wbqs77lCUgq92AiSQqD3Byb2R1Y3Rpb25RdWV1ZQ`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698:32,message,message,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-412133698,3,['message'],['message']
Integrability,"AC: Confirm that this is reproducible behavior in the latest version of Cromwell, and given thats the case, confirm that job/workflow failure messages make it over to the workflow logs (not just the server logs)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466:142,message,messages,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-444617466,1,['message'],['messages']
Integrability,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2516,rout,routed,2516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,3,"['message', 'rout']","['messages', 'routed']"
Integrability,"According to the PR description, the WDL 1.0 spec did not have an opinion here, so we let our 1.0 implementation change. The `development` version of WDL says; >In the event that there is no protocol the import is resolved **relative** to the location of the current document. If a protocol-less import starts with `/` it will be interpreted as starting from the root of the host in the resolved URL. so I think it would be most pragmatic to close this issue with a documentation-update PR. @tlangs does this sound fair to you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883:191,protocol,protocol,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451608883,2,['protocol'],"['protocol', 'protocol-less']"
Integrability,"According to the [jetbrains](http://www.jetbrains.org/intellij/sdk/docs/basics/getting_started/plugin_compatibility.html) website, we might just need to add a dependency and hey-presto, it'll be picked up for all other jetbrains IDEs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1264#issuecomment-303756279:159,depend,dependency,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1264#issuecomment-303756279,1,['depend'],['dependency']
Integrability,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:60,message,message,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245,2,['message'],['message']
Integrability,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:35,integrat,integration,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719,1,['integrat'],['integration']
Integrability,"Actually now that I think about it more I don't understand why the two with intermediate values can't be overridden. Is it bad to provide a default value that may depend on something else? I can think of cases where that would be useful. . The middle change is very helpful -- I'm willing to put up with a bit of potential confusion in order to have it. . One other question: will this affect what gets listed as ""requested"" by `wdltool inputs`? I'd like for everything that *can* be supplied to be in there even if I provided a default value.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323820125:163,depend,depend,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323820125,1,['depend'],['depend']
Integrability,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:723,synchroniz,synchronization,723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169,3,['synchroniz'],['synchronization']
Integrability,"After the changes above, I believe the various Specs should run as expected:; - `sbt alltests:test`; - `sbt notests:test`; - `sbt nodocker:test`; - `sbt dbms:test`; - `sbt integration:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681:172,integrat,integration,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/558#issuecomment-196466681,1,['integrat'],['integration']
Integrability,"Again 2779. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-a47da50b-5587-413b-bbc6-4773a965cb41/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373); at org.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183:81,message,message,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-454822183,1,['message'],['message']
Integrability,"Ah gotcha! To summarize:. - no changes are being made to the (scala) cromwell code to integrate a backend; - the specification of the backend still happens on the level of the pipeline, via the backend.conf; - of which we can provide an example from the `cromwell.examples.conf`. So I just need to write that example :) Did I get that right this time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055:86,integrat,integrate,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413692055,1,['integrat'],['integrate']
Integrability,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:63,Depend,Depending,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039,1,['Depend'],['Depending']
Integrability,"Ah, oh well, thanks for the second set of eyes. https://github.com/broadinstitute/cromwell/commit/2682c001d99823098e655acd1dd7a3062a68f495 has your change implemented with some ~nasty~ reflection that hopefully the rest of the DSP-Batcher's will accept, and the test re-enabled. CI running here to see if it breaks anything:; https://travis-ci.com/broadinstitute/cromwell/builds/116462548. Assuming this works, I think the changes will pass all of our existing unit and integration tests!. We can get two others to review. I should abstain due to our collaboration on the code. ðŸ˜‰",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160:470,integrat,integration,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504500160,1,['integrat'],['integration']
Integrability,"Aha. So maybe we can just default in our own Noop DSN to silence the error. ToL: The lack-of-a-DSN-message is also [Logback adjacent](https://docs.sentry.io/clients/java/modules/logback/#usage). Someday I'll figure out how the hell to use logback/Joran. On first glance it looks a lot like HOCON's embedded default `application.conf` that can be overriden via `-Dconfig.file=â€¦` except one is supposed to use [`-Dlogback.configurationFile=â€¦`](https://logback.qos.ch/manual/configuration.html#configFileProperty). But while I ""get"" HOCON's mechanics I do not yet ""get"" best practices for logback [overrides/includes](https://stackoverflow.com/a/23737143/3320205).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690:99,message,message,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620#issuecomment-389034690,1,['message'],['message']
Integrability,Ahhh I remember asking myself why no test failed when I changed this log message.; Those test should be fixable easily,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371351341:73,message,message,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371351341,1,['message'],['message']
Integrability,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:70,message,messages,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152,1,['message'],['messages']
Integrability,All the dependencies I removed are pulled in transitively from wdl4s. T,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2646#issuecomment-331251916:8,depend,dependencies,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2646#issuecomment-331251916,1,['depend'],['dependencies']
Integrability,Allows for removal of dependency on backend in the various *logs and metadata endpoints. Be careful with how this impacts hashing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885:22,depend,dependency,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/430#issuecomment-181537885,1,['depend'],['dependency']
Integrability,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:178,message,messages,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981,1,['message'],['messages']
Integrability,"Also, if I change the type of the output from `Object` to `Map[String, String]` I get a similar error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Map[String, String] name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853:163,message,message,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351556853,2,['message'],['message']
Integrability,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:17,wrap,wrapped,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057,1,['wrap'],['wrapped']
Integrability,"Also. File locks are not that good. But locking via the database would be ideal for horicromtal. If I get some pointers I can implement a database lock. This will require an extra table or something, so I need some pointers on how this should be ideally integrated in cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488:254,integrat,integrated,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498644488,1,['integrat'],['integrated']
Integrability,Alternate command syntax for those investigating: `sbt 'server/run server'`. ~The original caused an error for me.~ EDIT: The original error may have been an unrelated error that pops up sometimes during `sbt clean compile`. I'm not sure of the default dependencies for `sbt */run` but it does appear that [`*/package`](https://www.scala-sbt.org/1.0/docs/Running.html#Common+commands) is being invoked and zipping up the class files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739:253,depend,dependencies,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3624#issuecomment-389207739,1,['depend'],['dependencies']
Integrability,"Amazing! That is the right way to deal with multi project. I don't know if you remember Geoff, Fransico also asked for this approach and I shared quite few links on gitter. But, anyways that's an amazing start for decoupling projects or in other words defining dependency chain. :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/527#issuecomment-194107947:261,depend,dependency,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/527#issuecomment-194107947,1,['depend'],['dependency']
Integrability,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:233,wrap,wrap,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558,1,['wrap'],['wrap']
Integrability,And here is the exception message:. ```; [ERROR] [12/06/2016 13:58:30.046] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] Unable to create actor for ActorRef Actor[akka://; cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; java.lang.RuntimeException: Unable to create actor for ActorRef Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor/KeyValue#988818050]; at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:81); at cromwell.server.CromwellRootActor$$anonfun$1.applyOrElse(CromwellRootActor.scala:80); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.lang.reflect.Constructor.newInstance(Constructor.java:423); at akka.util.Reflect$.instantiate(Reflect.scala:65); at akka.actor.ArgsReflectConstructor.produce(IndirectActorPro,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974:26,message,message,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748#issuecomment-265155974,1,['message'],['message']
Integrability,And if one chooses to go the delete-the-examples route please also look at deleting any wdl code that is no longer needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3023#issuecomment-349955857:49,rout,route,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3023#issuecomment-349955857,1,['rout'],['route']
Integrability,And/or include a link to stderr in the failure message?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292:47,message,message,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848#issuecomment-272218292,1,['message'],['message']
Integrability,"Another error w/ this test:. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2444/testReport/junit/cromwell.core.actor/RobustClientHelperSpec/RobustClientHelper_should_reset_timeout_when_backpressured_is_received/. ```; java.lang.AssertionError: assertion failed: received unexpected message RealMessage(ServiceUnreachable,TestActor[akka://TestSystem-78f39f37-cc73-481d-8e7a-e59e623aa020/user/$$i]) after 0 millis; at akka.testkit.TestKitBase.expectNoMsg_internal(TestKit.scala:696); at akka.testkit.TestKitBase.expectNoMessage(TestKit.scala:661); at akka.testkit.TestKitBase.expectNoMessage$(TestKit.scala:660); at akka.testkit.TestKit.expectNoMessage(TestKit.scala:896); at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:140); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692); at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674); at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14); at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750); at org.scalates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054:344,message,message,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351#issuecomment-451186054,1,['message'],['message']
Integrability,"Another example of a workflow complete failure possibly due to grabbing the hash from google. Workflow 0c7da038-172a-4081-8850-c87fec05f4c1. ```; 2016-04-26 20:52:05,279 cromwell-system-akka.actor.default-dispatcher-28 ERROR - WorkflowActor [UUID(0c7da038)]: Completion work failed for call HaplotypeCaller:46.; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:461,message,message,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,2,['message'],['message']
Integrability,Another thing I've noticed is that a ton of our log messages are at debug level but most people aren't using debug level logging. Oopsie Daisy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-225260047:52,message,messages,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-225260047,1,['message'],['messages']
Integrability,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:197,wrap,wrapped,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429,1,['wrap'],['wrapped']
Integrability,"Apologies, that metadata appears to have disappeared, but the same issue is referenced here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set/p1?new=1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538:179,message,message-must-have-path-set,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-342953538,1,['message'],['message-must-have-path-set']
Integrability,Appears to be related to wrapping it up into the implicit sub-workflow for the inner scatter (`ScatterElementToGraphNode:103`),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751#issuecomment-395825435:25,wrap,wrapping,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751#issuecomment-395825435,1,['wrap'],['wrapping']
Integrability,Are there any other changes which will have to be wrapped up in the RELEASE 0.17 umbrella?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646:50,wrap,wrapped,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175232646,1,['wrap'],['wrapped']
Integrability,Are you creating a .zip file of dependencies to submit to Cromwell alongside the root WDL?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4482#issuecomment-464202824:32,depend,dependencies,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4482#issuecomment-464202824,1,['depend'],['dependencies']
Integrability,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:78,message,message,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991,1,['message'],['message']
Integrability,"As I thought, that message was nothing. Once stderr had more than zero bytes it doesn't show up (and the file on GCS is correct)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192326140:19,message,message,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192326140,1,['message'],['message']
Integrability,"As a **Cromwell dev**, I want **Cromwell to follow akka protocols of handling unexpected messages**, so that I can **avoid excessive LinesOfCode**.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748:56,protocol,protocols,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328963748,2,"['message', 'protocol']","['messages', 'protocols']"
Integrability,"As a **Cromwell dev**, I want **to explore the cost/benefits of using AsyncAppender for our logs**, so that **we can decide whether we should adopt it.**; - effort: Small spike; - risk: Small to Medium; - business value: Small to Medium, depending on the results of the spike",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812:238,depend,depending,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-344952812,1,['depend'],['depending']
Integrability,"As a **Redteam member**, I want **to improve the Cromwell release process**, so that **it gets easier every time we release**.; * Effort: depends on the issue; * Risk: Small, depends on the issue; * Business value: Medium",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666:138,depend,depends,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2404#issuecomment-332631666,2,['depend'],['depends']
Integrability,"As a **user running workflows and setting up configs**, I want **Cromwell to (fail nicely?) when I reference a pluggable backend class that's not on the classpath**, so that I can **(still run my workflow? get a nice error message?)**. @geoffjentry ; - Effort: **Small?**; - Risk: **Small**; - Business value: **Small**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953:223,message,message,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683#issuecomment-328531953,1,['message'],['message']
Integrability,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:63,message,messages,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436,2,['message'],['messages']
Integrability,"As a **user running workflows**, I want **to see fewer unhelpful log messages**, so that **performance improves and it is easier to find important messages**. More information and improvement ideas in the [Logging spec](https://docs.google.com/document/d/1Dc37EaPDoWXacSSzLgCdndx9zo5k6EmE5tvg-2fisPo/edit#). - effort: Small to Medium; - risk: Small; - Currently we show too many log messages, which degrades performance. We risk showing too few, but I think it's a risk we can mitigate.; - business value: Medium ; - Our logs are where users go to debug workflows, and currently they are a haystack to pick through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674:69,message,messages,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807#issuecomment-344984674,3,['message'],['messages']
Integrability,"As a **user running workflows**, I want to **be able to specify the backend in workflow options**, so that **Cromwell only uses the default backend when no other is specified, and notifies the user that it is using the default**. - Effort: X-Small to Small; - Risk: Small; - Note that some WDL may break, consider ways to deprecate (with warning messages in the WDLs).; - Business value: Small, for now; - This may change as Cromwell supports more backends and more users operate in a multi-backend environment",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075:346,message,messages,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312#issuecomment-335814075,1,['message'],['messages']
Integrability,"As a **user running workflows**, I want to **see a timing diagram or other useful error message when my workflow has failed before making any calls**, so that **I know why I don't see the timing diagram like I expect.**; - Effort: Small; - Risk: X-Small; - Business value: Small; - I haven't heard any mention of this for a while from customers or other internal folk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398:88,message,message,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1887#issuecomment-345359398,1,['message'],['message']
Integrability,"As a follow up I did some extra testing. As cromwell evaluates imports from `$PWD`. This means that ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; yields different results depending on the current working directory. In my opinion this is not desirable behavior. The small patch code that I wrote does not solve this issue. If cromwell is run from the same directory as the workflow.wdl it works, but in other cases it does not. . In an ideal case ; ```; java -jar /some/absolute/path/cromwell-<version>.jar /another/absolute/path/workflow.wdl; ```; will always lead to the same result no matter what $PWD is. This makes workflows reproducible and easy to be reused. ; This means that cromwell should use the absolute parent path of `workflow.wdl` to evaluate its imports from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775:227,depend,depending,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-367252775,1,['depend'],['depending']
Integrability,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:316,depend,depend,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321,1,['depend'],['depend']
Integrability,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:183,message,message,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398,1,['message'],['message']
Integrability,"As tech debt, I maybe would like to see things like `unwrapOutputValues` broken into utility functions. Not going to hold this PR up any longer figuring out this exact refactoring, but looking at current code I'm picturing something along the starting lines of:. ``` scala; // Sort of like Future.sequence, but with; // unwrapOutputValues's Failure(new Throwable(messages.mkString)); def sequenceMessages[T](in: Seq[Try[T]]): Try[T]. // If there are any failures, uses sequenceMessages to create the Failure; def unwrapValues[K,V](in: Map[K,Try[V]]): Try[Map[K,V]]; ```. :+1: For merging for the current code @cjllanwarne",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449:363,message,messages,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/112#issuecomment-124156449,1,['message'],['messages']
Integrability,"Assuming I'm not too far from the reality, my opinion on this is that ; - I like that the actors don't have to know about the actual reference of the Metadata service. Meaning I'm ok with the push to the event stream approach.; - I like less that metadata is pushed only when actors change state. Their data can be changed (and is changed) when they receive a message but don't necessarily transition. And if I'm understanding this correctly this is not enough to capture that.; For example, the `WorkflowExecutionActor` state are basically `NotRunning`, `Running`, `Done`, we can only update metadata between those states. So there's no update of call status in real-time, outputs etc... Now from your comment I understand you want to do this by having the backend actors publishing metadata, but ; 1) nothing guarantees that they will, or with which format; 2) there are states that could only be known by then engine and that we might want to publish (`JobInitializing`, `Finalizing` etc...). Even more generally, I think the engine is the ultimate decider of what the state of a call is in the workflow, and relying on the backend to get this information seems a bit wrong.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534:360,message,message,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219042534,1,['message'],['message']
Integrability,"At a guess, because the implementation of globbing is backend-dependent (or really, filesystem dependent) and therefore depends on having a task for context.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452187156:62,depend,dependent,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452187156,3,['depend'],"['dependent', 'depends']"
Integrability,"B_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2190,message,message,2190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,"Based on extensive log-examination, I believe this change has somehow broken the ability of a CWL workflow to survive restarts. Some suites pass some of the time due to a confluence of (1) getting lucky and avoiding a restart and (2) retries. It is very often the case that a test case only succeeds on the second or third try. Because I don't want anything to slip through the cracks due to probability, I'm not personally going to call this green until I see zero `Could not read from gs://cloud-cromwell-dev/cromwell_execution/travis/` messages.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066:539,message,messages,539,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066,1,['message'],['messages']
Integrability,Based on the following:; ```; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; ```. it looks like something isn't getting written to S3.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440778241:180,message,message,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440778241,1,['message'],['message']
Integrability,"Benchmarking results. ::Benchmark JsonEditor with circe.includeExcludeJson(_, None, Some(NonEmptyList.of(<some exclude key>)))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Exclude keys traversal: json size MB -> 14): 22.716215929999994 ms; Parameters(Exclude keys traversal: json size MB -> 32): 222.96187330999993 ms. ::Benchmark JsonEditor with circe.includeJson(_, NonEmptyList.one(""message""))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Include keys traversal: json size MB -> 14): 23.666746470000003 ms; Parameters(Include keys traversal: json size MB -> 32): 151.92092569000005 ms",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444:511,message,message,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444,1,['message'],['message']
Integrability,Bonus awesomeness â€“ removing this backend nerfs the vulnerable JDOM dependency that we would [otherwise have to upgrade](https://broadworkbench.atlassian.net/browse/BW-1228). ```; root(develop)> | 81> whatDependsOn org.jdom jdom2 2.0.6; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] | +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; [info] | ; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890:68,depend,dependency,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890,1,['depend'],['dependency']
Integrability,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:89,integrat,integration,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['integrat'],['integration']
Integrability,"Btw, the closes ""Closes"" in the commit message and the PR name are pointing to the wrong id.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928245:39,message,message,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242928245,1,['message'],['message']
Integrability,Build 3730. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3499629773500006 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$4(WorkflowFailSlowSpec.scala:30); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030:196,message,message,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-467169030,1,['message'],['message']
Integrability,Builds locally but Travis is NOT happy about class conflicts with the new dependency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314223845:74,depend,dependency,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314223845,1,['depend'],['dependency']
Integrability,"By default, the CloudFormation template will only give access to the bucket you specified at creation time as well as `gatk-test-data/*` and `broad-references/*`. To be able to access data in additional buckets you would need to grant `s3.*` to these resources through a policy that grants access to the bucket that you attach to `Cromwell-ServerStac-Ec2InstanceRole` (The exact name of the role depends on the name you gave the stack and some random characters cloud formation adds to prevent name collisions). In addition you need to add the same (or equivalent) policy to `GenomicsW-GenomicsEnvBatchInstance` role. This grants the batch worker EC2s access to the bucket. The policy would look something like:; ```json; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:*""; ],; ""Resource"": [; ""arn:aws:s3:::my-bucket-name"",; ""arn:aws:s3:::my-bucket-name/*""; ],; ""Effect"": ""Allow"",; ""Sid"": ""S3BucketAllowAllObjectOps""; }; ]; }; ```. In the `GenomicsEnvBatchJobRole` you would also need to attach a more restricted policy similar to:. ```json; {; ""Version"": ""2012-10-17"",; ""Statement"": [; {; ""Action"": [; ""s3:Delete*"",; ""s3:PutBucket*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name"",; ""Effect"": ""Deny""; },; {; ""Action"": [; ""s3:ListBucket*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name"",; ""Effect"": ""Allow""; },; {; ""Action"": [; ""s3:*""; ],; ""Resource"": ""arn:aws:s3:::my-bucket-name/*"",; ""Effect"": ""Allow""; }; ]; } ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-611697894:396,depend,depends,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-611697894,1,['depend'],['depends']
Integrability,Can that nice error message be removed from its `Option` container?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252:20,message,message,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289603252,1,['message'],['message']
Integrability,Can you also patch this in `wdltool` and any other dependencies that are as yet un-cromwell-repo-ified?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904:51,depend,dependencies,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2755#issuecomment-337259904,1,['depend'],['dependencies']
Integrability,Closing because it solves the wrong problem. The `WorkflowStoreCoordinatedWriteActor` mediates writes to this table from within the same Cromwell; multiple Cromwells will still interfere with one another by simultaneous calls to `fetchStartableWorkflows`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414701301:86,mediat,mediates,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414701301,1,['mediat'],['mediates']
Integrability,Closing for now - might be worth considering depending on what comes out of the retry meeting,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-275135813:45,depend,depending,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-275135813,1,['depend'],['depending']
Integrability,Closing for now. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996:100,rout,route,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996,2,['rout'],['route']
Integrability,Closing for now. Thank you all for looking. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088:127,rout,route,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088,2,['rout'],['route']
Integrability,Closing since the error message now contains (a) that a file was missing (b) the appropriate file name,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224:24,message,message,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-276483224,1,['message'],['message']
Integrability,"Closing this, as the above now fails fast during Materialization in cromwell/develop. Previously this would try to run several jobs and then fail (possibly) hours later. For the curious, from going through the git history, I believe https://github.com/broadinstitute/cromwell/pull/2647 is the first time this started failing fast in cromwell. From that commit's [Dependencies.scala](https://github.com/broadinstitute/cromwell/pull/2647/files#diff-0ecdbc5a001d52fb34f5eafb7cd1aaa6), here are the four commits on the wdl4s side around that time too: https://github.com/broadinstitute/wdl4s/compare/ba89da9...f63dc02",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700#issuecomment-345403662:363,Depend,Dependencies,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700#issuecomment-345403662,1,['Depend'],['Dependencies']
Integrability,Companion.scala:20); 	at akka.http.scaladsl.settings.ParserSettings$.default(ParserSettings.scala:119); 	at cromwell.webservice.CromwellApiService.$anonfun$workflowRoutes$68(CromwellApiService.scala:233); 	at akka.http.scaladsl.server.Directive$.$anonfun$addByNameNullaryApply$2(Directive.scala:134); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anon,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:1671,Rout,RouteConcatenation,1671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,Concatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$an,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3043,Rout,RouteConcatenation,3043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"ConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: command: ""/bin/bash"" ""/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/execution/script.submit""; [2017-01-20 09:31:16,78] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: job id: 2329; [2017-01-20 09:31:16,79] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918:4219,Message,Message,4219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875#issuecomment-274088918,1,['Message'],['Message']
Integrability,"Confirmed that AWS SDK has a dependency on netty version 4.1.22.Final from [line 93 of AWS SDK pom.xml](https://github.com/aws/aws-sdk-java-v2/blob/2.0.0-preview-9/pom.xml#L93). `<netty.version>4.1.22.Final</netty.version>`; ; This is likely coming from very old version of Scala HTTP client `val sttpV = ""0.0.16""` current version is `v.1.1.12` https://github.com/softwaremill/sttp/releases",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850:29,depend,dependency,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381408850,1,['depend'],['dependency']
Integrability,"Confirmed the error is improved in v39:. Looks similar to this. ; ```""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Please check the log file for more details: gs://ss_cromwell_bucket/cromwell-execution/exceed_disk_size/d142f233-f72a-40a6-9f84-8b8a2ead32e7/call-simple_localize_and_fetch_size/simple_localize_and_fetch_size.log.""```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379:70,message,message,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809#issuecomment-480981379,1,['message'],['message']
Integrability,Could we just add a `synchronized` if it's only a problem in the tests ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436831363:21,synchroniz,synchronized,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-436831363,1,['synchroniz'],['synchronized']
Integrability,"Couple of Qs (either way it's a better situation than current):. - Is it possible for write requests to enter into the metadata service after the check events message?; - What happens if the DB is still busy writing data (as you noted last week, that can get backed up)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289779759:159,message,message,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289779759,1,['message'],['message']
Integrability,"Cromwell Really Should Support pub-sub for workflow status notifications. There is definitely some kind of code in there already today, but it is not used in production and I don't know how complete it is. That said, it should be possible now to use the `/query` endpoint to get information about multiple workflows at once, such as all currently-running workflows, or all workflows started after X time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591356898:31,pub-sub,pub-sub,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591356898,1,['pub-sub'],['pub-sub']
Integrability,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:75,depend,dependencyTree,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087,2,"['bridg', 'depend']","['bridge', 'dependencyTree']"
Integrability,"Cromwell itself doesn't implement streaming for tasks in either WDL or CWL, mostly. The one exception, depending on how broadly one wants to define streaming, is Cromwell can understand a WDL ""hint"" (in quotes as there's not an official concept) that a cloud native path in a `File` variable can be handed directly to the task instead of converted to a local file on the POSIX filesystem. This is using the same sort of markup that DNANexus is using in the task metadata of the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734:103,depend,depending,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734,1,['depend'],['depending']
Integrability,Dependent on the completion of #4239,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-439428795:0,Depend,Dependent,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-439428795,2,['Depend'],['Dependent']
Integrability,Depending on how many words you have left... is this the right level to explain where the dotted lined call-input expressions come from (/what they're for)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5254#issuecomment-548511548:0,Depend,Depending,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5254#issuecomment-548511548,1,['Depend'],['Depending']
Integrability,"Depending on the issue, a cloud SQL issue could cause anything from a failed migration and delayed release to data loss and restore from backup. (The first is extremely more likely.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475:0,Depend,Depending,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726#issuecomment-328538475,1,['Depend'],['Depending']
Integrability,"Depending on what your monitoring script does and how long your command takes to run it's possible that the task finishes before the monitoring script had time to write / flush anything into the monitoring log.; I ran this and got an empty log . ```; task t {; command {; echo ""hey""; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```. but this gave me a non-empty one . ```; task t {; command {; sleep 5; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034:0,Depend,Depending,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034,1,['Depend'],['Depending']
Integrability,"Depending on your definition of `actual` it is actual, the devs themselves confirmed this :) Whether or not it can ever add up to be meaningful for us, who knows. . To be clear I think at most this would be a tiny effect, it just seemed like something which could take 5 mins to do as opposed to some of our real problems ;) As PO I wouldn't worry too much about this ticket :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504:0,Depend,Depending,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504,1,['Depend'],['Depending']
Integrability,Depends on #1115,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1121#issuecomment-230542274:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1121#issuecomment-230542274,2,['Depend'],['Depends']
Integrability,Depends on #1116,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1123#issuecomment-230548868:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1123#issuecomment-230548868,1,['Depend'],['Depends']
Integrability,Depends on #1280,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1119#issuecomment-239529170:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119#issuecomment-239529170,1,['Depend'],['Depends']
Integrability,Depends on #2660,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2661#issuecomment-332222843:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2661#issuecomment-332222843,1,['Depend'],['Depends']
Integrability,Depends on #482 #483,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/515#issuecomment-193897087:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/515#issuecomment-193897087,1,['Depend'],['Depends']
Integrability,Depends on #782,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/783#issuecomment-217204424:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/783#issuecomment-217204424,1,['Depend'],['Depends']
Integrability,"Depends on #782, #783",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/668#issuecomment-217204672:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/668#issuecomment-217204672,5,['Depend'],['Depends']
Integrability,"Depends on #785 (which it is currently prioritized ahead of in Ready for Development), #788 (which is currently in development), and #668 (in review)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-219158010:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-219158010,1,['Depend'],['Depends']
Integrability,Depends on (or maybe even duplicates) #658,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1162#issuecomment-234028065:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162#issuecomment-234028065,1,['Depend'],['Depends']
Integrability,"Depends on having call-scoped ~~BackendActors~~ Backends, which currently are a ways off.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/500#issuecomment-191929454:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/500#issuecomment-191929454,1,['Depend'],['Depends']
Integrability,Depends upon the completion on #1005,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1034#issuecomment-232369842:0,Depend,Depends,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1034#issuecomment-232369842,1,['Depend'],['Depends']
Integrability,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:311,depend,dependencies,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178,1,['depend'],['dependencies']
Integrability,"Did you see that ""token reclaimed"" message or not?. If not, we might also want to add some deathwatching around the JABJEA so that at least we don't run forever if it crashes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-261998434:35,message,message,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-261998434,1,['message'],['message']
Integrability,Different stack trace but same exception:. ```; Caused by: java.io.IOException: insufficient data written; at sun.net.www.protocol.http.HttpURLConnection$StreamingOutputStream.close(HttpURLConnection.java:3540); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:81); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:63); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:57); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:496); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-322288854:122,protocol,protocol,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535#issuecomment-322288854,1,['protocol'],['protocol']
Integrability,Do you have an example before & after?. It seems like the output would contain `[First $limitBytes bytes]` from `annotatedContentAsStringWithLimit` which is a pretty strange thing to have in the middle of an error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536:214,message,message,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536,1,['message'],['message']
Integrability,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:40,depend,dependency,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,2,['depend'],['dependency']
Integrability,"Does anybody know what happens when versions of dependencies differ between `cromwell` and `cromwell-backend`? I can find out empirically, though I wonder if there are any consequences of this we should be aware of... and if that is a bad thing, is there any way we can define dependencies so they are in lockstep with Cromwell?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192386095:48,depend,dependencies,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192386095,2,['depend'],['dependencies']
Integrability,"Does your Cromwell routinely restart in the manner described in the ticket description? If you're using it in production, that seems less likely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4001#issuecomment-561696328:19,rout,routinely,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001#issuecomment-561696328,1,['rout'],['routinely']
Integrability,During a recent production fire we logged an unexpected message and the state of a `StandardCacheHitCopyingActor` FSM which enabled us to fix a previously unknown bug. Are we still going to be able to log that sort of information?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430665873:56,message,message,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430665873,1,['message'],['message']
Integrability,"E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-valinor/cromwell-execution/test/02306258-436a-4372-ab54-2dcd83c42b47/call-hello | 2018-11-21 15:09:10.588000 | string |; | 4762 | 02306258-436a-4372-ab54-2dcd83c42b47 | commandLine | test.hello | NULL | 1 | echo 'Hello World!' > ""helloWorld.txt"" | 2018-11-21 15:09:10.767000 | string |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1399,message,message,1399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['message'],['message']
Integrability,Edit: ~~Dependent on the completion of #3344~~,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4239#issuecomment-439428495:8,Depend,Dependent,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4239#issuecomment-439428495,1,['Depend'],['Dependent']
Integrability,Error message from this failed test on PAPI ðŸ¤” . ```Could not copy gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/home/travis/build/broadinstitute/cromwell/common-workflow-language/v1.0/v1.0/gs://centaur-cwl-conformance/cwl-inputs/Hello.java to gs://cloud-cromwell-dev/cromwell_execution/travis/linkfile.cwl/5c134cd8-80d5-47d1-a635-e4dd5df2356d/call-linkfile.cwl/Hello.java```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137:6,message,message,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377046137,1,['message'],['message']
Integrability,"Excellent. So if I fix that in my conf, the messages should go away,; right? Can I specify docker.hash-lookup.method in the workflow_options?. On Fri, Aug 11, 2017 at 1:31 PM, Thib <notifications@github.com> wrote:. > If the image is not on dockerhub but exists locally to where the Cromwell; > application is running then it should be able to find the hash if docker.hash-lookup.method; > = ""local""; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321872886>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkxZ61CdCMTLR5po4xUtPAM1MnJ0sks5sXI_2gaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321874576:44,message,messages,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321874576,1,['message'],['messages']
Integrability,"Explicit failure means success:. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Unexpected character 'r' at input index 0 (line 1, position 1), expected JSON Value:\nruhroh\n^\n""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538:61,message,message,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740#issuecomment-327936538,1,['message'],['message']
Integrability,"FWIW 2: it's also possible to ""fix"" the tests by pushing the flush interval back to ~2s for the tests - thus reducing the chance that the probe messages are sent at the same time as a regular flush message. The downside to that was that the tests were taking 20 seconds each, which didn't feel great (and even though unlikely, there was still a small chance of an accident happening)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038:144,message,messages,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898#issuecomment-486855038,2,['message'],"['message', 'messages']"
Integrability,"FWIW, in the GATK world we just blanket refuse to support anything Windows. Every now and then we get a question from someone who edited a file manually on a Windows box -- but it happens *maybe* twice a year. I wouldn't advocate for putting a huge amount of effort into this beyond recognizing the error and providing an informative message if possible...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193:334,message,message,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273881193,1,['message'],['message']
Integrability,"FYI if ScalaCheck sneaks back into our dependencies again, and someone wants to regression test this, `sbt ""project <project>"" test` can be shortened to `sbt <project>/test`, for example:; - `sbt root/test` (same as `sbt test`); - `sbt core/test`; - `sbt gcsFileSystem/test`; - `sbt root/test:test` (also same as `sbt test`); - `sbt engine/docker:test`; - `sbt services/alltests:test`; - etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344:39,depend,dependencies,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1516#issuecomment-254668344,1,['depend'],['dependencies']
Integrability,"FYI there's a hidden watermark at the top of the file that one can use in PRs to tell if the RESTAPI.md was manually or automatically updated. Example: https://github.com/broadinstitute/cromwell/blame/31/docs/api/RESTAPI.md#L1-L8. Also if one doesn't have a dev environment locally they can still use any public sbt docker. It will take a while as it downloads ~the entire internet~ all of the un-cached cromwell dependencies, but something like this will work:. ```shell; docker \; run \; --rm \; -v $PWD:$PWD \; -w $PWD \; hseeberger/scala-sbt \; sbt generateRestApiDocs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043:413,depend,dependencies,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3562#issuecomment-385389043,1,['depend'],['dependencies']
Integrability,"FYI: Places we allow expressions:. | Position | Notes |; | --- | --- |; | Workflow declarations | Inject a task with matching dependencies. Use ""Local"" backend... But what if Local is not configured (e.g. FC) |; | Task declarations | Use the same backend. Inject a preceeding task (maybe evaluate everything at once?) |; | Task outputs | Use the same backend We'd need to insert a new task afterwards and rewire following tasks. We'd also need to back-fill results once they're known |; | Subexpressions | In the same context as the main expression |; | Scatter signatures | Actually this one doesn't work right now (sad!) but we could probably treat them just like workflow declarations |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377:98,Inject,Inject,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256066377,3,"['Inject', 'depend']","['Inject', 'dependencies']"
Integrability,Failure and RetryableFailure are 2 different messages so in this case I think this is a no-op ? We just never return RetryableFailure messages in Local ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102:45,message,messages,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/756#issuecomment-217231102,2,['message'],['messages']
Integrability,File this under TOL but I'll channel my inner @kcibul and point out that at some point we should look and see if swagger actually *can* generate a java client for us. making a wrapper to de-suck the java would be a lot less work than building this out to completeness.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2474#issuecomment-317097312:176,wrap,wrapper,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2474#issuecomment-317097312,1,['wrap'],['wrapper']
Integrability,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, the WEA doesn't bypass EJEAs anymore. When they receive an abort message they'll die if they don't have a BJEA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443:128,message,message,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600#issuecomment-342586443,1,['message'],['message']
Integrability,"For TPUs, good question but I don't know. From the [doc](https://cloud.google.com/tpu/docs/quickstart) it seems relatively different, but AFAICT PAPI doesn't support that yet so it'll also depend on how what information they expect to be passed in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3463#issuecomment-377288935:189,depend,depend,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3463#issuecomment-377288935,1,['depend'],['depend']
Integrability,"For our use cases, Iâ€™d say put responsibility on the pipeline developer. If thereâ€™s a collision, which file â€œwinsâ€ may be undefined, at least in the beginning. In the future, it could be useful to have the status set to Failed. However, weâ€™d like to distinguish â€œpipeline failureâ€ from â€œoutput failureâ€ in an automated way. So if itâ€™s recognized as a Failure, then it should codify the error status to determine the cause of failure without having to parse the error message. Perhaps one could introduce FailedWithWarnings status or something better.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467:467,message,message,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-466243467,1,['message'],['message']
Integrability,"For the SFS backend (as I understand it), Cromwell looks for the presence of an `rc` file to determine whether the job has succeeded or failed. However, Slurm may terminate the job if it runs overtime or over memory and Cromwell will hang indefinitely. I was hoping there might be a way that Slurm could notify Cromwell that the job had failed. . Only suggestion I've got at the moment is submitting a secondary job with a `afternotok:` dependency, and if the original job fails, the secondary job will write a non-zero `rc` file into the execution directory so Cromwell knows that something has failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483902912:437,depend,dependency,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483902912,1,['depend'],['dependency']
Integrability,"For the record, we use a custom backend that we specify as default, and specify local for some tasks with; ```; runtime {; backend: ""Local""; }; ```; and I've noticed those WARN messages in the logs but wasn't really concerned as everything works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647:177,message,messages,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-282616647,1,['message'],['messages']
Integrability,"Forgot to update this. I'm fairly certain of two things:; - This is an artifact of using mock jes; - There's a subtle bug somewhere. I've not been able to replicate this. I'm still not sure _what_ happened exactly nor why but I will jot down what I saw in case this comes up again. There were 2 jobs out of the 20k scatters which found their way back into the engine with FailedRetryable errors. In the code there are only 2 places where those are created and both involve preemption. On the engine side at the moment there's a direct assumption when this happens that the job was indeed preempted. I'm not certain how exactly this led to wacky behavior (and admittedly don't completely remember the details) but it appeared that the original ""preempted"" jobs did in fact complete and the preempted jobs never ran. In the DB the errors were CromwellFatalErrors w/ 500 messages in them. This should be impossible considering that when these FailedRetyrable things are created they're stuffed with a preempted error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225:868,message,messages,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-261786225,1,['message'],['messages']
Integrability,Found a good discussion. Will put it in a comment. https://stackoverflow.com/questions/442564/avoid-synchronizedthis-in-java,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328:100,synchroniz,synchronizedthis-in-java,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439102328,1,['synchroniz'],['synchronizedthis-in-java']
Integrability,"From @droazen . Alright, the commit to use to build the jar to run GenomicsDBImport (using the instructions above) is: d4d97fcbb59efd9acbf8fabca7361b59512755bb. The tool is passing integration tests at this point, and it is completely worth your while to profile the current version and see how it compares to the SelectVariants approach. It's worth mentioning that in the next week or so we will add one additional argument to the tool which might further help performance. You can track the status of this here: https://github.com/broadinstitute/gatk/issues/2613. Hand-off complete -- have a good weekend everyone!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007:181,integrat,integration,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2196#issuecomment-296326007,1,['integrat'],['integration']
Integrability,"From @ruchim, suggestion for the usage message:. ```; run <WDL file> [<workflow inputs file>] [<workflow options file>] [<workflow metadata output path>] [<WDL zip file>] [<labels file>]. WDL file - WDL file that contains primary workflow to be run. Required. Inputs file - JSON file that has inputs that get passed to WDL file. Optional. Workflow options file - JSON file that contains workflow level options. Optional. Workflow metadata path - File path where workflow metadata is to be outputted. Optional (NOTE: provide an example??). WDL zip file - zip file containing workflows being imported by the primary workflow. Optional (Not required if primary workflow is importing workflows that exist inside the same directory where ; Cromwell is being run.) (NOTE: maybe that last line isn't required??). labels file - JSON file containing labels applied to all tasks in the WDL file. Optional; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1939#issuecomment-294157945:39,message,message,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1939#issuecomment-294157945,1,['message'],['message']
Integrability,"From SLACK:. ```; 1. Can you make sure CaaS conforms to this loose list â€” ask questions about what any of this means â€” https://broadinstitute.atlassian.net/wiki/spaces/DSDE/pages/229212218/New+Service+Checklist. 2. This is an example of the â€œpaperworkâ€ required â€” https://docs.google.com/document/d/1vv0thxw1ESyO6k9pP1R0qqo1atEiMn4f-ynx6ns972Y â€” weâ€™ll need one of these for CaaS; ```; ```; 1. System diagram â€” shows the different components and how they connect to each other. 2. Data flow diagram â€” slightly different â€” shows how the data moves through the system. This should have a paragraph or so description walking through the different steps the data takes. . 3. Network Diagram â€” Actual network connectivity between components and how they connect to each other (inlcuding ports/protocols). Yes, I know they all sound the same. Theyâ€™re really really similar. Theyâ€™re not the same.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3792#issuecomment-398081689:787,protocol,protocols,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3792#issuecomment-398081689,1,['protocol'],['protocols']
Integrability,"From discussion with @geoffjentry, the purpose is to take this kind of logging out of the code; if someone wants to see these messages, they change the configuration of akka, instead (see http://doc.akka.io/docs/akka/current/java/logging.html#Auxiliary_logging_options). I've updated the PR description to reflect this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423:126,message,messages,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260#issuecomment-430401423,1,['message'],['messages']
Integrability,"From my point of view, I'd recommend merging this one ~~as is~~ once the tests are happy. If we start seeing issues with too many shells being spawned we can look again at routing the shell requests. ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1346/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664:172,rout,routing,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243148664,1,['rout'],['routing']
Integrability,"From my testing, it seems that anything that runs a ""chmod""-like command disrupts the ACL-controlled permissions, leading to permission denied and/or other errors. I think if the configuration option wrapped any commands that did this, it would fix the issue. In the meantime I was able to come up with a few workarounds to fix the permissions so that we were happy with the system (moved some files around so cromwell wasn't accessing or trying to move anything past our ACL, and added a ""chmod o-wrx..."" command to my submit script), but a configuration option that did this by default would be great!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828:200,wrap,wrapped,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374703828,1,['wrap'],['wrapped']
Integrability,"From the metadata:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for \""gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/\"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", line 205, in GetActiveProjectAndAccount\n project_name = properties.VALUES.core.project.Get(validate=False)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1221, in Get\n required)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1501, in _GetProperty\n value = _GetPropertyWithoutDefault(prop, properties_file)\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 1539, in _GetPropertyWithoutDefault\n value = callback()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py\"", line 693, in _GetGCEProject\n return c_gce.Metadata().Project()\n File \""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/credentials/gce.py\"", line 104, in Project\n gce_read.GOOGLE_GCE_METADATA_PROJECT_URI)\n Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044:61,message,message,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298633044,2,"['Message', 'message']","['Message', 'message']"
Integrability,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:842,depend,depending,842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592,1,['depend'],['depending']
Integrability,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends donâ€™t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldnâ€™t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1007,message,message,1007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112,6,['message'],['message']
Integrability,"Given that the critical portion of this issue was related to a google bug that has been resolved, I'm happy to close the issue. The original report was about the ""unexpected actor death"" log message, but that turned out to be a red herring relative to the underlying google bug. @geoffjentry, I'll leave it up to you as to whether leaving this issue open serves a purpose to you, but my original problem is solved, I believe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207:191,message,message,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262136207,1,['message'],['message']
Integrability,"Good news: the error message is now spot on. Bad news: unmarshalling error. ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.5, Date: Mon, 28 Jan 2019 23:23:57 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: ...""; }),HttpProtocol(HTTP/1.1)); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447:21,message,message,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224#issuecomment-458344447,2,['message'],['message']
Integrability,"Good news? This would also band-aid the jobs-never-running problem reported last week. From the token logs: . 6:22 PM :; ```; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 3367; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 3947,; ""atLimit"" : true; },; ...; ```. At 6:26 PM the `JobExecutionTokenDispenserActor` crashed with a stack trace similar to the one in this PR description. 6:27 PM:. ```; ""tokenTypes"" : [; ""queue"" : {; ""groupsNeedingTokens"" : [; {; ""hogGroup"" : ""porcine-project"",; ""size"" : 5; }; ],; ...; ""poolState"" : {; ""hogGroups"" : [; {; ""hogGroup"" : ""porcine-project"",; ""used"" : 16,; ""atLimit"" : false; }; ],; ...; ```. So the crash of the `JobExecutionTokenDispenserActor` not only lost the token assignments, but also the hog queues. The loss of token assignments leads to the fairly harmless condition of Cromwell handing out more tokens than it actually should (though emitting thousands of scary log messages in the process). But the loss of the hog queues means that the 3367 jobs that needed tokens at 6:22 PM would never receive them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667:1004,message,messages,1004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909#issuecomment-488007667,1,['message'],['messages']
Integrability,"Good point of discussion. I defaulted to ""`sbt test` by default runs all tests, for now"". Will leave it up to whomever you recommend as second reviewer to decide if integration tests should be excluded from `sbt test` as part of this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078:165,integrat,integration,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169449078,1,['integrat'],['integration']
Integrability,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:147,message,message,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643,2,['message'],['message']
Integrability,"Got it. . So in 36 you're a bit stuck in that it's hardcoded into the `v2alpha1` version of Pipelines API. You could use the `v1alpha2` PAPI backend, depending on if you're using PAPIv2 for a specific reason or just because it's newer (side note: Google would really prefer people to be using `v2alpha1`). As of 36.1 (just released today) that docker image is only pulled when running CWL (unclear if you're using CWL or WDL) and is configurable in the configuration file via `CWL.versions.VERSION.output-runtime-extractor.docker-image`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513:150,depend,depending,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466545513,1,['depend'],['depending']
Integrability,"Got over aggressive with last minute dependency thinning, removing some reflective dependencies for tests, but the concept is what you see there. Fixing locally, will confirm via travis, then merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265:37,depend,dependency,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141#issuecomment-231486265,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Happened again last night:. ```; The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; ```. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:148,message,message,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,6,['message'],"['message', 'messages']"
Integrability,"Having a hard time finding the right place to unit/integration test these changes using existing specs/mocks/centaur. If tests are required, a good bit of test refactoring will need to follow to cut through the Akka-HTTP-`Route`-to-`SubmitActor`-to-`ServicesActor`s layers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439:51,integrat,integration,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318459439,2,"['Rout', 'integrat']","['Route', 'integration']"
Integrability,"He wants us to emit a log message on every HTTP call to a google service, and we still don't do that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893:26,message,message,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368#issuecomment-324470893,1,['message'],['message']
Integrability,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:467,depend,depends,467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988,1,['depend'],['depends']
Integrability,"Hello! We seem to be running into a similar issue on cromwell `34-bda9485`. After humming along without a problem for a while, we all of a sudden stopped being able to run workflows with zipped WDL imports. Looking at the metadata, we get:; ```json; ""failures"": [; {; ""message"": ""Workflow input processing failed"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""pipelines%2Fdna_seq%2FUnmappedBamToAlignedBam.wdl: Name or service not known""; },; {; ""causedBy"": [],; ""message"": ""java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)""; },...; ```; Is this the same issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244:269,message,message,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244,3,['message'],['message']
Integrability,"Here is a random proposal... `LOG_MODE` can be one of `pretty`, `standard`, or `none`. This determines how Cromwell will log to stdout. Defaults are `standard` for server and `pretty` for command line. Then, independent of `LOG_MODE`, users can turn on workflow logging by setting `LOG_ROOT`, which can be overridden via workflow option (currently called `workflow_log_dir`, but perhaps we can synchronize the names of `LOG_ROOT` and `workflow_log_dir`??). If neither are set, then don't write workflow log. thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188398063:394,synchroniz,synchronize,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-188398063,1,['synchroniz'],['synchronize']
Integrability,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:186,message,message,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,3,['message'],['message']
Integrability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:333,message,message,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,4,"['depend', 'message']","['dependency', 'message']"
Integrability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:64,message,message,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,1,['message'],['message']
Integrability,"Hey @francares, just re-opening the ticket so that when the dependencies related to BackendConfigs and ShadowWorkflowActor are resolved, it can be then be worked on in the future. Hope thats okay!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209958715:60,depend,dependencies,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209958715,1,['depend'],['dependencies']
Integrability,"Hey @gauravs90 , so I'm going to rephrase to see if I understand what's happening.; Actors (`WorkflowExecutionActor`, `WorkflowActor`, `BackendActor`s ...), when mixing in this `CromwellProfilerFsm` trait, publish their state and data to the world via the akka event stream, on every state transition. Another actor, akka (pun intended) `TheProfiler`, reads those messages, and depending on where they come from, creates custom MetadataEvents, which it pushes to the metadata service. Does that sound close from what you're doing ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219040152:364,message,messages,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219040152,2,"['depend', 'message']","['depending', 'messages']"
Integrability,"Hey @gauravs90 - this looks like it shares a bit of work with the stuff I did in my #707 PR, trying to get message processing as far through the system as possible without backends. Luckily, you've focused in a different place (the actual validation) so combining/merging them shouldn't be too tough. The big differences I can see:; - I did the Materialization in a shadow actor to avoid interrupting the main one; - I moved backend assignment into my ShadowMaterializeWorkflowDescriptorActor; - MaterializeWorkflowDescriptorActor creates a data-only EngineWorkflowDescriptor. Literally just a BackendWorkflowDescriptor plus backend assignments. Having looked at your code though, I'm now unsure which is better; - It turned out I wasn't 100% correct first time so there was a lot of tidying up in the interfaces between the lifecycle states :-/ . Anyway, I've added you as a reviewer on my PR so you can have a look at what I've done - it'd be nice to try to work out where these things should go and maybe rebase or merge these PRs since they're making changes in similar places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658:107,message,message,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/709#issuecomment-210438658,2,"['interface', 'message']","['interfaces', 'message']"
Integrability,"Hey @geoffjentry. The formula downloads the .jar from the Github releases page and [creates a little wrapper script](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L11) that's used to make `cromwell` available as a command-line tool. There's no need to compile from source. In order to update the formula for future releases, you can just submit a PR to Homebrew by updating the [url](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L4), [SHA](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L5), and [install steps](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L9-L12). I'd be happy to do this in the future for future releases -- just include this step in whatever release checklist you may use. Homebrew is a pretty well-established community, so there's not much to contribute on that end. Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076:101,wrap,wrapper,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076,1,['wrap'],['wrapper']
Integrability,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:167,message,message,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182,2,['message'],['message']
Integrability,"Hey @plsysu,. Cromwell doesn't take care of dependencies, it's usually managed via dockers. However, another option is to add something to the start of your command block that exports the JAVA_HOME variable to point to version 1.8. Maybe something like...; ```; command {; export JAVA_HOME=`/usr/libexec/java_home -v 1.8`; java -jar gatk.jar ...; }; ```. Hope that works!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4410#issuecomment-440495817:44,depend,dependencies,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4410#issuecomment-440495817,1,['depend'],['dependencies']
Integrability,"Hey @rhpvorderman, I've started to use this for our workflows and seems to be working well! Props for this change :). I've got a small suggestion (not enough to raise an issue, and only if you're already making other changes), it would be awesome if Cromwell could log a message to say that it's copying files. I watch for that because then I know the task is starting properly. . Unrelated to that, I was wondering what hurdles might have to be overcome to devise a hashing-strategy based on your new `cached-copy` (that's not File / md5). You've mentioned [before](https://github.com/broadinstitute/cromwell/issues/2620#issuecomment-482565332) that this might be possible as it doesn't depend on the final path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924:271,message,message,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507916924,2,"['depend', 'message']","['depend', 'message']"
Integrability,"Hey Jing,. Regarding call caching misses -- there should be something message in the workflow metadata for why it failed, will you be able to share workflow metadata from the second workflow that fails to cache?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120:70,message,message,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440498120,1,['message'],['message']
Integrability,"Hi @AlekseiLitkovetc,. > I would like to know which two tests should pass with non-default credentials. Back in March when I filed the ticket we were only [including tests for four](https://github.com/broadinstitute/cromwell/blob/38/src/ci/bin/testCentaurAws.sh#L24-L27) of our dozens of Centaur tests. After the AWS hackathon, the test coverage expanded to only [exclude a few](https://github.com/broadinstitute/cromwell/blob/39/src/ci/bin/testCentaurAws.sh#L25-L53) remaining tests. So, the original minimum A/C was to see these tests passing with non-default creds:; - [hello](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/hello.test); - [long_cmd](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/standardTestCases/long_cmd.test); - [haplotypecaller.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/haplotypecaller.aws.test); - [singlesample.aws](https://github.com/broadinstitute/cromwell/blob/42/centaur/src/main/resources/integrationTestCases/singlesample.aws.test). > could you please prompt where I can find a link to Jenkins?. Our Jenkins servers are only internally accessible to Broad employees because of Jenkins continued problems with security. Perhaps one day we'll move off Travis to another public CI-as-a-Service that will run tests longer than 180 minutes, such as CircleCI or Google Cloud Build. Then we could migrate all tests to one place.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164:916,integrat,integrationTestCases,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-503831164,2,['integrat'],['integrationTestCases']
Integrability,"Hi @Bek - thanks to eagle eyed colleague @cjllanwarne - you're using the interpolation if/else syntax outside of a `task` `command` block, however you can instead use `if true_or_false then ""Foo"" else ""Bar""`. Also note that there's another issue lurking. In your `call` block you need to add `input:`, `input: message = var`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4963#issuecomment-491859810:310,message,message,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963#issuecomment-491859810,1,['message'],['message']
Integrability,"Hi @EvanTheB could you check something for me - you should be seeing a message like `Cromwell will watch for an rc file *and* double-check every {} seconds to make sure this job is still alive` when you start your job? (assuming `INFO` level logging is enabled). Then, with that background polling ongoing throughout the job run, if a full iteration of `exit-poll-timeout` has passed since the job stopped running, Cromwell will then mark the job as failed. If that gives you enough to put something more helpful into the docs that would be awesome! If not, I can maybe clarify a bit more? Otherwise we should hopefully be able to cycle round to improving this documentation _eventually_ (though unfortunately I can't make any stronger promises on an ETA than that!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172:71,message,message,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172,1,['message'],['message']
Integrability,"Hi @Redmar-van-den-Berg, you're correct, there appears to be a bug in our draft-2 parser which is failing to catch this. To answer ""which is correct"", the requirement to wrap values in arrays was not being enforced correctly but it now is. In your example you can do this with the array literal syntax, eg:; ```wdl; call ls {; input: files = [ i ]; }; ```. I have added a test for our WDL 1.0 support which **is** catching this properly, so if you're able to upgrade your workflows from WDL draft-2 to WDL 1.0, then `womtool validate` will give you the correct answer. If not, I'll leave this open as a bug since it certainly *should* be picked up by womtool. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219:170,wrap,wrap,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-454545219,1,['wrap'],['wrap']
Integrability,"Hi @TMiguelT, I worked on relative imports in Cromwell quite recently. The ideas about specifying the ""start point"" within the zip file did come up, but in the end people seemed more interested in relative HTTP imports (which is what I focussed on). I have two potential ideas for you which hopefully don't need Cromwell code changes. Hopefully these will help you - if not let us know!. ## Submit by URL. If you have a new version of Cromwell - since these changes were relatively recent - then you could try submitting the workflow to Cromwell by URL (based on your relative path, I'd guess the github hosted location you want would be https://raw.githubusercontent.com/h3abionet/h3agatk/1.0.1/workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl). ## Call into the relatively nested file. If submit by URL is out, you could perhaps make a top level ""wrapper"" workflow which immediately imports and calls `workflows/GATK/GATK-complete-WES-Workflow-h3abionet.cwl`. This should let you access it while keeping it's location relative to the other files in the repo safe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212:856,wrap,wrapper,856,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449025212,1,['wrap'],['wrapper']
Integrability,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:630,message,message,630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170,2,"['integrat', 'message']","['integration', 'message']"
Integrability,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:42,integrat,integrates,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,1,['integrat'],['integrates']
Integrability,"Hi @cjllanwarne -; I pushed a new commit. Can you check it now? Tests are failing because they expect messages formatted differently. I'll fix them if you say that other things are okay.; The contents of the file are still read twice, but now the main logic is located in one method. I tried to do this with one reading, but it caused a lot of headaches :) If necessary, I can explain in more detail what problems arise when reading a file only once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518:102,message,messages,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518,1,['message'],['messages']
Integrability,Hi @dheiman - I know you're already aware as you're on the openwdl mailing list but just wanted to point out that w/ the move to the OpenWDL governance Cromwell issues aren't the path to effect change in WDL. . I've been looking at how to transfer WDL related issues from this repo to that one but in the meantime note that the way to have changes find their way into the spec would be to discuss the topic on the mail list/gitter and ultimately to open a PR with proposed changes to the spec. See the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process) for more details.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-339786407:507,protocol,protocol,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-339786407,1,['protocol'],['protocol']
Integrability,"Hi @dinvlad ...; - The ""Docker"" backend referenced in application.conf is just using straight docker on the host machine. There's also ""JES"" backend which is using the google genomics pipelines API, which is effectively docker-as-a-service; - There are indeed plans to support both AWS and Azure. Some (probably crude at first) support for one of the two is expected within a couple of months. Over the course of then ext few quarters we expect to support both as well as other cloud vendors as well. In terms of how they'd be done, the answer is It Depends. There's the budding [GA4GH Task Execution API](https://github.com/ga4gh/task-execution-schemas) which was heavily inspired by the google genomics pipeline API. Our hope is to see other cloud vendors support this API, which would make our lives easier. Assuming that doens't happen, we've experimented a bit with Azure, and the remote docker approach has made the most sense. The actual outcome is not set in stone at the moment. Does this answer your questions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352:550,Depend,Depends,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239537352,1,['Depend'],['Depends']
Integrability,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... ðŸ˜„,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:486,rout,route,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956,1,['rout'],['route']
Integrability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:544,message,message,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,10,['message'],"['message', 'messages']"
Integrability,"Hi @freeseek,. (replying here because this is the currently open issue). The message; ```; 400 Bucket is requester pays bucket but no user project provided.; ```; should be addressed by specifying your project in the config as described [here](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059:77,message,message,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059,1,['message'],['message']
Integrability,"Hi @jainh,. The runtime section is very backend-implementation specific, but to answer your question in the abstract:. The wdl spec [says](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#runtime-section) that ""Values can be any expression â€¦"". For example:. Valid wdl string:; ```; runtime {; my_key: ""a 'b c' d""; }; ```. Valid wdl array:; ```; runtime {; my_key: [""a"", ""b c"", ""d""]; }; ```. The following however is **invalid** according to the spec as the keys are duplicated:; ```; runtime {; my_key: ""a""; my_key: ""b c""; my_key: ""d""; }; ```. It is then up to the backend to decide and implement what keys and values it will accept. The config backend is currently implemented to [only](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/DeclarationValidation.scala#L43-L50) support primitive wdl types (WdlInteger, WdlString, WdlFloat, WdlBoolean) and their optional wrappers. While it does not support them, one could update that code to support arrays of values too. Meanwhile, the JES backend already does support arrays for some attributes, e.g. for [`zones`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L127) and [`disks`](https://github.com/broadinstitute/cromwell/blob/839ea1e456b929d6149430f4d7d3805f8c235d3f/supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesRuntimeAttributes.scala#L142).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343:987,wrap,wrappers,987,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-332307343,1,['wrap'],['wrappers']
Integrability,"Hi @jjackzhn, this is really more of a WDL question than a Cromwell question (and if you'd like to change how this works, there is an active community managing the WDL spec [here](https://github.com/openwdl/wdl)). In the meantime, you can convert any `X?` into a non-optional `X` by using `select_first` ([see docs](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#x-select_firstarrayx)). Here's your `scatter` example modified to include `select_first`:. ```wdl; Array[String]? strings. # Put 'strings' into an Array[Array[String]?] with one entry: [strings1]; # Then, select the first value in that array which is defined.; # (Note: the workflow will fail if strings is not defined!); Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; ```. If you want to be safe in case the value is not supplied, you can wrap that into an `if`, but note that the output will also be optional now:. ```wdl; Array[String]? strings. if (defined(strings)) {; Array[String] strings_not_optional = select_first([strings1]). scatter (str in strings_not_optional) { ; call testtask{ input: str = str }; }; }. output {; # Let's imagine that testtask has a ""String out_string""; # Because it's wrapped in an 'if', it's now an optional output:; String? out_string = testtask.out_string; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738:916,wrap,wrap,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428327738,2,['wrap'],"['wrap', 'wrapped']"
Integrability,"Hi @kcibul it's about consolidating all of our various devops-y things into a more unified manner. Ideally we'd like to have the Push To DockerHub wrapped into our Jenkins. @hjfbynara can explain more, but from a strategic point, unifying how we move our Dockers around is a good thing, particularly for security and accountability. You can all meet but we're trying to make our DevOps work more unified. Please find me for this meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453:147,wrap,wrapped,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654#issuecomment-259459453,1,['wrap'],['wrapped']
Integrability,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:200,message,message,200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045,2,['message'],['message']
Integrability,"Hi @vsoch,. Lot's of good stuff here on first glance. I'll dive deeper over the weekend. For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the `.circleci/config.yml` into a script, or multiple scripts if necessary?. On a related note, based on your expertise I may want to pick your brain to go over our [existing CI scripts](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L38-L39) too as we move to Circle, or perhaps something even ~shinier~ [newer](https://news.ycombinator.com/item?id=17602838). Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228:110,depend,depending,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228,1,['depend'],['depending']
Integrability,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:365,integrat,integration,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921,1,['integrat'],['integration']
Integrability,Hi @yihming - thanks for providing the detail and log message. Please try removing the trailing `/` from the network url. so use `projects/gred-cumulus-sb-01-991a49c4/global/networks/vpc-cumulus-sb-01` instead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645:54,message,message,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645,1,['message'],['message']
Integrability,"Hi Jeff,. But the Google Storage backend endpoints still operate on the idea of an exponential backoff, as both are REST-based:. https://cloud.google.com/storage/docs/xml-api/overview. https://cloud.google.com/storage/docs/json_api/. So there is no way around needing to wait. Maybe having an exponential backoff would help instead of the `Thread.sleep(retryInterval.toMillis.toInt)` of 500 milliseconds, as now you're synchronizing a subset of threads to wake up at the same time - becoming similar to a [Thundering herd problem](https://en.wikipedia.org/wiki/Thundering_herd_problem) - which in this case is a subcluster of threads that would succeed instead of just one. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238893135:419,synchroniz,synchronizing,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238893135,1,['synchroniz'],['synchronizing']
Integrability,"Hi Sean,. Yeah I noticed that, and glad that it is in the process of getting fixed. Though this is precisely why we must have more control over the whole process, as creation of instances are basically just a cascade of events which get tied to an [**Operation**](https://developers.google.com/resources/api-libraries/documentation/compute/v1/java/latest/index.html?com/google/api/services/compute/model/Operation.html), through which you can interface with the VMs that are building or running. I have fairly high confidence that this is basically what is happening underneath the Google service endpoint when performing a [**RunPipelineRequest**](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53) through the [Pipeline API here](https://github.com/googleapis/googleapis/blob/c4899b3f0cef2caa73bb1a32baf00f54c8a49921/google/genomics/v1alpha2/pipelines.proto#L51-L53):. ```; rpc RunPipeline(RunPipelineRequest) returns (google.longrunning.Operation) {; option (google.api.http) = { post: ""/v1alpha2/pipelines:run"" body: ""*"" };; }; ```. If you think of this as a graph of best-effort networked dependent triggers via APIs, you can stabilize this to make it more predictable and scalable. It is just too obvious that we can collectively definitely make this better at this stage - as we already have the tools - and especially since we'll soon have nested workflows via https://github.com/broadinstitute/cromwell/issues/1532, which should be assumed to make the current number of operations in flight grow by several orders of magnitude. ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994:443,interface,interface,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260533994,2,"['depend', 'interface']","['dependent', 'interface']"
Integrability,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:922,message,message,922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094,4,['message'],['message']
Integrability,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:368,Message,Message,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['Message'],['Message']
Integrability,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:118,integrat,integration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852,1,['integrat'],['integration']
Integrability,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:62,message,messages,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717,4,['message'],"['message', 'messages']"
Integrability,"I agree that optional task inputs not being overridable from the inputs json is a bug and should be fixed. Regarding defaults vs fixing values, you're both right - values like that might be intended as inputs or might be intended as fixed intermediate values depending on the context and it's really hard to deduce which one an author intended just based on the WDL file. . That kind of confusion is exactly why WDL 1.0 (you're currently writing in draft-2) is adding `input` sections. Ie:. ```wdl; workflow foo {; input {; # input with default; Int threads = 1; }; }; ```; vs; ```wdl; workflow foo {; input {; ...; }; ; # intermediate value: cannot be overridden; Int threads = 1; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386302299:259,depend,depending,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3528#issuecomment-386302299,1,['depend'],['depending']
Integrability,"I also cannot seem to make the correct zip format. I get a message like this:; ```; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""/tmp/2995465807561959992.zip5068472134395796754/imports/zipped.wdl""; }; ],; ""message"": ""Workflow input processing failed""; ```. @ruchim have you worked out how to make the zip correctly?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3501#issuecomment-397130609:59,message,message,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501#issuecomment-397130609,3,['message'],['message']
Integrability,"I also encountered the udocker-singularity route in the discussion on cwltool singularity integration. Maybe it is an idea to take a closer look on the udocker-singularity implementation as a starting point for workflow tool singularity usage. . Or maybe not, because you will lose HPC friendly singularity features this way!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052:43,rout,route,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358310052,2,"['integrat', 'rout']","['integration', 'route']"
Integrability,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:556,Message,Message,556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,2,['Message'],['Message']
Integrability,"I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610:311,message,message,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610,1,['message'],['message']
Integrability,"I am not familiar with that error message. From a bit of Googling it looks like [this](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9) may be relevant. Assuming `cloud-lifesciences` is Google's project hosting the image that Cloud Life Sciences is trying to use to spin up the worker VM, you may need to add `projects/cloud-lifesciences` to your organization's [trusted image projects](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9#:~:text=the%20trusted%20image-,projects,-.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292:34,message,message,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292,1,['message'],['message']
Integrability,"I am of two minds on this. I would be happy to chat. Nonetheless, for now, let's not create impediments within the sprint by having outside work being a burden. Next sprint you can create a task that has only a few points relating to its integration not the actual coding and testing. . Thumb typed for added typos. > On Jun 19, 2015, at 10:19 AM, Scott Frazer notifications@github.com wrote:; > ; > Heh yeah.. though that velocity will be artificially high because a lot of the work I do for Cromwell (including this PR and the next PR) I do off hours.; > ; > â€”; > Reply to this email directly or view it on GitHub.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678:238,integrat,integration,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113549678,1,['integrat'],['integration']
Integrability,I assume the Blues have been made aware that successes will now be wrapped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171066630:67,wrap,wrapped,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171066630,1,['wrap'],['wrapped']
Integrability,"I believe I've found the issues with the jes and bad label tests, but I can't figure out why the other two failed. As far as I can tell, the relevant files for these two tests are. * [google_labels.wdl](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/google_labels/google_labels.wdl); * [wrapper.wdl](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/google_labels/wrapper.wdl); * [good_options.json](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/google_labels/good_options.json); * [google_labels_good.test](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/google_labels_good.test); * [google_labels_subworkflows.test](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/google_labels_subworkflows.test). It seems like the subworkflows test is just calling the good labels wdl as a subworkflow to check that it still passes, so I'm just going to assume that they are both failing for the same issue. The travis log, or at least the parts that I checked, don't contain much information besides the fact that the workflow failed. I'm not really sure how to approach debugging the test",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497359100:341,wrap,wrapper,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497359100,2,['wrap'],['wrapper']
Integrability,"I believe it is implicitly imported as a transitive dependency, which is to say we don't have any actual import statements for it... it will just disappear automagically with Alibaba.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175368391:52,depend,dependency,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175368391,1,['depend'],['dependency']
Integrability,"I believe that I am running into this problem with a batch of workflows.; I have a Cromwell instance running on GCE (launched via `docker-compose ... up`). Cromwell had gotten stuck accepting new workflow requests, so I shut it down and it didn't go down cleanly. After restarting Cromwell, I see:. ```; cromwell_1 | 2018-05-24 16:03:29,668 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; cromwell_1 | common.exception.AggregatedMessageException: Error(s):; cromwell_1 | Workflow a07583dd-f571-44bf-abb7-5bf281dfd249 in state Running and restarted = false cannot be started and should not have been fetched.; ```. with a lengthy list of workflows listed with the same error message. All of these workflows came to a stop. In fact, querying cromwell, I saw:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":0,""jobs"":0}; ```. I have been able to now restart cromwell and submit new workflows and get them running, but these other workflows were fairly well along. I would like to get them started again. What is the best way to do this?. ```; $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""32-c07d8d9-SNAP""}; ```. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784:725,message,message,725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784,1,['message'],['message']
Integrability,"I could see this being a place where workpulling from a central manager vs. pushing from a central router would be worth doing in the first go. Since these by definition are going to be slower operations although falling back to our typical ""let's see if it blows up before we get around to it"" can also work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-274619482:99,rout,router,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831#issuecomment-274619482,1,['rout'],['router']
Integrability,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:578,message,messages,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,5,['message'],"['message', 'messages']"
Integrability,I don't approve PR's but that seems like the obvious route to take.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-433518217:53,rout,route,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-433518217,1,['rout'],['route']
Integrability,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,integrat,integrations,244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633,2,['integrat'],['integrations']
Integrability,"I don't know what's going on with the labels. Ruchi, Gemma and I discussed this when I was Acting Delivery Czar for a day in your absence. It definitely was in the sprint at one point. A/C: at a minimum emulate the exception above and confirm that a running WorkflowActor does not crash and hang forever without making progress. Some very nice-to-haves would be understanding what actually is going on here; could we be failing faster with conspicuous broken credentials? Also de-scary any error messages a la the other `OneForOneStrategy` tickets as appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562:496,message,messages,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562,1,['message'],['messages']
Integrability,"I don't see any error logging associated with this cromwell hash. However, I did see this:. ```; 2016-05-03 10:14:45,314 cromwell-system-akka.actor.default-dispatcher-17 ERROR - BackendCallExecutionActor [UUID(643d3c46):CollectUnsortedReadgroupBamQualityMetrics:22]: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:349,message,message,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,4,['message'],['message']
Integrability,I don't see the dependency anymore so it looks good to me !,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1190#issuecomment-289556748:16,depend,dependency,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1190#issuecomment-289556748,1,['depend'],['dependency']
Integrability,I don't think it's been improved no. I have no idea how often users encounter this. It could be added as a low hanging fruit for User improvement though as it's not a big deal to make the error message more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391:194,message,message,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391,1,['message'],['message']
Integrability,"I don't think we override the entrypoint that's correct. If you're using a ""ConfigBackend"" technically you can choose what the docker command is so you could do something like ; ```; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash ${script}; """"""; ```. And in your WDL command call whatever the original entrypoint of your docker was (`/opt/FastQC/wrapper.sh` in your case).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007:451,wrap,wrapper,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-300922007,1,['wrap'],['wrapper']
Integrability,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:435,Message,Message,435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,1,['Message'],['Message']
Integrability,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:463,depend,dependencies,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941,1,['depend'],['dependencies']
Integrability,"I find that the wrapper bash script (...`/execution/script`) that Cromwell generates tries to capture stdout and stderr in a convoluted way:; - it redirects the command's stdout (and stderr, separately) to a named pipe (a.k.a. FIFO); - it than captures the results from the FIFO, and uses `tee` to make a copy to .../execution/stdout (and stderr, respectively); - it doesn't do anything to the stdout of `tee`. So, `tee` writes another copy to it's own stdout.; - SLURM captures `tee`'s stdout (inherited from the parent script) and writes it to ...`/execution/stdout`. Similary for stderr. So, both copies generated by ""tee"" end up in ...`/exection/stdout`! The output is *duplicated*! This causes problems with subsequent steps in the WDL script. To work around this, I've changed the `-o` and `-e` options to:; ```; -o ${out}.slurm -e ${err}.slurm; ```; noting that `${out}` has the same value as `${cwd}/execution/stdout` in my environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393:16,wrap,wrapper,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393,1,['wrap'],['wrapper']
Integrability,"I get the same error message when e.g. renaming an arbitrary `.txt` file to `.zip` - that's not necessarily what's happening, but a clue that the zip itself may be bad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859:21,message,message,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859,1,['message'],['message']
Integrability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:43,message,message,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,2,['message'],['message']
Integrability,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:21,message,message,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437,1,['message'],['message']
Integrability,"I have encountered the same problem, either local mode or server mode. version : comwell41; wdl: version 1. - error info: . ```; ""submission"": ""2019-06-28T08:36:56.384Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""/tmp/imports_workflow_63e53e21-b200-46b2-9653-db79983d6c1d_3805297415647673436.zip4786068963842572955/bcftools-task/bcftoolsView.wdl""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ```. - my wdl header. ```; version 1.0. import ""bcftools-task/bcftoolsView.wdl"" as select; import ""beagle-task/prephasing.wdl"" as prephasing. ```. - unzip -v impute_human_beagle_v1.zip. ```; Archive: impute_human_beagle_v1.zip; Length Method Size Cmpr Date Time CRC-32 Name; -------- ------ ------- ---- ---------- ----- -------- ----; 655 Defl:N 319 51% 06-28-2019 15:52 d52896d1 bcftools-task/bcftoolsView.wdl; 694 Defl:N 384 45% 06-28-2019 14:24 552eeedb beagle-task/prephasing.wdl; -------- ------- --- -------; 1349 703 48% 2 files; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137:247,message,message,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137,2,['message'],['message']
Integrability,I have to admit I'm among the guilty here - we make the `run` mode available to PacBio customers (via a Python wrapper that provides a friendlier CLI) who prefer to use the command line. Are there drawbacks to this from a black-box user perspective?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473:111,wrap,wrapper,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473,1,['wrap'],['wrapper']
Integrability,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:294,wrap,wrapped,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506,2,['wrap'],['wrapped']
Integrability,"I just found out that Cromwell-Singularity integration will be on the agenda on Winter Codefest 2018, starting tomorrow! See https://docs.google.com/document/d/1RlDUWRFqMcy4V2vvkA1_ENsVo6TXge2wIO_Nf73Itk0/edit#heading=h.xg79ql4rt605. You can join in (also remotely) by checking this file: https://docs.google.com/spreadsheets/d/1o4xDUgl2iu_CgFuDpB1swtG8XVZK3aifvKlhh5qagyI/edit#gid=0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186:43,integrat,integration,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-358296186,1,['integrat'],['integration']
Integrability,"I just want to point out that this used to work in Cromwell 29, so some sort of regression has happened such that sub workflows aren't working anymore. I'm not sure what kind of sub workflow integration tests you guys have, but it looks like they aren't comprehensive enough. Feel free to add this one to your test suite (it's actually not a super complicated sub workflow). . This is pretty important to some of the work we're doing with Gaddy to get the somatic genome pipeline ready (we can't run the samples for him). And the ultimate goal of this project is to bring more users to FireCloud...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823:191,integrat,integration,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358515823,1,['integrat'],['integration']
Integrability,"I like the idea here but I think we're probably now over the boundary of ""do it sync and then wrap the result in Future.successful"". A hidden cost to `isAlive` is that we have to create a shell process and wait for it to complete every time we query a job's status using it. @geoffjentry @kshakir if we have to fire up a new shell process for every status query, I will re-suggest my previous idea of putting the whole thing behind a routed actor pool so that we can at least rate-limit it... ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349:94,wrap,wrap,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242777349,2,"['rout', 'wrap']","['routed', 'wrap']"
Integrability,"I like this:; * it makes our dependency on bash explicit; * users can always turn it back if it doesn't work for them. I'm curous whether the `qsub` lets you specify what shell to use as a separate parameter rather than having to revert to full ""binary command submission"" mode?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3489#issuecomment-379263786:29,depend,dependency,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3489#issuecomment-379263786,1,['depend'],['dependency']
Integrability,"I may not know sbt enough to know if there is a way around, but by putting the backend implementations into the backend project, I think the backend jar will contain all the backend implementations, which is not its role.; It's providing an interface for any potential backend implementor and they might not want to also get all our ""supported"" backends with it.; My understanding of pluggable backend is that we have self-contained jars that only need the backend interface (and messages etc..) and cromwell-core. Those jars can be plugged-in and out seemingly in the conf file, and will be used by the engine. I think it's a good idea to also do that for our own backend implementations, even if they're then aggregated into the master cromwell jar.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209547143:241,interface,interface,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209547143,3,"['interface', 'message']","['interface', 'messages']"
Integrability,"I need this ability to label Google VMs for resource tracking, but have been thus far unable to have a VM labelled correctly. The jobs submit and run, but the labels do not show up. . From the documentation here (https://cromwell.readthedocs.io/en/develop/wf_options/Google/), it's not clear where the google-specific options are added, so I tried the following: ; ```; {; ""default_runtime_attributes"":{; ""zones"":""us-east1-b"", ; ""google_labels"": {""custom-label"":""custom-value""}; }; }; ```; I submit (Cromwell v42) with:; ```; curl -X POST ""<CROMWELL URL>/api/workflows/v1"" \; -H ""accept: application/json"" \; -H ""Content-Type: multipart/form-data"" \; -F ""workflowSource=@main.wdl"" ; -F ""workflowInputs=@inputs.json"" \; -F ""workflowOptions=@options.json"" \; -F ""workflowType=WDL"" \; -F ""workflowTypeVersion=draft-2""; ```. That submits/runs fine, but when I check the VM that spins up, I only see the two labels of `cromwell-workflow-id` and `wdl-task-name`. If I change the options JSON to anything else, e.g.; ```; {; ""default_runtime_attributes"":{""zones"":""us-east1-b""},; ""google_labels"": {""custom-label"":""custom-value""}; }; ```; then it fails to submit, returning:; ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Invalid workflow options provided: Unsupported key/value pair in WorkflowOptions: google_labels -> {\""custom-label\"":\""custom-value\""}""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533:1195,message,message,1195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533,2,['message'],['message']
Integrability,"I ran a couple workflows, against 0.19 and against develop (considering the workflow id wrapping issue is resolved). Here are some differences I found, there might be other that those workflow didn't catch.; - The ""Collector"" of a scatter is present in the metadata in develop, and wasn't in 0.19. Oddly it doesn't contain its output though : . ```; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ```; - `stdout` and `stderr` are missing (**Local only**); - `runtimeAttributes` is missing ; Completely missing on local.; On JES, only attributes in the WDL show up, those for which the default value was used are missing.; - `executionEvents` is missing (even if there is none, there is an attribute with an empty list in 0.19); - `cache` is missing (**Local only**); e.g. ```; ""cache"": {; ""allowResultReuse"": true; }; ```; - `inputs` at the call level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `inputs` at the workflow level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `outputs` at the workflow level is missing if there are no inputs (`""outputs"" : {}` was present in 0.19); - Scatter keys are shown in develop's metadata as a normal call:. ```; ""w.$scatter_0"": [; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ]; ```; - `submission` is missing; - In 0.19, all ""first level"" (non-shards) calls would appear in the metadata right away, with a `NotStarted` status and some basic available information:. ```; ""example.gatherUltimateAnalysis"": [; {; ""executionStatus"": ""NotStarted"",; ""shardIndex"": -1,; ""outputs"": {},; ""runtimeAttributes"": {},; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""array"": ""ultimateAnalysis.out""; },; ""backend"": ""JES"",; ""attempt"": 1,; ""executionEvents"": []; }; ]; ```. In develop, a call appears in the metadata only at runtime; - `backendStatus` has been renamed to `jesOperationStatus` (JES status)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341:88,wrap,wrapping,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341,1,['wrap'],['wrapping']
Integrability,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:196,wrap,wrap,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,1,['wrap'],['wrap']
Integrability,"I recently went through a scenario where there were two ways of combining a long list of intervals into 50 scatters -- depending on whether the actual intervals were contiguous or not--though its uncommon for them to be contiguous. My point is -- there are going to be times where it wont be obvious how to combine intervals ---I feel it may be best to publish an explicit task, and let users customize",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-496543757:119,depend,depending,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605#issuecomment-496543757,1,['depend'],['depending']
Integrability,"I saw that, but since `File?` isn't a compound type, I'm under the impression that example was for comparison to the actual compound types. That's the inconsistency I'm not groking -- since `File?` isn't an example of a compound type, it seems that example's existence implies that something that accepts a `File` should also accept a `File?`, which is indeed the case with Cromwell's integration for size() but not basename() or sub(). . If we relied entirely on what the spec's headings and examples said as being the only acceptable inputs, then basename() wouldn't work on `File` at all because the spec says it actually takes in a `String`, not a `File`, and has no `File` examples. Since basename() works on `File` it seems Cromwell is already going beyond what the 1.0 spec explicitly says.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450:385,integrat,integration,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450,1,['integrat'],['integration']
Integrability,I should have made this explicit in the previous comment: . I don't currently see a way we can support underscores in bucket names as long as we're using Google's GCS NIO filesystem. But I do think Cromwell can and should fail with useful and timely error messages when presented with bucket names that will not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174:256,message,messages,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174,1,['message'],['messages']
Integrability,"I solved it by myself. This issue is caused by region. I use default setting for region in aws.conf.; When I change to region accordingly (In my case, it is set to ap-northeast-2), it works well. I think the error message about this needs improvement a little more precisely. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-northeast-2""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965:214,message,message,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965,1,['message'],['message']
Integrability,"I still need to get my [terminology straight](http://martinfowler.com/articles/mocksArentStubs.html), but either a mock or a stub would have probably sufficed. I mainly wanted to feel like the code was ""self-documented"" a little in the tests. Instead, I put in a detector for a `cromwell-account.conf` that when present runs an integration test against the live ""gcr.io"". TODO: I still need to clean up access token caching, but there's lots of other code that may be critiqued.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172:328,integrat,integration,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-161046172,1,['integrat'],['integration']
Integrability,"I submitted the regular single_sample.wdl with the VIR_1923 .JSON that's; there...I guess I don't have permissions or something to access the files; that are listed. On Thu, Apr 14, 2016 at 9:01 AM, meganshand notifications@github.com; wrote:. > Not sure if this is a separate issue or not, but when @knoblett; > https://github.com/knoblett and I were submitting a workflow yesterday; > we got the exact same error message (submitted with Swagger). The issue for; > her was that there was an input that was specified to be a File type, but; > in reality it was just a String (so I'm guessing the issue was similar in; > that it couldn't find the ""file""). Unfortunately, it validated just fine,; > but we weren't able to submit it.; > ; > I'd be happy to provide the WDL and JSON files (both the broken version; > and the fixed version) but they won't attach in a github comment.; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921:415,message,message,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921,1,['message'],['message']
Integrability,"I take that back, the spec might not explicitly state this. However, that has always been the intention - the only ""order"" is via dependency",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235939607:130,depend,dependency,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235939607,1,['depend'],['dependency']
Integrability,"I think I covered this in another issue ""don't bother sending the message that nobody's waiting for"". I'll see if I can find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-298001488:66,message,message,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-298001488,1,['message'],['message']
Integrability,I think besides the log messages this is good. @cjllanwarne is the second reviewer,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/123#issuecomment-125791759:24,message,messages,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/123#issuecomment-125791759,1,['message'],['messages']
Integrability,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:70,message,messages,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855,1,['message'],['messages']
Integrability,I think one needs to be mindful of the current logging situation however when theyâ€™re adding logging messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3994#issuecomment-412110793:101,message,messages,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3994#issuecomment-412110793,1,['message'],['messages']
Integrability,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,adapter,adapter,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720,2,['adapter'],['adapter']
Integrability,"I think the ""CaaS"" comment at the end should be put into the headline (ie something like `CaaS wraps 404 into 500 on releaseHold requests`). And we should probably check this works in other cases where 404s might be returned, not just `releaseHold`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863:95,wrap,wraps,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406642863,1,['wrap'],['wraps']
Integrability,I think the PAPI Centaur `/bin/bash` dependency is purely an artifact of having a job shell effectively hardcoded to `/bin/bash` for the previous 31 releases of Cromwell so that unintentionally `/bin/bash` dependent WDLs were written into the test suite. ðŸ™‚,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761:37,depend,dependency,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697#issuecomment-392810761,2,['depend'],"['dependency', 'dependent']"
Integrability,"I think the interface part of this is done, but the implementation is definitely not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2523#issuecomment-326039856:12,interface,interface,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2523#issuecomment-326039856,1,['interface'],['interface']
Integrability,"I think this [happened](https://github.com/broadinstitute/wdl4s/blob/develop/wom/src/main/scala/wdl4s/wom/expression/WomExpression.scala#L23) as part of my ""wrap WDL expressions in WOM"" work, so this might be closeable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2521#issuecomment-326039066:157,wrap,wrap,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2521#issuecomment-326039066,1,['wrap'],['wrap']
Integrability,I think this depends on #652 (implement execution),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-213414391:13,depend,depends,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/655#issuecomment-213414391,1,['depend'],['depends']
Integrability,"I think this is actually a separate problem w.r.t. how cromwell constructs its script file. If you look in your GCS folder for the exec.sh, you'll probably see something like:. ```; echo hello && exit 1; cat $? > rc.txt; ```. Although there's no error message returned via the REST api, if you look in the server logs I suspect you'll see something along the lines of ""rc file not found""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109:252,message,message,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109,1,['message'],['message']
Integrability,"I think this is great! I would like @ruchim to take a look and see what she thinks about turning off issue creation after a certain date. Also, if we should close out all of the existing tickets after that date. Jira is open for folks to join and look at our board so this shouldn't be too big of a pain, but I don't know how we would get the message out to everyone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4995#issuecomment-495620362:343,message,message,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4995#issuecomment-495620362,1,['message'],['message']
Integrability,I think we should be aware when we have actors getting messages that are unexpected as it is almost certainly a bug.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467167480:55,message,messages,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467167480,1,['message'],['messages']
Integrability,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:392,depend,dependency,392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532,5,"['depend', 'message']","['dependency', 'message']"
Integrability,"I thought we were going to try @aednichols's idea for autocommitting the heartbeat writes (still batched, just not wrapped in one big transaction) to avoid having to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369:115,wrap,wrapped,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369,1,['wrap'],['wrapped']
Integrability,"I tried the same thing without call caching on gsa5, same version of cromwell as above. It's much faster, but still gets slower throughout the workflow. The scatter is made of three jobs each depends on the previous one. The first job, is submitted very quickly to SGE (same job was taking 5 secs per job to retrieve from call caching as per previous note). The second job is submitting faster than before, at 5 secs per job, with the following jstack:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-100"" #166 prio=5 os_prio=0 tid=0x00002b59440ad000 nid=0x17ac7 runnable [0x00002b5b700fe000]; java.lang.Thread.State: RUNNABLE; 	at scala.collection.Iterator$class.exists(Iterator.scala:919); 	at scala.collection.AbstractIterator.exists(Iterator.scala:1336); 	at scala.collection.IterableLike$class.exists(IterableLike.scala:77); 	at scala.collection.AbstractIterable.exists(Iterable.scala:54); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:88); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData$$anonfun$cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1$1.apply(WorkflowExecutionActorData.scala:87); 	at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247); 	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259); 	at scala.collection.AbstractTraversable.filter(Traversable.scala:104); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorData.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActorData$$upstreamFailed$1(WorkflowExecutionActorData.scala:87); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecuti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397:192,depend,depends,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278442397,1,['depend'],['depends']
Integrability,"I tried to further decompose `core` into submodules to be more exact about what I was bringing in to `languageFactory`, but I was ending up with 50 different sub-core projects and honestly I don't think super fine-grain imports are such a big deal now that the circular dependency problems are gone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3239#issuecomment-363230176:270,depend,dependency,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3239#issuecomment-363230176,1,['depend'],['dependency']
Integrability,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:35,message,message,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231,1,['message'],['message']
Integrability,I would like to merge this interface so I can continue with other task until all dependencies are satisfied. @cjllanwarne and @scottfrazer Can you review this small piece of code?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/678#issuecomment-208430231:27,interface,interface,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/678#issuecomment-208430231,2,"['depend', 'interface']","['dependencies', 'interface']"
Integrability,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:153,message,messages,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637,2,['message'],['messages']
Integrability,"I'd be okay with this, except; 1. I don't currently know how to have the test watch messages go from WorkflowActor to CallActor without logging.; 2. The test should not assume the second start message would be sent from the WorkflowActor before the first CallActor picks up its start message and begins running, and possibly even completes. . Log scraping is supposed to be easy with TestKit, something like:. ```; EventFilter.error(message = ""some message"", occurrences = 1) intercept {; // do something which should trigger such a log message; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394:84,message,messages,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394,6,['message'],"['message', 'messages']"
Integrability,I'd encourage interested parties (e.g. @drozen) to directly interface w/ OpenWDL,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344400450:60,interface,interface,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344400450,1,['interface'],['interface']
Integrability,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:159,message,message,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,3,"['Adapter', 'message']","['Adapters', 'message']"
Integrability,"I'll largely defer to @cjllanwarne, @mcovarr, or @danbills on the specifics, but it seems that you could specify the runtime attribute you need and how to interpret it by customizing the SLURM backend in the config:. https://cromwell.readthedocs.io/en/stable/backends/SLURM/. If I'm reading the docs correctly, it might be possible to inject your `module load` command into the `--wrap` argument.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382:335,inject,inject,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-505658382,2,"['inject', 'wrap']","['inject', 'wrap']"
Integrability,"I'll post here the full error message of the failed task, ok? The full Workflow is huge and probably has sensitive information on other tasks. ```python; failures: [{; causedBy: [{; causedBy: [{; causedBy: [],; message: ""Task MakeAnalysisReadyBam.BaseRecalibrator:9:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 19: unexpected exit status 1 was not ignored [Delocalization] Unexpected exit status 1 while running "" / bin / sh - c retry() { for i in `seq 3`; do gsutil - h\ ""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/st",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:30,message,message,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,2,['message'],['message']
Integrability,"I'm (obviously) not terribly good at spotting problems in the logic, but it seems good to me... I share your desire to make the interface a little better somehow... But perhaps that's a ticket for later?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-207589925:128,interface,interface,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-207589925,1,['interface'],['interface']
Integrability,"I'm _still_ working on implementing [this](https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318533390) integration test. At the moment, the develop cromwell client won't let centaur pass bad JSON for the test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778:120,integrat,integration,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2511#issuecomment-319978778,1,['integrat'],['integration']
Integrability,"I'm definitely ok with any ""ticket(s) for later"", as long as Miguel's comments in the parent PR are addressed. Retries should be _outside_ of the `Await.result()`, and by association should not retry within `runTransaction`. This PR is only one example that does an outer retry, choosing implementation details such as:; 1. Communicating ""whoops, please retry"" via a functional parameter `shouldRetry`, instead of wrapping the JDBC exception to our own expected-exception-for-retries, such as ~~`cromwell.actor.TransientException`~~ `lethall.actor.TransientException`.; 2. Invoking `withRetry` via an overloaded `DataAccess` method, instead having the caller invoke `withRetry`. In general, if a similar but different version of `withRetry` outside `runTransaction` appears, it'll get a thumbsup from me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-207650934:414,wrap,wrapping,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-207650934,1,['wrap'],['wrapping']
Integrability,"I'm fine with pushing this in without integration tests, assuming @tovanadler has manually tested. :+1: . Nominating @Horneth as second reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938:38,integrat,integration,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/356#issuecomment-169420938,1,['integrat'],['integration']
Integrability,"I'm getting the same error with this wdl: [mutect2-replicate-validation.zip](https://github.com/broadinstitute/cromwell/files/2242528/mutect2-replicate-validation.zip). And here is the dependency file: [wdl-dependencies.zip](https://github.com/broadinstitute/cromwell/files/2242529/wdl-dependencies.zip). This happens in versions 30 and up. When I run this in v29, cromwell doesn't throw an error but it hangs after completing the first task. . tsato@gsa5:novaseq: java -jar $wom validate mutect2-replicate-validation.wdl; Exception in thread ""main"" java.lang.RuntimeException: This workflow contains a cyclic dependency on m2.Mutect2.filtered_vcf; 	at wdl.draft2.model.Scope.childGraphNodesSorted(Scope.scala:53); 	at wdl.draft2.model.Scope.childGraphNodesSorted$(Scope.scala:44); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:46); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:46); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:97); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:14); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:10); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition$(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$ops$$anon$1.toWomWorkflowDefinition(WomWorkflowDefinitionMak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:185,depend,dependency,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,4,['depend'],"['dependencies', 'dependency']"
Integrability,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:550,message,messages,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,2,['message'],"['message', 'messages']"
Integrability,"I'm late to the party on this, but:. > Then chains of tasks could effectively become one task. I don't think merging of tasks works if you have certain resource or software dependencies, eg: inside a docker container. From a software engineering POV, is it easy / possible to detect and facilitate streaming between tasks like this, especially if they're scheduled as completely separate jobs? To me it sounds super difficult, like you'd have like a ""fuzzy"" dependency graph, and you could end up streaming your result data between nodes or tasks (and even worse if you're running on the cloud). (@mr-c, you've talked about this a [few times](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)). > parallel, rather than sequentially. Mostly, but what happens if two of the inputs are technically streamable, or even more complicated how would `stdin` fit into this. The [CWL documentation](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineTool) says that it requires the path (eg: [`$(inputs.stdinRef.path)`](; https://www.biostars.org/p/258614/#290536)) which to me sounds like it isn't exactly streamable, but `stdout` [implicitly is?](https://www.commonwl.org/v1.0/CommandLineTool.html#stdout) WDL in the version [1.0 spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#language-specification) doesn't include any reference to 'stream', so I'm surprised to see the DNAnexus adding a separate tagging mechanism for this optimisation. _Late edit: reformatting for clarity_; Engine support:. - Cromwell (not supported) [[source](https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734)]; - CWLTool (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644)]; - Toil (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)]. But piping (named and anonymous) is super easy in WDL because you have a command line, and in CWL yo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417:173,depend,dependencies,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417,2,['depend'],"['dependencies', 'dependency']"
Integrability,I'm pretty sure if you see this message it means that it wasn't able to get the hash at all,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882:32,message,message,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882,1,['message'],['message']
Integrability,"I'm trying it with 16g right now. It still slows down abruptly after completing the first task inside of the scatter. The first task is fast, but then the second task (which depends on the first) is slower.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289:174,depend,depends,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289,1,['depend'],['depends']
Integrability,"I'm wondering if this is related to cromwell creating new job definitions for **every** new call, versus using parameter substitution to modify the inputs for a single job definition? There may be some sort of backend issue with the integration to the AWS APIs that and old job definition is being called incorrectly instead of yet another new definition being created with the correct inputs? . This would track with the workflow log saying that the job definition already exists and then re-using a job that has inputs for a completely different sample.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451:233,integrat,integration,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004#issuecomment-501250451,1,['integrat'],['integration']
Integrability,"I've been playing around with different settings and testing this out a lot. I like it!. I think I might have found a bug with writing to GCS buckets. I'm using this as my options file:. ```; {; ""workflow_log_dir"": ""gs://sfrazer-dev/foobar"",; ""call_logs_dir"": ""gs://sfrazer-dev/foobar/calls""; }; ```. And I ran a workflow both locally and with JES. In both cases it seemed to write the call logs just fine. However, it seems the workflow log got lost somewhere. It did, however, create a file `gs://sfrazer-dev/foobar` with the contents of the file being `foobar`. I also got this odd error message. Indeed the stderr file that was uploaded was zero bytes. However, it is supposed to be zero bytes! I'll try again and send something else to stderr. Might just be a spurious error when your files happen to be zero bytes... ```; [2016-03-04 10:02:32,132] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log; [2016-03-04 10:02:34,490] [info] Got 'range not satisfiable' for reading gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log at position 0; assuming empty.; [2016-03-04 10:02:34,692] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886:591,message,message,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886,1,['message'],['message']
Integrability,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:796,message,messages,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,1,['message'],['messages']
Integrability,"I've gone through and updated the dependency graph of updating sttp, you can view diff at https://github.com/delagoya/cromwell/tree/update-depversions . Still one more set of errors to fix: ; ```; root(update-depversions)> | 31>; [info] Compiling 4 Scala sources to $HOME/src/cromwell/womtool/target/scala-2.12/classes...; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:46: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val thisLevelNodesAndLinks: NodesAndLinks = callsAndDeclarations foldMap { graphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:56: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val subGraphNodesAndLinks: NodesAndLinks = subGraphs foldMap { wdlGraphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:26: private val clusterCount in object GraphPrint is never used; [error] private val clusterCount: AtomicInteger = new AtomicInteger(0); [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:40: local default argument in method listAllGraphNodes is never used; [error] def upstreamLinks(wdlGraphNode: WdlGraphNode, graphNodeName: String, suffix: String = """"): Set[String] = wdlGraphNode.upstream collect {; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:54: value foldMap is not a member of Set[wom.graph.GraphNode]; [error] graph.nodes foldMap nodesAndLinks _; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:89: private method nodesAndLinks in class WomGraph is never used; [error] private def nodesAndLinks(graphNode: GraphNode): NodesAndLinks = {; [error] ^; [error] 6 errors found; [error] (womtool/compile:compileIncremental) Compilation failed; [error] Total time: 4 s, completed Apr 16, 2018 9:00:54 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626:34,depend,dependency,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626,1,['depend'],['dependency']
Integrability,"I've had a quick look at the code but I'm struggling with the Scala as usual, so I don't think I'd be up to making a PR to fix this. Will the tmpdirMin and outdirMin statements both map to `disk XXX SSD`, or will they be different disks with a specific mount path? I suppose they should be separate, in order to make sure each has at least the amount of disk space specified. And if they are different disks, how will I know where to mount them? I can't work out what the TMPDIR and HOME are being set to in a Cromwell job, and actually it will vary depending on the backend, I imagine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4507#issuecomment-449268020:550,depend,depending,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4507#issuecomment-449268020,1,['depend'],['depending']
Integrability,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:104,depend,dependent,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159,1,['depend'],['dependent']
Integrability,"IIRC the main driver for this was to be able to turn off cache copying. Google bills a bucket owner for egress and not the account copying out of the bucket, so a bucket owner is potentially at the mercy of Cromwell's cache hit routing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364:228,rout,routing,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324207364,1,['rout'],['routing']
Integrability,"IMO we should document this internally, since IMO the ""forever home"" for the process is to be wrapped up into the publish script for a new client version to be created every time we publish a new Cromwell version. Until then, I'd put the ""how to make clients"" docs either in our releasing Cromwell google doc, or as another heading in https://github.com/broadinstitute/cromwell/tree/develop/processes/release_processes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505468847:94,wrap,wrapped,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505468847,1,['wrap'],['wrapped']
Integrability,"If I recall correctly, it was proposed to not follow this route because the state consisted of the child workflow actor references in it, and was used to kill (or maybe abort) the workflow it was executing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310:58,rout,route,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-236298310,1,['rout'],['route']
Integrability,"If redness persists, consult your wdl4s reviewers (so that PR gets merged and the dependent artifact is published).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1818#issuecomment-270945817:82,depend,dependent,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1818#issuecomment-270945817,1,['depend'],['dependent']
Integrability,If that test is chronically failing due to Docker Hub flakiness we should tag it as Integration so it doesn't break our builds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762:84,Integrat,Integration,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166384762,1,['Integrat'],['Integration']
Integrability,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:39,integrat,integration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034,1,['integrat'],['integration']
Integrability,"If this error happened in production, the Cromwell process would terminate quickly and... presumably restarted by k8s or something. With these changes, the Cromwell process will sleep instead of terminating. Will this negatively impact startup time? I guess it would depend on how long a cold start takes to start initializing backends; I don't know how long that takes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756:267,depend,depend,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756,1,['depend'],['depend']
Integrability,"If we go this route -- let's make sure we know how we're going to verify; that the right behavior is happening. I'm not sure this can actually be; done... so verifying it is key. it also might end up being the same thing as setting the batch size to 1; which if that is acceptable is much easier to understand and maintain. On Fri, Nov 16, 2018 at 10:29 AM Ruchi <notifications@github.com> wrote:. > Dependent on the completion of #4239; > <https://github.com/broadinstitute/cromwell/issues/4239>; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-439429215>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g5nogt7ob-GFp0zPjnpHEOmkUs4Vks5uvtntgaJpZM4Xcgj4>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-439430669:14,rout,route,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-439430669,2,"['Depend', 'rout']","['Dependent', 'route']"
Integrability,"If we're going the metadata route, which I think I like too, given that w're ok with the fact that it will likely nearly double the size of the full metadata response, I think there's no point merging this because half of it will be deleted in the next PR no ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2335#issuecomment-306906462:28,rout,route,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2335#issuecomment-306906462,1,['rout'],['route']
Integrability,"If you are calling Cromwell in run mode, can you wrap it in a script followed by an AWS CLI command to copy the workflow log?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-448359294:49,wrap,wrap,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-448359294,1,['wrap'],['wrap']
Integrability,"If you want to alert general users of this solution, I can publish a FAQ on the support forum. If you'd rather it be limited to developer-minded users (i.e. the ones likely to scour the Cromwell repo), then merely having this ticket in your repository will be good. If you don't want to encourage it, then add a message to this thread about why you would discourage this--though what has already been said above may be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-255756516:312,message,message,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-255756516,1,['message'],['message']
Integrability,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:412,depend,dependencies,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,2,"['depend', 'message']","['dependencies', 'message']"
Integrability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:51,message,message,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,1,['message'],['message']
Integrability,"In previous versions of cromwell, I did not have this issue using the same inputs (WDL, local_application.conf, json, and dependent files referenced by the json).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-254813125:122,depend,dependent,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594#issuecomment-254813125,1,['depend'],['dependent']
Integrability,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:123,message,message,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502,2,['message'],['message']
Integrability,Initial implementation complete. Not yet closing this issue as integration tests are not yet operable and there are still several TODOs in the code. See commit f788704.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3426#issuecomment-382881466,1,['integrat'],['integration']
Integrability,"Integration test added in #4488, and FYI a patch added in #4508.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472:0,Integrat,Integration,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4414#issuecomment-453374472,1,['Integrat'],['Integration']
Integrability,"Interesting, could you look at the content of `stdout` and `stderr` for those stuck tasks ? Does it look like it's still doing work ?; It looks like you're running this locally on your machine ? How big are the samples in your `test.json` ?; I don't think the docker dead letters message are actually the issue (even though I'm not sure why they're appearing I'll look int that).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367032664:280,message,message,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367032664,1,['message'],['message']
Integrability,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:30,message,message,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640,1,['message'],['message']
Integrability,"Is this ""always on""?. I would have thought we would want a way to stop this log message happening 10,000 times per JES job?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193282727:80,message,message,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/507#issuecomment-193282727,1,['message'],['message']
Integrability,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:49,depend,dependent,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457,4,"['depend', 'synchroniz']","['dependent', 'depends', 'synchronization']"
Integrability,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:1038,Message,Message,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,1,['Message'],['Message']
Integrability,"It also depends on the situation Cromwell is dealing with, in other circumstances I/O was the main limiter for instance",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929479:8,depend,depends,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366#issuecomment-370929479,1,['depend'],['depends']
Integrability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:118,integrat,integration,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['integrat'],['integration']
Integrability,"It is turned on in Dev, evidence: https://sentry.io/broad-institute/firecloud-dev/issues/572001494/. Not sure what the protocol is to migrate these settings to Staging/Prod/Perf? @davidbernick ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394767306:119,protocol,protocol,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3709#issuecomment-394767306,1,['protocol'],['protocol']
Integrability,"It may be that youâ€™re running Cromwell 52 or later with an older AWS; CloudFormation built infrastructure. Can you share which build of Cromwell; youâ€™re using and the build/ version/ origin of the CloudFormation template?. On Tue, Jul 21, 2020 at 8:18 PM Sri Paladugu <notifications@github.com>; wrote:. > This can happen if the job fails meaning that an rc.txt file isnâ€™t; > created. It would be worth looking at the CloudWatch log for the batch job.; > â€¦ <#m_-7712250081708699723_>; > On Tue, Jul 21, 2020 at 4:07 PM Sri Paladugu *@*.***> wrote: Is there any; > progress on this issue? I am the getting the following exception:; > IOException: Could not read from; > s3:///results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt; > Caused by: java.nio.file.NoSuchFileException: s3://; > s3.amazonaws.com/s3bucketname/results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt; > â€” You are receiving this because you are subscribed to this thread. Reply; > to this email directly, view it on GitHub <#4687 (comment); > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662079379>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMJZ66Z5PIAEUX3IBLR4XYPZANCNFSM4G23FFUQ; > .; >; > Cloudwatch logs contained the following message: ""/bin/bash:; > /var/scratch/fetch_and_run.sh: Is a directory""; >; > â€”; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENOHHXQP6VC5XUGZ5TR4YV5XANCNFSM4G23FFUQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-670978468:1326,message,message,1326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-670978468,1,['message'],['message']
Integrability,"It ran the task and I don't see how it could have done that otherwise.. On Fri, Aug 11, 2017 at 10:13 AM, Thib <notifications@github.com> wrote:. > I'm pretty sure if you see this message it means that it wasn't able to; > get the hash at all; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321823882>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk3eHGDQKVm_OJyuRuQ8i9BrfJ1bqks5sXGGJgaJpZM4O0GvF>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321825726:180,message,message,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321825726,1,['message'],['message']
Integrability,"It should be fairly easy to add yes, we can also drill down deep in the caused by chain or not depending on how wide/specific we want to be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139:95,depend,depending,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139,1,['depend'],['depending']
Integrability,"It should be in-place (e.g. the source and destination tables are in the same database). . It would be nice to be automagic (e.g. liquibase), but just having a script in the repository to do the upgrade would be fine as well. It's really just for existing production customers who need to bridge the gap (GOTC/FC)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031:289,bridg,bridge,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/789#issuecomment-226603031,1,['bridg'],['bridge']
Integrability,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:716,message,message,716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178,1,['message'],['message']
Integrability,"It turns out the syntax change; https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather. I wonder if the documentation could be improved?. all the examples I found by googling things like 'wdl array iteration' found links to version 1 example. eventually, I stumbled on the idea of searching for wdl gather. Gather is not a standard term in computer science. Iterating over arrays does not require a scatter task. . I understand it is hard to write front ends with good error messages. I wonder if there is a way to write a something that checks for wdl version incompatibilities. I belive my womtool reported my wdl was valid. . Kind regards. Andy. Also there is a type in the code example https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather . ; ```; call sum {input: ints = inc.increment}; ```. should be; ```; call sum {input: ints = inc.incremented}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519:502,message,messages,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519,1,['message'],['messages']
Integrability,"It was taking 12-14 seconds between the time the GetStatus message was received by the ServiceRegistryActor and the time it was received by the EngineMetadataServiceActor. All sorts of other test stuff was executing in the interim, but the timeout for status queries previously defaulted to 10 seconds so that wasn't going to work. Added an explicit ~~30~~ 60 second ~~dilated~~ timeout as in many other spots.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006:59,message,message,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1057#issuecomment-227978006,1,['message'],['message']
Integrability,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:444,synchroniz,synchronized,444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,5,['synchroniz'],"['synchronization', 'synchronized']"
Integrability,"It would be very nice to support Mesos, as it is a very nice framework for in house cloud computing systems. Not everyone is able to launch their jobs into the cloud. I see you already have support for Yarn here: . http://cromwell.readthedocs.io/en/develop/backends/Spark/. And we also have this:; ```; A not so widely known fact is that Spark has its root in Mesos: it was ; initially developed at the AMPLab as a proof-of-concept Mesos ; framework to demonstrate how easy and fast ; it is to develop a distributed platform on top of Mesos; ```; taken from here : ; * https://mesosphere.com/blog/spark-mesos-shared-history-and-future-mesosphere-hackweek/. Spark and Mesos was really closely integrated, though I see that Spark has created their own scheduler, Mesos is still a very good way of running Spark jobs. It would be a very nice addition to the Chromwell framework! . Mesos is used in many other Big Data cloud environments outside of the Bioinformatics pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576:692,integrat,integrated,692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576,1,['integrat'],['integrated']
Integrability,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:7,depend,dependency,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193,2,['depend'],['dependency']
Integrability,"It's a theoretical possibility that we might write compressed to local disk, but unless I'm missing something, I don't think there's any way in Cromwell today that it could happen?. So my $0.02 would be that if we have an option in the IoActor to compress a file on write, then the least surprising outcome is that it gets compressed - regardless of FS. If for whatever reason in the future we want to write then read compressed files to local disk, we can always cross the decompression-on-read bridge when we come to it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551232213:496,bridg,bridge,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551232213,1,['bridg'],['bridge']
Integrability,"It's also worth noting that `MetadataPutFailed` manages to get logged both when we send it in the metadata service and where it's used, but we don't do anything other than those log messages. It seems like one would be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976:182,message,messages,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1811#issuecomment-269422976,1,['message'],['messages']
Integrability,"It's technically not what we call the IoActor anymore but instead _some_ actor that will route I/O related commands to either the IoActor or its proxy, so I came up with ""endpoint"" but I'm totally open to other naming suggestions !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-353687364:89,rout,route,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-353687364,1,['rout'],['route']
Integrability,"Ive also come across a few scenarios where a task was run that produced different output, depending on whether or not some flag was set in an input file. In most cases I just name the files the same, or use a glob, but there have been some scenarios where the resulting names were totally different with different extensions and I had to create seperate tasks for them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-328092858:90,depend,depending,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-328092858,1,['depend'],['depending']
Integrability,Just a heads up that the changes incoming on `mlc_existence` as currently structured move the execution of workflow-ID-dependent APIs (which is most of them) from `CromwellApiService` to `EngineMetadataServiceActor`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232187693:119,depend,dependent,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152#issuecomment-232187693,1,['depend'],['dependent']
Integrability,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. ðŸ™‚",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:103,message,message,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516,2,['message'],['message']
Integrability,"Just wonder why the runtime block has to be key-value pairs? `spark-submit` has tons of attributes and new attributes may be added in the future. Can the runtime block just be wrapped as string and passed to `spark-submit`?. If it has to be key-value pair, can the key be something like ""additionalArgs"" and the value be a string of containing attributes the user wants to add? for example:; `""additionalArgs"": ""--conf 'xx -Dxx' --name xx""`; In this way, if `spark-submit` has new attributes in the future, cromwell doesn't need to updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537:176,wrap,wrapped,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2640#issuecomment-331604537,1,['wrap'],['wrapped']
Integrability,LSocketImpl.writeRecord(SSLSocketImpl.java:717) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.sendChangeCipherSpec(Handshaker.java:1077) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.sendChangeCipherAndFinish(ClientHandshaker.java:1222) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.serverHelloDone(ClientHandshaker.java:1134) ~[na:1.8.0_72]; at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:348) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) ~[na:1.8.0_72]; at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.Run.status(R,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:8732,protocol,protocol,8732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,1,['protocol'],['protocol']
Integrability,"Lastly,; ```; 2019-01-31 20:30:56,569 INFO - changelog.xml: changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne: ChangeSet changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne ran successfully in 8ms; 2019-01-31 20:30:56,593 ERROR - changelog.xml: changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Change Set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne failed. Error: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSER",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:708,message,message,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,2,['message'],['message']
Integrability,"Leave this as a placeholder for now - it might not make sense depending on how the WOM stuff shakes out, but it might make a ton of sense.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-313468808:62,depend,depending,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-313468808,1,['depend'],['depending']
Integrability,"Lenthall doesn't have any other code changes for cromwell at the moment, as the dependency is currently listed at 0.19 stable. I don't like updating lenthall _to_ a snapshot unless needed, as from what I can tell sbt is [broken](http://stackoverflow.com/questions/37225775/idea-sbt-unable-to-reparse-warning) on snapshot resolving at the moment, with a recommendation of basically ""[just don't use snapshot dependencies with sbt](https://github.com/sbt/sbt/issues/2687#issuecomment-236586241)"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707:80,depend,dependency,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589#issuecomment-254657707,2,['depend'],"['dependencies', 'dependency']"
Integrability,Let's reply with honesty rather than sending an unrecognised message to an actor which probably has nothing to do with call caching anymore,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1108#issuecomment-229980714:61,message,message,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1108#issuecomment-229980714,1,['message'],['message']
Integrability,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:379,message,message,379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228,1,['message'],['message']
Integrability,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:4,message,message,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['message'],['message']
Integrability,Looking forward to some walkthrough as well to understand the new Path wrappers but this is really cool.; Is there a way this could get us rid of the `xxxProxy` classes ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276198438:71,wrap,wrappers,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276198438,1,['wrap'],['wrappers']
Integrability,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:575,depend,dependency,575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,6,['depend'],"['dependencies', 'dependency']"
Integrability,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:366,message,message,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279,1,['message'],['message']
Integrability,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:71,message,message,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499,2,['message'],['message']
Integrability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:32,message,message,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['message'],['message']
Integrability,"Lots of changes here for reviewer comments: this now works for expression dirents that evaluate to arrays of `WomSingleFile`s and not just ""scalar"" `WomSingleFile`s, although there isn't currently a test for arrays. We could possibly PR such a test into CWL (it's the same as the scalar test with the JS expression wrapped in square brackets)? We definitely shouldn't PR any tests for expression dirents that evaluate to `WomMaybePopulatedFile`s (scalar or array) since we wouldn't pass such a test atm. ðŸ˜¦",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377043537:315,wrap,wrapped,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3460#issuecomment-377043537,1,['wrap'],['wrapped']
Integrability,"Luckily, we now have a simple wrapper that can turn `WomValue`s into `WomExpression`s (which simply return the value given to them), so runtime attributes can be WomExpression, and the values given in the default runtime attributes can be easily converted into expressions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943:30,wrap,wrapper,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943,2,['wrap'],['wrapper']
Integrability,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs ðŸ˜…. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Meâ„¢.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:440,depend,depend,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698,1,['depend'],['depend']
Integrability,Merging as the failing test is that same stupid integration test w/ dockerhub failing on develop,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014:48,integrat,integration,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/337#issuecomment-166383014,1,['integrat'],['integration']
Integrability,Might depend on #688 / #683,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/692#issuecomment-208380569:6,depend,depend,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/692#issuecomment-208380569,1,['depend'],['depend']
Integrability,"Modify the `script-epilogue` to remove the sync operation. I also encountered the sync problem, and removing sync has not caused any identifiable issues. Cromwell will not start dependent jobs until the rc file is written to disk by the job and polled Cromwell, so I suspect this is safe. . See:; https://gatkforums.broadinstitute.org/wdl/discussion/9368/how-difficult-would-it-be-to-get-cromwell-working-on-slurm",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022:178,depend,dependent,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347#issuecomment-435468022,1,['depend'],['dependent']
Integrability,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:759,message,messages,759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897,2,['message'],['messages']
Integrability,"Most of the remaining backend dependencies on engine look straightforward to move to backend or core, though there are a few prizes like the direct DB updates from JES and Local backends. No heroics required, but do what's possible to remove unnecessary dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844:30,depend,dependencies,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/554#issuecomment-197620844,2,['depend'],['dependencies']
Integrability,"Must have forgotten this info... 1) develop (cromwell-23-79f6e12-SNAPSHOT.jar); 2) No, cannot believe I forgot again...; 3) No. On Fri, Nov 4, 2016 at 9:43 AM, Chris Llanwarne notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220; > - Is this .21, .22 or develop?; > - Did you capture the 'CMD \' thread dump from the JVM?; > - Was there an error message from Cromwell before it got stuck?; > ; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk5vBVr6UFscUsg3sazpo1H9pyVgMks5q6zaXgaJpZM4Ko1_r; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488:369,message,message,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488,1,['message'],['message']
Integrability,My comment to resurrect a hidden comment was also just hidden - so I'll drop it here instead :). You mentioned caching before as something that needs to be added to the BackendActor. Did you mean for the caching writing or reading? And did you foresee other command messages for the BackendActor being necessary for that/those?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200936859:266,message,messages,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200936859,1,['message'],['messages']
Integrability,My gut feeling is to be extremely relaxed about the first point. The WorkflowExecutionActor has already exited at this point so nothing exists that would be sending messages to the metadata service. The only problem would be if messages get reordered somehow which IMO is unlikely,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755:165,message,messages,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289783755,2,['message'],['messages']
Integrability,NB this will also require a fixup of https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/joint-discovery-gatk/joint-discovery-gatk4.wdl#L341,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102:120,integrat,integrationTestCases,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326#issuecomment-368974102,1,['integrat'],['integrationTestCases']
Integrability,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:45,message,message,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475,1,['message'],['message']
Integrability,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:21,contract,contract,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867,2,['contract'],"['contract', 'contracts']"
Integrability,"NOTE: depending on reviewer comments, will log TODOs as new tickets, or continue to try and refactor for this PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197415719:6,depend,depending,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/566#issuecomment-197415719,1,['depend'],['depending']
Integrability,"No longer blocked. It appears that the service account used by centaur tests for requester-pays testing is not compatible with the bucket used in `arrays` centaur-integration-test. Options:; - Use a workflow option to override-the-override, the service account back to the ""original"" service account specified in the JSON; - Reconfigure the centaur-integration-tests to not use an override of the requester-pays service account",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696:163,integrat,integration-test,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3846#issuecomment-409588696,2,['integrat'],"['integration-test', 'integration-tests']"
Integrability,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134,1,['integrat'],['integration']
Integrability,"No worries, that's what PRs are for. ðŸ˜‰ . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:622,message,message,622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991,1,['message'],['message']
Integrability,"Not a dumb question :smile: It shouldn't be a problem because `isValidPath` in the `GoogleCloudStorage` interface won't return true for `out` as a path, so the IoManager will skip `GcsInterface` and try `SharedInterface`. However it's not great because when it's actually a ""relative GCS path"", it wouldn't pass `isValidPath` and we wouldn't even have chance to ""correct"" the path and make it absolute before trying to read. This is why if you look at the ioInterface that is assigned in case of a JesBackend in the `WorkflowDescriptor`, it's directly the `GcsInterface`, not an `IoManager`. This way we skip the `isValidPath` check.. It's looks a bit hacky but in some sense there's no reason to have an IoManager with only one interface, and like you said there's no reason to provide a SharedFS interface to a JesBackend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162699613:104,interface,interface,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162699613,3,['interface'],['interface']
Integrability,"Not sure if this is a separate issue or not, but when @knoblett and I were submitting a workflow yesterday we got the exact same error message (submitted with Swagger). The issue for her was that there was an input that was specified to be a File type, but in reality it was just a String (so I'm guessing the issue was similar in that it couldn't find the ""file""). Unfortunately, it validated just fine, but we weren't able to submit it. . I'd be happy to provide the WDL and JSON files (both the broken version and the fixed version) but they won't attach in a github comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581:135,message,message,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581,1,['message'],['message']
Integrability,"Note for reviewers verifying testing changes: Travis, `sbt test`, and `sbt 'test-only cromwell.engine.db.slick.SlickDataAccessSpec'` no longer run the database integration tests. One must use either:. ``` bash; sbt 'integration:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. or:. ``` bash; sbt 'alltests:test-only cromwell.engine.db.slick.SlickDataAccessSpec'; ```. @cjllanwarne First reviewer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505:160,integrat,integration,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398#issuecomment-173965505,2,['integrat'],['integration']
Integrability,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:224,depend,depending,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757,2,['depend'],['depending']
Integrability,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:522,message,message,522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346,1,['message'],['message']
Integrability,"Note to those trying to reproduce-- the wdl is reported to run fine on the combination of:; 1. Local; 2. Small inputs. It should reproducibly fail on the combination of:; 1. Current JES / Firecloud (cromwell 0.18); 2. Normal (aka large) input files. The JES logs showed that it started localizing, and then four minutes later began delocalizing files. The STAR stderr printed that the program was starting, but there was no other error other message in the stderr nor jes log. AFAIK, there did not seem to be any other current indication via the Firecloud interface as to why the job was exiting prematurely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497:442,message,message,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497,2,"['interface', 'message']","['interface', 'message']"
Integrability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:204,depend,dependabot,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172,10,['depend'],['dependabot']
Integrability,"OOI, what's the use-case for this? I'm not saying this code change is bad (as a code change it looks totally fine) but the description is flagging up warning signs in my head. The WDL spec claims that workflows are robust to calls being specified in **any order** since the DAG is 100% implied by inter-call dependencies rather than list-order. Eg this should be fine:. ```; workflow x {; call b { input: i = a.i }; call a; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879:308,depend,dependencies,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879,1,['depend'],['dependencies']
Integrability,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:88,message,message,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236,3,['message'],['message']
Integrability,"Oh that's wicked! I haven't seen / heard of registries (being run with Singularity!) in the wild - did you use the same base and convert to Singularity recipes? Did you run as an instance, or run as instances with singularity-compose? Apologies for many questions, you've greatly peaked my interest! . Interesting story - the _very first_ design for a Singularity Registry (I was doing back in 2016/early 2017) was totally based with Singularity - but because we didn't have instances proper yet, there were too many issues to make it feasible to develop (and I jumped to Docker). But now that we have those dependencies / instances, I could definitely look at it again. The caveat (I suspect) is that we'd still need to run services with sudo - did you get around that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537245722:608,depend,dependencies,608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537245722,1,['depend'],['dependencies']
Integrability,Oh whoops never mind then. Agreed we should not be wrapping successes. :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171071369:51,wrap,wrapping,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171071369,1,['wrap'],['wrapping']
Integrability,"Oh, and `sbt assembly` already had _all_ of its tests disabled by build.sbt, so @geoffjentry's request that ""when one checks out the code, sbt assembly not fail on an integration test"" holds true without any other modifications. :wink:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288:167,integrat,integration,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441288,1,['integrat'],['integration']
Integrability,"Ok, just for clarification I will do the following:; 1. Remove validateRuntimeValue from wdl4s and create a PR in that repo.; 2. Add validateMemoryValue to JesInitializationActor.; 3. Refactor validateMemoryValue to return similar message than the other ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683:231,message,message,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212581683,1,['message'],['message']
Integrability,"Okay so my understanding of this so far (I'm just reiterating what you have in your diagram for my own benefit):. `ShadowWorkflowActor` has 4 ""lifecycle"" states as actors:; - `MaterializeWorkflowDescriptorActor` - basic validation and `WorkflowDescriptor` creation; - `EngineWorkflowInitializationActor` - spawns 1 or more `BackendWorkflowInitializationActor`, then aggregates results from all of these and message back `ShadowWorkflowActor`; - `EngineWorkflowExecutionActor` - sent a Start or Restart message, performs the execution, sends back result of execution. Spawns `BackendWorkflowExecutionActor`s; - `EngineWorkflowFinalizationActor` - do post-workflow termination actions. Spawns `BackendWorkflowFinalizationActor`s. _I'm just thinking out loud here... if you think this is stupid, I won't feel bad if you ignore me_. I guess the only comments on this scheme are about naming... . I feel like `MaterializeWorkflowDescriptorActor` should follow the same naming scheme and maybe be called something like `EngineWorkflowDescriptorActor` or `EngineWorkflowParserActor`. I'm also not a huge fan of prepending `Engine` onto these actors... maybe we can just drop `Engine`?; - `WorkflowDescriptorActor`; - `WorkflowInitializationActor`; - `WorkflowExecutionActor`; - `WorkflowFinalizationActor`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545:407,message,message,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209515545,2,['message'],['message']
Integrability,"One dependency to be removed, then ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/895/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/895#issuecomment-222008720:4,depend,dependency,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/895#issuecomment-222008720,1,['depend'],['dependency']
Integrability,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:152,message,message,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,2,['message'],['message']
Integrability,"Out of curiosity, did you observe a behavior that makes you think that tasks are not being run as early as they could be ?; Cromwell periodically traverses the ""unstarted"" nodes to determine if all their dependencies are satisfied, and if so starts the task, which should effectively run all tasks as soon as possible. *There is an exception to this for sub-workflows, tasks depending on a sub workflow output will only be run once the whole sub-workflow completes, even if this specific output is available before that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019:204,depend,dependencies,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019,2,['depend'],"['dependencies', 'depending']"
Integrability,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:311,message,messages,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,2,"['message', 'wrap']","['messages', 'wrapping']"
Integrability,"Overall, a code-based start of discussion. See also individual commits messages. Some or all could be integrated into #1198.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562:71,message,messages,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1199#issuecomment-234843562,2,"['integrat', 'message']","['integrated', 'messages']"
Integrability,PS Basically I'd like to close these ugly gaps in my pipeline ; ![image](https://github.com/broadinstitute/cromwell/assets/57629300/ebe89fb6-8420-486d-b52f-653f29fc73c5). The gap between `rc` generation and the `Status change from Running to Done` message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820:248,message,message,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820,1,['message'],['message']
Integrability,"Per Khalid's comments above we may want a new ticket to address the inconsistency in the WA / WEA supervision strategy wrt parent / child interactions: when the WEA died the the WA's supervision strategy said `Stop` (do not restart the crashed WEA), but then the WA sent the defunct WEA an abort message. The WA then parked itself in `WorkflowAbortingState` and waited for a `WorkflowExecutionAbortedResponse` that would never be sent from a WEA that no longer existed. . While the changes in this PR prevent the WEA from crashing in this specific (and alarmingly ordinary) circumstance (hooray!), I agree there are still general structural issues in the WA / WEA relationship we should address separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219:296,message,message,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219,1,['message'],['message']
Integrability,"Per standup this morning we've decided to close this. The requested functionality is problematic on a few levels:. JSON runtime attributes are currently interpreted as WdlExpressions after going through coercion, which presently is a backend-dependent process as only the backend knows the accepted runtime attribute coercions. While currently it might appear safe to treat the default runtime attribute JSON values as WDL with a `WdlExpression.fromString`, that's an assumption of WDL / JSON expression equivalence not made elsewhere in Cromwell and it seems questionable to pioneer that here. Currently Cromwell's backend assignments happen at initialization time, but in the future Cromwell's backend dispatch is likely to become more sophisticated and dynamic. It therefore wouldn't be possible to say at workflow initialization time the backend to which a call was fated, which would mean the default runtime attribute handling would need to happen in the various `FooRuntimeAttributes` classes as it does now. The refactor proposed here would actually remove the default runtime attribute handling at call execution time and therefore make dynamic dispatch more difficult to add in the future. Finally, while it appears technically possible to doctor tasks with workflow option-derived default runtime attributes in `MaterializeWorkflowDescriptorActor`, this makes for some pretty hacky code. I discovered at least 3 spots that needed to be updated:; - backendAssignment values; - NamespaceWithWorkflow -> tasks; - NamespaceWithWorkflow -> workflow -> children -> tasks. That last item was particularly hideous since `Workflow#children` is explicitly write-once. I got around this with subclassing, but I felt bad about myself afterward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247:242,depend,dependent,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-238635247,1,['depend'],['dependent']
Integrability,"Per the direction of the conversation on your integration testing doc, it sounds like there's a sentiment to disable integration (and maybe Docker) tests by default. . The wheel has chosen @scottfrazer but I suspect @geoffjentry may have some opinions here as well. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169450477,2,['integrat'],['integration']
Integrability,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:179,message,message,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631,4,['message'],['message']
Integrability,"Potential tech talk subject: WTH is `core` for? When I first made it, I intended it to be a grab bag of stuff that everything else sat on top. This is not the first time someone else had a differing view (e.g. someone had said at one point that they thought it was a place to put stuff that everything _did_ depend on, not just _could_ depend on). I don't really care what the answer is, although I'm thinking the nomenclature is no longer accurate and perhaps should be changed. Something to talk about after standup?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223559375:308,depend,depend,308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223559375,2,['depend'],['depend']
Integrability,"Probably, depending on what you mean exactly. The real impetus is for monitoring tools though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327:10,depend,depending,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327,1,['depend'],['depending']
Integrability,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After theyâ€™ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:897,rout,routinely,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,1,['rout'],['routinely']
Integrability,"Quoting @geoffjentry from earlier comment so that it's not lost:. > @gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676:257,inject,injection,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676,2,['inject'],['injection']
Integrability,"Re: `WorkflowManagerActorSpec`, over in my current PR I bumped up the event message timeouts, since it seemed to be taking more than 3 seconds for `ScatterWdl` to finish running.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194:76,message,message,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153539194,1,['message'],['message']
Integrability,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:302,rout,route,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065,5,"['rout', 'wrap']","['route', 'routes', 'wrapped', 'wrapping']"
Integrability,"Re: the `hsqldb compliant` commit: There should be multiple ways to check at runtime the database type, and then customize the code/SQL within the `CustomTaskChange`. For example, matching on the `liquibase.database.Database`, or go further and use the `Database.getDatabaseProductName`. The latter is a wrapper to `java.sql.Connection.getMetadata.getDatabaseProductName`. For HSQL it should return `HsqlDatabaseProperties.PRODUCT_NAME`, or the hard coded string `""MySQL""`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-243643308:304,wrap,wrapper,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1277#issuecomment-243643308,1,['wrap'],['wrapper']
Integrability,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:591,message,message,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['message'],['message']
Integrability,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:151,message,message,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126,1,['message'],['message']
Integrability,Red thumb required for two minor WOM changes (two files changed at the bottom of the PR - one gets a scaladoc and the other gets an error message improvement),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065:138,message,message,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065,1,['message'],['message']
Integrability,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:373,wrap,wrapper,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685,2,['wrap'],['wrapper']
Integrability,"Regarding testing, we could at the very least interrogate the message going to the IoActor in the `CarbonitingMetadataFreezerActorSpec` to make sure it's asking for the file to be compressed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754:62,message,message,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754,1,['message'],['message']
Integrability,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:190,wrap,wrapped,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848,2,['wrap'],['wrapped']
Integrability,"Results:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14192,message,message,14192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:152,message,message,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,4,['message'],['message']
Integrability,Safe to merge - the only centaur fails are due to changed message syntax in centaur and unrelated to this change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183:58,message,message,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1616#issuecomment-256419183,1,['message'],['message']
Integrability,"Sample of a possible new log message thread:; ```; [INFO] [...] [.../TestJesApiQueryManager-1262117937] Running with 1 PAPI request workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller1 has been removed and replaced by statusPoller2 in the pool of 1 workers; [WARN] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 terminated. 0 run creation requests, 5 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; [INFO] [...] [.../TestJesApiQueryManager-1262117937] PAPI request worker statusPoller2 has been removed and replaced by statusPoller3 in the pool of 1 workers. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147:29,message,message,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671#issuecomment-479685147,1,['message'],['message']
Integrability,"Saw DSDEEPB-1549 was already filed. Minor time scaling issue for jenkins, then :+1: as far as I'm concerned. Btw, anyone have any idea what's up with travis-to-coveralls integration's SSL errors? We don't have coverage results anymore? :crying_cat_face:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086:170,integrat,integration,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086,1,['integrat'],['integration']
Integrability,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:87,message,message,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496,2,['message'],['message']
Integrability,"Sayeth Aaron,; >(1) The way the APIs (all Google Cloud APIs, in theory) work is that any *default* value is omitted. So there isn't really a list: any field that has the default value would be omitted. >(2) The default values are the 'zero' values for the primitive types. This is an artifact of the Protocol Buffer to JSON conversion: https://developers.google.com/protocol-buffers/docs/proto3#json",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-477174761:300,Protocol,Protocol,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-477174761,2,"['Protocol', 'protocol']","['Protocol', 'protocol-buffers']"
Integrability,See #4414 for more info. We needed a reproducible test for deadlocks. This adds PR adds a deadlock-creating-test at the high-level using multiple cromwells in a Docker Compose. FYI- an earlier PR #4415 added very-very-low level deadlocks at the SQL/Database interface level.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4488#issuecomment-446621811:258,interface,interface,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4488#issuecomment-446621811,1,['interface'],['interface']
Integrability,"See my slack message. Unless you've fixed it recently, this is still an; issue. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> have you had a chance to retry; > it? If it's no longer a problem, we'll close it.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291561369>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_tbc5EfZFXbnN-7A0l7quaQjZgsks5rsnRBgaJpZM4KHqX4>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291571578:13,message,message,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1489#issuecomment-291571578,1,['message'],['message']
Integrability,Should I keep the error message to `JES` or should it say `Pipelies API`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751:24,message,message,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751,1,['message'],['message']
Integrability,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:273,synchroniz,synchronization,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,1,['synchroniz'],['synchronization']
Integrability,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:198,integrat,integration,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434,1,['integrat'],['integration']
Integrability,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:862,depend,dependencies,862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054,2,['depend'],['dependencies']
Integrability,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:23,message,message,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112,2,['message'],"['message', 'messages']"
Integrability,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:287,message,message,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623,2,['message'],['message']
Integrability,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so youâ€™ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:175,message,messages,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763,4,['message'],"['message', 'messages']"
Integrability,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:274,depend,dependent,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['depend'],['dependent']
Integrability,"So here is a final update. I have tried running Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:847,message,message,847,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,1,['message'],['message']
Integrability,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:21,wrap,wrapped,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,3,['wrap'],"['wrap', 'wrapped']"
Integrability,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:138,message,message,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067,1,['message'],['message']
Integrability,SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:241); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.runBatch(JesPollingActor.scala:67); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.cromwell$backend$impl$jes$statuspolling$JesPollingActor$$handleBatch(JesPollingActor.scala:58); at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anonfun$receive$1.applyOrElse(JesPollingActor.scala:36); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.statuspolling.JesPollingActor.aroundReceive(JesPollingActor.scala:22); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:3016,protocol,protocol,3016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['protocol'],['protocol']
Integrability,"Some information that may assist in debugging what is going on. I logged onto production to see what the current openfile count is and it is over 90K. Since we now have sufficient headroom to further investigate (ie java is not running out of open file handles) - I ran lsof on the java proc to see what it tells me. . First I looked at the beginning part of the ""path/name"" for the open file to see if there was some common part. awk ' { print $9 } ' < /tmp/lsof.out | cut -d '/' -f2 | sort | uniq -c | sort -nr; 50277 pipe; 25137 [eventpoll]; 14242 cromwell-workflow-logs; 1094 protocol:; 69 (stat:; 17 usr; 12 tmp; 6 dev; 2 etc; 2; 1 var; 1 app; 1 NAME. Obviously ""pipe"" and ""eventpoll"" are important areas to investigate to see if this is normal behavior or something odd. The third largest consumer ""cromwell-workflow-logs"" seemed interesting and if you look more closely I noticed that of the 14k over 11k are files that no longer exist (were deleted) but cromwell is maintaining an open file handle to it. . egrep cromwell-workflow-logs /tmp/lsof.out | awk ' { print $NF } ' | sort | uniq -c | head -4; 11541 (deleted); 1 /cromwell-workflow-logs/workflow.0005b906-d7be-4214-9943-0647a92c2c8e.log; 1 /cromwell-workflow-logs/workflow.000c5b14-0da4-4c2c-9a3b-f50377471820.log; 1 /cromwell-workflow-logs/workflow.001655a7-2c75-4a0f-b7cc-ba8ec96781ec.log. I also ran ls on the directory inside the container to see how many files exist. Of course the exact numbers don't line up because all these commands were run at different times (lsof was a snapshot ran at a specific time and my ls command is current time). docker exec -it cromwell_app_1 ls /cromwell-workflow-logs | wc -l; 2699. So at least one place that warrants further investigation is the code that reads/writes the cromwell-workflow-logs - something in there is not closing their file handle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-396592770:580,protocol,protocol,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716#issuecomment-396592770,1,['protocol'],['protocol']
Integrability,"Some of the ci builds are still failing bc of dependency conflicts, fixing that up now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7123#issuecomment-1529854583:46,depend,dependency,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7123#issuecomment-1529854583,1,['depend'],['dependency']
Integrability,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:125,depend,depending,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,3,"['Depend', 'depend']","['Depending', 'depending']"
Integrability,"Sorry @cjllanwarne , I just tested with:; ```; [; {; ""includeSubworkflows"": ""false""; }; ]; ``` ; on https://cromwell.caas-prod.broadinstitute.org which has version `36-fde91e6`, the problem appeared to me again:. ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@23304acb rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@233013e3[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 2178366]""; }; ```; Maybe the query is too ambitious? I should use pagination or more restrictive query instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873#issuecomment-455261953:241,message,message,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873#issuecomment-455261953,1,['message'],['message']
Integrability,"Sorry if this is a dumb question, but on a `SharedFilesystemBackend` that's using GCS inputs for a call, is it possible that `out` could be correctly interpreted as a ""GCS relative path""? i.e. wouldn't that have to be a local file? Per the docs and my understanding of this code, this only localizes down GCS inputs and doesn't delocalize outputs, and it didn't seem there's the concept of a GCS scratch bucket that could serve as the basis of a relative path. +1 to removing the possibility that JesBackend workflows could get their hands on a shared file system IO interface. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568:567,interface,interface,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568,1,['interface'],['interface']
Integrability,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:311,depend,dependencies,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159,1,['depend'],['dependencies']
Integrability,"Still fighting some dependency issues, there are two tests where I can only seem to get one or the other working. closing for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569:20,depend,dependency,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2092#issuecomment-289249569,1,['depend'],['dependency']
Integrability,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:49,message,messages,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983,2,['message'],['messages']
Integrability,"Sure, but generally I would advocate to leave open PRs that you might want other contributors to see (that they have not been fully addressed / might be integrated in the future). There isn't any harm in leaving it open, until it's irrelevant and has no chance of being looked over again!. This is of course my 0.02 and just an opinion, so if you feel strongly about closing it, that is up to you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416321838:153,integrat,integrated,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416321838,1,['integrat'],['integrated']
Integrability,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. â€¦2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:120,message,messages,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929,1,['message'],['messages']
Integrability,"TL;DR ðŸ‘ post rebase and minor comments. ToL 1: I dislike that one can't `Paths.get(Paths.get(""gs://bucket/path"").toString)`, and one must insert a `.toUri` for this to work. Hopefully this will be updated over in GCS NIO in the future and we can convert all the `.toUri.toString` back to `.toString`. ToL 2: Already discussed with Thibault. I concur that I don't like the proxy classes for long term. I'm picturing a custom cromwell wrapper that externally retries `Path.copy(): Unit`. Right now the proxy `.copy()` themselves are retrying internally while blocking the `Unit`. This custom wrapper should have a different behavior. Possibly a method returning a scala `Future`, even better Akka actors using messages and supervisors!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1519/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854:433,wrap,wrapper,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1519#issuecomment-255566854,3,"['message', 'wrap']","['messages', 'wrapper']"
Integrability,"TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076:86,message,messages,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076,1,['message'],['messages']
Integrability,"Tagging @jainh here just so I can collapse the conversation into a single place & not both here & PR #1216 . I'm fine w/ this since it shouldn't affect the outside behavior - technically the contract from Cromwell is that it is free to launch any runnable call at any point it wants. I do want to make sure that the ""WDL order"" change is never documented anywhere so that we're free to change it at any time, but IIRC #1216 doesn't do that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235972513:191,contract,contract,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235972513,1,['contract'],['contract']
Integrability,"Tests continue to run well. I forgot about documentation though, both a short blurb in the `/CHANGELOG.md`, and that linking over to as much documentation as needed under `/docs`. Not sure if you've used MkDocs before, but [the live docs are hosted here](https://cromwell.readthedocs.io/en/stable/search.html?q=mysql), and the docs can be tested locally by running `mkdocs serve` from the cromwell root directory and then browsing to http://localhost:8000. If you run into any issues drop us a message here. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504575501:494,message,message,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504575501,1,['message'],['message']
Integrability,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:218,integrat,integration,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533,1,['integrat'],['integration']
Integrability,"Thank you @aednichols . 1: I understand that it's free, and I'm only owed what I've paid for. :-). 2: A Unix tool running in the `WDLTesting/src/wdl`directory would report that `WDLTesting/src/wdl/Child/ChildWF.wdl` does not exist. But because it's being called from the main workflow, that still works. It seems to be that you can say ""all file references based off the launching directory work"", or ""no file references based off the launching directory work, you have to evaluate the reference based on the location of the file that made the reference"". Saying ""sometimes it will work, and sometimes it won't. We know you've been depending on this behavior, but we're nuking it anyway""? That, I would say, is rather user unfriendly. 3: All that said, that change, making a called workflow behave significantly differently from the starting workflow, was at least easily dealt with. (I wrote a python script to update the import statements.) But the change that called workflows no longer get passed anything from the JSON file. Given a choice between adding 10 - 50 parameters to each sub-workflow call, and just sticking with draft-2, we're sticking with draft-2. So, thank you for your time, and good luck going forward",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013:632,depend,depending,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013,1,['depend'],['depending']
Integrability,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations â†’ docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1387,message,message,1387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981,1,['message'],['message']
Integrability,"Thank you for looking into this! Since you encountered this problem in real life, is there any chance you have a minimal WDL / options that reproduces the issue? If so we can take care of turning that into a Centaur integration test so this doesn't regress.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986:216,integrat,integration,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986,1,['integrat'],['integration']
Integrability,"Thank you for the quick reply, Adam!. I wish the error message was a little more helpful, but this was definitely the issue and I have it working now!! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904:55,message,message,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904,1,['message'],['message']
Integrability,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:77,message,message,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421,1,['message'],['message']
Integrability,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:418,message,message,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,1,['message'],['message']
Integrability,"Thanks @illusional, I've come to a very similar configuration, albeit for singularity 3.X. I ended up settling on this:. ```; submit-docker = """"""; export SINGULARITY_CACHEDIR=/data/cephfs/punim0751/singularity_cache; module load Singularity/3.0.3-spartan_gcc-6.2.0; IMAGE=/data/cephfs/punim0751/${docker}; singularity build --sandbox $IMAGE docker://${docker} > /dev/null; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --userns -B ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. Just two things I'd like to discuss. Firstly, because you are pulling the docker image inside the sbatch script, this depends on the cluster you're working on allowing network access for the workers. While that is possible on our local cluster, my discussion with some sysadmins made me realise that this wasn't necessarily commonplace, and even on our cluster they strongly discouraged me from relying too heavily on it. This made me look for a solution that was even more generalizable. This is why I `singularity build` the image before I submit it, using the head node. This ensures that all network-requiring work is done on the head node, where network access is guaranteed. I also make sure to set a cache directory, so we don't download the same docker image multiple times in the case of a scatter job etc. Of course, if you do have network access for your workers and the admins have no issue with you using it, pulling the image from the worker is probably a better option to avoid hogging the head node. The second main difference in my config is that the singularity binary I was using did not have `setuid` permissions, meaning that I had to use the sandbox format, and run the image using `--userns`. This is obviously only required if your sysadmins don't trust `singularity`, but I think it's important to demonstrate a way of runni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475:554,wrap,wrap,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461281475,2,"['depend', 'wrap']","['depends', 'wrap']"
Integrability,"Thanks @kshakir! . Mint team just noticed a similar issue for a few of our workflows, where the workflow status in Cromwell is ""running"" but the VM instance is no longer running. These specific workflows were ""stuck"" on the first task and did not start any subworkflows. When trying to abort the workflow, I get the following error: . ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 467b64cc-9aa4-4eaf-85ef-16ed4d540d4c because no workflow with that ID is in progress""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046:364,message,message,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046,1,['message'],['message']
Integrability,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:590,message,messages,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,1,['message'],['messages']
Integrability,"Thanks for reporting this, you're right that those files should not have the same name. The files within the script are meant to capture the stdout and stderr of the user command only and are required for both WDL and CWL support. The files on the `qsub` command line are meant to capture all stdout and stderr including those log messages. The structure of the script has changed somewhat with Cromwell 32 but I suspect this problem still exists so we'll look at making a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393103059:331,message,messages,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393103059,1,['message'],['messages']
Integrability,"Thanks for reporting, @asmoe4. A few questions:; - Which Cromwell version are you using?; - Have you ever run this workflow successfully on a previous version of Cromwell?; - Does the workflow run on a backend other than AWS? You could try e.g. the [local backend](https://cromwell.readthedocs.io/en/stable/backends/Local/). ---. Developer notes:; - I've never seen the error message `error in opening zip file` before, a quick Google suggests it's coming straight out of `java.util.zip`; - The only recent zip change I can think of is one that Shouldn't Break Anything, https://github.com/broadinstitute/cromwell/pull/4399. If this is a regression (based on reported Cromwell version), that's where I'd start looking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608:376,message,message,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608,1,['message'],['message']
Integrability,"Thanks for reporting, @vruano - I've modified the title to reflect your discovery that this was caused by an empty scatter and that better logs from Cromwell would be nice when debugging. I think the yellow triangle in the UI would be a FireCloud change - perhaps you'd want to ask them to make a change too to make this even more visible? (the Cromwell team can only change what logs we produce and the error messages on failure, not how the UI interprets what we give them!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899:410,message,messages,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899,1,['message'],['messages']
Integrability,"Thanks for the brain dump. Chatting w/ some of our devops folks, they will work with us to move to CircleCI at some point. For now the team doesn't have the expertise nor bandwidth to evaluate how to do so securely. For example, we have several Hashicorp Vault rendered-secrets in our CI builds that should stay in the CI and not get vacuumed up into a docker image. I still want to ensure your code lives on, so for now [I submitted a PR](https://github.com/broadinstitute/cromwell/pull/4038) that takes takes your work above, wraps the `docker build` in a portable script, then adds it as [a parallel regression test](https://travis-ci.org/broadinstitute/cromwell/builds/420635707) under our common CI scripts. Whenever we move over from Travis to Circle it should move with the other scripts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052:528,wrap,wraps,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416272052,1,['wrap'],['wraps']
Integrability,Thanks for the quick replies. You're saying this happens for only a few shards in the same scatter ? If that's the case it would suggest this is some sort of transient failure of gsutil to authenticate properly but I'm not sure why that would result in this error message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112:264,message,message,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112,1,['message'],['message']
Integrability,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history ðŸ˜¡ so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:68,integrat,integrated,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,2,['integrat'],['integrated']
Integrability,"Thanks, ; It is a bit confusing since the usage message is `-p, --imports <value> A directory or zipfile to search for workflow imports` which seems to say both would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438699662:48,message,message,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438699662,1,['message'],['message']
Integrability,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:111,inject,injection,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855,2,['inject'],['injection']
Integrability,"That's actually pretty tough to do with the way Cromwell's backend interface is currently architected. We've run into this with HPC clusters as well. By the time something makes it to the backend interface it's already at the granularity of a single shard, and it's only on the **other** side of the backend interface that the code can be aware of the potential concept of an array job. It's certainly not an insurmountable problem (it's just code) but it'd be a fairly major architectural shift, and we know that there are users who have private backend implementations and thus we try not to change the interface as much as possible",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4707#issuecomment-469889900:67,interface,interface,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4707#issuecomment-469889900,4,['interface'],['interface']
Integrability,That's also a nice option indeed but streams only work if they can really run next to each other. If there are multiple tasks depending on the same file this become more difficult I think. Maybe a config value where the user can define if a subworkflow need to be completed or not to continue?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400586197:126,depend,depending,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400586197,1,['depend'],['depending']
Integrability,The AWS dependencies needs to be pruned. The entire known universe of AWS libraries [were pulled in](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/project/Dependencies.scala#L230) during the work-in-progress towards a new backend for cromwell. The image below was produced using [GrandPerspective](http://grandperspectiv.sourceforge.net/) on the expanded cromwell 33 jar. It shows the amount of AWS libraries that have been assembled transitively by that one `aws-sdk-java` [blanket](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.0.0-preview-9) dependency. ![cromwell_expanded](https://user-images.githubusercontent.com/791985/43986825-3ccde314-9ce5-11e8-8260-c9bbb66d3623.png). The dependencies should be slimmed down to only the required libs.; We don't need to have AWS [Route53](https://aws.amazon.com/route53/) etc. zipped in the jar to run workflows on the upcoming backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086:8,depend,dependencies,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086,4,"['Depend', 'depend']","['Dependencies', 'dependencies', 'dependency']"
Integrability,The EJEA is sending RecoverJobCommand or ExecuteJobCommand depending on if the workflow is restarting or not. So I think this is already done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623:59,depend,depending,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623,1,['depend'],['depending']
Integrability,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:422,Depend,Dependencies,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814,1,['Depend'],['Dependencies']
Integrability,"The `thread.sleep` command would need to be added to whichever actor(s) is actually submitting messages to the API. This doesn't strike me as too onerous for the developers, but you're right, it's definitely part of the scala and not the config files. Some minimal exception catching is also called for. Rather than throttling concurrent _jobs_ it probably makes more sense to limit the number and frequency of concurrent _workflow submissions_:; ```# Cromwell ""system"" settings; system {; ; # Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000 # No practical limit on the number of total workflows. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 4 # Too conservative?. # Number of seconds between workflow launches; new-workflow-poll-rate = 5 # Too conservative?; }; ```; This should stagger submissions without limiting the total amount of work being done. The number of threads available to the backend-dispatcher also appears to settable. You could create an artificial bottleneck there to protect AWS's API.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279:95,message,messages,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279,1,['message'],['messages']
Integrability,The current behavior depends on if this time out would cause GCP to return a preemption signal or another termination signal. They're not the same buckets of retries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293673942:21,depend,depends,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293673942,1,['depend'],['depends']
Integrability,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:155,depend,dependency,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532,1,['depend'],['dependency']
Integrability,"The effective difference is not all that noticeable, honestly... log messages will consistently have the right tag information and we'll be able to deliver server logs for individual workflows to users, but besides that they should look very familiar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-153211676:69,message,messages,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/254#issuecomment-153211676,1,['message'],['messages']
Integrability,"The error is different nowadays and apparently downstream of the fail. The underlying issue appears to be with not hydrating a `WomMaybePopulatedFile` output from a dependent job on restart: . ```; java.lang.UnsupportedOperationException: value is not available: WomMaybePopulatedFile(None,None,None,None,None,List()); 	at wom.values.WomMaybePopulatedFile.$anonfun$value$2(WomFile.scala:244); 	at scala.Option.getOrElse(Option.scala:121); 	at wom.values.WomMaybePopulatedFile.value(WomFile.scala:244); 	at cwl.internal.EcmaScriptEncoder.encodeFile(EcmaScriptEncoder.scala:102); 	at cwl.internal.EcmaScriptEncoder.encodeFileOrDirectory(EcmaScriptEncoder.scala:90); 	at cwl.internal.EcmaScriptEncoder.encode(EcmaScriptEncoder.scala:39); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$3(EcmaScriptUtil.scala:105); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScrip",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:165,depend,dependent,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['depend'],['dependent']
Integrability,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:76,message,message,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173,1,['message'],['message']
Integrability,The hash failures are expected with http inputs and should not be the cause of your workflow failure. Also we don't currently support `http` in engine filesystems. Do you see any other error messages than might provide some insight into what's happening?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166:191,message,messages,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166,1,['message'],['messages']
Integrability,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:208,wrap,wrap,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019,1,['wrap'],['wrap']
Integrability,"The jar size isn't viewed to be an issue. We can definitely trim dependencies going forward, but closing this for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-473106092:65,depend,dependencies,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-473106092,1,['depend'],['dependencies']
Integrability,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:438,depend,dependencies,438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685,2,['depend'],['dependencies']
Integrability,"The messages are logging the size of the list being (re-)added to the `BatchRequest`, not what's inside the possibly stale `ArrayList` inside the `BatchRequest` object.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408:4,message,messages,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408,1,['message'],['messages']
Integrability,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:14,message,message,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666,2,"['Message', 'message']","['Message', 'message']"
Integrability,"The next error is:; ```; 2019-01-31 19:38:58,499 INFO - changelog.xml: changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi: ChangeSet changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$Enhanc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:631,message,message,631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['message'],['message']
Integrability,"The one consistent thing should be: there should always be a ""message"" element in each entry, regardless of new/old format",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842811:62,message,message,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842811,1,['message'],['message']
Integrability,"The opinions from other engine developers in the WDL spec makes it sound like they're quite relaxed about this doubling scheme, so IMO it's fine to go ahead with this as-is. Depending on whether I get my way or not, we might or might not see a spec change in WDL 2.0 with a maximum memory ~requirement~ `runtime` attribute, above which Cromwell should not go, but that would be a bridge for us to cross in the future. TL;DR: ðŸ‘",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499549114:174,Depend,Depending,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499549114,2,"['Depend', 'bridg']","['Depending', 'bridge']"
Integrability,"The output which is produced by STAR might be named differently depending on the settings used. Currently this isn't supported yet in this task, but it might be in the future. Because of this I didn't want to make any assumptions on the naming of these files, in order to maintain flexibility in the use of these settings. After some consideration, though, for this specific pipeline I suppose these settings won't (or shouldn't) be changed. So, for now: Yes, the path to the BAI can be calculated in a more stable way. It still seems a bit awkward, though, that a rerun might try to produce a different file from the original run, but I'll close this for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395677185:64,depend,depending,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395677185,1,['depend'],['depending']
Integrability,The problem might be that `bamIndexPath` is an [optional String](https://github.com/biowdl/tasks/blob/d8fd75696ef2c04d0cc2876e77b38e80e0c37e6d/samtools.wdl#L4) whose value will vary depending on the path to the index file rather than its content. I think this should be a File type instead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395373551:182,depend,depending,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717#issuecomment-395373551,1,['depend'],['depending']
Integrability,"The pros / cons sort of mirror each other.; One pro of changing it is what @ffinfo suggested where you don't have to wait for the whole subworkflow to start downstream tasks.; However if your subworkflow is a coherent unit in the sense that it only really is successful if all of its calls complete successfully it might not be the desired behavior.; For example in the WDLs above, if task `Cat` is an expensive operation and the sleep task ends up failing, you could potentially have wasted time running `Cat` unnecessarily.; Of course this can be mitigated by having `Cat` depend on `Sleep`, but it's some sort of ""fake"" dependency. To be fair this behavior already exists with scatters so it might not be that much of a deal, but I remember it was brought up at the time. I think people could be surprised either way. Another not-quite-similar-but-related example is streaming of files from one task to the other, which [CWL lets you specify explicitly](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandInputParameter) (see `streamable` field).; You could imagine a scenario like this (if WDL had a similar streamable concept):. ```wdl. task A {; command {; ./my_script_generating_data.sh > streamable_out; echo ""hello"" > out; }; output {; File streamable_out = ""streamable_out""; File out = ""out""; }; parameter_meta {; streamable_out: {; ""streamable"": true; }; }; }. task B {; File in; command {; cat in | my_script_reading_data.sh; }; }. workflow w {; call A; call B { input: in = A.streamable_out }; call B as B2 { input: in = A.out }; }; ```. Where A and B would actually run simultaneously but B2 would have to wait for A to complete.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499:575,depend,depend,575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499,2,['depend'],"['depend', 'dependency']"
Integrability,"The reason for this is that there are different GCSFileSystems with different ""roots"". `gcsFilesystem` is rooted to the the workflow gcs directory. `GcsOutputsFileSystem` is rooted to the output gcs directory where the outputs are to be copied.; To convert a string to a GCS path we need a gcsFIlesystem, but depending on the string we'll use a different filesystem (the source string path will use the file system rooted to the workflow directory and the destination path will use the filesystem rooted to the output directory).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170649787:309,depend,depending,309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170649787,1,['depend'],['depending']
Integrability,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:128,message,message,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152,4,['message'],['message']
Integrability,"The tasks we run tend to have variable memory and storage requirements depending on the dataset we're processing in any given run. It would be nice to be able to set just minimum values and have Cromwell calculate what it should actually request based on ""some logic"" relating to the size of the input -> where the ""some logic"" is the difficult bit of course. For some tasks we have pretty good expectations of how the needs will relate to inputs, eg if I'm just copying over the same data with minor changes, but for others it could be hairy. . Frankly I don't think this should be made a priority, because my naive impression is that it will be really hard to do well, and the result will be a convenience, but nothing earth-shattering. There's a lot of other stuff I would want to have first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265:71,depend,depending,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301220265,1,['depend'],['depending']
Integrability,"The token message is a safety-catch in the token distributer. If the job actor exits in an unexpected way without returning its execution token, the token distributer will spot that and reclaim the token anyway. . So the underlying issue here is that the job actor is crashing or exiting inappropriately. EDIT: Here, ""job actor"" == `EJEA`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188:10,message,message,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612#issuecomment-255765188,1,['message'],['message']
Integrability,"There is a second issue that came up after Google fixed the first issue that is oddly ironic given your comment. If they hit API quotas, it appears that they were immediately failing the operations, leading to some odd error messages passed through to Cromwell (and then to me). In a sense, they were failing too quickly. Tough balancing act, it seems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439:225,message,messages,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439,1,['message'],['messages']
Integrability,There is an open ticket on the akka github for this: https://github.com/akka/akka-http/issues/907; It's out of our control AFAICT. We've taken every precaution to shutdown things in order. The message is harmless although confusing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332242945:193,message,message,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332242945,1,['message'],['message']
Integrability,"There is something that could be better in terms of coherence I think, it's not critical though so I've let it as is for now.; In the DataAccess.scala which is basically the Database API, some methods take `Call` objects and some take FQNs (actually now that we added the index it's respectively `CallKey`s or `ExecutionDatabaseKey`s).; But the slick implementation only needs the FQN and the index to do its magic, at no point the actual Call object is used. I think it could be nice to use only ExecutionDatabaseKey-like types (which are basically wrappers for (FQN, index)), instead of Call objects. Plus this helps decoupling binding from the engine :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/156#issuecomment-135144035:550,wrap,wrappers,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/156#issuecomment-135144035,1,['wrap'],['wrappers']
Integrability,"There's a good forum post on this topic here, including not only `includeKey` and `excludeKey` but also increasing Akka HTTP timeouts. We might want to change those timeout defaults in `reference.conf` if users are seeing this routinely even with those filters. https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694:227,rout,routinely,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-326019694,1,['rout'],['routinely']
Integrability,"These changes appear to break submissions in service account mode. Removing the `GenomicsScopes.all` from `GoogleScopes` fixes the problem but (presumably) will break `application-default`:. ```; 2015-12-21 14:05:11,203 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:378,message,message,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,3,['message'],['message']
Integrability,"These changes are primarily focused on getting as much code into Standard as possible. Getting JES and SFS perfect wasn't a goal here though. There's a lot more work that could be done to tighten up each of those backends. Regarding JES/GCSFS: the proxy classes, that inject the custom file system providers, could likely be merged with `GcsPath`. I also have my eye on further cleaning up a lot of the path mapping, someday. For example JES calls something like `callRoot.resolve(path.stripPrefix(""/""))` in two different classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649:268,inject,inject,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1930#issuecomment-276229649,1,['inject'],['inject']
Integrability,"These tests are possibly valuable but will depend on having an engine database, so this should really be 0.21 and not 0.20.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1040#issuecomment-228485970:43,depend,depend,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1040#issuecomment-228485970,1,['depend'],['depend']
Integrability,These types should not depend on anything WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2660#issuecomment-332220784:23,depend,depend,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2660#issuecomment-332220784,1,['depend'],['depend']
Integrability,This PR includes all of the changes in https://github.com/broadinstitute/cromwell/pull/5710 . Can you create this PR such that it depends on that PR/branch and only includes the new changes so that it is easier to review just these changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669271678:130,depend,depends,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669271678,1,['depend'],['depends']
Integrability,"This PR merges into @Horneth's branch. Good news: The tests-formerly-known-as-root are now running under `server`!; Bad news:; ```; *** 3 TESTS FAILED ***; SimpleWorkflowActorSpec:; A WorkflowActor should ; - start, run, succeed and die *** FAILED *** (10 seconds, 174 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; SimpleWorkflowActorSpec:; A WorkflowActor should ; - fail when a call fails *** FAILED *** (10 seconds, 35 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 1 messages on InfoFilter(None,Right(Starting calls: wf_goodbye.goodbye:NA:1$),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.CromwellTestKitSpec$.waitForInfo(CromwellTestKitSpec.scala:129); at cromwell.SimpleWorkflowActorSpec.$anonfun$startingCallsFilter$1(SimpleWorkflowActorSpec.scala:186); ...; WorkflowExecutionActorSpec:; WorkflowExecutionActor ; - should allow a backend to tell it to retry... up to a point *** FAILED *** (10 seconds, 170 milliseconds); java.lang.AssertionError: timeout (10 seconds) waiting for 3 messages on InfoFilter(None,Right(Starting calls: wf_hello.hello),false); at akka.testkit.EventFilter.intercept(TestEventListener.scala:114); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActorSpec.$anonfun$new$1(WorkflowExecutionActorSpec.scala:84); ```. Still, as this isn't merging into develop feel free to press merge if you'd like.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580:345,message,messages,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3375#issuecomment-371344580,3,['message'],['messages']
Integrability,"This appears to be the way it already works. ``` scala; private def preempted(errorCode: Int, errorMessage: Option[String], jobDescriptor: BackendCallJobDescriptor, logger: WorkflowLogger): Boolean = {; def isPreemptionCode(code: Int) = code == 13 || code == 14. try {; errorCode == 10 && errorMessage.isDefined && isPreemptionCode(extractErrorCodeFromErrorMessage(errorMessage.get)) && jobDescriptor.preemptible; } catch {; case _: NumberFormatException | _: StringIndexOutOfBoundsException =>; logger.warn(s""Unable to parse JES error code from error message: ${errorMessage.get}, assuming this was not a preempted VM.""); false; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007:552,message,message,552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007,1,['message'],['message']
Integrability,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:69,integrat,integration,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307,1,['integrat'],['integration']
Integrability,This could be wrapper endpoint or backwards-compatible changes to the existing one or both,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4892#issuecomment-486346405:14,wrap,wrapper,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4892#issuecomment-486346405,1,['wrap'],['wrapper']
Integrability,This does indeed depend on #482,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/483#issuecomment-193898794:17,depend,depend,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/483#issuecomment-193898794,1,['depend'],['depend']
Integrability,This is great! We should wait for https://github.com/broadinstitute/centaur/pull/193 (fixing our integration tests) to merge but otherwise looking good to me!; ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2280/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444:97,integrat,integration,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280#issuecomment-302444444,1,['integrat'],['integration']
Integrability,"This is not the same as #695. #719 is, our workflow metadata response to a massive wdl is a massively slow build of metadata. This metadata includes lots of trips to the database, lots of conversion from DB objects to JSON-friendly objects, and possibly retrieves values that aren't needed depending on the consumer. In addition to investigating what should be returned by the existing metadata endpoint, we may want two additional metadata builders:; 1. For showing timing diagrams -- doesn't need things such as the output paths, etc.; 2. For running the existing 3-types of final calls -- doesn't need all the existing metadata, and definitely doesn't need to be created 3x, once per type of final call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-213249737:290,depend,depending,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-213249737,1,['depend'],['depending']
Integrability,"This looks remarkably similar to the change made in https://github.com/broadinstitute/cromwell/pull/4952 (and seems to re-invent the ""don't read too much"" logic with a slightly different maximum size). I wonder whether it would be possible to combine the ""read from stderr"" logic from these two changes to always happen in the same place - even if the resulting message text then ends up going in different directions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364:362,message,message,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364,1,['message'],['message']
Integrability,"This might be conflating two issues, but in case it is related, another error we are consistently seeing that seems dependent on which docker container we use (which may be a red herring, but that's all I can narrow it down to), we'll run something and get this error: ; ```; ""callCaching"": {; ""hashFailures"": [; {; ""message"": ""[Attempted 1 time(s)] - NoSuchFileException: s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz""; }; ]; }; ],; ```. Meanwhile, the input location of the input file is this:; ```; ""inputs"": {; ""input_fastq"": {; ""format"": null,; ""location"": ""s3://some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""size"": null,; ""secondaryFiles"": [],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; },; ```; So it's being given a valid S3 URL but then when it's trying to get the hash, it's looking at an invalid S3 URL (the one with s3.amazonaws.com isn't valid, but wasn't supplied by us). Thoughts? Is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066:116,depend,dependent,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066,3,"['depend', 'message']","['dependent', 'message']"
Integrability,"This might not be an issue for the wdl focus group (in favor of upcoming situations). It depends on if this starts becoming more of a Cromwell implementation thing or a WDL thing. I think it is purely the latter, and if that's the case it should be moved to the wdl repo and left for that triage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-332979103:89,depend,depends,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2446#issuecomment-332979103,1,['depend'],['depends']
Integrability,"This passes the K.I.S.S. principle, but I personally _hate_ the ask pattern, as it leads to timeouts and exceptions when the system gets busy. One should be theoretically be able to have the two actors to `tell` / `receive` messages instead, though I don't have the code ready at this second.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332:224,message,messages,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/308#issuecomment-161824332,1,['message'],['messages']
Integrability,"This seems to be interestingly connected with the spec change https://github.com/openwdl/wdl/pull/315 (so cc @patmagee). As that PR is currently written, we would be fine to do the scheme like this, because the `memory` section says ""you can provide as much memory as you want, as long as it's over this amount"", but it feels like we're in danger of writing non-portable WDLs like this because the incentive is to write a small value first and rely on the doubling to catch you if necessary. FWIW I'd rather go down the route of:; - `memory` is treated as the ""guaranteed to work"" ceiling amount; - We could start by having a much lower `memory_to_try_first` attribute representing the first value to try; - If the task fails, we can then double it from the low baseline, until either the task succeeds or we reach the `memory` ceiling, and at that point we don't try any further. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634:520,rout,route,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634,1,['rout'],['route']
Integrability,"This should incorporate PR feedback delta:; 1. Scott's suggestion for input expression storage and evaluation.; 2. Jeff's suggestions for eliminating the `CheckExecutionStatus` message and handler case.; 3. My suggestion for putting mutable workflow state behind an actor. I'll try to address 2 and 3 in the time remaining in the sprint, 1 will have to wait for motivating user stories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643:177,message,message,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643,2,['message'],['message']
Integrability,"This was deliberately ""not allowed"", to try to force people to pass through the inputs and outputs to their workflows (ie so that the interface to a workflow was stable even if extra tasks were added or removed from its internal workings). In other words to encourage:. ```wdl; version 1.0. workflow Test2 {; input {; String? passthrough_text; }. call Echo {; input: text = passthrough_text; }. output {; }; }; ```. This would allow you to swap out the internal call to `Echo` for something else, or rename the call, or add another call after it, or replace the entire workflow itself with a single task, etc, etc... and nobody who's calling `Test2` needs to worry about your internal refactorings. They just `call Test2` and supply the input and are done. Now having said that, I've pretty much changed my mind about this being something to enforce rather than just something to encourage, and would ideally like to go down the route of saying ""best practices are to pass through inputs and outputs but it's not enforced because quite often it's super-annoying""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870:134,interface,interface,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870,2,"['interface', 'rout']","['interface', 'route']"
Integrability,Those dropped bits are where I stick my steganographic messages to people,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537214275:55,message,messages,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537214275,1,['message'],['messages']
Integrability,"Though I guess if they really are pluggable backends, then somebody can create their own SBT project and include whatever they want as long as it exposes a class that adheres to the Actor interface. So we really can't stop differing versions of Akka or any other shared package, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192392206:188,interface,interface,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192392206,1,['interface'],['interface']
Integrability,"Tl;DR: I think ""our one-size-fits-all"" logging is the real problem here. . Hiding one message to help one user type (and I'm suspicious that this is more signal than cause) might cause other problems for other user types.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3994#issuecomment-412110084:86,message,message,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3994#issuecomment-412110084,1,['message'],['message']
Integrability,"To @geoffjentry's first point, should there be a quiescence period in SWRA? i.e. after finding there are no pending writes it could transition to a ReChecking state, send itself a ReCheck message on a timer, and check again. To the second point, admittedly I didn't check this but I assume the transaction has committed after which Cromwell can't really make any additional durability guarantees?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289781672:188,message,message,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2101#issuecomment-289781672,1,['message'],['message']
Integrability,"To clarify after discussion with @mcovarr:; This is a per-workflow limitation: each workflow chooses not to start new jobs until it sees its number of ""queued jobs"" go under a certain limit.; It doesn't protect against someone starting 10 million workflows with 1 job. That's what the token dispenser does.; It would make it more difficult to distinguish between a job that can't be started because its dependencies are not fulfilled and a job that is not being started because there are too many queued jobs already: they would both have the same `NotStarted` status. We could introduce a new status but this has a cost in terms of educating users etc...; This seems TechTalk worthy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370571661:403,depend,dependencies,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370571661,1,['depend'],['dependencies']
Integrability,"To elaborate the idea a bit further... I'd like to be able to make the following transformations:. Easiest (add a dependent step):; ```wdl; workflow foo{; call A{}; }; ```; and ; ""Add B to FOO""; Becomes:; ```wdl; workflow FOO{; call A{}. call B{ inputs:; foo=A.foo}; }; ```; 2. Harder (replace a task); ```wdl; workflow FOO{; call A{}. call B{inputs: foo=A.foo}. call C{inputs: bar=B.bar}; } ; ```; and ; ""override B in FOO by B_Prime"". Becomes:; ```wdl; workflow FOO{; call A{}. call B_Prime{inputs: foo=A.foo}. call C{inputs: bar=B_Prime.bar}; }; ```; 3. even harder (replace a single call by multiple calls); ```wdl; workflow FOO{; call A{}. call B{inputs: foo=A.foo}. call C{inputs: bar=B.bar}; } ; ```; and ; ""override B in FOO by B_Prime_one and B_Prime_two"". Becomes:; ```wdl; workflow FOO{; call A{}. call B_Prime_one{inputs: foo=A.foo}; call B_Prime_two{inputs: baz=B_prime.baz}; call C{inputs: bar=B_Prime_two.bar}; }; ```; 3 might be solvable by implementing B_Prime_one and B_Prime_two as a workflow, and then invoking 2) and replacing call B by a call to that workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3315#issuecomment-368136214:114,depend,dependent,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3315#issuecomment-368136214,1,['depend'],['dependent']
Integrability,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:250,integrat,integration,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712,1,['integrat'],['integration']
Integrability,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:270,message,messages,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,2,['message'],['messages']
Integrability,ToL: . The title made me a little nervous that this was adding more coupling that would make things hard in a world where these services are different implementations or instantiated on remote servers. But since it looks like it's all being directed through the standard interfaces in the service registry it all looks OK. And I assume we can still give the `serviceRegistryActor: ActorRef` argument to a new service that's created remotely.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367017359:271,interface,interfaces,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3294#issuecomment-367017359,1,['interface'],['interfaces']
Integrability,"ToL: I was wondering whether 404 is the right code here, but I think it probably is... . My concern is that ""can't diff because your job is pre-28"" should be distinguishable from ""can't diff because your job doesn't exist"". EDIT: I think it's OK because the message string is appropriately different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2372#issuecomment-310696609:258,message,message,258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2372#issuecomment-310696609,1,['message'],['message']
Integrability,"ToL: I'm a little afraid of the arbitrary number 5... Is it at all easy to make the `WorkflowSummaryActor` send a `ExciseFromCache` message when it sees a status message for a workflow?. Otherwise, (and especially since this makes Centaur great again), ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/965/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/965#issuecomment-224280285:132,message,message,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/965#issuecomment-224280285,2,['message'],['message']
Integrability,"ToL:. Looking at this, I wonder whether an easier route to the upgrade script is to make another implementation of this `WdlWriter` typeclass for draft 2 `WdlNamespace`. It leaves us further away from funneling draft 2 and draft 3 through the same object mode (and the massive code deletion we'd get from that)l, but it might be a much more expedient (and maybe safer?) way of achieving the upgrade script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833:50,rout,route,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3601#issuecomment-387452833,1,['rout'],['route']
Integrability,"Totally TOL, but I think just replacing. ``` scala; case MetadataQuery(w, None, None) => IndexedJsonObject(Map(w.id.toString -> MetadataBuilderActor.parseWorkflowEventsToIndexedJsonValue(eventsList))).toJson.asJsObject; ```. with. ``` scala; case MetadataQuery(w, _, _) => MetadataBuilderActor.parseWorkflowEvents(eventsList); ```. in `processMetadataResponse`; would give you what you want. Also, if we don't want the id wrapping the metadata maybe we still want it as an attribute ? . ``` scala; case MetadataQuery(w, _, _) => JsObject(MetadataBuilderActor.parseWorkflowEvents(eventsList).fields + (""workflowId"" -> JsString(w.id.toString))); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/938#issuecomment-223711434:422,wrap,wrapping,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/938#issuecomment-223711434,1,['wrap'],['wrapping']
Integrability,"True about `sbt assembly`, but do we want to run integration tests by default with `sbt test`? Currently these are on by default and the `.travis.yml` is set up not to run them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441877:49,integrat,integration,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/357#issuecomment-169441877,1,['integrat'],['integration']
Integrability,"True. This workflow works fine with local backends and FC without normal_allelic_counts. I guess cromwell read null as a file based on line 141 in file ""ModelSegments.java "". Then, it return the message ""the local copy message must have path set."". Not sure... ModelSegments.java ; 141 private File inputNormalAllelicCountsFile = null;",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-343304266:195,message,message,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791#issuecomment-343304266,2,['message'],['message']
Integrability,"Two errors on the testing:; ```; - should successfully run curl *** FAILED *** (1 minute, 37 seconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: bb88b541-3f1a-490c-9121-7685b4ab54b3). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Job curl_wf.newsgrab:NA:1 exited with return code 6 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ""message"" : ""Workflow failed""; }; ]; ```. ```; info] PubSubMetadataServiceActorSpec:; [info] A PubSubMetadataActor with a subscription should ; [info] - should create the requested subscription *** FAILED *** (17 milliseconds); [info] java.lang.AssertionError: received 1 excess messages on InfoFilter(None,Left(Creating subscription baz),true); [info] at akka.testkit.EventFilter.intercept(TestEventListener.scala:116); [info] at cromwell.services.metadata.impl.pubsub.PubSubMetadataServiceActorSpec.$anonfun$new$9(PubSubMetadataServiceActorSpec.scala:40); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); [info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); ```; I don't see how these are caused by this PR. I would gladly fix them if I would know how.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025:330,message,message,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025,3,['message'],"['message', 'messages']"
Integrability,Unit tests are passing. Centaur provides the integration tests we should be using - no need for separate tests. Centaur script was added in rev 3bd9b6a. Closing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3286#issuecomment-394931718:45,integrat,integration,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3286#issuecomment-394931718,1,['integrat'],['integration']
Integrability,Unit-tested functionality in this PR is better integration-tested by #4488 and #4508.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4415#issuecomment-453374229:47,integrat,integration-tested,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4415#issuecomment-453374229,1,['integrat'],['integration-tested']
Integrability,Update is based on this analysis:. ![workflow_duration_by_status](https://user-images.githubusercontent.com/791985/117333982-7d2e8f80-ae67-11eb-95eb-3cf8f76fa77b.png). See BT-272 for the R script. Edit: Filtered out the workflows that run the individual tests as workflows. Those wrapper workflows were the ones failing (as expected) after ~90 minutes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214:280,wrap,wrapper,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214,1,['wrap'],['wrapper']
Integrability,"Update:. ```; $ curl http://localhost:8080/api/engine/v1/stats; {; ""workflows"": 0,; ""jobs"": 0; }; ```. Except, when using `top` I see several jobs running with the workflow ID: 7b3cdd40-2c3c-4533-be62-06b7d9135546. ```; java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --reference /root/case_gatk_acnv_workflow/**7b3cdd40-2c3c-4533**-b+; ```. Status gives me: . ```; {; ""status"": ""fail"",; ""message"": ""Failed lookup attempt for workflow ID 7b3cdd40-2c3c-4533-be62-06b7d9135546""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460:398,message,message,398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249237460,1,['message'],['message']
Integrability,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:527,message,messages,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302,2,['message'],['messages']
Integrability,"We are ready to submit PR with fix for this issue and we performed manual testing on several backends (AWS, GCP, Local), but there are some troubles with creation of integration test for that: in particular, we didn't find the way to pass cromwell options to cromwell running in server mode. Is this possible and is integration test required for this issue? @wleepang",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385:166,integrat,integration,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-519482385,2,['integrat'],['integration']
Integrability,"We can't really make our images private because we want our workflows to be publicly accessible, especially for Terra users. We can make mirrors of our GCR image repositories across regions -- hopefully that will eliminate this type of event for the most part. But we'll still be dependent on our users to to use the right mirrors (as @freeseek just noted above).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729:280,depend,dependent,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729,1,['depend'],['dependent']
Integrability,"We get 'fail to delocalize' as the error message over a bunch of failure types. In particular, when the task failed with return code 0. This makes failures hard to debug in FireCloud, as users have to dig down into logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864:41,message,message,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864,1,['message'],['message']
Integrability,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:131,inject,injection,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893,2,['inject'],['injection']
Integrability,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:91,message,message,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357,2,['message'],['message']
Integrability,"We run into the same problem with our production pipelines (i.e. not having a clean output directory structure unless we do some magic). So this would be a huge improvement to us.; @ruchim Throwing an exception and marking the job as failed would be the best. As this makes sure the pipeline developers can easily spot when there are file collisions. Cromwell exiting with an exit code other than 0 after that will also make it easier. Another argument for having a flattened output structure: Continuous integration. We want to know if our pipeline produces certain files and these files have a certain content. However with the current output folder structure it is impossible to know where files are going to end up, so it is hard to test them.; We circumvent this by setting an ""outputDir"" variable in our workflows and tasks which is absolute, so the files are copied to one place. But this is less than ideal. We should not have to hack in WDL as a replacement for functionality in the execution engine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705:505,integrat,integration,505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-474726705,1,['integrat'],['integration']
Integrability,We're expecting the resolver to look at the root of the submitted zip file when resolving files. We treat all imports as full paths from the root of the dependencies file structure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451570239:153,depend,dependencies,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4515#issuecomment-451570239,1,['depend'],['dependencies']
Integrability,"Well depending on how different a given language becomes between versions we might want to route to different processor implementations. But again, when N > 2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981:5,depend,depending,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2346#issuecomment-325698981,2,"['depend', 'rout']","['depending', 'route']"
Integrability,"Well, here's a use case. I want to run the same workflow on exomes and on whole genomes, and some of my parameters take different defaults depending on the data type. It would be swell to be able to say e.g. `my_param = param_values[data_type]` assuming I've set up my defaults as maps with e.g. 'wgs' and 'exome' as keys, and I can somewhere set `data_type = 'wgs'` (because presumably several values would need to be switched) (by the way, does wdl have enums?). So I'd have defaults that are variable references -- but I might decide to use something else entirely and just input my_workflow.my_param = 5 in my json for whatever reason. . Is that crazy/wrong?. I guess I could instead do the override by injecting the value I want into `param_values`...? But then I'm constrained to work with whatever `data_types` have been planned for and can't add something different on the fly. . Does any of this make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098:139,depend,depending,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2565#issuecomment-323826098,2,"['depend', 'inject']","['depending', 'injecting']"
Integrability,"What do you recommend instead on a personal laptop? Even 55 by the way is; not reliable and fails or stucks randomly. I feel like it is more of a; macOS issue, as I never had these on Ubuntu using same setup. 4 Åžub 2023 Cmt 00:01 tarihinde Adam Nichols ***@***.***> ÅŸunu; yazdÄ±:. > Desktop Docker is not the most reliable platform for real work in; > Cromwell, but it is interesting that a specific version broke it.; >; > Do you have time to do a git bisect between 55 and current?; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHAHMHBVIQIXGMY5RKKIW7TWVVW37ANCNFSM6AAAAAAUQB7A2U>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366:814,Message,Message,814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366,1,['Message'],['Message']
Integrability,What does that mean? How do we get you guys to prioritize work on it?; (This is directed at Jeff's comment that we should interface elsewhere),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344402249:122,interface,interface,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-344402249,1,['interface'],['interface']
Integrability,What is the current behavior doing exactly? I've seen this message before and thought it was just an alarmist wording of a nbd condition but now I'm thinking I might have had that wrong ðŸ˜¬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665245650:59,message,message,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665245650,1,['message'],['message']
Integrability,"What is the recommendation for resolving this problem?; I am getting the following:. ```; {; u'status': u'fail',; u'message': u'Task slick.basic.BasicBackend$DatabaseDef$$anon$2@2dbcf781 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@6dbdf3be[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 550175]'; }; ```. when calling the `query` endpoint.; It happens episodically. If I call `query` again, it often responds just fine. I'm particularly curious about the message indicating:. ```; queued tasks = 1000; ```. There is not much going on with this instance:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":24,""jobs"":115}. $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""33-215cca9-SNAP""}; ```. How should I interpret having 1000 queued tasks?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329:116,message,message,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394779329,2,['message'],['message']
Integrability,"When I run this I get a lot of unexpected messages. Presumably we just want to drop these acks:. ```; 2016-05-24 16:42:22,511 cromwell-system-akka.actor.default-dispatcher-30 INFO - Status change from Initializing to Running; 2016-05-24 16:42:22,515 cromwell-system-akka.actor.default-dispatcher-30 ERROR - Unexpected message MetadataPutAcknowledgement(PutMetadataAction(MetadataEvent(MetadataKey(da17555a-11bc-4e72-a338-a2f177718435,Some(MetadataJobKey(DeliciousFileSpam.rotateInner,None,1)),backendStatus),MetadataValue(Running),2016-05-24 16:42:22.511))).; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221395379:42,message,messages,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/883#issuecomment-221395379,2,['message'],"['message', 'messages']"
