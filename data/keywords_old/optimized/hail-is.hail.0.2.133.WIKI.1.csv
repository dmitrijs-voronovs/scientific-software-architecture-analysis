quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"s; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:93827,checkpoint,checkpoint,93827,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['checkpoint'],['checkpoint']
Availability,"s; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; TupleExpression. View page source. TupleExpression. class hail.expr.TupleExpression[source]; Expression of type ttuple.; >>> tup = hl.literal((""a"", 1, [1, 2, 3])). Attributes. dtype; The data type of the expression. Methods. count; Do not use this method. index; Do not use this method. __class_getitem__ = <bound method GenericAlias of <class 'hail.expr.expressions.typed_expressions.TupleExpression'>>. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Index into the tuple.; Examples; >>> hl.eval(tup[1]); 1. Parameters:; item (int) – Element index. Returns:; Expression. __gt__(other); Return self>value. __le__(other); Return self<=value. __len__()[source]; Returns the length of the tuple.; Examples; >>> len(tup); 3. Returns:; int. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expression",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:1303,error,error,1303,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['error'],['error']
Availability,"s=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests on two assumptions:. - an individual's first ``k`` principal component coordinates fully; describe their allele-frequency-relevant ancestry, and. - the relationship between ancestry ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:3921,down,down,3921,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['down'],['down']
Availability,"s]). Notes; The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the PLINK mendel; formats, resembling; the .mendel, .fmendel, .imendel, and .lmendel formats,; respectively.; First table: all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:49862,error,errors,49862,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"sary representational properties of the VDS."""""". rd = self.reference_data; vd = self.variant_data. def error(msg):; raise ValueError(f'VDS.validate: {msg}'). rd_row_key = rd.row_key.dtype; if (; not isinstance(rd_row_key, hl.tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9265,error,error,9265,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"sary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [4]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:1230,avail,available,1230,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['avail'],['available']
Availability,"se. replace; Replace substrings matching pattern1 with pattern2 using regex. reverse; Returns the reversed value. split; Returns an array of strings generated by splitting the string at delim. startswith; Returns whether substr is a prefix of the string. strip; Returns a copy of the string with whitespace removed from the start and end. translate; Translates characters of the string using mapping. upper; Returns a copy of the string, but with lower case letters converted to upper case. __add__(other)[source]; Concatenate strings.; Examples; >>> hl.eval(s + ' jumped over the lazy dog'); 'The quick brown fox jumped over the lazy dog'. Parameters:; other (StringExpression) – String to concatenate. Returns:; StringExpression – Concatenated string. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Slice or index into the string.; Examples; >>> hl.eval(s[:15]); 'The quick brown'. >>> hl.eval(s[0]); 'T'. Parameters:; item (slice or Expression of type tint32) – Slice or character index. Returns:; StringExpression – Substring or character at index item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; Boole",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StringExpression.html:2581,error,error,2581,docs/0.2/hail.expr.StringExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StringExpression.html,1,['error'],['error']
Availability,"search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; Allowed Operators; Extra. job_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are pending, ready, creating, running, live, cancelled, error, failed, bad, success, done. instance; str; =, ==, !=, =~, !~; use this to search for all jobs that ran on a given worker. instance_collection; str; =, ==, !=, =~, !~; use this to search for all jobs in a given pool. Example: user = johndoe; Example: billing_project = johndoe-trial; Example: instance_collection = standard. Combining Multiple Statements; Example: Searching for batches in a time window; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. Example: Searching for batches that have run since June 2023 that cost more than $5 submitted by a given user; start_time >= 2023-06-01; cost > 5.00; user = johndoe. Example: Searching for failed batches where the batch name contains pca; state = failed; name =~ pca. Example: Searching for jobs within a given range of ids; job_id >= 1000; job_id < 2000. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/advanced_search_help.html:2805,error,error,2805,docs/batch/advanced_search_help.html,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html,1,['error'],['error']
Availability,"seful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; pro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86849,error,errors,86849,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"sfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where the optional suffix is m representing millicpu.; Omitting a suffix means the value is in cpu.; For the ServiceBackend, cores must be a power of; two between 0.25 and 16.; Examples; Set the job’s CPU requirement to 250 millicpu:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4445,echo,echo,4445,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"sh` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8825,avail,available,8825,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['avail'],['available']
Availability,"shopping. In total,; we will have created 9 jobs that run in parallel as we did not define any dependencies; between the jobs. >>> b = hb.Batch(name='nested-scatter-1'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); >>> b.run(). We can implement the same example as above with a function that implements the inner; for loop. The do_chores function takes a Batch object to add new jobs; to and a user name for whom to create chore jobs for. Like above, we create 9 independent; jobs. However, by structuring the code into smaller functions that take batch objects,; we can create more complicated dependency graphs and reuse components across various computational; pipelines.; >>> def do_chores(b, user):; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'). >>> b = hb.Batch(name='nested-scatter-2'); >>> for user in ['Alice', 'Bob', 'Dan']:; ... do_chores(b, user); >>> b.run(). Lastly, we provide an example of a more complicated batch that has an initial; job, then scatters jobs per user, then has a series of gather / sink jobs; to wait for the per user jobs to be done before completing. >>> def do_chores(b, head, user):; ... chores = []; ... for chore in ['make-bed', 'laundry', 'grocery-shop']:; ... j = b.new_job(name=f'{user}-{chore}'); ... j.command(f'echo ""user {user} is doing chore {chore}""'); ... j.depends_on(head); ... chores.append(j); ... sink = b.new_job(name=f'{user}-sink'); ... sink.depends_on(*chores); ... return sink. >>> b = hb.Batch(name='nested-scatter-3'); >>> head = b.new_job(name='head'); >>> user_sinks = []; >>> for user in ['Alice', 'Bob', 'Dan']:; ... user_sink = do_chores(b, head, user); ... user_sinks.append(user_sink); >>> final_sink = b.new_job(name='final-sink'); >>> final_sink.depend",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:9699,echo,echo,9699,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"sion keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""; assert 'url' in doc, doc; assert 'version' in doc, doc; assert 'reference_genome' in doc, doc; if cloud in doc['url']:; return DatasetVersion(doc['url'][cloud], doc['version'], doc['reference_genome']); else:; return None. @staticmethod; def get_region(name: str, versions: List['DatasetVersion'], region: str) -> List['DatasetVersion']:; """"""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""; available_versions = []; for version in versions:; if version.in_region(name, region):; version.url = version.url[region]; available_versions.append(version); return available_versions. def __init__(self, url: Union[dict, str], version: Optional[str], reference_genome: Optional[str]):; self.url = url; self.version = version; self.reference_genome = reference_genome. def in_region(self, name: str, region: str) -> bool:; """"""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:2813,avail,available,2813,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['avail'],['available']
Availability,"sisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:55521,avail,available,55521,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['avail'],['available']
Availability,"somal region of chromosome Y. sequence_context; Return the reference genome sequence at the locus. window; Returns an interval of a specified number of bases around the locus. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. property contig; Returns the chromosome.; Examples; >>> hl.eval(locus.contig); '1'. Returns:; StringExpression – The chromosome for this locus. property contig_idx; Returns the chromosome.; Examples; >>> hl.eval(locus.contig_idx); 0. Returns:; StringExpression – The index of the chromosome for this locus. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Expo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:2629,error,error,2629,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['error'],['error']
Availability,"source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""; return Env.fs().copy(src, dest). [docs]def hadoop_exists(path: str) -> bool:; """"""Returns ``True`` if `path` exists. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().exists(path). [docs]def hadoop_is_file(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a file. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_file(path). [docs]def hadoop_is_dir(path: str) -> bool:; """"""Returns ``True`` if `path` both exists and is a directory. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().is_dir(path). [docs]def hadoop_stat(path: str) -> Dict[str, Any]:; """"""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""; return Env.fs().stat(path).to_legacy_dict(). [docs]def hadoop_ls(path: str) -> List[Dict[str, Any]]:; """"""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:5192,error,error,5192,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['error'],['error']
Availability,"specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed job will still run. In the case of cancelled,; it takes time to cancel a batch, especially for larger batches. Individual jobs cannot be cancelled or deleted. Instead, you can cancel the entire batch with the “Cancel”; button next to the row for that batch. You can also delete a batch with the “Delete” button. Warning; Deleting a batch only removes it from the UI. You will still be billed for a deleted batch. The UI has an advanced search mode with a custom query language to find batches and jobs.; Learn more on the Advanced Search Help page. Important Notes. Warning; To avoid expensive egress charges, input and output files should be located in buckets; that are multi-regional in the United States because Batch runs jobs in any US region. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:13389,failure,failure,13389,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,2,['failure'],['failure']
Availability,"split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """""". split = split_multi(ds, keep_star=keep_star, left_aligned=left_aligned, permit_shuffle=permit_shuffle). row_fields = set(ds.row); update_rows_expression = {}; if vep_root in row_fields:; update_rows_expression[vep_root] = split[vep_root].annotate(**{; x: split[vep_root][x].filter(lambda csq: csq.allele_num == split.a_index); for x in (; 'intergenic_consequences',; 'motif_fe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:122464,error,error,122464,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['error']
Availability,"squared=0.1527377521613834,; f_stat=1.2704081632653061,; multiple_p_value=0.5314327326007864,; n=4). Regress blood pressure against an intercept (1), genotype, age, and; the interaction of genotype and age:; >>> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; In relation to; lm.summary; in R, linreg(y, x = [1, mt.x1, mt.x2]) computes; summary(lm(y ~ x1 + x2)) and; linreg(y, x = [mt.x1, mt.x2], nested_dim=0) computes; summary(lm(y ~ x1 + x2 - 1)).; More generally, nested_dim defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; beta (tarray of tfloat64):; Estimated regression coefficient for each covariate.; standard_error (tarray of tfloat64):; Estimated standard error for each covariate.; t_stat (tarray of tfloat64):; t-statistic for each covariate.; p_value (tarray of tfloat64):; p-value for each covariate.; multiple_standard_error (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:28561,error,error,28561,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['error'],['error']
Availability,"ssTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:58335,error,errors,58335,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"ssing elements are ignored. Parameters:. array (ArrayNumericExpression); unique (bool). Returns:; Expression of type tint32. hail.expr.functions.corr(x, y)[source]; Compute the; Pearson correlation coefficient; between x and y.; Examples; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; Only indices where both x and y are non-missing will be included in the; calculation.; If x and y have length zero, then the result is missing. Parameters:. x (Expression of type array<tfloat64>); y (Expression of type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:17852,toler,tolerance,17852,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"st (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}\). Wald; p_value; float64; Wald p-value testing \(\beta_1 = 0\). LRT, Firth; beta; float64; fit effect coefficient,; \(\hat\beta_1\). LRT, Firth; chi_sq_stat; float64; deviance statistic. LRT, Firth; p_value; float64; LRT / Firth p-value testing; \(\beta_1 = 0\). Score; chi_sq_stat; float64; score statistic. Score; p_value; float64; score p-value testing \(\beta_1 = 0\). For the Wald and likelihood ratio tests, Hail fits the logistic model for; each row using Newton iteration and only emits the above fields; when the maximum likelihood estimate of the coefficients converges. The; Firth test uses a modified form of Newton iteration. To help diagnose; convergence issues, Hail also emits three fields which summarize the; iterative fitting process:. Test; Field; Type; Value. Wald, LRT, Firth; fit.n_iterations; int32; number of iterations until; convergence",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:9335,error,error,9335,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['error'],['error']
Availability,"st be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:51969,error,error,51969,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['error'],['error']
Availability,"st that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:11299,checkpoint,checkpoint,11299,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"stall with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipython command. $ SPARK_HOME=/path/to/spark/ \; HAIL_HOME=/path/to/hail/ \; PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/build/distributions/hail-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by runnin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:3180,echo,echo,3180,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['echo'],['echo']
Availability,"start. [28]:. p = hl.plot.histogram(mt.sample_qc.call_rate, range=(.88,1), legend='Call Rate'); show(p). [Stage 24:> (0 + 1) / 1]. [29]:. p = hl.plot.histogram(mt.sample_qc.gq_stats.mean, range=(10,70), legend='Mean Sample GQ'); show(p). [Stage 27:> (0 + 1) / 1]. Often, these metrics are correlated. [30]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 30:> (0 + 1) / 1]. Removing outliers from the dataset will generally improve association results. We can make arbitrary cutoffs and use them to filter:. [31]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); print('After filter, %d/284 samples remain.' % mt.count_cols()). [Stage 32:> (0 + 1) / 1]. After filter, 250/284 samples remain. Next is genotype QC. It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error.; In a low-depth dataset like 1KG, it is hard to detect bad genotypes using this metric, since a read ratio of 1 alt to 10 reference can easily be explained by binomial sampling. However, in a high-depth dataset, a read ratio of 10:100 is a sure cause for concern!. [32]:. ab = mt.AD[1] / hl.sum(mt.AD). filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))). fraction_filtered = mt.aggregate_entries(hl.agg.fraction(~filter_condition_ab)); print(f'Filtering {fraction_filtered * 100:.2f}% entries out of downstream analysis.'); mt = mt.filter_entries(filter_condition_ab). [Stage 34:> (0 + 1) / 1]. Filtering 3.60% entries out of downstream analysis. [ ]:. Variant QC is a bit more of the same: we can use the variant_qc function to produce a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:15914,error,error,15914,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['error'],['error']
Availability,"stributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""; if hl.current_backend().requires_lowering:; tmp = hl.utils.new_temp_file(). if len(self.key) == 0:; uid = Env.get_uid(); tmp2 = hl.utils.new_temp_file(); self.checkpoint(tmp2); ht = hl.read_table(tmp2).add_index(uid).key_by(uid); ht.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n).key_by().drop(uid); else:; # checkpoint rather than write to use fast codec; self.checkpoint(tmp); return hl.read_table(tmp, _n_partitions=n). return Table(; ir.TableRepartition(; self._tir, n, ir.RepartitionStrategy.SHUFFLE if shuffle else ir.RepartitionStrategy.COALESCE; ); ). [docs] @typecheck_method(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'Table':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.Table`; Table with at most",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:94043,checkpoint,checkpoint,94043,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['checkpoint'],['checkpoint']
Availability,"strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""; return _fses[requester_pays_config].open(path, mode, buffer_size). [docs]def copy(src: str, dest: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/result",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:2972,error,error,2972,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['error'],['error']
Availability,"sult = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results.append(result); continue. j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results.appen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8435,checkpoint,checkpoint,8435,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"t GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71575,error,errors,71575,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"t as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partitioning(path)[source]¶; Write partitioning.json.gz file for legacy VDS file. Parameters:path (str) – path to VDS file. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:25555,down,down,25555,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['down'],['down']
Availability,"t isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found recor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:10358,error,error,10358,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"t the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statistics from a dataset using the; expression language. Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:3345,down,download,3345,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,1,['down'],['download']
Availability,"t to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2565,down,down,2565,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['down'],['down']
Availability,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113184,down,downcoding,113184,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcoding']
Availability,"t-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; We will explain by example. Consider a hypothetical 3-allelic; variant:; A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi_hts() will create two biallelic variants (one for each; alternate allele) at the same position; A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT or PGT field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1.; The biallelic alt AD entry is just the multiallelic AD entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries.; The biallelic DP is the same as the multiallelic DP.; The biallelic PL entry for a genotype g is the minimum over PL entries; for multiallelic genotypes that downcode to g. For example, the PL for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45.; Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic AD entry; for an allele is just the sum of the multiallelic AD entries for alleles; that map to that allele. Similarly, the biallelic PL entry for a genotype is; th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:88345,down,downcoded,88345,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['down'],['downcoded']
Availability,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4566,error,errors,4566,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,8,['error'],['errors']
Availability,"t: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_missing(rd.END), hl.agg.take((rd.row_key, rd.col_key), 5)),; end_before_position=hl.agg.filter(rd.END < rd.locus.position, hl.agg.take((rd.row_key, rd.col_key), 5)),; ); if VariantDataset.ref_block_max_length_field in rd.globals:; rbml = rd[VariantDataset.ref_block_max_length_field]; end_exprs['blocks_too_long'] = hl.agg.filter(; rd.END - rd.locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_with_ref_max_len = len([mt for mt in mts if fd in mt.globals]); any_ref_max = n_with_ref_max_len > 0; all_ref_max = n_with_ref_max_len == len(mts). # if some mts have max ref len but not all, drop it;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:11375,error,error,11375,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"t; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referenc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33387,error,error,33387,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['error'],['error']
Availability,"t_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). def call_hemi(kid_pp, parent, parent_pp, kid_ad_ratio):; p_data_given_dn = parent_pp[0] * kid_pp[1] * DE_NOVO_PRIOR; p_het_in_parent = 1 - (1 - prior) ** 4; p_data_given_missed_het = (parent_pp[1] + parent_pp[2]) * kid_pp[2] * p_het_in_parent; p_de_novo = p_data_given_dn / (p_data_given_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:32066,failure,failure,32066,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['failure'],['failure']
Availability,"t_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int]) – Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Crea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4542,failure,failures,4542,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['failure'],['failures']
Availability,"t_type),; key='interval',; ); else:; key_dtype = calling_intervals.key.dtype; if (; len(key_dtype) != 1; or not isinstance(calling_intervals.key[0].dtype, hl.tinterval); or calling_intervals.key[0].dtype.point_type != vds.reference_data.locus.dtype; ):; raise ValueError(; f""'impute_sex_chromosome_ploidy': expect calling_intervals to be list of intervals or""; f"" table with single key of type interval<locus>, found table with key: {key_dtype}""; ). rg = vds.reference_data.locus.dtype.reference_genome. par_boundaries = []; for par_interval in rg.par:; par_boundaries.append(par_interval.start); par_boundaries.append(par_interval.end). # segment on PAR interval boundaries; calling_intervals = hl.segment_intervals(calling_intervals, par_boundaries). # remove intervals overlapping PAR; calling_intervals = calling_intervals.filter(; hl.all(lambda x: ~x.overlaps(calling_intervals.interval), hl.literal(rg.par)); ). # checkpoint for efficient multiple downstream usages; info(""'impute_sex_chromosome_ploidy': checkpointing calling intervals""); calling_intervals = calling_intervals.checkpoint(new_temp_file(extension='ht')). interval = calling_intervals.key[0]; (any_bad_intervals, chrs_represented) = calling_intervals.aggregate((; hl.agg.any(interval.start.contig != interval.end.contig),; hl.agg.collect_as_set(interval.start.contig),; )); if any_bad_intervals:; raise ValueError(; ""'impute_sex_chromosome_ploidy' does not support calling intervals that span chromosome boundaries""; ). if len(rg.x_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple X contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ); if len(rg.y_contigs) != 1:; raise NotImplementedError(; f""reference genome {rg.name!r} has multiple Y contigs, this is not supported in 'impute_sex_chromosome_ploidy'""; ). kept_contig_filter = hl.array(chrs_represented).map(lambda x: hl.parse_locus_interval(x, reference_genome=rg)); vds = VariantDataset(; hl.filter_intervals(vds.referen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:14901,checkpoint,checkpoint,14901,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,6,"['checkpoint', 'down']","['checkpoint', 'checkpointing', 'downstream']"
Availability,"ta', 'standard_error', 't_stat', 'p_value']; computed_row_fields = {; field_name: per_y_list.map(lambda one_y: one_y[field_name][row_idx]); for field_name in computed_row_field_names; }; pass_through_rows = {field_name: block[field_name][row_idx] for field_name in row_field_names}. if not is_chained:; computed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant us",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:25796,checkpoint,checkpoint,25796,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['checkpoint'],['checkpoint']
Availability,"table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_localize=True, *, _timed=False)[source]; Collect the rows of the table into a local list.; Examples; Collect a list of all X records:; >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; This method returns a list whose elements are of type Struct. Fields; of these ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:15272,checkpoint,checkpoint,15272,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['checkpoint'],['checkpoint']
Availability,"tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*) -- Readable representation of Mendel error.; ; **Second table:** errors per nuclear family. This table contains one row per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155632,error,error,155632,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['error']
Availability,"tart with a range table with an array of random boolean values:; >>> ht = hl.utils.range_table(100); >>> ht = ht.annotate(arr = hl.range(0, 5).map(lambda _: hl.rand_bool(0.5))). Aggregate to compute the fraction True per element:; >>> ht.aggregate(hl.agg.array_agg(lambda element: hl.agg.fraction(element), ht.arr)) ; [0.54, 0.55, 0.46, 0.52, 0.48]. Notes; This function requires that all values of array have the same length. If; two values have different lengths, then an exception will be thrown.; The f argument should be a function taking one argument, an expression of; the element type of array, and returning an expression including; aggregation(s). The type of the aggregated expression returned by; array_agg() is an array of elements of the return type of f. Parameters:. f – Aggregation function to apply to each element of the exploded array.; array (ArrayExpression) – Array to aggregate. Returns:; ArrayExpression. hail.expr.aggregators.downsample(x, y, label=None, n_divisions=500)[source]; Downsample (x, y) coordinate datapoints. Parameters:. x (NumericExpression) – X-values to be downsampled.; y (NumericExpression) – Y-values to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:33133,down,downsample,33133,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['down'],['downsample']
Availability,"tarting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes. (#10662) Fixed; partitioning logic in hl.import_plink.; (#10669); NDArrayNumericExpression.sum() now works correctly on ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) I",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:58425,error,error,58425,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"taset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, Kid. 3; HomRef; ~HomRef; HomVar; Auto; Dad, Kid. 4; ~HomRef",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50345,error,errors,50345,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"taset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; With the default root and y a single expression, the following row-indexed; fields are added. <row key fields> (Any) – Row key fields.; <pass_through fields> (Any) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:3574,error,error,3574,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['error'],['error']
Availability,"tches.; Example: name = pca_pipeline; Example: name =~ pca. Predefined Keyword Expression; The left hand side of the statement is a special Batch-specific keyword which can be one of the values; listed in the tables below. Allowed operators are dependent on the type of the value expected for each; keyword, but can be one of =, ==, !=, >, >=, <, <=, =~, !~.; The right hand side is the value to search against. Keywords. Keyword; Value Type; Allowed Operators; Extra. cost; float; =, ==, !=, >, >=, <, <=. duration; float; =, ==, !=, >, >=, <, <=; Values are rounded to the millisecond. start_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. end_time; date; =, ==, !=, >, >=, <, <=; ISO-8601 datetime string. Example: cost >= 1.00; Example: duration > 5; Example: start_time >= 2023-02-24T17:15:25Z. Keywords specific to searching for batches. Keyword; Value Type; Allowed Operators; Extra. batch_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are running, complete, success, failure, cancelled, open, closed. user; str; =, ==, !=, =~, !~. billing_project; str; =, ==, !=, =~, !~. Example: state = running; Example: user = johndoe; Example: billing_project = johndoe-trial. Keywords specific to searching for jobs in a batch. Keyword; Value Type; Allowed Operators; Extra. job_id; int; =, ==, !=, >, >=, <, <=. state; str; =, ==, !=; Allowed values are pending, ready, creating, running, live, cancelled, error, failed, bad, success, done. instance; str; =, ==, !=, =~, !~; use this to search for all jobs that ran on a given worker. instance_collection; str; =, ==, !=, =~, !~; use this to search for all jobs in a given pool. Example: user = johndoe; Example: billing_project = johndoe-trial; Example: instance_collection = standard. Combining Multiple Statements; Example: Searching for batches in a time window; start_time >= 2023-02-24T17:15:25Z; end_time <= 2023-07-01T12:35:00Z. Example: Searching for batches that have run since June 2023 that cos",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/advanced_search_help.html:2375,failure,failure,2375,docs/batch/advanced_search_help.html,https://hail.is,https://hail.is/docs/batch/advanced_search_help.html,1,['failure'],['failure']
Availability,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14960,error,errors,14960,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"ter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. tran",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:2300,error,errors,2300,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"ter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; Creates a block matrix from a binary file. log; Element-wise natural lo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7916,checkpoint,checkpoint,7916,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['checkpoint'],['checkpoint']
Availability,"ter_missing]); Returns the product of values in the collection. sum(collection[, filter_missing]); Returns the sum of values in the collection. cumulative_sum(a[, filter_missing]); Returns an array of the cumulative sum of values in the array. argmin(array[, unique]); Return the index of the minimum value in the array. argmax(array[, unique]); Return the index of the maximum value in the array. corr(x, y); Compute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:3519,toler,tolerance,3519,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"ters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations : :obj:`int`; The maximum number of iterations when fitting the logistic null model. Defaults to 25.; null_tolerance : :obj:`float`; The null model logisitic regression converges when the errors is less than this. Defaults to; 1e-6.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for why this differs from the paper. - p_value : :obj:`.tfloat64`, the test p-value for the null hypothesis that the genotypes; have no linear influence on the phenotypes. - fault : :obj:`.tint32`, the fault flag from :func:`.pgenchisq`. The global fields are:. - n_complete_samples : :obj:`.tint32`, the number of samples with neither a missing; phenotype nor a missing covariate. - y_residual : :obj:`.tint32`, the residual phenotype fr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:95669,error,errors,95669,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['errors']
Availability,"tes with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44262,checkpoint,checkpoint,44262,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['checkpoint'],['checkpoint']
Availability,"th categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool, optional) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.models.Plot if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.qq(pvals, label=None, title='Q-Q plot', xlabel='Expected -log10(p)', ylabel='Observed -log10(p)', size=6, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:9018,down,downsample,9018,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['downsample']
Availability,"th now also support async; functions. Bug fixes. (#11962) Fix error; (logged as (#11891)); in VCF combiner when exactly 10 or 100 files are combined.; (#11969) Fix; import_table and import_lines to use multiple partitions when; force_bgz is used.; (#11964) Fix; erroneous “Bucket is a requester pays bucket but no user project; provided.” errors in Google Dataproc by updating to the latest; Dataproc image version. Version 0.2.96; Released 2022-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:48594,error,error,48594,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:42890,failure,failures,42890,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['failure'],['failures']
Availability,"the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.; Hail makes it easy to visualize results! Let’s make a Manhattan plot:. [39]:. p = hl.plot.manhattan(gwas.p_value); show(p). This doesn’t look like much of a skyline. Let’s check whether our GWAS was well controlled using a Q-Q (quantile-quantile) plot. [40]:. p = hl.plot.qq(gwas.p_value); show(p). Confounded!; The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.; The linear_regression_rows function can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.; The pca function produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The hwe_normalized_pca function does the same, using HWE-normalized genotypes for the PCA. [41]:. eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). [Stage 158:> (0 + 1) / 1]. [42]:. pprint(eigenvalues). [18.084111467840707,; 9.984076405601847,; 3.540687229805949,; 2.655598108390125,; 1.596852701724399,; 1.5405241027955296,; 1.507713504116216,; 1.4744976712480349,; 1.467690539034742,; 1.4461994473306554]. [43]:. pcs.show(5, width=100). sscoresstrarray<float64>; ""HG00096""[1.22e-01,2.81e-01,-1.10e-01,-1.27e-01,6.68e-02,3.29e-03,-2.26e-02,4.26e-02,-9.30e-02,1.83e-01]; ""HG00099""[1.14e-01,2.89e-01,-1.06e-01,-6.78e-02,4.72e-02,2.87e-02,5.28e-03,-1.57e-0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:20804,error,error,20804,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['error'],['error']
Availability,"the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int]) – Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. new_bash_job; Initialize a BashJob object with default memory, storage, image, and CPU settings (defined in Batch) upon batch creation. new_job; Alias for Batch.new_bash_job(). new_python_job; Initialize a new PythonJob object with default Python image, memory, storage, and CPU settings (defined in Batch) upon batch creation. read_input; Create a new input resource file object representing a single file. read_input_group; Create a new resource group representing a mapping of identifier to input resource files. run; Execute a batch. select_jobs; Select all jobs in the batch whose name matches pattern. write_output; Write resource file or resource file group to an output destination. static from_batch_id(batch_id, *args, **kwargs); Create a Batch from an existing batch id.; Notes; Can only be used with the ServiceBackend.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:4626,failure,failures,4626,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['failure'],['failures']
Availability,"the inner recursive call to `triangle1(8)` by; adding 9 to the result. The second function is tail recursive: the result of `triangle2(9, 0)` is; the same as the result of the inner recursive call, `triangle2(8, 9)`. Example; -------; To find the sum of all the numbers from n=1...10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55. Let's say we want to find the root of a polynomial equation:; >>> def polynomial(x):; ... return 5 * x**3 - 2 * x - 1. We'll use `Newton's method<https://en.wikipedia.org/wiki/Newton%27s_method>`; to find it, so we'll also define the derivative:. >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at :math:`x_0 = 0`, we'll compute the next step :math:`x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}`; until the difference between :math:`x_{i}` and :math:`x_{i+1}` falls below; our convergence threshold:. >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; -------; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters; ----------; f : function ( (marker, \*args) -> :class:`.Expression`; Function of one callable marker, denoting where the recursive call (or calls) is located,; and many `args`, the loop variables.; typ : :class:`str` or :class:`.HailType`; Type the loop returns.; args : variable-length args of :class:`.Expression`; Expressions to initialize the loop values.; Returns; -------; :class:`.Expression`; Result of the loop with `args` as initial loop value",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:3093,error,error,3093,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['error'],['error']
Availability,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __or__(other)[source]; Return True if at least one of the left and right arguments is True.; Examples; >>> hl.eval(t | f); True. >>> hl.eval(t | na); True. >>> hl.eval(f | na); None. The & and | operators have higher priority than comparison; operators like ==, <, or >. Parentheses are often; necessary:; >>> x = hl.literal(5). >>> hl.eval((x < 10) | (x > 20)); True. Parameters:; other (BooleanExpression) – Right-side operand. Returns:; BooleanExpression – True if either left or right is True. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html:4227,error,error,4227,docs/0.2/hail.expr.BooleanExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.BooleanExpression.html,1,['error'],['error']
Availability,"the left side is smaller than or equal to the right side. __lt__(other); Less-than comparison.; Examples; >>> hl.eval(x < 5); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is smaller than the right side. __mod__(other); Compute the left modulo the right number.; Examples; >>> hl.eval(32 % x); 2. >>> hl.eval(7 % y); 2.5. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – Remainder after dividing the left by the right. __mul__(other); Multiply two numbers.; Examples; >>> hl.eval(x * 2); 6. >>> hl.eval(x * y); 13.5. Parameters:; other (NumericExpression) – Number to multiply. Returns:; NumericExpression – Product of the two numbers. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. __neg__(); Negate the number (multiply by -1).; Examples; >>> hl.eval(-x); -3. Returns:; NumericExpression – Negated number. __pow__(power, modulo=None); Raise the left to the right power.; Examples; >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters:. power (NumericExpression); modulo – Unsupported argument. Returns:; Expression of type tfloat64 – Result of raising left to the right power. __sub__(other); Subtract the right number from the left.; Examples; >>> hl.eval(x - 2); 1. >>> hl.eval(x - y); -1.5. Parameters:; other (NumericExpression) – Number to subtract. Returns:; NumericExpression – Difference of the two numbers. __truediv__(other); Divide two numbers.; Examples; >>> hl.eval(x / 2); 1.5. >>> hl.eval(y / 0.1); 4",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Float32Expression.html:3410,error,error,3410,docs/0.2/hail.expr.Float32Expression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Float32Expression.html,3,['error'],['error']
Availability,"the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tm",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:8483,error,error,8483,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['error'],['error']
Availability,"the start point. Methods. contains; Tests whether a value is contained in the interval. overlaps; True if the the supplied interval contains any value in common with this one. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(value)[source]; Tests whether a value is contained in the interval.; Examples; >>> hl.eval(interval.contains(3)); True. >>> hl.eval(interval.contains(11)); False. Parameters:; value – Object with type matching the interval point type. Returns:; BooleanExpression – True if value is contained in the interval, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. property end; Returns the end poin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:2103,error,error,2103,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['error'],['error']
Availability,"then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113587,down,downcode,113587,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcode']
Availability,"this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119009,down,downcoded,119009,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcoded']
Availability,"thm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pairs or unrelated pairs. Note that \(g_{is}\) is the number of alternate alleles. Hence, for; multi-allelic variants, a value of 2 may indicate two distinct alternative; alleles rather than a homozygous variant genotype. To enforce the latter,; either filter or split multi-allelic variants first.; The resulting table has the first 3, 4, 5, or 6 fields below, depending on; the statistics parameter:. i (col_key.dtype) – First sample. (key field); j (col_key.dtype) – Second sample. (key field); kin (tfloat64) – Kinship estimate, \(\widehat{\phi_{ij}}\).; ibd2 (tfloat64) – IBD2 estimate, \(\widehat{k^{(2)}_{ij}}\).; ibd0 (tfloat64) – IBD0 estimate, \(\widehat{k^{(0)}_{ij}}\).; ibd1 (tfloat64) – IBD1 estimate, \(\widehat{k^{(1)}_{ij}}\). Here col_key refers to the column key of the source matrix table,; and col_key.dtype is a struct containing the column key fields.; There is one row for each pair of distinct samples (columns), whe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:19101,reliab,reliably,19101,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['reliab'],['reliably']
Availability,"tion DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1940,avail,available,1940,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,1,['avail'],['available']
Availability,"tion to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73551,error,error,73551,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"tion, reference_genome='default') -> BooleanExpression:; """"""Returns ``True`` if `contig` and `position` is a valid site in `reference_genome`. Examples; --------. >>> hl.eval(hl.is_valid_locus('1', 324254, 'GRCh37')); True. >>> hl.eval(hl.is_valid_locus('chr1', 324254, 'GRCh37')); False. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; position : :class:`.Expression` of type :py:data:`.tint`; reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :class:`.BooleanExpression`; """"""; return _func(""isValidLocus"", tbool, contig, position, type_args=(tlocus(reference_genome),)). [docs]@typecheck(locus=expr_locus(), is_female=expr_bool, father=expr_call, mother=expr_call, child=expr_call); def mendel_error_code(locus, is_female, father, mother, child):; r""""""Compute a Mendelian violation code for genotypes. >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child1)); None. >>> hl.eval(hl.mendel_error_code(locus, True, father, mother, child2)); 7. Note; ----; Ignores call phasing, and assumes diploid and biallelic. Haploid calls for; hemiploid samples on sex chromosomes also are acceptable input. Notes; -----; In the table below, the copy state of a locus with respect to a trio is; defined as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`.LocusExpression.in_autosome`:. - Auto -- in autosome or in PAR, or in non-PAR of X and female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+------------+---------------+; | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:164179,error,error,164179,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"tional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:11129,down,download,11129,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['down'],['download']
Availability,"tions of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71392,fault,fault,71392,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['fault'],['fault']
Availability,"to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8061,resilien,resilient,8061,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,3,"['Checkpoint', 'resilien']","['Checkpointing', 'resilient']"
Availability,"tor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6696,avail,available,6696,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['avail'],['available']
Availability,"tput_dir, 'HGDP.vcf.bgz'); sample_annotations_path = os.path.join(output_dir, 'HGDP_annotations.txt'); gene_annotations_path = os.path.join(output_dir, 'ensembl_gene_annotations.txt'). if (; overwrite; or not _dir_exists(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, 'HGDP.vcf.bgz'); source = resources['HGDP_matrix_table']; info(f'downloading HGDP VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['HGDP_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16, reference_genome='GRCh38').write(; matrix_table_path, overwrite=True; ). tmp_sample_annot = os.path.join(tmp_dir, 'HGDP_annotations.txt'); source = resources['HGDP_annotations']; info(f'downloading HGDP annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['HGDP_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('HGDP files found'). [docs]def get_movie_lens(output_dir, overwrite: bool = False):; """"""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:5574,down,downloading,5574,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"tr) – Source file URI.; dest (str) – Destination file URI. hail.utils.hadoop_exists(path)[source]; Returns True if path exists. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_file(path)[source]; Returns True if path both exists and is a file. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_is_dir(path)[source]; Returns True if path both exists and is a directory. Parameters:; path (str). Returns:; bool. hail.utils.hadoop_stat(path)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. hail.utils.hadoop_ls(path)[source]; Returns information about files at path.; Notes; Raises an error if path does not exist.; If path is a file, returns a list with one element. If path is a; directory, returns an element for each file contained in path (does not; search recursively).; Each dict element of the result list contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; list [dict]. hail.utils.hadoop_scheme_supported(scheme)[source]; Returns True if the Hadoop filesystem supports URLs with the given; scheme.; Examples; >>> hadoop_scheme_supported('gs') . Notes; URLs with the https scheme are only supported if they are specifically; Azure Blob Storage URLs of the form https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>. Parameters:; scheme (str). Returns:; bool. hail.utils.copy_log(path)[source]; Attempt to copy the session log to a hadoop-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:8014,error,error,8014,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['error'],['error']
Availability,"tr) – expression to compute one endpoint.; j (str) – expression to compute another endpoint.; tie_breaker – Expression used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22271,redundant,redundant,22271,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['redundant'],['redundant']
Availability,"tr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. - `pat_id` (:py:data:`.tstr`) -- Paternal ID. (key field); - `mat_id` (:py:data:`.tstr`) -- Maternal ID. (key field); - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - Hem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:6831,error,errors,6831,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- Mendel error code, see below. **Second table:** errors per nuclear family. This table contains one row; per nuclear family, keye",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:5037,error,errors,5037,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"tructure of the “.imendel” PLINK format detailed ; below.; Columns:. s (String) – Sample ID (key column).; fid (String) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This met",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126362,error,error,126362,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['error']
Availability,"ts will currently only agree for variants with no; missing genotypes. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:15206,toler,tolerance,15206,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['toler'],['tolerance']
Availability,"ts(fs, matrix_table_path); or not _file_exists(fs, sample_annotations_path); or not _file_exists(fs, vcf_path); or not _file_exists(fs, gene_annotations_path); ):; init_temp_dir(); tmp_vcf = os.path.join(tmp_dir, '1kg.vcf.bgz'); source = resources['1kg_matrix_table']; info(f'downloading 1KG VCF ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, resources['1kg_matrix_table'], tmp_vcf); cluster_readable_vcf = _copy_to_tmp(fs, local_path_uri(tmp_vcf), extension='vcf.bgz'); info('importing VCF and writing to matrix table...'); hl.import_vcf(cluster_readable_vcf, min_partitions=16).write(matrix_table_path, overwrite=True). tmp_sample_annot = os.path.join(tmp_dir, '1kg_annotations.txt'); source = resources['1kg_annotations']; info(f'downloading 1KG annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_sample_annot). tmp_gene_annot = os.path.join(tmp_dir, 'ensembl_gene_annotations.txt'); source = resources['1kg_ensembl_gene_annotations']; info(f'downloading Ensembl gene annotations ...\n' f' Source: {source}'); sync_retry_transient_errors(urlretrieve, source, tmp_gene_annot). hl.hadoop_copy(local_path_uri(tmp_sample_annot), sample_annotations_path); hl.hadoop_copy(local_path_uri(tmp_gene_annot), gene_annotations_path); hl.hadoop_copy(local_path_uri(tmp_vcf), vcf_path); info('Done!'); else:; info('1KG files found'). [docs]def get_hgdp(output_dir, overwrite: bool = False):; """"""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""; fs = Env.fs(). if not _dir_exists(fs, output_dir):; fs.mkdir(output_dir). matrix_table_path = os.path.join(output_dir, 'HGDP.mt'); vcf_path = os.path.join(output_dir, 'HGDP.vcf.bgz'); s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:3604,down,downloading,3604,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,2,['down'],['downloading']
Availability,"ts. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:5917,down,downsampled,5917,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['down'],['downsampled']
Availability,"tstruct); or len(rd_row_key) != 1; or not rd_row_key.fields[0] == 'locus'; or not isinstance(rd_row_key.types[0], hl.tlocus); ):; error(f""expect reference data to have a single row key 'locus' of type locus, found {rd_row_key}""). vd_row_key = vd.row_key.dtype; if (; not isinstance(vd_row_key, hl.tstruct); or len(vd_row_key) != 2; or not vd_row_key.fields == ('locus', 'alleles'); or not isinstance(vd_row_key.types[0], hl.tlocus); or vd_row_key.types[1] != hl.tarray(hl.tstr); ):; error(; f""expect variant data to have a row key {{'locus': locus<rg>, alleles: array<str>}}, found {vd_row_key}""; ). rd_col_key = rd.col_key.dtype; if not isinstance(rd_col_key, hl.tstruct) or len(rd_row_key) != 1 or rd_col_key.types[0] != hl.tstr:; error(f""expect reference data to have a single col key of type string, found {rd_col_key}""). vd_col_key = vd.col_key.dtype; if not isinstance(vd_col_key, hl.tstruct) or len(vd_col_key) != 1 or vd_col_key.types[0] != hl.tstr:; error(f""expect variant data to have a single col key of type string, found {vd_col_key}""). if 'END' not in rd.entry or rd.END.dtype != hl.tint32:; error(""expect field 'END' in entry of reference data with type int32""). if check_data:; # check cols; ref_cols = rd.col_key.collect(); var_cols = vd.col_key.collect(); if len(ref_cols) != len(var_cols):; error(; f""mismatch in number of columns: reference data has {ref_cols} columns, variant data has {var_cols} columns""; ). if ref_cols != var_cols:; first_mismatch = 0; while ref_cols[first_mismatch] == var_cols[first_mismatch]:; first_mismatch += 1; error(; f""mismatch in columns keys: ref={ref_cols[first_mismatch]}, var={var_cols[first_mismatch]} at position {first_mismatch}""; ). # check locus distinctness; n_rd_rows = rd.count_rows(); n_distinct = rd.distinct_by_row().count_rows(). if n_distinct != n_rd_rows:; error(f'reference data loci are not distinct: found {n_rd_rows} rows, but {n_distinct} distinct loci'). # check END field; end_exprs = dict(; missing_end=hl.agg.filter(hl.is_m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:9491,error,error,9491,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['error'],['error']
Availability,"turn False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False. if reorder_fields:; globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select(*row_order). if self._type != other._type:; print(f'Table._same: types differ:\n {self._type}\n {other._type}'); return False. left = self; left = left.select_globals(left_globals=left.globals); left = left.group_by(key=left.key).aggregate(left_row=hl.agg.collect(left.row_value)). right = other; right = right.select_globals(right_globals=right.globals); right = right.group_by(key=right.key).aggregate(right_row=hl.agg.collect(right.row_value)). t = left.join(right, how='outer'). mismatched_globals, mismatched_rows = t.aggregate(; hl.tuple((; hl.or_missing(~_values_similar(t.left_globals, t.right_globals, tolerance, absolute), t.globals),; hl.agg.filter(; ~hl.all(; hl.is_defined(t.left_row),; hl.is_defined(t.right_row),; _values_similar(t.left_row, t.right_row, tolerance, absolute),; ),; hl.agg.take(t.row, 10),; ),; )); ). columns, _ = shutil.get_terminal_size((80, 10)). def pretty(obj):; pretty_str = pprint.pformat(obj, width=columns); return ''.join(' ' + line for line in pretty_str.splitlines(keepends=True)). is_same = True; if mismatched_globals is not None:; print(f""""""Table._same: globals differ:; Left:; {pretty(mismatched_globals.left_globals)}; Right:; {pretty(mismatched_globals.right_globals)}""""""); is_same = False. if len(mismatched_rows) > 0:; print('Table._same: rows differ:'); for r in mismatched_rows:; print(f"""""" Row mismatch at key={r.key}:; Left:; {pretty(r.left_row)}; Right:; {pretty(r.right_row)}""""""); is_same = False. return is_same. [docs] def collect_by_key(self, name: str = 'values') -> 'Table':; """"""Collect values for each unique key into an array. .. include:: _templates/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:129153,toler,tolerance,129153,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['toler'],['tolerance']
Availability,"turn res. return hl.rbind(cdf, compute). @typecheck(raw_cdf=expr_struct()); def _result_from_raw_cdf(raw_cdf):; levels = raw_cdf.levels; item_weights = (; hl._stream_range(hl.len(levels) - 1); .flatmap(; lambda l: hl._stream_range(levels[l], levels[l + 1]).map(; lambda i: hl.struct(level=l, value=raw_cdf['items'][i]); ); ); .aggregate(lambda x: hl.agg.group_by(x.value, hl.agg.sum(hl.bit_lshift(1, x.level)))); ); weights = item_weights.values(); ranks = weights.scan(lambda acc, weight: acc + weight, 0); values = item_weights.keys(); return hl.struct(values=values, ranks=ranks, _compaction_counts=raw_cdf._compaction_counts). @typecheck(k=expr_int32, left=expr_struct(), right=expr_struct()); def _cdf_combine(k, left, right):; t = tstruct(levels=tarray(tint32), items=tarray(tfloat64), _compaction_counts=tarray(tint32)); return _func('approxCDFCombine', t, k, left, right). @typecheck(cdf=expr_struct(), failure_prob=expr_oneof(expr_float32, expr_float64), all_quantiles=bool); def _error_from_cdf(cdf, failure_prob, all_quantiles=False):; """"""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """""". def compute_sum(cdf):; s = hl.sum(; hl.range(0, hl.len(cdf._compaction_counts)).map(lambda i: cdf._compaction_counts[i] * (2 ** (2 * i))); ); return s / (cdf.ranks[-1] ** 2). def update_grid_size(p, s):; return 4 * hl.sqrt(hl.log(2 * p / failure_prob) / (2 * s)). def compute_grid_size(s):; return hl.fold(lambda p, i: update_grid_size(p, s), 1 / failure_prob, hl.range(0, 5)). def compute_single_error(s, failure_prob=failure",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:5163,error,error,5163,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['error'],['error']
Availability,"turns the size of a collection. map; Transform each element of a collection. size; Returns the size of a collection. starmap; Transform each element of a collection of tuples. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. all(f)[source]; Returns True if f returns True for every element.; Examples; >>> hl.eval(a.all(lambda x: x < 10)); True. Notes; This method returns True if the collection is empty. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; BooleanExpression. – True if f returns True for every element, False otherwise. any(f)[source]; Returns True if f returns True for any element.; Examples; >>> hl.eval(a.any(lambda x: x % 2 == 0)); True. >>> hl.eval(s3.any(lambda x: x[0] == 'D')); False. Notes; This method always returns False for empty collections. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:2443,error,error,2443,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['error'],['error']
Availability,"typ, indices, aggs). [docs]@typecheck(arrays=expr_oneof(expr_stream(expr_any), expr_array(expr_any)), key=sequenceof(builtins.str)); def keyed_intersection(*arrays, key):; """"""Compute the intersection of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; arrays; key. Returns; -------; :class:`.ArrayExpression`; """"""; return _union_intersection_base(; 'keyed_intersection',; arrays,; key,; lambda key_var, vals_var: hl.tuple((key_var, vals_var)),; lambda res: res.filter(lambda x: hl.fold(lambda acc, elt: acc & hl.is_defined(elt), True, x[1])).map(; lambda x: x[1].first(); ),; ). [docs]@typecheck(arrays=expr_oneof(expr_stream(expr_any), expr_array(expr_any)), key=sequenceof(builtins.str)); def keyed_union(*arrays, key):; """"""Compute the distinct union of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; exprs; key. Returns; -------; :class:`.ArrayExpression`; """"""; return _union_intersection_base(; 'keyed_union',; arrays,; key,; lambda keys_var, vals_var: hl.fold(; lambda acc, elt: hl.coalesce(acc, elt), hl.missing(vals_var.dtype.element_type), vals_var; ),; lambda res: res,; ). [docs]@typecheck(collection=expr_oneof(expr_array(), expr_set()), delimiter=expr_str); def delimit(collection, delimiter=',') -> StringExpression:; """"""Joins elements of `collection` into single string delimited by `delimiter`. Examples; --------. >>> a = ['Bob', 'Charlie', 'Alice', 'Bob', 'Bob']. >>> hl.eval(hl.delimit(a)); 'Bob,Charlie,Alice,Bob,Bob'. Notes; -----; If the element type of `collection` is not :py:data:`.tstr`, then the; :func:`str` function will be called on each element before joining with; the delimiter. Parameters; ----------; collection : :class:`.ArrayExpression` or :class:`.SetExpression`; Collection.; delimiter : str or :class:`.StringExpressio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:146516,down,downstream,146516,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downstream']
Availability,"type :py:data:`.tfloat64`; """"""; return _func(""entropy"", tfloat64, s). @typecheck(x=expr_any, trunc=nullable(expr_int32)); def _showstr(x, trunc=None):; if trunc is None:; return _func(""showStr"", tstr, x); return _func(""showStr"", tstr, x, trunc). [docs]@typecheck(x=expr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; --------. >>> hl.eval(hl.triangle(3)); 6. Notes; -----; The calculation is ``n *",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105389,down,downcode,105389,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"type is significantly associated with the genotype:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 1.78e+02 | 1.68e-04 | 0 |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:73456,fault,fault,73456,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['fault'],['fault']
Availability,"type(GT=0, AD=[12, 0], DP=12, GQ=36, PL=[0, 36, 420])]. Integrate sample annotations¶; Hail treats variant and sample annotations as first-class citizens.; Annotations are usually a critical part of any genetic study. Sample; annotations are where you’ll store information about sample phenotypes,; ancestry, sex, and covariates. Variant annotations can be used to store; information like gene membership and functional impact for use in QC or; analysis.; In this tutorial, we demonstrate how to take a text file and use it to; annotate the samples in a VDS.; iPython supports various cell “magics”. The %%sh magic is one which; interprets the cell with bash, rather than Python. We can use this to; look at the first few lines of our annotation file. This file contains; the sample ID, the population and “super-population” designations, the; sample sex, and two simulated phenotypes (one binary, one discrete). In [11]:. %%sh; head data/1kg_annotations.txt | column -t. sh: 1: column: not found; head: write error: Broken pipe. This file can be imported into Hail with; HailContext.import_table.; This method produces a; KeyTable; object. Think of this as a Pandas or R dataframe that isn’t limited by; the memory on your machine – behind the scenes, it’s distributed with; Spark. In [12]:. table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'). 2018-10-18 01:26:28 Hail: INFO: Reading table to impute column types; 2018-10-18 01:26:28 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed). A good way to peek at the structure of a KeyTable is to look at its; schema. In [13]:. print(table.schema). Struct{Sample:String,Population:String,SuperPopulation",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:6289,error,error,6289,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['error'],['error']
Availability,"type_expr left; hand side with separator. If the left hand side is empty:; `` = expr. then the dot (.) is omitted. Parameters:; variant_expr (str or list of str) – Variant annotation expressions.; genotype_expr (str or list of str) – Genotype annotation expressions.; key (str or list of str) – List of key columns.; separator (str) – Separator to use between sample IDs and genotype expression left-hand side identifiers. Return type:KeyTable. mendel_errors(pedigree)[source]¶; Find Mendel errors; count per variant, individual and nuclear; family. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:; >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped). Export all mendel errors to a text file:; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""). Annotate variants with the number of Mendel errors:; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""). Notes; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment.; The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; PLINK mendel formats. The four; tables contain the following columns:; First table: all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the “.mendel” PLINK format detailed below.; Columns:. fid (String) – Family ID.; s (String) – Proband ID.; v (Variant) – Variant in which the error was found.; code (Int) – Mendel error code, see below.; error (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:123675,error,errors,123675,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['error'],['errors']
Availability,"t’s do a GWAS!¶; First, we need to restrict to variants that are :. common (we’ll use a cutoff of 1%); uncorrelated (not in linkage disequilibrium). Both of these are easy in Hail. In [43]:. common_vds = (vds; .filter_variants_expr('va.qc.AF > 0.01'); .ld_prune(memory_per_core=256, num_cores=4)). 2018-10-18 01:26:50 Hail: INFO: Running LD prune with nSamples=843, nVariants=9085, nPartitions=4, and maxQueueSize=257123.; 2018-10-18 01:26:50 Hail: INFO: LD prune step 1 of 3: nVariantsKept=8478, nPartitions=4, time=351.375ms; 2018-10-18 01:26:51 Hail: INFO: LD prune step 2 of 3: nVariantsKept=8478, nPartitions=12, time=1.184s; 2018-10-18 01:26:52 Hail: INFO: Coerced sorted dataset; 2018-10-18 01:26:52 Hail: INFO: LD prune step 3 of 3: nVariantsKept=8478, time=481.478ms. In [44]:. common_vds.count(). Out[44]:. (843L, 8555L). These filters removed about 15% of sites (we started with a bit over; 10,000). This is NOT representative of most sequencing datasets! We; have already downsampled the full thousand genomes dataset to include; more common variants than we’d expect by chance.; In Hail, the association tests accept sample annotations for the sample; phenotype and covariates. Since we’ve already got our phenotype of; interest (caffeine consumption) in the dataset, we are good to go:. In [45]:. gwas = common_vds.linreg('sa.CaffeineConsumption'); pprint(gwas.variant_schema). 2018-10-18 01:26:52 Hail: INFO: Running linear regression on 843 samples with 1 covariate including intercept... Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:21466,down,downsampled,21466,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['downsampled']
Availability,"ue = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; missing_label (str) – Label to use when a point is missing data for a categorical label. Returns:; bokeh.plotting.figure if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.manhattan(pvals, locus=None, title=None, size=4, hover_fields=None, collect_all=None, n_divisions=500, significance_line=5e-08)[source]; Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters:. pvals (Float64Expression) – P-values to be plotted.; locus (LocusExpression, optional) – Locus values to be plotted.; title (str, optional) – Title of the plot.; size (int) – Size of markers in screen space units.; hover_fields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:13114,down,downsample,13114,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['down'],['downsample']
Availability,"ue if that key is not present. items; Returns an array of tuples containing key/value pairs in the dictionary. key_set; Returns the set of keys in the dictionary. keys; Returns an array with all keys in the dictionary. map_values; Transform values of the dictionary according to a function. size; Returns the size of the dictionary. values; Returns an array with all values in the dictionary. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other); Return self>=value. __getitem__(item)[source]; Get the value associated with key item.; Examples; >>> hl.eval(d['Alice']); 43. Notes; Raises an error if item is not a key of the dictionary. Use; DictExpression.get() to return missing instead of an error. Parameters:; item (Expression) – Key expression. Returns:; Expression – Value associated with key item. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other); Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.DictExpression.html:2016,error,error,2016,docs/0.2/hail.expr.DictExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.DictExpression.html,1,['error'],['error']
Availability,"uester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:; >>> with hfs.open( ; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:; >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: ; ... f.write('result1: %s\n' % result1); ... f.write('result2: %s\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:; >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: ; ... print(unpack('<f', bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:5464,error,error,5464,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['error'],['error']
Availability,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:15509,toler,tolerance,15509,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['toler'],['tolerance']
Availability,"ullable(int)); def sample_rows(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each row with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its rows. >>> small_dataset = dataset.sample_rows(0.01). Notes; -----; Although the :class:`MatrixTable` returned by this method may be; small, it requires a full pass over the rows of the sampled object. Parameters; ----------; p : :obj:`float`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_rows(hl.rand_bool(p, seed)). [docs] @typecheck_method(p=numeric, seed=nullable(int)); def sample_cols(self, p: float, seed=None) -> 'MatrixTable':; """"""Downsample the matrix table by keeping each column with probability ``p``. Examples; --------; Downsample the dataset to approximately 1% of its columns. >>> small_dataset = dataset.sample_cols(0.01). Parameters; ----------; p : :obj:`float`; Probability of keeping each column.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.MatrixTable`; Matrix table with approximately ``p * n_cols`` column.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter_cols(hl.rand_bool(p, seed)). [docs] @typecheck_method(fields=dictof(str, str)); def rename(self, fields: Dict[str, str]) -> 'MatrixTable':; """"""Rename fields of a matrix table. Examples; --------. Rename column key `s` to `SampleID`, still keying by `SampleID`. >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key `s` to `info`, and the row field `info` to `vcf_info`:. >>> dataset_result = dataset.rename({'s': 'info', 'info': 'vcf_info",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:128801,Down,Downsample,128801,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Down'],['Downsample']
Availability,"ult is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') . Notes; If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus.; If a dataset does not have unique rows for each key (consider the; gencode genes, which may overlap; and clinvar_variant_summary,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters:. rel (MatrixTable or Table) – The relational object to which to add annotations.; names (varargs of str) – The names of the datasets with which to annotate rel. Returns:; MatrixTable or Table – The relational object rel, with the annotations from names; added. property available_datasets; List of names of available annotation datasets. Returns:; list – List of available annotation datasets. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:3473,avail,available,3473,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['avail'],['available']
Availability,"unc:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value ``[10, 2]`` is split, each split; site will contain the same array ``[10, 2]``. The provided allele index; field `a_index` can be used to select the value corresponding to the split; allele's position:. >>> split_ds = hl.split_multi_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:119765,down,downcoding,119765,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['down'],['downcoding']
Availability,"unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""; left_key = self.key.dtype; for (; i,; ht,; ) in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(; f""'union': table {i} has a different key.""; f"" Expected: {left_key}\n""; f"" Table {i}: {ht.key.dtype}""; ). if not (unify or ht.row.dtype == self.row.dtype):; raise ValueError(; f""'union': table {i} has a different row type.\n""; f"" Expected: {self.row.dtype}\n""; f"" Table {i}: {ht.row.dtype}\n""; f"" If the tables have the same fields in different orders, or some\n""; f"" common and some unique fields, then the 'unify' parameter may be\n""; f"" able to coerce the tables to a common type.""; ); all_tables = [self]; all_tables.extend(tables). if unify and not len(set(ht.row_value.dtype for ht in all_tables)) =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:85654,error,error,85654,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['error'],['error']
Availability,"unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:11615,down,downcode,11615,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['down'],['downcode']
Availability,"updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """""". require_row_key_variant(ds, ""split_multi""); new_id = Env.get_uid(); is_table = isinstance(ds, Table). old_row = ds.row if is_table else ds._rvrow; kept_alleles = hl.range(1, hl.len(old_row.alleles)); if not keep_star:; kept_alleles = kept_alleles.filter(lambda i: old_row.alleles[i] != ""*""). def new_struct(variant, i):; return hl.struct(alleles=variant.alleles, locus=variant.locus, a_index=i, was_split=hl.len(old_row.alleles) > 2). def split_rows(expr, rekey):; if isinstance(ds, MatrixTable):; mt = ds.annotate_rows(**{new_id: expr}).explode_rows(new_id); if rekey:; mt = mt.key_rows_by(); else:; mt = mt.key_rows_by('locus'); new_row_expr = mt._rvrow.annotate(; locus=mt[new_id]['loc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:114164,error,error,114164,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['error'],['error']
Availability,"ur images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possible job states; are as follows:. Pending - A job is waiting for its dependencies to complete; Ready - All of a job’s dependencies have completed, but the job has not been scheduled to run; Running - A job has been scheduled to run on a worker; Success - A job finished with exit code 0; Failure - A job finished with exit code not equal to 0; Error - The Docker container had an error (ex: out of memory). Clicking on a specific job will take you to a page with the logs for each of the three containers; run per job (see above) as well as a copy of the job spec and detailed; information about the job such as where the job was run, how long it took to pull the image for; each container, and any error messages.; To see all batches you’ve submitted, go to https://batch.hail.is. Each batch will have a current state,; number of jobs total, and the number of pending, succeeded, failed, and cancelled jobs as well as the; running cost of the batch (computed from completed jobs only). The possible batch states are as follows:. open - Not all jobs in the batch have been successfully submitted.; running - All jobs in the batch have been successfully submitted.; success - All jobs in the batch have completed with state “Success”; failure - Any job has completed with state “Failure” or “Error”; cancelled - Any job has been cancelled and no jobs have completed with state “Failure” or “Error”. Note; Jobs can still be running even if the batch has been marked as failure or cancelled. In the case of; ‘failure’, other jobs that do not depend on the failed ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:12621,error,error,12621,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['error'],['error']
Availability,"urce mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadcast:; unexpected_axes = axes - expected_axes; strictness = ''; else:; unexpected_axes = axes if axes != expected_axes else set(); strictness = 'strictly '. if unexpected_axes:; # one or more out-of-scope fields; refs = get_refs(expr); bad_refs = []; for name, inds in refs.items():; if broadcast:; bad_axes = inds.axes.intersection(unexpected_axes); if bad_axes:; bad_refs.append((name, inds)); elif inds.axes != expected_axes:; bad_refs.append((name, inds)). assert len(bad_refs) > 0; errors.append(; ExpressionException(; ""scope violation: '{caller}' expects an expression {strictness}indexed by {expected}""; ""\n Found indices {axes}, with unexpected indices {stray}. Invalid fields:{fields}{agg}"".format(; caller=caller,; strictness=strictness,; expected=list(expected_axes),; axes=list(indices.axes),; stray=list(unexpected_axes),; fields=''.join(; ""\n '{}' (indices {})"".format(name, list(inds.axes)) for name, inds in bad_refs; ),; agg=''; if (unexpected_axes - aggregation_axes); else ""\n '{}' supports aggregation over axes {}, ""; ""so these fields may appear inside an aggregator function."".format(caller, list(aggregation_axes)),; ); ); ). if aggregations:; if aggregation_axes:; # the expected axes of aggregated expressions are the expected axes + axes aggregated over; expected_agg_axes = expected_axes.union(aggregation_axes). for agg in aggregations:; assert isinstance(agg, Aggregation); refs = get_refs(*agg.exprs); agg_axes = agg.agg_axes(). # check for stray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:2487,error,errors,2487,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['error'],['errors']
Availability,"ure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1306,avail,available,1306,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['avail'],['available']
Availability,"urns:; MatrixTable – Filtered matrix table. filter_entries(expr, keep=True)[source]; Filter entries of the matrix. Parameters:. expr (bool or BooleanExpression) – Filter expression.; keep (bool) – Keep entries where expr is true. Returns:; MatrixTable – Filtered matrix table. Examples; Keep entries where the sum of AD is greater than 10 and GQ is greater than 20:; >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; When expr evaluates to missing, the entry will be removed regardless of; keep. Note; This method does not support aggregation. Notes; The expression expr will be evaluated for every entry of the table.; If keep is True, then entries where expr evaluates to True; will be kept (the filter removes the entries where the predicate; evaluates to False). If keep is False, then entries where; expr evaluates to True will be removed (the filter keeps the; entries where the predicate evaluates to False).; Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity – that is, that the; number of entries is smaller than the product of count_rows(); and count_cols(). To re-densify a filtered matrix table, use the; unfilter_entries() method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. Filtered entries are not included in the entries() table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt.annotate_cols(col_n = hl.agg.count",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:33239,down,downstream,33239,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['down'],['downstream']
Availability,"us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,; u'mean': 30.682263230349086,; u'min': 0.0,; u'nNotMissing': 10776455L,; u'stdev': 26.544770565260993,; u'sum': 330646029.00001156}. Count¶; The count aggregator is pretty simple - it counts the number of; elements in the aggregable. In [26]:. vds.query_genotypes('gs.count()'). Out[26]:. 10961000L. In [27]:. vds.num_samples * vds.count_variants(). Out[27]:. 10961000L. There’s one genotype per sample per variant, so the count of gs is; equal to the number of samples times the number of variants, or about 11; million",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:11470,avail,available,11470,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['avail'],['available']
Availability,"us(contig_holder, position, rg). gen_table = gen_table.annotate(locus=locus, alleles=alleles, rsid=rsid, varid=varid); gen_table = gen_table.annotate(; entries=gen_table.data[last_rowf_idx + 1 :]; .map(lambda x: hl.float64(x)); .grouped(3); .map(lambda x: hl.struct(GP=x)); ); if skip_invalid_loci:; gen_table = gen_table.filter(hl.is_defined(gen_table.locus)). sample_table_count = sample_table.count() - 2 # Skipping first 2 unneeded rows in sample file; gen_table = gen_table.annotate_globals(cols=hl.range(sample_table_count).map(lambda x: hl.struct(col_idx=x))); mt = gen_table._unlocalize_entries('entries', 'cols', ['col_idx']). sample_table = sample_table.tail(sample_table_count).add_index(); sample_table = sample_table.annotate(s=sample_table.text.split(' ')[0]); sample_table = sample_table.key_by(sample_table.idx); mt = mt.annotate_cols(s=sample_table[hl.int64(mt.col_idx)].s). mt = mt.annotate_entries(; GP=hl.rbind(; hl.sum(mt.GP),; lambda gp_sum: hl.if_else(; hl.abs(1.0 - gp_sum) > tolerance, hl.missing(hl.tarray(hl.tfloat64)), hl.abs((1 / gp_sum) * mt.GP); ),; ); ); mt = mt.annotate_entries(; GT=hl.rbind(; hl.argmax(mt.GP),; lambda max_idx: hl.if_else(; hl.len(mt.GP.filter(lambda y: y == mt.GP[max_idx])) == 1,; hl.switch(max_idx); .when(0, hl.call(0, 0)); .when(1, hl.call(0, 1)); .when(2, hl.call(1, 1)); .or_error(""error creating gt field.""),; hl.missing(hl.tcall),; ),; ); ); mt = mt.filter_entries(hl.is_defined(mt.GP)). mt = mt.key_cols_by('s').drop('col_idx', 'file', 'data'); mt = mt.key_rows_by('locus', 'alleles').select_entries('GT', 'GP'); return mt. [docs]@typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; delimiter=str,; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; force_bgz=bool,; filter=nullable(str),; find_replace=nullable(sized_tupleof(str, str)),; force=bool,; sour",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:52729,toler,tolerance,52729,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['toler'],['tolerance']
Availability,"use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""];",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2925,down,downstream,2925,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['down'],['downstream']
Availability,"ut=str,; overwrite=bool,; stage_locally=bool,; _codec_spec=nullable(str),; _read_if_exists=bool,; _intervals=nullable(sequenceof(anytype)),; _filter_intervals=bool,; ); def checkpoint(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; ) -> 'Table':; """"""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str)); def write(self, output: str, overwrite=False, stage_locally: bool = False, _codec_spec: Optional[str] = None):; """"""Write to disk. Examples; --------. >>> table1.write('output/table1.ht', overwrite=True). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_table`. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major out",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:59540,checkpoint,checkpoint,59540,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['checkpoint'],['checkpoint']
Availability,"ute the Pearson correlation coefficient between x and y. binary_search(array, elem); Binary search array for the insertion point of elem. hail.expr.functions.abs(x)[source]; Take the absolute value of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.abs(-5)); 5. >>> hl.eval(hl.abs([1.0, -2.5, -5.1])); [1.0, 2.5, 5.1]. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.approx_equal(x, y, tolerance=1e-06, absolute=False, nan_same=False)[source]; Tests whether two numbers are approximately equal.; Examples; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters:. x (NumericExpression); y (NumericExpression); tolerance (NumericExpression); absolute (BooleanExpression) – If True, compute abs(x - y) <= tolerance. Otherwise, compute; abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022).; nan_same (BooleanExpression) – If True, then NaN == NaN will evaluate to True. Otherwise,; it will return False. Returns:; BooleanExpression. hail.expr.functions.bit_and(x, y)[source]; Bitwise and x and y.; Examples; >>> hl.eval(hl.bit_and(5, 3)); 1. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_or(x, y)[source]; Bitwise or x and y.; Examples; >>> hl.eval(hl.bit_or(5, 3)); 7. Notes; See the Python wiki; for more information about bit operators. Parameters:. x (Int32Expression or Int64Expression); y (Int32Expression or Int64Expression). Returns:; Int32Expression or Int64Expression. hail.expr.functions.bit_xor(x, y)[source]; Bitwise exclusive-or x and y.; Examples; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; See the Python wiki; for more information about bit o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:3975,toler,tolerance,3975,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,"val. If the .bed file has four or more columns, then Hail will store the fourth column in the table:. interval (Interval) - Genomic interval.; target (String) - Fourth column of .bed file. UCSC bed files can have up to 12 fields, ; but Hail will only ever look at the first four. Hail ignores header lines in BED files. Caution; UCSC BED files are 0-indexed and end-exclusive. The line “5 100 105” will contain; locus 5:105 but not 5:100. Details here. Parameters:path (str) – Path to .bed file. Return type:KeyTable. static import_fam(path, quantitative=False, delimiter='\\\\s+', missing='NA')[source]¶; Import PLINK .fam file into a key table.; Examples; Import case-control phenotype data from a tab-separated PLINK .fam file into sample; annotations:; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam'). In Hail, unlike PLINK, the user must explicitly distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype without quantitative=True will return an error; (unless all values happen to be 0, 1, 2, and -9):; >>> fam_kt = KeyTable.import_fam('data/myStudy.fam', quantitative=True). Columns; The column, types, and missing values are shown below. ID (String) – Sample ID (key column); famID (String) – Family ID (missing = “0”); patID (String) – Paternal ID (missing = “0”); matID (String) – Maternal ID (missing = “0”); isFemale (Boolean) – Sex (missing = “NA”, “-9”, “0”). One of:. isCase (Boolean) – Case-control phenotype (missing = “0”, “-9”, non-numeric or the missing argument, if given.; qPheno (Double) – Quantitative phenotype (missing = “NA” or the missing argument, if given. Parameters:; path (str) – Path to .fam file.; quantitative (bool) – If True, .fam phenotype is interpreted as quantitative.; delimiter (str) – .fam file field delimiter regex.; missing (str) – The string used to denote missing values.; For case-control, 0, -9, and non-numeric are also treated; as missing. Returns:Key table with information from .fam file. Return ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:13548,error,error,13548,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['error'],['error']
Availability,"variates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowered_poisson_regression_rows/x', x). row_exprs = _get_regression_row_fields(mt, pass_through, '_lowered_poisson_regression_rows'); mt = mt._select_all(; row_exprs=dict(pass_through=hl.struct(**row_exprs)),; col_exprs=dict(y=y, covariates=covariates),; entry_exprs=dict(x=x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covari",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:64138,toler,tolerance,64138,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"variates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:154917,error,errors,154917,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; NumericExpression. View page source. NumericExpression. class hail.expr.NumericExpression[source]; Expression of numeric type.; >>> x = hl.literal(3). >>> y = hl.literal(4.5). Attributes. dtype; The data type of the expression. Methods. __add__(other)[source]; Add two numbers.; Examples; >>> hl.eval(x + 2); 5. >>> hl.eval(x + y); 7.5. Parameters:; other (NumericExpression) – Number to add. Returns:; NumericExpression – Sum of the two numbers. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Divide two numbers with floor division.; Examples; >>> hl.eval(x // 2); 1. >>> hl.eval(y // 2); 2.0. Parameters:; other (NumericExpression) – Dividend. Returns:; NumericExpression – The floor of the left number divided by the right. __ge__(other)[source]; Greater-than-or-equals comparison.; Examples; >>> hl.eval(y >= 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than or equal to the right side. __gt__(other)[source]; Greater-than comparison.; Examples; >>> hl.eval(y > 4); True. Parameters:; other (NumericExpression) – Right side for comparison. Returns:; BooleanExpression – True if the left side is greater than the right side. __le__(other)[source]; Less-than-or-equals compar,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NumericExpression.html:1347,error,error,1347,docs/0.2/hail.expr.NumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NumericExpression.html,1,['error'],['error']
Availability,"ve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83756,checkpoint,checkpoints,83756,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['checkpoint'],['checkpoints']
Availability,"vec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec @ residual])]). fisher00 = null_fit.fisher; fisher01 = ((covmat.T * mu) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). test_fit = _poisson_fit(X, yvec, b, mu, score, fisher, max_iterations, tolerance); if test == 'lrt':; return ht.select(test_fit=test_fit, **lrt_test(X, null_fit, test_fit), **ht.pass_through).select_globals(; 'null_fit'; ); assert test == 'wald'; return ht.select(test_fit=test_fit, **wald_test(X, test_fit), **ht.pass_through).select_globals('null_fit'). def _poisson_fit(; X: NDArrayNumericExpression, # (N, K); y: NDArrayNumericExpression, # (N,); b: NDArrayNumericExpression, # (K,); mu: NDArrayNumericExpression, # (N,); score: NDArrayNumericExpression, # (K,); fisher: NDArrayNumericExpression, # (K, K); max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Poisson(exp(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim == 2; assert y.ndim == 1; assert b.ndim == 1; assert mu.ndim == 1; assert score.ndim == 1; assert fisher.ndim == 2. dtype = numerical_regression_fit_dtype; blank_struct = hl.struct(**{k: hl.missing(dtype[k]) for k in dtype}). def fit(recur, iteration, b, mu, score, fisher):; def cont(exploded, delta_b, max_delta_b):; log_lkhd = y @ hl.log(mu) - mu.sum(). next_b = b + delta_b; next_mu = hl.exp(X @ next_b); next_score = X.T @ (y - next_mu); next_fisher = (next_mu * X.T) @ X. return (; hl.case(); .when(; exploded | hl.is_nan(delta_b[0]),; blank_struct.annotate(n_iterations=iteration, log_lkhd=log_lkhd, converged=False, exploded=True),; ); .when(; max_delta_b < toleranc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:67875,toler,tolerance,67875,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"verview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions; Expression. View page source. Expression. class hail.expr.Expression[source]; Base class for Hail expressions.; Attributes. dtype; The data type of the expression. Methods. collect; Collect all records of an expression into a local list. describe; Print information about type, index, and dependencies. export; Export a field to a text file. show; Print the first few records of the expression to the console. summarize; Compute and print summary information about the expression. take; Collect the first n records of an expression. __eq__(other)[source]; Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __ge__(other)[source]; Return self>=value. __gt__(other)[source]; Return self>value. __le__(other)[source]; Return self<=value. __lt__(other)[source]; Return self<value. __ne__(other)[source]; Returns True if the two expressions are not equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for inequality comparison. Returns:; BooleanExpression – True if the two expressions are not equal. collect(_localize=True)[source]; Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Ext",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.Expression-1.html:1433,error,error,1433,docs/0.2/hail.expr.Expression-1.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.Expression-1.html,2,['error'],['error']
Availability,"visions=nullable(int),; significance_line=nullable(numeric),; ); def manhattan(; pvals: 'Float64Expression',; locus: 'Optional[LocusExpression]' = None,; title: 'Optional[str]' = None,; size: int = 4,; hover_fields: 'Optional[Dict[str, Expression]]' = None,; collect_all: 'Optional[bool]' = None,; n_divisions: 'Optional[int]' = 500,; significance_line: 'Optional[Union[int, float]]' = 5e-8,; ) -> Plot:; """"""Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters; ----------; pvals : :class:`.Float64Expression`; P-values to be plotted.; locus : :class:`.LocusExpression`, optional; Locus values to be plotted.; title : str, optional; Title of the plot.; size : int; Size of markers in screen space units.; hover_fields : Dict[str, :class:`.Expression`], optional; Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all : bool, optional; Deprecated - use `n_divisions` instead.; n_divisions : int, optional.; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; significance_line : float, optional; p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If ``None``, no line is added. Returns; -------; :class:`bokeh.models.Plot`; """"""; if locus is None:; locus = pvals._indices.source.locus. ref = locus.dtype.reference_genome. if hover_fields is None:; hover_fields = {}. hover_fields['locus'] = hail.str(locus). pvals = -hail.log10(pvals). source_pd = _collect_scatter_plot_data(; ('_global_locus', locus.global_position()),; ('_pval', pvals),; fields=hover_fields,; n_divisions=_downsampling_factor('manhattan', n_divisions, collect_all),; ); source_pd['p_value'] = [10 ** (-p) for p in source_pd['_pval']]; source_pd['_contig'] = [locus.split("":"")[0] for locus in source_pd['locus']]. observed_contigs = [contig for contig in ref.contigs.copy() if contig in set(source_pd['_contig'])]. contig_ticks = [ref.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:52482,down,downsample,52482,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['down'],['downsample']
Availability,"w page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that take",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1424,avail,available,1424,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['avail'],['available']
Availability,"w per nuclear family in the dataset.; This table closely reflects the structure of the "".fmendel"" PLINK format detailed below. ; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **father** (*String*) -- Paternal ID.; - **mother** (*String*) -- Maternal ID.; - **nChildren** (*Int*) -- Number of children in this nuclear family.; - **nErrors** (*Int*) -- Number of Mendel errors in this nuclear family.; - **nSNP** (*Int*) -- Number of Mendel errors at SNPs in this nuclear family.; ; **Third table:** errors per individual. This table contains one row per individual in the dataset, ; including founders. This table closely reflects the structure of the "".imendel"" PLINK format detailed ; below.; ; Columns:; ; - **s** (*String*) -- Sample ID (key column).; - **fid** (*String*) -- Family ID.; - **nErrors** (*Int*) -- Number of Mendel errors found involving this individual.; - **nSNP** (*Int*) -- Number of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:157134,error,error,157134,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['error'],['error']
Availability,"w_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = res",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7679,checkpoint,checkpoint,7679,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"we might want to run the calls; for multiple windows at a time in a single job along with the checkpointing; mechanism to check if the result for the window has already completed.; Building on the solution above for checkpointing, we need two for loops; instead of one to ensure we still get an even number of jobs in each; batch while not rerunning previously completed windows.; First, we create a results array that is the size of the number of windows; indices = local_df_y.index.to_list(); results = [None] * len(indices). We identify all of the windows whose checkpoint file already exists; and append the inputs to the results list in the correct position in the; list to ensure the ordering of results is consistent. We also create; a list that holds tuples of the window to compute, the index of that; window, and the checkpoint path.; inputs = []. for i, window in enumerate(indices):; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:10507,checkpoint,checkpoint,10507,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['checkpoint'],['checkpoint']
Availability,"where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few ent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77882,error,error,77882,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74770,avail,available,74770,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['avail'],['available']
Availability,"will be; determined from their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the metadata parameter to supply a dictionary; with the relevant information. See; get_vcf_metadata() for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured.; The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the head",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:50222,down,downstream,50222,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['down'],['downstream']
Availability,"with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:62201,toler,tolerance,62201,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['toler'],['tolerance']
Availability,"wnsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:47387,error,error,47387,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['error'],['error']
Availability,"x),; ); # FIXME: the order of the columns is irrelevant to regression; mt = mt.key_cols_by(). mt = mt.filter_cols(hl.all(hl.is_defined(mt.y), *[hl.is_defined(mt.covariates[i]) for i in range(k)])). mt = mt.annotate_globals(; **mt.aggregate_cols(; hl.struct(; yvec=hl.agg.collect(hl.float(mt.y)),; covmat=hl.agg.collect(mt.covariates.map(hl.float)),; n=hl.agg.count(),; ),; _localize=False,; ); ); mt = mt.annotate_globals(; yvec=(; hl.case(); .when(mt.n - k - 1 >= 1, hl.nd.array(mt.yvec)); .or_error(; hl.format(""_lowered_poisson_regression_rows: insufficient degrees of freedom: n=%s, k=%s"", mt.n, k); ); ),; covmat=hl.nd.array(mt.covmat),; n_complete_samples=mt.n,; ); covmat = mt.covmat; yvec = mt.yvec; n = mt.n_complete_samples. logmean = hl.log(yvec.sum() / n); b = hl.nd.array([logmean, *[0 for _ in range(k - 1)]]); mu = hl.exp(covmat @ b); residual = yvec - mu; score = covmat.T @ residual; fisher = (mu * covmat.T) @ covmat; mt = mt.annotate_globals(null_fit=_poisson_fit(covmat, yvec, b, mu, score, fisher, max_iterations, tolerance)); mt = mt.annotate_globals(; null_fit=hl.case(); .when(mt.null_fit.converged, mt.null_fit); .or_error(; hl.format(; '_lowered_poisson_regression_rows: null model did not converge: %s',; mt.null_fit.select('n_iterations', 'log_lkhd', 'converged', 'exploded'),; ); ); ); mt = mt.annotate_rows(mean_x=hl.agg.mean(mt.x)); mt = mt.annotate_rows(xvec=hl.nd.array(hl.agg.collect(hl.coalesce(mt.x, mt.mean_x)))); ht = mt.rows(). covmat = ht.covmat; null_fit = ht.null_fit; # FIXME: we should test a whole block of variants at a time not one-by-one; xvec = ht.xvec; yvec = ht.yvec. if test == 'score':; chi_sq, p = _poisson_score_test(null_fit, covmat, yvec, xvec); return ht.select(chi_sq_stat=chi_sq, p_value=p, **ht.pass_through).select_globals('null_fit'). X = hl.nd.hstack([covmat, xvec.T.reshape(-1, 1)]); b = hl.nd.hstack([null_fit.b, hl.nd.array([0.0])]); mu = sigmoid(X @ b); residual = yvec - mu; score = hl.nd.hstack([null_fit.score, hl.nd.array([xvec ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:66009,toler,tolerance,66009,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['toler'],['tolerance']
Availability,"x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; data = _generate_hist2d_data(x, y, bins, range).to_pandas(). # Use python prettier float -> str function; data['x'] = data['x'].apply(lambda e: str(float(e))); data['y'] = data['y'].apply(lambda e: str(float(e))). mapper: ColorMapper; if log:; mapper = LogColorMapper(palette=colors, low=data.c.min(), high=data.c.max()); else:; mapper = LinearColorMapper(palette=colors, low=data.c.min(), high=data.c.max()). x_axis = sorted(set(data.x), key=lambda z: float(z)); y_axis = sorted(set(data.y), key=lambda z: float(z)); p = figure(; title=title,; x_range=x_axis,; y_range=y_axis,; x_axis_location=""above"",; width=width,; height=height,; tools=""hover,save,pan,box_zoom,reset,wheel_zoom"",; toolbar_location='below',; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:18648,avail,available,18648,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['avail'],['available']
Availability,"x_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets GT to the genotype with the minimum PL. Note; that if the genotype changes (as in the example), the PLs; are re-normalized (shifted) so that the most likely genotype has a PL of; 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard any; probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:48426,down,downcode,48426,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['down'],['downcode']
Availability,"x_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:14871,down,down,14871,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['down'],['down']
Availability,"x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; --------. >>> hl.eval(hl.triangle(3)); 6. Notes; -----; The calculation is ``n * (n + 1) / 2``. Parameters; ----------; n : :class:`.Expression` of type :py:data:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""triangle"", tint32, n). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof(expr_set(), expr_array())); def filter(f: Callable, collection):; """"""Returns a new",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105751,down,downcode,105751,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcode']
Availability,"xed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of rows. Parameters; ----------; name : :class:`str`; Name for row index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_rows(**{name: hl.scan.count()}). [docs] @typecheck_method(name=str); def add_col_index(self, name: str = 'col_idx') -> 'MatrixTable':; """"""Add the integer index of each column as a new column field. Examples; --------. >>> dataset_result = dataset.add_col_index(). Notes; -----; The field added is type :py:data:`.tint32`. The column index is 0-indexed; the values are found in the range; ``[0, N)``, where ``N`` is the total number of columns. Parameters; ----------; name: :class:`str`; Name for column index field. Returns; -------; :class:`.MatrixTable`; Dataset with new field.; """"""; return self.annotate_cols(**{name: hl.scan.count()}). @typecheck_method(other=matrix_table_type, tolerance=numeric, absolute=bool, reorder_fields=bool); def _same(self, other, tolerance=1e-6, absolute=False, reorder_fields=False) -> bool:; entries_name = Env.get_uid('entries_'); cols_name = Env.get_uid('columns_'). fd_f = set if reorder_fields else list. if fd_f(self.row) != fd_f(other.row):; print(f'Different row fields: \n {list(self.row)}\n {list(other.row)}'); return False; if fd_f(self.globals) != fd_f(other.globals):; print(f'Different globals fields: \n {list(self.globals)}\n {list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:113376,toler,tolerance,113376,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,4,['toler'],['tolerance']
Availability,"xperimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4808,avail,available,4808,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['avail'],['available']
Availability,"xplicitly adding all input files as input resource; files to the batch so to make sure the same code can run in all scenarios. Files that are already; in a Docker image do not need to be read as inputs to the batch. Output Files; All files generated by Batch are temporary files! They are copied as appropriate between jobs; for downstream jobs’ use, but will be removed when the batch has completed. In order to save; files generated by a batch for future use, you need to explicitly call Batch.write_output().; The first argument to Batch.write_output() can be any type of ResourceFile which includes input resource; files and job resource files as well as resource groups as described below. The second argument to write_output; should be either a local file path or a google storage file path when using the LocalBackend.; For the ServiceBackend, the second argument must be a google storage file path.; >>> b = hb.Batch(name='hello-input'); >>> j = b.new_job(name='hello'); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Resource Groups; Many bioinformatics tools treat files as a group with a common file; path and specific file extensions. For example, PLINK; stores genetic data in three files: *.bed has the genotype data,; *.bim has the variant information, and *.fam has the sample information.; PLINK can take as an input the path to the files expecting there will be three; files with the appropriate extensions. It also writes files with a common file root and; specific file extensions including when writing out a new dataset or outputting summary statistics.; To enable Batch to work with file groups, we added a ResourceGroup object; that is essentially a dictionary from file extension name to file path. When creating; a ResourceGroup in a Job (equivalent to a JobResourceFile),; you first need to use the method BashJob.declare_resource_group() to declare the files; in the resource group explicitly before referring t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:13067,echo,echo,13067,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['echo'],['echo']
Availability,"xpr_any); def str(x) -> StringExpression:; """"""Returns the string representation of `x`. Examples; --------. >>> hl.eval(hl.str(hl.struct(a=5, b=7))); '{""a"":5,""b"":7}'. Parameters; ----------; x. Returns; -------; :class:`.StringExpression`; """"""; if x.dtype == tstr:; return x; else:; return _func(""str"", tstr, x). [docs]@typecheck(c=expr_call, i=expr_int32); def downcode(c, i) -> CallExpression:; """"""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""; return _func(""downcode"", tcall, c, i). @typecheck(pl=expr_array(expr_int32)); def gq_from_pl(pl) -> Int32Expression:; """"""Compute genotype quality from Phred-scaled probability likelihoods. Examples; --------. >>> hl.eval(hl.gq_from_pl([0, 69, 1035])); 69. Parameters; ----------; pl : :class:`.Expression` of type :class:`.tarray` of :obj:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""gqFromPL"", tint32, pl). [docs]@typecheck(n=expr_int32); def triangle(n) -> Int32Expression:; """"""Returns the triangle number of `n`. Examples; --------. >>> hl.eval(hl.triangle(3)); 6. Notes; -----; The calculation is ``n * (n + 1) / 2``. Parameters; ----------; n : :class:`.Expression` of type :py:data:`.tint32`. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; """"""; return _func(""triangle"", tint32, n). [docs]@typecheck(f=func_spec(1, expr_bool), collection=expr_oneof",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:105664,down,downcoded,105664,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['down'],['downcoded']
Availability,xpression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.GroupedMatrixTable method). (hail.MatrixTable method). (hail.Table method). diagonal() (hail.linalg.BlockMatrix method). (in module hail.nd). dict() (in module hail.expr.functions). DictExpression (class in hail.expr). difference() (hail.expr.SetExpression method). distinct() (hail.Table method). distinct_by_col() (hail.MatrixTable method). distinct_by_row() (hail.MatrixTable method). dnorm() (in module hail.expr.functions). downcode() (in module hail.expr.functions). downsample() (in module hail.expr.aggregators). dpois() (in module hail.expr.functions). drop() (hail.expr.StructExpression method). (hail.MatrixTable method). (hail.Table method). dtype (hail.expr.ArrayExpression property). (hail.expr.ArrayNumericExpression property). (hail.expr.BooleanExpression property). (hail.expr.CallExpression property). (hail.expr.CollectionExpression property). (hail.expr.DictExpression property). (hail.expr.Expression property). (hail.expr.Float32Expression property). (hail.expr.Float64Expression property). (hail.expr.Int32Expression property). (hail.expr.Int64Expression property). (hail.expr.IntervalExpression property). (hail.expr.LocusExpression property). (hail.expr.NDArrayExpression property). (hail.expr.NDArrayNumericExpression property). (hail.expr.NumericExpression property). (hail.expr.SetExpression property). (hail.expr.StringExpression property). (hail.expr.StructExpression property). (hail.expr.TupleExpression property). dtype() (in module hail.expr.types). E. ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:18200,down,downsample,18200,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['down'],['downsample']
Availability,"xpressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.isdir('data/1kg.vds') and os.path.isfile('data/1kg_annotations.txt'):; print('All files are present and accounted for!'); else:; import sys; sys.stderr.write('Downloading data (~50M) from Google Storage...\n'); import urllib; import tarfile; urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',; 'tutorial_data.tar'); sys.stderr.write('Download finished!\n'); sys.stderr.write('Extracting...\n'); tarfile.open('tutorial_data.tar').extractall(); if not (os.path.is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:1483,avail,available,1483,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['avail'],['available']
Availability,"y 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters:; is_spot (bool) – If False, this job will be run on non-spot instances. Return type:; Self. Returns:; Same job object. storage(storage); Set the job’s storage size.; Examples; Set the job’s disk requirements to 10 Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:7200,echo,echo,7200,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['echo'],['echo']
Availability,"y ID.; - `children` (:py:data:`.tint32`) -- Number of children in this nuclear family.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this nuclear family.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors at SNPs in this; nuclear family. **Third table:** errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the `Implicated` in the table below. - (column key of `dataset`) (:py:data:`.tstr`) -- Sample ID (key field).; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual.; - `snp_errors` (:py:data:`.tint64`) -- Number of Mendel errors involving this; individual at SNPs. **Fourth table:** errors per variant. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - `errors` (:py:data:`.tint64`) -- Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; `Plink classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR) of X and Y; defined by the reference genome and the autosome is defined by; :meth:`~.LocusExpression.in_autosome`. - Auto -- in autosome or in PAR or female child; - HemiX -- in non-PAR of X and male child; - HemiY -- in non-PAR of Y and male child. `Any` refers to the set \{ HomRef, Het, HomVar, NoCall \} and `~`; denotes complement in this set. +------+---------+---------+--------+----------------------------+; | Code | Dad | Mom | Kid | Copy State | Implicated |; +======+=========+=========+========+============+=========",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:7198,error,errors,7198,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"y exports the contents of ``va.info`` to the INFO field. No other annotations besides ``va.info`` are exported. The genotype schema must have the type :py:class:`~hail.expr.TGenotype` or :py:class:`~hail.expr.TStruct`. If the type is; :py:class:`~hail.expr.TGenotype`, then the FORMAT fields will be GT, AD, DP, GQ, and PL (or PP if ``export_pp`` is True).; If the type is :py:class:`~hail.expr.TStruct`, then the exported FORMAT fields will be the names of each field of the Struct.; Each field must have a type of String, Char, Int, Double, or Call. Arrays and Sets are also allowed as long as they are not nested.; For example, a field with type ``Array[Int]`` can be exported but not a field with type ``Array[Array[Int]]``.; Nested Structs are also not allowed. .. caution::. If samples or genotypes are filtered after import, the value stored in ``va.info.AC`` value may no longer reflect the number of called alternate alleles in the filtered VDS. If the filtered VDS is then exported to VCF, downstream tools may produce erroneous results. The solution is to create new annotations in ``va.info`` or overwrite existing annotations. For example, in order to produce an accurate ``AC`` field, one can run :py:meth:`~hail.VariantDataset.variant_qc` and copy the ``va.qc.AC`` field to ``va.info.AC``:. >>> (vds.filter_genotypes('g.gq >= 20'); ... .variant_qc(); ... .annotate_variants_expr('va.info.AC = va.qc.AC'); ... .export_vcf('output/example.vcf.bgz')). :param str output: Path of .vcf file to write. :param append_to_header: Path of file to append to VCF header.; :type append_to_header: str or None. :param bool export_pp: If true, export linear-scaled probabilities (Hail's `pp` field on genotype) as the VCF PP FORMAT field. :param bool parallel: If true, return a set of VCF files (one per partition) rather than serially concatenating these files.; """""". self._jvdf.exportVCF(output, joption(append_to_header), export_pp, parallel). [docs] @handle_py4j; @convertVDS; @typecheck_method(o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:58627,down,downstream,58627,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['down'],['downstream']
Availability,"y list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:17178,error,error,17178,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['error'],['error']
Availability,"y of tstr) – Variant alleles, key field.; (column key of dataset) (tstr) – Proband ID, key field.; fam_id (tstr) – Family ID.; mendel_code (tint32) – Mendel error code, see below. Second table: errors per nuclear family. This table contains one row; per nuclear family, keyed by the parents. pat_id (tstr) – Paternal ID. (key field); mat_id (tstr) – Maternal ID. (key field); fam_id (tstr) – Family ID.; children (tint32) – Number of children in this nuclear family.; errors (tint64) – Number of Mendel errors in this nuclear family.; snp_errors (tint64) – Number of Mendel errors at SNPs in this; nuclear family. Third table: errors per individual. This table contains one row per; individual. Each error is counted toward the proband, father, and mother; according to the Implicated in the table below. (column key of dataset) (tstr) – Sample ID (key field).; fam_id (tstr) – Family ID.; errors (tint64) – Number of Mendel errors involving this; individual.; snp_errors (tint64) – Number of Mendel errors involving this; individual at SNPs. Fourth table: errors per variant. locus (tlocus) – Variant locus, key field.; alleles (tarray of tstr) – Variant alleles, key field.; errors (tint64) – Number of Mendel errors in this variant. This method only considers complete trios (two parents and proband with; defined sex). The code of each Mendel error is determined by the table; below, extending the; Plink classification.; In the table, the copy state of a locus with respect to a trio is defined; as follows, where PAR is the pseudoautosomal region (PAR) of X and Y; defined by the reference genome and the autosome is defined by; in_autosome(). Auto – in autosome or in PAR or female child; HemiX – in non-PAR of X and male child; HemiY – in non-PAR of Y and male child. Any refers to the set { HomRef, Het, HomVar, NoCall } and ~; denotes complement in this set. Code; Dad; Mom; Kid; Copy State | Implicated. 1; HomVar; HomVar; Het; Auto; Dad, Mom, Kid. 2; HomRef; HomRef; Het; Auto; Dad, Mom, K",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:50288,error,errors,50288,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['errors']
Availability,"y the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; This method can be called more than once. It’s behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command.; To declare a resource file of type JobResourceFile, use either; the get attribute syntax of job.{identifier} or the get item syntax of; job[‘identifier’]. If an object for that identifier doesn’t exist,; then one will be created automatically (only allowed in the; command() method). The identifier name can be any valid Python; identifier such as ofile5000.; All JobResourceFile are temporary files and must be written to; a permanent location using Batch.write_output() if the output; needs to be saved.; Only resources can be referred to in commands. Referencing a; batch.Batch or",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:1914,echo,echo,1914,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,2,['echo'],['echo']
Availability,"y: bool = False,; _codec_spec: Optional[str] = None,; _read_if_exists: bool = False,; _intervals=None,; _filter_intervals=False,; _drop_cols=False,; _drop_rows=False,; ) -> 'MatrixTable':; """"""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""; hl.current_backend().validate_file(output). if not _read_if_exists or not hl.hadoop_exists(f'{output}/_SUCCESS'):; self.write(output=output, overwrite=overwrite, stage_locally=stage_locally, _codec_spec=_codec_spec); _assert_type = self._type; _load_refs = False; else:; _assert_type = None; _load_refs = True; return hl.read_matrix_table(; output,; _intervals=_intervals,; _filter_intervals=_filter_intervals,; _drop_cols=_drop_cols,; _drop_rows=_drop_rows,; _assert_type=_assert_type,; _load_refs=_load_refs,; ). [docs] @typecheck_method(; output=str, overwrite=bool, stage_locally=bool, _codec_spec=nullable(str), _partitions=nullable(expr_any); ); def write(; self,; output: str,; overwrite: bool = False,; stage_locally: bool = False,; _codec_spec: Optional[str] = None,; _partitions=None,; ):; """"""Write to disk. Examples; --------. >>> dataset.write('output/dataset.mt'). .. include:: _templates/write_warning.rst. See Also; --------; :func:`.read_matrix_table`. P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:81223,checkpoint,checkpoint,81223,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['checkpoint'],['checkpoint']
Availability,"y:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pn",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71353,fault,fault,71353,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['fault'],['fault']
Availability,"yTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contains one row per Mendel error in the dataset;; it is possible that a variant or sample may be found on more than one row. This table closely; reflects the structure of the "".mendel"" PLINK format detailed below.; ; Columns:; ; - **fid** (*String*) -- Family ID.; - **s** (*String*) -- Proband ID.; - **v** (*Variant*) -- Variant in which the error was found.; - **code** (*Int*) -- Mendel error code, see below. ; - **error** (*String*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:155052,error,errors,155052,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['error'],['errors']
Availability,"y_expr(dataset.is_hom_ref).checkpoint(hl.utils.new_temp_file()); IS_HET = BlockMatrix.from_entry_expr(dataset.is_het).checkpoint(hl.utils.new_temp_file()); IS_HOM_VAR = BlockMatrix.from_entry_expr(dataset.is_hom_var).checkpoint(hl.utils.new_temp_file()); NOT_MISSING = (IS_HOM_REF + IS_HET + IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()). total_possible_ibs = NOT_MISSING.T @ NOT_MISSING. ibs0_pre = (IS_HOM_REF.T @ IS_HOM_VAR).checkpoint(hl.utils.new_temp_file()); ibs0 = ibs0_pre + ibs0_pre.T. is_not_het = IS_HOM_REF + IS_HOM_VAR; ibs1_pre = (IS_HET.T @ is_not_het).checkpoint(hl.utils.new_temp_file()); ibs1 = ibs1_pre + ibs1_pre.T. ibs2 = total_possible_ibs - ibs0 - ibs1. Z0 = ibs0 / expectations.e00; Z1 = (ibs1 - Z0 * expectations.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:7539,checkpoint,checkpoint,7539,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['checkpoint'],['checkpoint']
Availability,"ym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Mendel; error, keyed by the variant and proband id. - `locus` (:class:`.tlocus`) -- Variant locus, key field.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Variant alleles, key field.; - (column key of `dataset`) (:py:data:`.tstr`) -- Proband ID, key field.; - `fam_id` (:py:data:`.tstr`) -- Family ID.; - `mendel_code` (:py:data:`.tint32`) -- ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4911,error,errors,4911,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['errors']
Availability,"{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). \]. \[\begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56639,error,error,56639,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['error'],['error']
Availability,"{list(other.globals)}'); return False; if fd_f(self.col) != fd_f(other.col):; print(f'Different col fields: \n {list(self.col)}\n {list(other.col)}'); return False; if fd_f(self.entry) != fd_f(other.entry):; print(f'Different row fields: \n {list(self.entry)}\n {list(other.entry)}'); return False. if reorder_fields:; entry_order = list(self.entry); if list(other.entry) != entry_order:; other = other.select_entries(*entry_order). globals_order = list(self.globals); if list(other.globals) != globals_order:; other = other.select_globals(*globals_order). col_order = list(self.col); if list(other.col) != col_order:; other = other.select_cols(*col_order). row_order = list(self.row); if list(other.row) != row_order:; other = other.select_rows(*row_order). if list(self.col_key) != list(other.col_key):; print(f'different col keys:\n {list(self.col_key)}\n {list(other.col_key)}'); return False. return self._localize_entries(entries_name, cols_name)._same(; other._localize_entries(entries_name, cols_name), tolerance, absolute; ). @typecheck_method(caller=str, s=expr_struct()); def _select_entries(self, caller, s) -> 'MatrixTable':; base, cleanup = self._process_joins(s); analyze(caller, s, self._entry_indices); return cleanup(MatrixTable(ir.MatrixMapEntries(base._mir, s._ir))). @typecheck_method(caller=str, row=expr_struct()); def _select_rows(self, caller, row) -> 'MatrixTable':; analyze(caller, row, self._row_indices, {self._col_axis}); base, cleanup = self._process_joins(row); return cleanup(MatrixTable(ir.MatrixMapRows(base._mir, row._ir))). @typecheck_method(caller=str, col=expr_struct(), new_key=nullable(sequenceof(str))); def _select_cols(self, caller, col, new_key=None) -> 'MatrixTable':; analyze(caller, col, self._col_indices, {self._row_axis}); base, cleanup = self._process_joins(col); return cleanup(MatrixTable(ir.MatrixMapCols(base._mir, col._ir, new_key))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'MatrixTable':; base,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:114880,toler,tolerance,114880,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['toler'],['tolerance']
Availability,"} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:68825,error,error,68825,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['error']
Availability,"}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} \cdot {} &\mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}; \right). .. math::; \begin{aligned}; \mathrm{P}(x = (AA, AA, AB) \mid m) = &\left(; \begin{aligned}; &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AB); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.). While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood. These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. - ``DR`` refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; - ``DP`` refers to the read depth (DP field) of the proband.; - ``AB`` refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; - ``AC`` refers to the count of alternate alleles across all individuals; in the dataset at the site.; - ``p`` refers to :math:`\mathrm{P_{\text{de novo}}}`.; - ``min_p`` refers to the `min_p` function parameter. HIGH-quality SNV:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:24688,error,error,24688,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['error'],['error']
Availability,"’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row value",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:4536,down,down,4536,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['down'],['down']
Availability,". (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:38605,error,errors,38605,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['error'],['errors']
Availability,"; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference genome from a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7240,avail,available,7240,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['avail'],['available']
Availability,"﻿. . Getting Started — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Running Hail locally; Building Hail from source; Running on a Spark cluster; Running on a Cloudera Cluster; Running in the cloud; Building with other versions of Spark 2. BLAS and LAPACK; Running the tests. Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Getting Started. View page source. Getting Started¶; You’ll need:. The Java 8 JDK.; Spark 2.0.2. Hail is compatible with Spark 2.0.x and 2.1.x.; Python 2.7 and Jupyter Notebooks. We recommend the free Anaconda distribution. Running Hail locally¶; Hail uploads distributions to Google Storage as part of our continuous integration suite.; You can download a pre-built distribution from the below links. Make sure you download the distribution that matches your Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, availa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:812,down,download,812,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['down'],['download']
Availability,"﻿. . Overview — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Overview. View page source. Overview¶; This notebook is designed to provide a broad overview of Hail’s; functionality, with emphasis on the functionality to manipulate and; query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by; population stratification.; Each notebook starts the same: we import the hail package and create; a HailContext. This; object is the entry point for most Hail functionality. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import matplotlib.patches as mpatches; from collections import Counter; from math import log, isnan; from pprint import pprint; %matplotlib inline. Installing and importing; seaborn is optional; it; just makes the plots prettier. In [3]:. # optional; import seaborn. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:219,down,download,219,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['down'],['download']
Availability,"﻿. . Overview: module code — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Overview: module code. All modules for which code is available; hail.context; hail.dataset; hail.expr; hail.keytable; hail.kinshipMatrix; hail.ldMatrix; hail.representation.annotations; hail.representation.genotype; hail.representation.interval; hail.representation.pedigree; hail.representation.variant; hail.utils. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/index.html:318,avail,available,318,docs/0.1/_modules/index.html,https://hail.is,https://hail.is/docs/0.1/_modules/index.html,1,['avail'],['available']
Availability,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:220,down,download,220,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,2,['down'],['download']
Availability,"﻿. . Using the expression language to slice, dice, and query genetic data — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Using the expression language to slice, dice, and query genetic data. View page source. Using the expression language to slice, dice, and query genetic data¶; This notebook uses the Hail expression language to query, filter, and; annotate the same thousand genomes dataset from the overview. We also; cover how to compute aggregate statistics from a dataset using the; expression language.; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use; throughout the notebook. In [2]:. from pprint import pprint. Check for tutorial data or download if necessary¶; This cell downloads the necessary data from Google Storage if it isn’t; found in the current working directory. In [3]:. import os; if os.path.is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:433,down,download,433,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['down'],['download']
Availability,"﻿. Batch Service — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch Service. View page source. Batch Service. Warning; The Batch Service is currently only available to Broad Institute affiliates. Please contact us if you are interested in hosting a copy of the Batch; Service at your institution. Warning; Ensure you have installed the Google Cloud SDK as described in the Batch Service section of; Getting Started. What is the Batch Service?; Instead of executing jobs on your local computer (the default in Batch), you can execute; your jobs on a multi-tenant compute cluster in Google Cloud that is managed by the Hail team; and is called the Batch Service. The Batch Service consists of a scheduler that receives job; submission requests from users and then executes jobs in Docker containers on Google Compute; Engine VMs (workers) that are shared amongst all Batch users. A UI is available at https://batch.hail.is; that allows a user to see job progress and access logs. Sign Up; For Broad Institute users, you can sign up at https://auth.hail.is/signup.; This will allow you to authenticate with your Broad Institute email address and create; a Batch Service account. A Google Service Account is created; on your behalf. A trial Batch billing project is also created for you at; <USERNAME>-trial. You can view these at https://auth.hail.is/user.; To create a new Hail Batch billing project (separate from the automatically created trial billing; project), send an inquiry using this billing project creation form.; To modify an existing Hail Batch billing project, send an inquiry using this; billing project modification form. File Loca",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:509,avail,available,509,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['avail'],['available']
Availability,"﻿. Hail | ; Datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets. View page source. Datasets. Warning; All functionality described on this page is experimental and subject to; change. This page describes genetic datasets that are hosted in public buckets on both; Google Cloud Storage and Amazon S3. Note that these datasets are stored in; Requester Pays buckets on GCS, and are available in; both the US-CENTRAL1 and EUROPE-WEST1 regions. On AWS, the datasets are shared; via Open Data on AWS and are in buckets; in the US region.; Check out the load_dataset() function to see how to load one of these; datasets into a Hail pipeline. You will need to provide the name, version, and; reference genome build of the desired dataset, as well as specify the region; your cluster is in and the cloud platform. Egress charges may apply if your; cluster is outside of the region specified.; Schemas for Available Datasets. Schemas. Search. name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets.html:737,avail,available,737,docs/0.2/datasets.html,https://hail.is,https://hail.is/docs/0.2/datasets.html,1,['avail'],['available']
Availability,"﻿. Hail | ; Hail Tutorials. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials. View page source. Hail Tutorials. To take Hail for a test drive, go through our tutorials. These can be viewed here in the; documentation, but we recommend instead that you run them yourself with Jupyter by; downloading the archive (.tar.gz); and running the following:pip install jupyter; tar xf tutorials.tar.gz; jupyter notebook tutorials/. Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials-landing.html:835,down,downloading,835,docs/0.2/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.2/tutorials-landing.html,1,['down'],['downloading']
Availability,"﻿. Hail | ; Numeric functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Numeric functions. View page source. Numeric functions; Numeric functions. abs(x); Take the absolute value of a numeric value, array or ndarray. approx_equal(x, y[, tolerance, absolute, ...]); Tests whether two numbers are approximately equal. bit_and(x, y); Bitwise and x and y. bit_or(x, y); Bitwise or x and y. bit_xor(x, y); Bitwise exclusive-or x and y. bit_lshift(x, y); Bitwise left-shift x by y. bit_rshift(x, y[, logical]); Bitwise right-shift x by y. bit_not(x); Bitwise invert x. bit_count(x); Count the number of 1s in the in the two's complement binary representation of x. exp(x). expit(x). is_nan(x). is_finite(x). is_infinite(x). log(x[, base]); Take the logarithm of the x with base base. log10(x). logit(x). sign(x); Returns the sign of a numeric value, array or ndarray. sqrt(x). int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit integer expression. int64(x); Convert to a 64-bit integer expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. floor(x). ceil(x). uniroot(f, min, max, *[, max_iter, epsilon, ...]); Finds a root of the function f within the interval [min, max]. Numeric collection functions. min(*exprs[, filter_missing]); Returns the minimum elem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:826,toler,tolerance,826,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['toler'],['tolerance']
Availability,﻿. Hail | ; Overview: module code. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.c,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/index.html:470,avail,available,470,docs/0.2/_modules/index.html,https://hail.is,https://hail.is/docs/0.2/_modules/index.html,1,['avail'],['available']
Availability,"﻿. Hail | ; VariantDatasetCombiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; VariantDatasetCombiner. View page source. VariantDatasetCombiner. class hail.vds.combiner.VariantDatasetCombiner[source]; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets.; Examples; A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets:; gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:813,failure,failure-tolerant,813,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['failure'],['failure-tolerant']
Availability,"﻿. Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:929,error,error,929,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['error'],['error']
Availability,"﻿. Hail | ; hail.vds.split_multi. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.split_multi. View page source. hail.vds.split_multi. hail.vds.split_multi(vds, *, filter_changed_loci=False)[source]; Split the multiallelic variants in a VariantDataset. Parameters:. vds (VariantDataset) – Dataset in VariantDataset representation.; filter_changed_loci (bool) – If any REF/ALT pair changes locus under min_rep(), filter that; variant instead of throwing an error. Returns:; VariantDataset. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html:1054,error,error,1054,docs/0.2/vds/hail.vds.split_multi.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.split_multi.html,1,['error'],['error']
Availability,"﻿. Hail | ; hail.vds.store_ref_block_max_length. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.store_ref_block_max_length. View page source. hail.vds.store_ref_block_max_length. hail.vds.store_ref_block_max_length(vds_path)[source]; Patches an existing VDS file to store the max reference block length for faster interval filters.; This method permits vds.filter_intervals() to remove reference data not overlapping a target interval.; This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; vds.truncate_reference_blocks() to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS.; Examples; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') . See also; vds.filter_intervals(), vds.truncate_reference_blocks(). Parameters:; vds_path (str). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html:1151,down,downstream,1151,docs/0.2/vds/hail.vds.store_ref_block_max_length.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.store_ref_block_max_length.html,1,['down'],['downstream']
Availability,"﻿. Hail | ; hailtop.batch Python API. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; hailtop.batch Python API. View page source. hailtop.batch Python API; The Hail Batch Service is a multi-tenant elastic compute cluster for analyzing datasets in the cloud. It; is available in both Microsoft Azure and Google Cloud Platform. At this time, the; Hail-maintained Batch Service is only available for users with a Broad Institute affiliation. However, there are; instructions available for how to deploy the Hail Batch Service in your own projects in our GitHub repository.; To learn more about the Hail Batch Service, take a look at our documentation.; The Python library hailtop.batch is a client library for defining workflows for the Hail Batch Service to execute.; To learn more about the Python client library, there is a tutorial and; cookbooks with detailed examples. The API documentation is available here. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/batch_api.html:645,avail,available,645,docs/0.2/batch_api.html,https://hail.is,https://hail.is/docs/0.2/batch_api.html,4,['avail'],['available']
Deployability," # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8350,pipeline,pipeline,8350,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['pipeline'],['pipeline']
Deployability," '3'). >>> hl.eval(t[2]); '3'. Parameters:; iterable (an iterable of Expression) – Tuple elements. Returns:; TupleExpression. hail.expr.functions.array(collection)[source]; Construct an array expression.; Examples; >>> s = {'Bob', 'Charlie', 'Alice'}. >>> hl.eval(hl.array(s)); ['Alice', 'Bob', 'Charlie']. Parameters:; collection (ArrayExpression or SetExpression or DictExpression). Returns:; ArrayExpression. hail.expr.functions.empty_array(t)[source]; Returns an empty array of elements of a type t.; Examples; >>> hl.eval(hl.empty_array(hl.tint32)); []. Parameters:; t (str or HailType) – Type of the array elements. Returns:; ArrayExpression. hail.expr.functions.set(collection)[source]; Convert a set expression.; Examples; >>> s = hl.set(['Bob', 'Charlie', 'Alice', 'Bob', 'Bob']); >>> hl.eval(s) ; {'Alice', 'Bob', 'Charlie'}. Returns:; SetExpression – Set of all unique elements. hail.expr.functions.empty_set(t)[source]; Returns an empty set of elements of a type t.; Examples; >>> hl.eval(hl.empty_set(hl.tstr)); set(). Parameters:; t (str or HailType) – Type of the set elements. Returns:; SetExpression. hail.expr.functions.dict(collection)[source]; Creates a dictionary.; Examples; >>> hl.eval(hl.dict([('foo', 1), ('bar', 2), ('baz', 3)])); {'bar': 2, 'baz': 3, 'foo': 1}. Notes; This method expects arrays or sets with elements of type ttuple; with 2 fields. The first field of the tuple becomes the key, and the second; field becomes the value. Parameters:; collection (DictExpression or ArrayExpression or SetExpression). Returns:; DictExpression. hail.expr.functions.empty_dict(key_type, value_type)[source]; Returns an empty dictionary with key type key_type and value type; value_type.; Examples; >>> hl.eval(hl.empty_dict(hl.tstr, hl.tint32)); {}. Parameters:. key_type (str or HailType) – Type of the keys.; value_type (str or HailType) – Type of the values. Returns:; DictExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/constructors.html:7165,update,updated,7165,docs/0.2/functions/constructors.html,https://hail.is,https://hail.is/docs/0.2/functions/constructors.html,1,['update'],['updated']
Deployability," (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None); show(p). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:7472,update,updated,7472,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['update'],['updated']
Deployability," ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30755,update,update,30755,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability," ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_generation_tup, idx):; prev_size = prev_generation_tup[1]; n_new = hl.int32(hl.floor(prev_size * generation_size_multiplier)); new_generation = hl.range(n_new).map(; lambda i: hl.struct(; s=hl.str('generation_') + hl.str(idx + 1) + hl.str('_idx_') + hl.str(i),; original=hl.missing('str'),; mother=hl.rand_int32(0, prev_size),; father=hl.rand_int32(0, prev_size),; ); ); return (new_generation, (prev_size + n_new) if keep_founders else n_new). ht = ht.annotate_globals(; generations=hl.range(n_rounds).scan(; lambda prev, idx: make_new_generation(prev, idx), (ht.generation_0, hl.len(ht.generation_0)); ); ). def simulate_mating_calls(prev_generation_calls, new_generation):; new_samples = new_generation.map(; lambda samp: hl.call(; prev_generation_calls[samp.mother][hl.rand_int32(0, 2)],; prev_generation_calls[samp.father][hl.rand_int32(0, 2)],; ); ); if keep_founders:; return prev_generation_calls.extend(new_samples); else:; return new_samples. ht = ht.annotate(; __new_entries=hl.fold(; lambda prev_calls, generation_metadata: simulate_mating_calls(prev_calls, generation_metadata[0]),; ht.__entries.GT,; ht.generations[1:],; ).map(lambda gt: hl.struct(GT=gt)); ); ht = ht.annotate_globals(; __new_cols=ht.generations.flatmap(lambda x: x[0]) if keep_founders else ht.generations[-1][0]; ); ht = ht.drop('__entries', '__cols', 'generation_0', 'generations'); return ht._unlocalize_entries('__new_entries', '__new_cols', list('s')). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:3481,update,updated,3481,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,2,['update'],['updated']
Deployability," +-------+-------+-----+-------+-------+----------+-------+----------+; +----------------+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42409,pipeline,pipeline,42409,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability," 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulip",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:50454,release,release,50454,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability," : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """""". Env.backend().add_liftover(self.name, chain_file, dest_reference_genome.name); if dest_reference_genome.name in self._liftovers:; raise KeyError(f""Liftover already exists from {self.name} to {dest_reference_genome.name}.""); if dest_reference_genome.name == self.name:; raise ValueError(f'Destination reference genome cannot have the same name as this reference {self.name}.'); self._liftovers[dest_reference_genome.name] = chain_file. [docs] @typecheck_method(global_pos=int); def locus_from_global_position(self, global_pos: int) -> 'hl.Locus':; """""" ""; Constructs a locus from a global position in reference genome.; The inverse of :meth:`.Locus.position`. Examples; --------; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters; ----------; global_pos : int; Zero-based global base position along the reference genome. Returns; -------; :class:`.Locus`; """"""; if global_pos < 0:; raise ValueError(f""global_pos must be non-negative, got {global_pos}""). if self._global_positions_list is None:; # dicts are in insertion order as of 3.7; self._global_positions_list = list(self.global_positions_dict.values()). global_positions = self._global_positions_list; contig = self.contigs[bisect_right(global_positions, global_pos) - 1]; contig_pos = self.global_positions_dict[contig]. if global_pos >= contig_pos + self.lengths[contig]:; raise ValueError(f""global_pos {global_pos} exceeds length of reference genome {self}.""). return hl.Locus(contig, global_pos - contig_pos + 1, self). rg_type.set(ReferenceGenome). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:16681,update,updated,16681,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['update'],['updated']
Deployability," = right.localize_entries('right_entries', 'right_cols'). ht = left_t.join(right_t, how='outer'); ht = ht.annotate_globals(; left_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.left_cols.map(lambda x: hl.tuple([x[f] for f in left.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; right_keys=hl.group_by(; lambda t: t[0],; hl.enumerate(ht.right_cols.map(lambda x: hl.tuple([x[f] for f in right.col_key])), index_first=False),; ).map_values(lambda elts: elts.map(lambda t: t[1])),; ); ht = ht.annotate_globals(; key_indices=hl.array(ht.left_keys.key_set().union(ht.right_keys.key_set())); .map(lambda k: hl.struct(k=k, left_indices=ht.left_keys.get(k), right_indices=ht.right_keys.get(k))); .flatmap(; lambda s: hl.case(); .when(; hl.is_defined(s.left_indices) & hl.is_defined(s.right_indices),; hl.range(0, s.left_indices.length()).flatmap(; lambda i: hl.range(0, s.right_indices.length()).map(; lambda j: hl.struct(k=s.k, left_index=s.left_indices[i], right_index=s.right_indices[j]); ); ),; ); .when(; hl.is_defined(s.left_indices),; s.left_indices.map(lambda elt: hl.struct(k=s.k, left_index=elt, right_index=hl.missing('int32'))),; ); .when(; hl.is_defined(s.right_indices),; s.right_indices.map(lambda elt: hl.struct(k=s.k, left_index=hl.missing('int32'), right_index=elt)),; ); .or_error('assertion error'); ); ); ht = ht.annotate(; __entries=ht.key_indices.map(; lambda s: hl.struct(left_entry=ht.left_entries[s.left_index], right_entry=ht.right_entries[s.right_index]); ); ); ht = ht.annotate_globals(; __cols=ht.key_indices.map(; lambda s: hl.struct(; **{f: s.k[i] for i, f in enumerate(left.col_key)},; left_col=ht.left_cols[s.left_index],; right_col=ht.right_cols[s.right_index],; ); ); ); ht = ht.drop('left_entries', 'left_cols', 'left_keys', 'right_entries', 'right_cols', 'right_keys', 'key_indices'); return ht._unlocalize_entries('__entries', '__cols', list(left.col_key)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:6017,update,updated,6017,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,2,['update'],['updated']
Deployability," Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1361,release,releases,1361,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['release'],['releases']
Deployability," Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Pre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1071,configurat,configuration,1071,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89345,pipeline,pipelines,89345,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:7642,release,released,7642,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_C,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:209,Install,Installation,209,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:205,Install,Installation,205,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:204,Install,Installation,204,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cult,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:200,Install,Installation,200,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:196,Install,Installation,196,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_f,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html:194,Install,Installation,194,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,6,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibro,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html:190,Install,Installation,190,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,4,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibrob,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html:189,Install,Installation,189,docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Muscle_Skeletal_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibrobla,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html:187,Install,Installation,187,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,4,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblas,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html:186,Install,Installation,186,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblast,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html:185,Install,Installation,185,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html:183,Install,Installation,183,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_a,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html:182,Install,Installation,182,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,4,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_al,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html:181,Install,Installation,181,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,4,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html:180,Install,Installation,180,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,8,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html:179,Install,Installation,179,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,4,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_s,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html:178,Install,Installation,178,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_ge,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html:173,Install,Installation,173,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html:171,Install,Installation,171,docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_high_quality.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html:170,Install,Installation,170,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_ass,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html:167,Install,Installation,167,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,12,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_asso,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html:166,Install,Installation,166,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_assoc,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html:165,Install,Installation,165,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html:164,Install,Installation,164,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associat,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html:162,Install,Installation,162,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,2,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability, Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Schemas; 1000_Genomes_HighCov_autosomes; 1000_Genomes_HighCov_chrX; 1000_Genomes_HighCov_chrY; 1000_Genomes_Retracted_autosomes; 1000_Genomes_Retracted_chrX; 1000_Genomes_Retracted_chrY; 1000_Genomes_autosomes; 1000_Genomes_chrMT; 1000_Genomes_chrX; 1000_Genomes_chrY; CADD; DANN; Ensembl_homo_sapiens_low_complexity_regions; Ensembl_homo_sapiens_reference_genome; GTEx_RNA_seq_gene_TPMs; GTEx_RNA_seq_gene_read_counts; GTEx_RNA_seq_junction_read_counts; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html:158,Install,Installation,158,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,14,"['Configurat', 'Install']","['Configuration', 'Installation']"
Deployability," Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:23784,install,installation,23784,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability," PositionScaleDiscrete(""y"", name=name, breaks=breaks, labels=labels). [docs]def scale_x_genomic(reference_genome, name=None):; """"""The default genomic x scale. This is used when the ``x`` aesthetic corresponds to a :class:`.LocusExpression`. Parameters; ----------; reference_genome:; The reference genome being used.; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleGenomic(""x"", reference_genome, name=name). [docs]def scale_color_discrete():; """"""The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_color_hue(). [docs]def scale_color_hue():; """"""Map discrete colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""color""). [docs]def scale_color_continuous():; """"""The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:12020,continuous,continuous,12020,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['continuous'],['continuous']
Deployability," This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to get a few datapoints as; you gradually increase the size of your small dataset to see if it’s scaling linearly. Estimating cost; Costs vary between cloud providers. This cost estimate is based on Google Cloud, but the same principles often apply to other providers.; Google charges by the core-hour, so we can convert so-called “wall clock time” (time elapsed from starting the cluster to stopping the cluster); to dollars-spent by multiplying it by the number of cores of each type and the price per core per hour of each type. At time of writing,; preemptible cores are 0.01 dollars per core hour and non-preemptible cores are 0.0475 dollars per core hour. Moreover, each core has an; additional 0.01 dollar “dataproc premium” fee. The cost of CPU cores for a cluster with an 8-core leader node; two non-preemptible, 8-core workers;; and 10 preemptible, 8-core workers running for 2 hours is:; 2 * (2 * 8 * 0.0575 + # non-preemptible workers; 10 * 8 * 0.02 + # preemptible workers; 1 * 8 * 0.0575) # leader (master) node. 2.98 USD.; There are additional charges for persistent disk and SSDs. If your leader node has 100 GB and your worker nodes have 40 GB each you can expect; a modest increase in cost, slightly less than a dollar. The cost per disk is prorated from a per-month rate; at time of writing it is 0.04 USD; per GB per month. SSDs are more than four times as expensive.; In general, once you know the wall clock time of your job, you can enter your cluster parameters into the; Google Cloud Pricing Calculator. and get a precise estimate; of cost using the latest prices. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:4067,update,updated,4067,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['update'],['updated']
Deployability," ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', 'GQ']):; ad_field = 'AD'; gt_field = 'GT'; else:; raise ValueError(; f""'compute_charr': require a VDS or MatrixTable with fields LAD/LAD/LGT/GQ/DP or AD/GT/GQ/DP,""; f"" found entry fields {list(mt.entry)}""; ); # Annotate reference allele frequency when it is not defined in the original data, and name it 'ref_AF'.; ref_af_field = '__ref_af'; if ref_AF is None:; n_samples = mt.count_cols(); if n_samples < 10000:; raise ValueError(; ""'compute_charr': with fewer than 10,000 samples, require a reference AF in 'reference_data_source'.""; ). n_alleles = 2 * n_samples; mt = mt.annotate_rows(**{ref_af_field: 1 - hl.agg.sum(mt[gt_field].n_alt_alleles()) / n_alleles}); else:; mt = mt.annotate_rows(**{ref_af_field: ref_AF}). # Filter to autosomal biallelic SNVs with reference allele frequency within the range (min_af, max_af); rg = mt.locus.dtype.reference_genome.name; if rg == 'GRCh37':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('1-22', reference_genome=rg)]); elif rg == 'GRCh38':; mt = hl.filter_intervals(mt, [hl.parse_locus_interval('chr1-chr22', reference_genome=rg)]); else:; mt = mt.filter_rows(mt.locus.in_autosome()). mt = mt.filter_rows(; (hl.len(mt.alleles) == 2); & hl.is_snp(mt.alleles[0], mt.alleles[1]); & (mt[ref_af_field] > min_af); & (mt[ref_af_field] < max_af); ). # Filter to variant calls with GQ above min_gq and DP within the range (min_dp, max_dp); ad_dp = mt['DP'] if 'DP' in mt.entry else hl.sum(mt[ad_field]); mt = mt.filter_entries(mt[gt_field].is_hom_var() & (mt.GQ >= min_gq) & (ad_dp >= min_dp) & (ad_dp <= max_dp)). # Compute CHARR; mt = mt.select_cols(charr=hl.agg.mean((mt[ad_field][0] / (mt[ad_field][0] + mt[ad_field][1])) / mt[ref_af_field])). mt = mt.select_globals(; af_min=min_af,; af_max=max_af,; dp_min=min_dp,; dp_max=max_dp,; gq_min=min_gq,; ). return mt.cols(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:63354,update,updated,63354,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['update'],['updated']
Deployability," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8142,pipeline,pipeline,8142,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipeline']
Deployability," [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud paramete",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:10139,configurat,configuration,10139,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['configurat'],['configuration']
Deployability," allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77668,integrat,integration,77668,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Deployability," already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:26945,pipeline,pipelines,26945,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool, optional) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to down",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:8027,continuous,continuous,8027,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability," between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30551,update,update,30551,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability," bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:36643,pipeline,pipelines,36643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_continuous()[source]; The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_discrete()[source]; The default discrete fill scale. This maps each discrete value to a fill color. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_hue()[source]; Map discrete fill colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_manual(*, values)[source]; A color scale that assigns strings to fill colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_fill_identity()[source]; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. Returns:; FigureAttribute – The scale to be applied. Fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:13684,continuous,continuous,13684,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability," creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2548,install,installed,2548,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['install'],['installed']
Deployability," dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2333,install,installed,2333,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['installed']
Deployability," diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail query but can be used standalone for Hail Batch users by; import hailtop.fs as hfs. Deprecations. (#12929) Hail no; longer officially supports Python 3.7.; (#12917) The; hail-az scheme for referencing blobs in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in fa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:34496,release,release,34496,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability," each row of entries is regarded as a vector with elements; defined by entry_expr and missing values mean-imputed per row.; The (i, j) element of the resulting block matrix is the correlation; between rows i and j (as 0-indexed by order in the matrix table;; see add_row_index()).; The correlation of two vectors is defined as the; Pearson correlation coeffecient; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors.; This method has two stages:. writing the row-normalized block matrix to a temporary file on persistent; disk with BlockMatrix.from_entry_expr(). The parallelism is; n_rows / block_size.; reading and multiplying this block matrix by its transpose. The; parallelism is (n_rows / block_size)^2 if all blocks are computed. Warning; See all warnings on BlockMatrix.from_entry_expr(). In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; BlockMatrix.write_from_entry_expr().; The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; ld_matrix().; To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters:. entry_expr (Float64Expression) – Entry-indexed numeric expression on matrix table.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Returns:; BlockMatrix – Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:23013,update,updated,23013,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['update'],['updated']
Deployability," evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is eith",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70416,integrat,integration,70416,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Deployability," exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9989,configurat,configuration,9989,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['configurat'],['configuration']
Deployability," experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Expressions. View page source. Expressions. eval(expression); Evaluate a Hail expression, returning the result. Expression; Base class for Hail expressions. ArrayExpression; Expression of type tarray. ArrayNumericExpression; Expression of type tarray with a numeric type. BooleanExpression; Expression of type tbool. CallExpression; Expression of type tcall. CollectionExpression; Expression of type tarray or tset. DictExpression; Expression of type tdict. IntervalExpression; Expression of type tinterval. LocusExpression; Expression of type tlocus. NumericExpression; Expression of numeric type. Int32Expression; Expression of type tint32. Int64Expression; Expression of type tint64. Float32Expression; Expression of type tfloat32. Float64Expression; Expression of type tfloat64. SetExpression; Expression of type tset. StringExpression; Expression of type tstr. StructExpression; Expression of type tstruct. TupleExpression; Expression of type ttuple. NDArrayExpression; Expression of type tndarray. NDArrayNumericExpression; Expression of type tndarray with a numeric element type. hail.expr.eval(expression)[source]; Evaluate a Hail expression, returning the result.; This method is extremely useful for learning about Hail expressions and; understanding how to compose them.; The expression must have no indices, but can refer to the globals; of a Table or MatrixTable.; Examples; Evaluate a conditional:; >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters:; expression (Expression) – Any expression, or a Python value that can be implicitly interpreted as an expression. Returns:; Any. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/expressions.html:2327,update,updated,2327,docs/0.2/expressions.html,https://hail.is,https://hail.is/docs/0.2/expressions.html,1,['update'],['updated']
Deployability," filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.; rna (bool) – If True, pair adenine (A) with uracil (U) instead of thymine (T). Returns:; StringExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:24707,update,updated,24707,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['update'],['updated']
Deployability," filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155753,update,updated,155753,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['updated']
Deployability," gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11700,install,installed,11700,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Stomach_all_snp_gene_associations. View page source. GTEx_eQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html:9679,update,updated,9679,docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Stomach_all_snp_gene_associations.html,1,['update'],['updated']
Deployability," gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_eQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html:9679,update,updated,9679,docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Thyroid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability," hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20474,install,installed,20474,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['install'],['installed']
Deployability," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7864,install,installed,7864,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['install'],"['install', 'installed']"
Deployability," is None:; legend = """". y_axis_label = 'Frequency'; if log:; y_axis_type = 'log'; else:; y_axis_type = 'linear'; fig = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ). y = np.array(data['ranks'][1:-1]) / data['ranks'][-1]; x = np.array(data['values'][1:-1]); min_x = data['values'][0]; max_x = data['values'][-1]; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5419,update,update,5419,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability," is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6802,configurat,configuration,6802,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability," lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:19174,integrat,integrate,19174,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integrate']
Deployability," log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5425,configurat,configuration,5425,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:119522,update,updated,119522,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['update'],['updated']
Deployability," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86170,pipeline,pipelines,86170,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following code splits a dataset mt which contains a mixture of split and; non-split variants.; >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; split_multi_hts(), which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:; >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:85377,update,updates,85377,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updates']
Deployability," n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Entry fields:; 'DP': int32; 'GQ': int32; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'GT': call; 'PGT': call; 'AD': array<int32>; 'PL': array<int32>; 'adj': bool; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html:34895,update,updated,34895,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_dense.html,1,['update'],['updated']
Deployability," n_non_ref: int64,; n_snp: int64,; n_transition: int64,; n_transversion: int64,; r_het_hom_var: float64,; r_insertion_deletion: float64,; r_ti_tv: float64; }; 'gnomad_sex_imputation': struct {; chr20_mean_dp: float32,; chrX_mean_dp: float32,; chrY_mean_dp: float32,; chrX_ploidy: float32,; chrY_ploidy: float32,; X_karyotype: str,; Y_karyotype: str,; sex_karyotype: str,; f_stat: float64,; n_called: int64,; expected_homs: float64,; observed_homs: int64; }; 'gnomad_population_inference': struct {; pca_scores: array<float64>,; pop: str,; prob_afr: float64,; prob_ami: float64,; prob_amr: float64,; prob_asj: float64,; prob_eas: float64,; prob_fin: float64,; prob_mid: float64,; prob_nfe: float64,; prob_oth: float64,; prob_sas: float64; }; 'gnomad_sample_qc_residuals': struct {; n_snp_residual: float64,; r_ti_tv_residual: float64,; r_insertion_deletion_residual: float64,; n_insertion_residual: float64,; n_deletion_residual: float64,; r_het_hom_var_residual: float64,; n_transition_residual: float64,; n_transversion_residual: float64; }; 'gnomad_sample_filters': struct {; hard_filters: set<str>,; hard_filtered: bool,; release_related: bool,; qc_metrics_filters: set<str>; }; 'gnomad_high_quality': bool; 'gnomad_release': bool; 'relatedness_inference': struct {; related_samples: set<struct {; s: str,; kin: float64,; ibd0: float64,; ibd1: float64,; ibd2: float64; }>,; related: bool; }; 'hgdp_tgp_meta': struct {; project: str,; study_region: str,; population: str,; genetic_region: str,; latitude: float64,; longitude: float64,; hgdp_technical_meta: struct {; source: str,; library_type: str; },; global_pca_scores: array<float64>,; subcontinental_pca: struct {; pca_scores: array<float64>,; pca_scores_outliers_removed: array<float64>,; outlier: bool; },; gnomad_labeled_subpop: str; }; 'high_quality': bool; ----------------------------------------; Key: ['s']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html:21063,update,updated,21063,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sample_metadata.html,1,['update'],['updated']
Deployability," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25555,pipeline,pipelines,25555,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['pipeline'],['pipelines']
Deployability," now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also added; hl.utils.genomic_range_table to quickly create a table keyed by; locus. Bug Fixes. (#12711) In Query on; Batch, fix null pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:38816,pipeline,pipeline,38816,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability," object is not mutable""); self.__dict__[key] = value. def __getattr__(self, item):; if item in self.__dict__:; return self.__dict__[item]. raise AttributeError(get_nice_attr_error(self, item)). def _copy_fields_from(self, other: 'ExprContainer'):; self._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same group",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5092,pipeline,pipeline,5092,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability," of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current working directory,; * `-t <name>` specifies the image name, and; * `-f <dockerfile>` specifies the Dockerfile file.; * A more complete description may be found `here: <https://docs.docker.com/engine/reference/com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:2066,update,update,2066,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,2,"['install', 'update']","['install', 'update']"
Deployability," panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_female. View page source. UK_Biobank_Rapid_GWAS_female. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html:10497,update,updated,10497,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_female.html,2,['update'],['updated']
Deployability," partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22166,pipeline,pipelines,22166,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability," pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94538,patch,patch,94538,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['patch', 'update']","['patch', 'update']"
Deployability," path=str, overwrite=bool); def write_expression(expr, path, overwrite=False):; """"""Write an Expression. In the same vein as Python's pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False). Example; -------; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters; ----------. expr : :class:`~.Expression`; Expression to write.; path : :class:`str`; Path to which to write expression.; Suggested extension: .he (hail expression).; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination. Returns; -------; None; """"""; source = expr._indices.source; if source is not None:; analyze('write_expression.expr', expr, source._global_indices); source = source.select_globals(__expr=expr); expr = source.index_globals().__expr; hl.utils.range_table(1).filter(False).key_by().drop('idx').annotate_globals(expr=expr).write(; path, overwrite=overwrite; ). [docs]@typecheck(path=str, _assert_type=nullable(hail_type)); def read_expression(path, _assert_type=None):; """"""Read an :class:`~.Expression` written with :func:`.experimental.write_expression`. Example; -------; >>> hl.experimental.write_expression(hl.array([1, 2]), 'output/test_expression.he'); >>> expression = hl.experimental.read_expression('output/test_expression.he'); >>> hl.eval(expression). Parameters; ----------. path : :class:`str`; File to read. Returns; -------; :class:`~.Expression`; """"""; _assert_table_type = None; _load_refs = True; if _assert_type:; _assert_table_type = ttable(hl.tstruct(expr=_assert_type), row_type=hl.tstruct(), row_key=[]); _load_refs = False; return hl.read_table(path, _assert_type=_assert_table_type, _load_refs=_load_refs).index_globals().expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html:2719,update,updated,2719,docs/0.2/_modules/hail/experimental/expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html,2,['update'],['updated']
Deployability," requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_ag",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:73667,pipeline,pipeline,73667,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability," running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5383,install,install,5383,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability," should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You shou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1824,install,install-on-cluster,1824,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install-on-cluster']
Deployability," so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl.is_defined(mt.PL),; hl.bind(; lambda unnorm: unnorm - hl.min(unnorm),; hl.range(0, hl.triangle(mt.alleles.length())).map(; lambda newi: hl.bind(; lambda newc: mt.P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:162155,update,update,162155,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['update']
Deployability," source. CaseBuilder. class hail.expr.builders.CaseBuilder[source]; Class for chaining multiple if-else statements.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(x.length() == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; case(), cond(), switch(). Attributes. Methods. default; Finish the case statement by adding a default case. or_error; Finish the case statement by throwing an error with the given message. or_missing; Finish the case statement by returning missing. when; Add a branch. default(then)[source]; Finish the case statement by adding a default case.; Notes; If no condition from a when() call is True,; then then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the case statement by throwing an error with the given message.; Notes; If no condition from a CaseBuilder.when() call is True, then; an error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the case statement by returning missing.; Notes; If no condition from a CaseBuilder.when() call is True, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(condition, then)[source]; Add a branch. If condition is True, then returns then. Warning; Missingness is treated similarly to cond(). Missingness is; not treated as False. A condition that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a CaseBuilder. Parameters:. condition (BooleanExpression); then (Expression). Returns:; CaseBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html:2673,update,updated,2673,docs/0.2/functions/hail.expr.builders.CaseBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.CaseBuilder.html,1,['update'],['updated']
Deployability," table, use flatten() first. Warning; Do not export to a path that is being read from in the same pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23944,pipeline,pipeline,23944,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability," the call contains two different alternate alleles. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return a0 > 0 and a1 > 0 and a0 != a1. [docs] def is_het_ref(self):; """"""True if the call contains one reference and one alternate allele. :rtype: bool; """"""; n = self.ploidy; if n < 2:; return False. assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; return (a0 == 0 and a1 > 0) or (a0 > 0 and a1 == 0). [docs] def n_alt_alleles(self):; """"""Returns the count of non-reference alleles. :rtype: int; """"""; n = 0; for a in self._alleles:; if a > 0:; n += 1; return n. [docs] @typecheck_method(n_alleles=int); def one_hot_alleles(self, n_alleles):; """"""Returns a list containing the one-hot encoded representation of the; called alleles. Examples; --------. >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; -----; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters; ----------; n_alleles : :obj:`int`; Number of total alleles, including the reference. Returns; -------; :obj:`list` of :obj:`int`; """"""; r = [0] * n_alleles; for a in self._alleles:; r[a] += 1; return r. [docs] def unphased_diploid_gt_index(self):; """"""Return the genotype index for unphased, diploid calls. Returns; -------; :obj:`int`; """"""; from hail.utils import FatalError. if self.ploidy != 2 or self.phased:; raise FatalError(; ""'unphased_diploid_gt_index' is only valid for unphased, diploid calls. Found {}."".format(repr(self)); ); a0 = self._alleles[0]; a1 = self._alleles[1]; assert a0 <= a1; return a1 * (a1 + 1) / 2 + a0. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:5872,update,updated,5872,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,2,['update'],['updated']
Deployability," the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output().; for window in local_df_y.index.to_list():; checkpoint = checkpoint_path(window); if hfs.exists(checkpoint):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8217,pipeline,pipeline,8217,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['pipeline'],['pipeline']
Deployability," the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92085,release,releases,92085,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['releases']
Deployability," to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. sum(axis=None)[source]; Sum out one or more axes of an ndarray. Parameters:; axis (int tuple) – The axis or axes to sum out. Returns:; NDArrayNumericExpression or NumericExpression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None); Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:11277,update,updated,11277,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['update'],['updated']
Deployability," to modify the result of the inner recursive call to triangle1(8) by; adding 9 to the result.; The second function is tail recursive: the result of triangle2(9, 0) is; the same as the result of the inner recursive call, triangle2(8, 9).; Example; To find the sum of all the numbers from n=1…10:; >>> triangle_f = lambda f, x, total: hl.if_else(x == 0, total, f(x - 1, total + x)); >>> x = hl.experimental.loop(triangle_f, hl.tint32, 10, 0); >>> hl.eval(x); 55; Let’s say we want to find the root of a polynomial equation:; >>> def polynomial(x):; … return 5 * x**3 - 2 * x - 1; We’ll use Newton’s method<https://en.wikipedia.org/wiki/Newton%27s_method>; to find it, so we’ll also define the derivative:; >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at \(x_0 = 0\), we’ll compute the next step \(x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}\); until the difference between \(x_{i}\) and \(x_{i+1}\) falls below; our convergence threshold:; >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters:. f (function ( (marker, *args) -> Expression) – Function of one callable marker, denoting where the recursive call (or calls) is located,; and many args, the loop variables.; typ (str or HailType) – Type the loop returns.; args (variable-length args of Expression) – Expressions to initialize the loop values. Returns:; Expression – Result of the loop with args as initial loop values. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:43508,update,updated,43508,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['update'],['updated']
Deployability," transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetada",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31233,update,update,31233,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability," two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK as described in the Getting; Started section and we have created a user account for you and given you a; billing project.; To authenticate your computer with the Batch service, run the following; command in a terminal window:; gcloud auth application-default login; hailctl auth login. Executing this command will take you to a login page in your browser window where; you can select your google account to authenticate with. If everything works successfully,; you should see a message “hailctl is now authenticated.” in your browser window and no; error messages in the terminal window. Submitting a Batch to the Service. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:7893,install,installed,7893,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['install'],['installed']
Deployability," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:19313,update,updated,19313,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['update'],['updated']
Deployability," type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:; GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use MatrixTable.select_entries() to rearrange these fields if; necessary.; The following new fields are generated:. old_locus (locus) – The old locus, before filtering and computing; the minimal representation.; old_alleles (array<str>) – The old alleles, before filtering an",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23919,update,update,23919,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability," wheel 712 Jan 25 17:19 index.tsv; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-00.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-01.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-02.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-03.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-04.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-05.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-06.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-07.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-08.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-09.tsv.bgz. $ zcat output/cols_files/part-00.tsv.bgz; #{""col_idx"":0}; row_idx x; 0 6.2501e-02; 1 7.0083e-01; 2 3.6452e-01; 3 4.4170e-01; 4 7.9177e-02; 5 6.2392e-01; 6 5.9920e-01; 7 9.7540e-01; 8 8.4848e-01; 9 3.7423e-01. Due to overhead and file system limits related to having large numbers; of open files, this function will iteratively export groups of columns.; The `batch_size` parameter can control the size of these groups. Parameters; ----------; mt : :class:`.MatrixTable`; path : :obj:`int`; Path (directory to write to.; batch_size : :obj:`int`; Number of columns to write per iteration.; bgzip : :obj:`bool`; BGZip output files.; header_json_in_file : :obj:`bool`; Include JSON header in each component file (if False, only written to index.tsv); """"""; if use_string_key_as_file_name and not (len(mt.col_key) == 1 and mt.col_key[0].dtype == hl.tstr):; raise ValueError(; f'parameter ""use_string_key_as_file_name"" requires a single string column key, found {list(mt.col_key.dtype.values())}'; ); hl.utils.java.Env.backend().execute(; hl.ir.MatrixToValueApply(; mt._mir,; {; 'name': 'MatrixExportEntriesByCol',; 'parallelism': batch_size,; 'path': path,; 'bgzip': bgzip,; 'headerJsonInFile': header_json_in_file,; 'useStringKeyAsFileName': use_string_key_as_file_name,; },; ); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html:3668,update,updated,3668,docs/0.2/_modules/hail/experimental/export_entries_by_col.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html,2,['update'],['updated']
Deployability," while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matrix Tables Locally; Some useful methods to interact with matrix tables locally are; MatrixTable.describe(), MatrixTable.head(), and; MatrixTable.sample_rows(). describe prints out the schema for all row; fields, column fields, entry fields, and global fields as well as the row keys; and column keys. head returns a new matrix table with only the first N rows.; sample_rows returns a new matrix table where the rows are randomly sampled with; frequency p.; To get the dimensions of the matrix table, use MatrixTable.count_rows(); and MatrixTable.count_cols(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:14109,update,updated,14109,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['updated']
Deployability," x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; intervals_type = hl.tarray(hl.tinterval(hl.tlocus(rg))); intervals = intervals_type._convert_from_json(obj['gvcf_import_intervals']); obj['gvcf_import_intervals'] = intervals. return VariantDatasetCombiner(**obj); return obj. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:34258,update,updated,34258,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['updated']
Deployability," x_range[0]) / x_bins; y_spacing = (y_range[1] - y_range[0]) / y_bins. def frange(start, stop, step):; from itertools import count, takewhile. return takewhile(lambda x: x <= stop, count(start, step)). x_levels = hail.literal(list(frange(x_range[0], x_range[1], x_spacing))[::-1]); y_levels = hail.literal(list(frange(y_range[0], y_range[1], y_spacing))[::-1]); grouped_ht = source.group_by(; x=hail.str(x_levels.find(lambda w: x >= w)), y=hail.str(y_levels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinsta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:22840,update,update,22840,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability," your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16277,pipeline,pipeline,16277,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability,"""""""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42795,pipeline,pipeline,42795,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability,"#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:32579,pipeline,pipelines,32579,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"& (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23472,update,update,23472,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144427,pipeline,pipelines,144427,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['pipeline'],['pipelines']
Deployability,"'GRCh37',; global_seed=None,; skip_logging_configuration=False,; jvm_heap_size=None,; _optimizer_iterations=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; ):; from hail.backend.local_backend import LocalBackend; from hail.backend.py4j_backend import connect_logger. log = _get_log(log); tmpdir = _get_tmpdir(tmpdir); optimizer_iterations = get_env_or_default(_optimizer_iterations, 'HAIL_OPTIMIZER_ITERATIONS', 3). jvm_heap_size = get_env_or_default(jvm_heap_size, 'HAIL_LOCAL_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {ve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20273,install,installed,20273,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['install'],['installed']
Deployability,"((1 - K) * P); cas = mt.filter_cols(mt.y_w_asc_bias == 1); con = mt.filter_cols(mt.y_w_asc_bias == 0).add_col_index(name='col_idx_' + uid); keep = round(p * n * (1 - K)) * [1] + round((1 - p) * n * (1 - K)) * [0]; con = con.annotate_cols(**{'keep_' + uid: hl.literal(keep)[hl.int32(con['col_idx_' + uid])]}); con = con.filter_cols(con['keep_' + uid] == 1); con = _clean_fields(con, uid); mt = con.union_cols(cas); return mt. [docs]@typecheck(mt=MatrixTable, y=oneof(expr_int32, expr_float64), K=oneof(int, float), exact=bool); def binarize(mt, y, K, exact=False):; r""""""Binarize phenotype `y` such that it has prevalence `K` = cases/(cases+controls); Uses inverse CDF of Gaussian to set binarization threshold when `exact` = False,; otherwise uses ranking to determine threshold. Parameters; ----------; mt : :class:`.MatrixTable`; :class:`.MatrixTable` containing phenotype to be binarized.; y : :class:`.Expression`; Column field of phenotype.; K : :obj:`int` or :obj:`float`; Desired ""population prevalence"" of phenotype.; exact : :obj:`bool`; Whether to get prevalence as close as possible to `K` (does not use inverse CDF). Returns; -------; :class:`.MatrixTable`; :class:`.MatrixTable` containing binary phenotype with prevalence of approx. `K`; """"""; if exact:; key = list(mt.col_key); uid = Env.get_uid(base=100); mt = mt.annotate_cols(**{'y_' + uid: y}); tb = mt.cols().order_by('y_' + uid); tb = tb.add_index('idx_' + uid); n = tb.count(); # ""+ 1"" because of zero indexing; tb = tb.annotate(y_binarized=tb['idx_' + uid] + 1 <= round(n * K)); tb, mt = tb.key_by('y_' + uid), mt.key_cols_by('y_' + uid); mt = mt.annotate_cols(y_binarized=tb[mt['y_' + uid]].y_binarized); mt = mt.key_cols_by(*map(lambda x: mt[x], key)); else: # use inverse CDF; y_stats = mt.aggregate_cols(hl.agg.stats(y)); threshold = stats.norm.ppf(1 - K, loc=y_stats.mean, scale=y_stats.stdev); mt = mt.annotate_cols(y_binarized=y > threshold); return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:35369,update,updated,35369,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,2,['update'],['updated']
Deployability,"(GQ) scores.; dp_bins : :class:`tuple` of :obj:`int`; Tuple containing cutoffs for depth (DP) scores.; dp_field : :obj:`str`; Name of depth field. If not supplied, DP or MIN_DP will be used, in that order. Returns; -------; :class:`.Table`; Hail Table of results, keyed by sample.; """""". require_first_key_field_locus(vds.reference_data, 'sample_qc'); require_first_key_field_locus(vds.variant_data, 'sample_qc'). if dp_field is not None:; ref_dp_field_to_use = dp_field; elif 'DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'DP'; elif 'MIN_DP' in vds.reference_data.entry:; ref_dp_field_to_use = 'MIN_DP'; else:; ref_dp_field_to_use = None. vmt = vds.variant_data; if 'GT' not in vmt.entry:; vmt = vmt.annotate_entries(GT=hl.vds.lgt_to_gt(vmt.LGT, vmt.LA)); allele_count, atypes = vmt_sample_qc_variant_annotations(global_gt=vmt.GT, alleles=vmt.alleles); variant_ac = Env.get_uid(); variant_atypes = Env.get_uid(); vmt = vmt.annotate_rows(**{variant_ac: allele_count, variant_atypes: atypes}); vmt_dp = vmt['DP'] if ref_dp_field_to_use is not None and 'DP' in vmt.entry else None; variant_results = vmt.select_cols(; **vmt_sample_qc(; global_gt=vmt.GT,; gq=vmt.GQ,; variant_ac=vmt[variant_ac],; variant_atypes=vmt[variant_atypes],; dp=vmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). rmt = vds.reference_data; rmt_dp = rmt[ref_dp_field_to_use] if ref_dp_field_to_use is not None else None; reference_results = rmt.select_cols(; **rmt_sample_qc(; locus=rmt.locus,; gq=rmt.GQ,; end=rmt.END,; dp=rmt_dp,; gq_bins=gq_bins,; dp_bins=dp_bins,; ); ).cols(). joined = reference_results[variant_results.key]; dp_bins_field = {}; if ref_dp_field_to_use is not None:; dp_bins_field['dp_bins'] = hl.tuple(dp_bins); joined_results = variant_results.transmute(**combine_sample_qc(joined, variant_results.row)); joined_results = joined_results.annotate_globals(gq_bins=hl.tuple(gq_bins), **dp_bins_field); return joined_results. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html:13298,update,updated,13298,docs/0.2/_modules/hail/vds/sample_qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html,2,['update'],['updated']
Deployability,"([(1 - x) / x for x in fst]).map(; lambda x: hl.rand_beta(ancestral * x, (1 - ancestral) * x); ),; entries=hl.repeat(hl.struct(), n_samples),; ),; af_dist,; ); ),; ). bn = bn._unlocalize_entries('entries', 'cols', ['sample_idx']). # entry info; p = hl.sum(bn.pop * bn.af) if mixture else bn.af[bn.pop]; q = 1 - p. if phased:; mom = hl.rand_bool(p); dad = hl.rand_bool(p); return bn.select_entries(GT=hl.call(mom, dad, phased=True)). idx = hl.rand_cat([q**2, 2 * p * q, p**2]); return bn.select_entries(GT=hl.unphased_diploid_gt_index_call(idx)). [docs]@typecheck(mt=MatrixTable, f=anytype); def filter_alleles(mt: MatrixTable, f: Callable) -> MatrixTable:; """"""Filter alternate alleles. .. include:: ../_templates/req_tvariant.rst. Examples; --------; Keep SNPs:. >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:. >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[allele_index - 1] > 0). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whet",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:153785,Update,Update,153785,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Update'],['Update']
Deployability,"(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31089,update,update,31089,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"(hl.missing(rd.entry.dtype), False), lambda acc: keep_last(acc, (elt, False)), keep_last; ),; ht.entries,; ),; ht.entries,; ).map(lambda tup: keep_last(tup[0], (tup[1], False))); ); ht_join = ht. ht = ht.key_by(); ht = ht.select(; to_shuffle=hl.enumerate(ht.prev_block).filter(; lambda idx_and_elt: hl.is_defined(idx_and_elt[1]) & idx_and_elt[1][1]; ); ); ht = ht.explode('to_shuffle'); rg = rd.locus.dtype.reference_genome; ht = ht.transmute(col_idx=ht.to_shuffle[0], entry=ht.to_shuffle[1][0]); ht_shuf = ht.key_by(; locus=hl.locus(hl.literal(rg.contigs)[ht.entry.contig_idx], ht.entry.start_pos, reference_genome=rg); ). ht_shuf = ht_shuf.collect_by_key(""new_starts""); # new_starts can contain multiple records for a collapsed ref block, one for each folded block.; # We want to keep the one with the highest END; ht_shuf = ht_shuf.select(; moved_blocks_dict=hl.group_by(lambda elt: elt.col_idx, ht_shuf.new_starts).map_values(; lambda arr: arr[hl.argmax(arr.map(lambda x: x.entry.END))].entry.drop('contig_idx', 'start_pos'); ); ). ht_joined = ht_join.join(ht_shuf.select_globals(), 'left'). def merge_f(tup):; (idx, original_entry) = tup. return (; hl.case(); .when(; ~(hl.coalesce(ht_joined.prev_block[idx][1], False)),; hl.coalesce(ht_joined.moved_blocks_dict.get(idx), original_entry.drop('contig_idx', 'start_pos')),; ); .or_missing(); ). ht_joined = ht_joined.annotate(new_entries=hl.enumerate(ht_joined.entries).map(lambda tup: merge_f(tup))); ht_joined = ht_joined.drop('moved_blocks_dict', 'entries', 'prev_block', 'contig_idx_row', 'start_pos_row'); new_rd = ht_joined._unlocalize_entries(; entries_field_name='new_entries', cols_field_name='cols', col_key=list(rd.col_key); ). rbml = hl.vds.VariantDataset.ref_block_max_length_field; if rbml in new_rd.globals:; new_rd = new_rd.drop(rbml). if isinstance(ds, VariantDataset):; return VariantDataset(reference_data=new_rd, variant_data=ds.variant_data); return new_rd. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:41193,update,updated,41193,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['update'],['updated']
Deployability,"(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_recursive(loop_ir.body); elif isinstance(loop_ir, ir.TailLoop):; if any(contains_recursive_call(x) for n, x in loop_ir.params):; raise TypeError(""parameters passed to inner loop can't contain recursive call!""); elif not isinstance(loop_ir, ir.Recur) and contains_recursive_call(loop_ir):; raise TypeError(""found recursive expression outside of tail position!""). @typecheck(recur_exprs=expr_any); def make_loop(*recur_exprs):; if len(recur_exprs) != len(args):; raise TypeError('Recursive call in loop has wrong number of arguments'); err = None; for i, (rexpr, expr) in enumerate(zip(recur_exprs, args)):; if rexpr.dtype != expr.dtype:; if err is None:; err = 'Type error in recursive call,'; err += f'\n at argument index {i}, loop arg type: {expr.dtype}, '; err += f'recur arg type: {rexpr.dtype}'; if err is not None:; raise TypeError(err); irs = [expr._ir for expr in recur_exprs]; indices, aggregations = unify_all(*recur_exprs); return construct_expr(ir.Recur(loop_name, irs, typ), typ, indices, aggregations). uid_irs = []; loop_vars = []. for expr in args:; uid = Env.get_uid(); loop_vars.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uid_irs.append((uid, expr._ir)). loop_f = to_expr(f(make_loop, *loop_vars)); if loop_f.dtype != typ:; raise TypeError(f""requested type {typ} does not match inferred type {loop_f.dtype}""); check_tail_recursive(loop_f._ir); indices, aggregations = unify_all(*args, loop_f). return construct_expr(ir.TailLoop(loop_name, loop_f._ir, uid_irs), loop_f.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:6348,update,updated,6348,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['update'],['updated']
Deployability,"(time=expr_str, format=expr_str, zone_id=expr_str); def strptime(time, format, zone_id):; """"""; Interpret a formatted datetime string as a Unix timestamp (number of seconds since 1970-01-01T00:00Z (ISO)). Examples; --------. >>> hl.eval(hl.experimental.strptime(""07/08/19 3:00:01 AM"", ""%D %l:%M:%S %p"", ""America/New_York"")); 1562569201. >>> hl.eval(hl.experimental.strptime(""Saturday, October 11, 1997. 05:45:23 AM"", ""%A, %B %e, %Y. %r"", ""GMT+2"")); 876541523. Notes; -----; The following formatting characters are supported in format strings: A a B b D d e F H I j k l M m n p R r S s T t U u V v W Y y z; See documentation here: https://linux.die.net/man/3/strftime. A zone id can take one of three forms. It can be an explicit offset, like ""+01:00"", a relative offset, like ""GMT+2"",; or a IANA timezone database (TZDB) identifier, like ""America/New_York"". Wikipedia maintains a list of TZDB identifiers here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Currently, the parser implicitly uses the ""en_US"" locale. This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string `""07/08/09""` and the format string `""%Y.%m.%d""`, this method will fail, since that's not specific; enough to determine seconds from. You can fix this by adding ""00:00:00"" to your date string and ""%H:%M:%S"" to your format string. Parameters; ----------; time : str or :class:`.Expression` of type :py:data:`.tstr`; The string from which to parse the time.; format : str or :class:`.Expression` of type :py:data:`.tstr`; The format string describing how to parse the time.; zone_id: str or :class:`.Expression` of type :py:data:`.tstr`; An id representing the timezone. See notes above. Returns; -------; :class:`~.Int64Expression`; The Unix timestamp associated with the given time string.; """"""; return _func(""strptime"", hl.tint64, time, format, zone_id). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/time.html:4310,update,updated,4310,docs/0.2/_modules/hail/experimental/time.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/time.html,2,['update'],['updated']
Deployability,"))),; regions=nullable(sequenceof(str)),; gcs_bucket_allow_list=nullable(dictof(str, sequenceof(str))),; copy_spark_log_on_error=nullable(bool),; ); def init(; sc=None,; app_name=None,; master=None,; local='local[*]',; log=None,; quiet=False,; append=False,; min_block_size=0,; branching_factor=50,; tmp_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6629,configurat,configuration,6629,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"); for version in self.versions; if version.maybe_index(key_expr, all_matches) is not None; ]; if len(compatible_indexed_values) == 0:; versions = [f'{(v.version, v.reference_genome)}' for v in self.versions]; raise ValueError(; f'Could not find compatible version of {self.name} for user'; f' dataset with key {key_expr.dtype}.\n'; f'This annotation dataset is available for the following'; f' versions and reference genome builds: {"", "".join(versions)}.'; ); else:; indexed_values = sorted(compatible_indexed_values, key=lambda x: x[1])[-1]. if len(compatible_indexed_values) > 1:; info(; f'index_compatible_version: More than one compatible version'; f' exists for annotation dataset: {self.name}. Rows have been'; f' annotated with version {indexed_values[1]}.'; ); return indexed_values[0]. [docs]class DB:; """"""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:9324,configurat,configuration,9324,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['configurat'],['configuration']
Deployability,"+ '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45994,Configurat,Configuration,45994,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Configurat'],['Configuration']
Deployability,", P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5229,install,installed,5229,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['install'],['installed']
Deployability,", ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:4419,configurat,configuration,4419,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,", call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakine",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31601,pipeline,pipelines,31601,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,", color=None)[source]; Creates a line plot with the area between the line and the x-axis filled in.; Supported aesthetics: x, y, fill, color, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8491,continuous,continuous,8491,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,",; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will chang",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37856,Configurat,Configuration,37856,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Configurat'],['Configuration']
Deployability,"-+-------+----------+----------+-------+. The same test, but using the original paper’s suggested weights which are derived from the; allele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper inclu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:73985,integrat,integration,73985,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"---+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93214,integrat,integration,93214,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Deployability,"-----+-------+----------+----------+-------+. The same test, but using the original paper's suggested weights which are derived from the; allele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._linear_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 2.39e+01 | 4.32e-01 | 0 |; | 1 | 9 | 1.69e+01 | 7.82e-02 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The pape",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:77492,integrat,integration,77492,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Deployability,"------------------------; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>; ----------------------------------------. Common Operations; Like tables, Hail provides a number of methods for manipulating data in a; matrix table.; Filter; MatrixTable has three methods to filter based on expressions:. MatrixTable.filter_rows(); MatrixTable.filter_cols(); MatrixTable.filter_entries(). Filter methods take a BooleanExpression argument. These expressions; are generated by applying computations to the fields of the matrix table:; >>> filt_mt = mt.filter_rows(hl.len(mt.alleles) == 2). >>> filt_mt = mt.filter_cols(hl.agg.mean(mt.GQ) < 20). >>> filt_mt = mt.filter_entries(mt.DP < 5). These expressions can compute arbitrarily over the data: the MatrixTable.filter_cols(); example above aggregates entries per column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:6134,update,update,6134,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['update']
Deployability,"--; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""color""). [docs]def scale_color_identity():; """"""A color scale that assumes the expression specified in the ``color`` aesthetic can be used as a color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""color""). [docs]def scale_color_manual(*, values):; """"""A color scale that assigns strings to colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""color"", values=values). [docs]def scale_fill_discrete():; """"""The default discrete fill scale. This maps each discrete value to a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:13148,continuous,continuous,13148,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['continuous'],['continuous']
Deployability,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46867,configurat,configuration,46867,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound**3:; return _svd(self.to_numpy(), full_matrices=False, compute_uv=compute_uv, overwrite_a=True); else:; return self._svd_gr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76686,configurat,configuration-dependent,76686,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['configurat'],['configuration-dependent']
Deployability,". Backend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; Backend. LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Backend. View page source. Backend. class hailtop.batch.backend.Backend(requester_pays_fses); Bases: ABC, Generic[RunningBatchType]; Abstract class for backends.; Methods. _async_run; Execute a batch. _run; See _async_run(). async_close. close; Close a Hail Batch Backend. requester_pays_fs. rtype:; RouterAsyncFS. validate_file. rtype:; None. abstract async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch.; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). _run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); See _async_run().; :rtype: Optional[TypeVar(RunningBatchType)]. Warning; This method should not be called directly. Instead, use batch.Batch.run(). async async_close(). close(); Close a Hail Batch Backend.; Notes; This method should be called after executing your batches at the; end of your script. requester_pays_fs(requester_pays_config). Return type:; RouterAsyncFS. async validate_file(uri, requester_pays_config=None). Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html:265,Configurat,Configuration,265,docs/batch/api/backend/hailtop.batch.backend.Backend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.Backend.html,1,['Configurat'],['Configuration']
Deployability,". BashJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; BashJob. PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BashJob. View page source. BashJob. class hailtop.batch.job.BashJob(batch, token, *, name=None, attributes=None, shell=None); Bases: Job; Object representing a single bash job to execute.; Examples; Create a batch object:; >>> b = Batch(). Create a new bash job that prints hello to a temporary file t.ofile:; >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file t.ofile to a permanent location; >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:; >>> b.run(). Notes; This class should never be created directly by the user. Use Batch.new_job(); or Batch.new_bash_job() instead.; Methods. command; Set the job's command to execute. declare_resource_group; Declare a resource group for a job. image; Set the job's docker image. command(command); Set the job’s command to execute.; Examples; Simple job with no output files:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file j.ofile that is written to a; permanent location:; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:; >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html:240,Configurat,Configuration,240,docs/batch/api/batch/hailtop.batch.job.BashJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.BashJob.html,1,['Configurat'],['Configuration']
Deployability,". Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; Locus. View page source. Locus. class hail.genetics.Locus[source]; An object that represents a location in the genome. Parameters:. contig (str) – Chromosome identifier.; position (int) – Chromosomal position (1-indexed).; reference_genome (str or ReferenceGenome) – Reference genome to use. Note; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. mt.locus.take(5). This is rare; it is much; more common to manipulate the LocusExpression object, which is; constructed using the following functions:. locus(); parse_locus(); locus_from_global_position(). Attributes. contig; Chromosome identifier. position; Chromosomal position (1-based). reference_genome; Reference genome. Methods. parse; Parses a locus object from a CHR:POS string. property contig; Chromosome identifier.; :rtype: str. classmethod parse(string, reference_genome='default')[source]; Parses a locus object from a CHR:POS string.; Examples; >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). Parameters:. string (str) – String to parse.; reference_genome (str or ReferenceGenome) – Reference genome to use. Default is default_reference(). Return type:; Locus. property position; Chromosomal position (1-based).; :rtype: int. property reference_genome; Reference genome. Returns:; ReferenceGenome. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html:2045,update,updated,2045,docs/0.2/genetics/hail.genetics.Locus.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html,1,['update'],['updated']
Deployability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Batch. Job; BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Batch. View page source. Batch. class hailtop.batch.batch.Batch(name=None, backend=None, attributes=None, requester_pays_project=None, default_image=None, default_memory=None, default_cpu=None, default_storage=None, default_regions=None, default_timeout=None, default_shell=None, default_python_image=None, default_spot=None, project=None, cancel_after_n_failures=None); Bases: object; Object representing the distributed acyclic graph (DAG) of jobs to run.; Examples; Create a batch object:; >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints “hello”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respective",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:236,Configurat,Configuration,236,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,1,['Configurat'],['Configuration']
Deployability,". Batch — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Batch. View page source. Batch; Batch is a Python module for creating and executing jobs. A job consists of a bash; command to run as well as a specification of the resources required and some metadata.; Batch allows you to easily build complicated computational pipelines with many jobs and numerous; dependencies. Batches can either be executed locally or with the Batch Service. Contents. Getting Started; Installation. Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; What is the Batch Service?; Sign Up; File Localization; Service Accounts; Billing; Setup; Submitting a Batch to the Service; Regions; Using the UI; Important Notes. Cookbooks; Clumping GWAS Results; Random Forest. Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Exact Match Expression; Partial Match Expression; Keyword Expression; Predefined Keyword Expression; Combining Multiple Statements. Python Version Compatibility Policy; Change Log. Indices and tables. Index; Search Page. Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/index.html:505,pipeline,pipelines,505,docs/batch/index.html,https://hail.is,https://hail.is/docs/batch/index.html,1,['pipeline'],['pipelines']
Deployability,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:264,Configurat,Configuration,264,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,1,['Configurat'],['Configuration']
Deployability,". BatchPoolFuture — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolFuture; BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:260,Configurat,Configuration,260,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,1,['Configurat'],['Configuration']
Deployability,". Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Getting Started; Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1085,deploy,deploying,1085,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['deploy'],['deploying']
Deployability,". Docker Resources — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; What is Docker?; Installation; Creating a Dockerfile; Building Images; Pushing Images. Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:889,install,install,889,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['install']
Deployability,". Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use the function hfs.open to read the data regardless; of where it’s located.; with hfs.open(df_y_path) as f:; local_df_y = pd.read",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4590,install,installed,4590,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['install'],['installed']
Deployability,". Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Requirements; Building Hail; Building the Docs and Website; Running the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1116,install,install,1116,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install']
Deployability,". Getting Started — Batch documentation. Batch; . Getting Started; Installation; Installing Batch on Mac OS X or GNU/Linux with pip; Installing the Google Cloud SDK; Try it out!. Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Getting Started. View page source. Getting Started. Installation; Batch is a Python module available inside the Hail Python package located at hailtop.batch. The; Batch Service additionally depends on the Google Cloud SDK. Installing Batch on Mac OS X or GNU/Linux with pip; Create a conda enviroment named; hail and install the Hail python library in that environment. If conda activate doesn’t work, please read these instructions; conda create -n hail python'>=3.9'; conda activate hail; pip install hail. Installing the Google Cloud SDK; If you plan to use the Batch Service (as opposed to the local-only mode), then you must additionally; install the Google Cloud SDK. Try it out!; To try batch out, open iPython or a Jupyter notebook and run:; >>> import hailtop.batch as hb; >>> b = hb.Batch(); >>> j = b.new_job(name='hello'); >>> j.command('echo ""hello world""'); >>> b.run(). You’re now all set to run the tutorial!. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/getting_started.html:683,install,install,683,docs/batch/getting_started.html,https://hail.is,https://hail.is/docs/batch/getting_started.html,3,['install'],['install']
Deployability,". HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.context. Source code for hail.context; from __future__ import print_function # Python 2 and 3 print compatibility. from hail.typecheck import *; from pyspark import SparkContext; from pyspark.sql import SQLContext. from hail.dataset import VariantDataset; from hail.expr import Type; from hail.java import *; from hail.keytable import KeyTable; from hail.stats import UniformDist, TruncatedBetaDist, BetaDist; from hail.utils import wrap_to_list. [docs]class HailContext(object):; """"""The main entry point for Hail functionality. .. warning::; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the :py:meth:`.HailContext.stop` method.; ; If passing in a Spark context, ensure that the configuration parameters ``spark.sql.files.openCostInBytes``; and ``spark.sql.files.maxPartitionBytes`` are set to as least 50GB. :param sc: Spark context, one will be created if None.; :type sc: :class:`.pyspark.SparkContext`. :param appName: Spark application identifier. :param master: Spark cluster master. :param local: Local resources to use. :param log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:1052,configurat,configuration,1052,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,". InputResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; InputResourceFile. JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; InputResourceFile. View page source. InputResourceFile. class hailtop.batch.resource.InputResourceFile; Bases: ResourceFile; Class representing a resource from an input file.; Examples; input is an InputResourceFile of the batch b; and is used in job j:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(name='hello'); >>> j.command(f'cat {input}'); >>> b.run(). Methods. source. rtype:; None. source(). Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html:317,Configurat,Configuration,317,docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.InputResourceFile.html,1,['Configurat'],['Configuration']
Deployability,". Job — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; Job. BashJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Job. View page source. Job. class hailtop.batch.job.Job(batch, token, *, name=None, attributes=None, shell=None); Bases: object; Object representing a single job to execute.; Notes; This class should never be created directly by the user. Use Batch.new_job(),; Batch.new_bash_job(), or Batch.new_python_job() instead.; Methods. always_copy_output; Set the job to always copy output to cloud storage, even if the job failed. always_run; Set the job to always run, even if dependencies fail. cloudfuse; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. cpu; Set the job's CPU requirements. depends_on; Explicitly set dependencies on other jobs. env. gcsfuse; Add a bucket to mount with gcsfuse. memory; Set the job's memory requirements. regions; Set the cloud regions a job can run in. spot; Set whether a job is run on spot instances. storage; Set the job's storage size. timeout; Set the maximum amount of time this job can run for in seconds. always_copy_output(always_copy_output=True); Set the job to always copy output to cloud storage, even if the job failed.; Notes; Can only be used with the backend.ServiceBackend.; Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:232,Configurat,Configuration,232,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,1,['Configurat'],['Configuration']
Deployability,". JobResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; JobResourceFile. ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; JobResourceFile. View page source. JobResourceFile. class hailtop.batch.resource.JobResourceFile(value, source); Bases: ResourceFile; Class representing an intermediate file from a job.; Examples; j.ofile is a JobResourceFile on the job`j`:; >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; All JobResourceFile are temporary files and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved.; Methods. add_extension; Specify the file extension to use. source. rtype:; Job. add_extension(extension); Specify the file extension to use.; Examples; >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; The default file name for a JobResourceFile is the name; of the identifier. Parameters:; extension (str) – File extension to use. Return type:; JobResourceFile. Returns:; JobResourceFile – Same resource file with the extension specified. source(). Return type:; Job. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html:313,Configurat,Configuration,313,docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.JobResourceFile.html,1,['Configurat'],['Configuration']
Deployability,". LocalBackend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; LocalBackend. ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; LocalBackend. View page source. LocalBackend. class hailtop.batch.backend.LocalBackend(tmp_dir='/tmp/', gsa_key_file=None, extra_docker_run_flags=None); Bases: Backend[None]; Backend that executes batches on a local computer.; .. rubric:: Examples; >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters:. tmp_dir (str) – Temporary directory to use.; gsa_key_file (Optional[str]) – Mount a file with a gsa key to /gsa-key/key.json. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable HAIL_BATCH_GSA_KEY_FILE.; extra_docker_run_flags (Optional[str]) – Additional flags to pass to docker run. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS. Methods. _async_run; Execute a batch. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(). Parameters:. batch (Batch) – Batch to execute.; dry_run (bool) – If True, don’t execute code.; verbose (bool) – If True, print debugging output.; delete_scratch_on_exit (bool) – If True, delete temporary directories with intermediate files. Return type:; None. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html:275,Configurat,Configuration,275,docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.LocalBackend.html,1,['Configurat'],['Configuration']
Deployability,". Python Version Compatibility Policy — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:504,release,released,504,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,2,['release'],['released']
Deployability,". PythonJob — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Batch; Job; BashJob; PythonJob; PythonJob. Resources; Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; PythonJob. View page source. PythonJob. class hailtop.batch.job.PythonJob(batch, token, *, name=None, attributes=None); Bases: Job; Object representing a single Python job to execute.; Examples; Create a new Python job that multiplies two numbers and then adds 5 to the result:; # Create a batch object with a default Python image. b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def multiply(x, y):; return x * y. def add(x, y):; return x + y. j = b.new_python_job(); result = j.call(multiply, 2, 3); result = j.call(add, result, 5). # Write out the str representation of result to a file. b.write_output(result.as_str(), 'hello.txt'). b.run(). Notes; This class should never be created directly by the user. Use Batch.new_python_job(); instead.; Methods. call; Execute a Python function. image; Set the job's docker image. call(unapplied, *args, **kwargs); Execute a Python function.; Examples; import json. def add(x, y):; return x + y. def multiply(x, y):; return x * y. def format_as_csv(x, y, add_result, mult_result):; return f'{x},{y},{add_result},{mult_result}'. def csv_to_json(path):; data = []; with open(path) as f:; for line in f:; line = line.rstrip(); fields = line.split(','); d = {'x': int(fields[0]),; 'y': int(fields[1]),; 'add': int(fields[2]),; 'mult': int(fields[3])}; data.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:244,Configurat,Configuration,244,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,1,['Configurat'],['Configuration']
Deployability,". PythonResult — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; PythonResult. View page source. PythonResult. class hailtop.batch.resource.PythonResult(value, source); Bases: Resource, str; Class representing a result from a Python job.; Examples; Add two numbers and then square the result:; def add(x, y):; return x + y. def square(x):; return x ** 2. b = Batch(); j = b.new_python_job(name='add'); result = j.call(add, 3, 2); result = j.call(square, result); b.write_output(result.as_str(), 'output/squared.txt'); b.run(). Notes; All PythonResult are temporary Python objects and must be written; to a permanent location using Batch.write_output() if the output needs; to be saved. In most cases, you’ll want to convert the PythonResult; to a JobResourceFile in a human-readable format.; Methods. as_json; Convert a Python result to a file with a JSON representation of the object. as_repr; Convert a Python result to a file with the repr representation of the object. as_str; Convert a Python result to a file with the str representation of the object. source; Get the job that created the Python result. as_json(); Convert a Python result to a file with a JSON representation of the object.; Examples; def add(x, y):; return {'result': x + y}. b = Batch(); j = b.new_python_job(name='add'); result = j.call(add, 3, 2); b.write_output(result.as_json(), 'output/add.json'); b.run(). Return type:; JobResourceFile. Returns:; JobResourceFile – A new resource file where the contents are a Python object; that has been converted to JSON. as_repr(); Convert a Python result to a file with the repr representation of the object.; Examples",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.PythonResult.html:307,Configurat,Configuration,307,docs/batch/api/resource/hailtop.batch.resource.PythonResult.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.PythonResult.html,1,['Configurat'],['Configuration']
Deployability,". Random Forest Model — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Random Forest; Introduction; Batch Code; Imports; Random Forest Function; Format Result Function; Build Python Image; Control Code. Add Checkpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Rand",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:355,Configurat,Configuration,355,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,1,['Configurat'],['Configuration']
Deployability,". Resource — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; Resource. ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; Resource. View page source. Resource. class hailtop.batch.resource.Resource; Bases: object; Abstract class for resources.; Methods. source. rtype:; Optional[Job]. abstract source(). Return type:; Optional[Job]. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.Resource.html:299,Configurat,Configuration,299,docs/batch/api/resource/hailtop.batch.resource.Resource.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.Resource.html,1,['Configurat'],['Configuration']
Deployability,". ResourceFile — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; ResourceFile. InputResourceFile; JobResourceFile; ResourceGroup; PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ResourceFile. View page source. ResourceFile. class hailtop.batch.resource.ResourceFile(value); Bases: Resource, str; Class representing a single file resource. There exist two subclasses:; InputResourceFile and JobResourceFile. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html:307,Configurat,Configuration,307,docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceFile.html,1,['Configurat'],['Configuration']
Deployability,". ResourceGroup — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Resource; ResourceFile; InputResourceFile; JobResourceFile; ResourceGroup; ResourceGroup. PythonResult. Batch Pool Executor; Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ResourceGroup. View page source. ResourceGroup. class hailtop.batch.resource.ResourceGroup(source, root, **values); Bases: Resource; Class representing a mapping of identifiers to a resource file.; Examples; Initialize a batch and create a new job:; >>> b = Batch(); >>> j = b.new_job(). Read a set of input files as a resource group:; >>> bfile = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'). Create a resource group from a job intermediate:; >>> j.declare_resource_group(ofile={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam'}); >>> j.command(f'plink --bfile {bfile} --make-bed --out {j.ofile}'). Reference the entire file group:; >>> j.command(f'plink --bfile {bfile} --geno 0.2 --make-bed --out {j.ofile}'). Reference a single file:; >>> j.command(f'wc -l {bfile.fam}'). Execute the batch:; >>> b.run() . Notes; All files in the resource group are copied between jobs even if only one; file in the resource group is mentioned. This is to account for files that; are implicitly assumed to always be together such as a FASTA file and its; index.; Methods. source. source(). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html:309,Configurat,Configuration,309,docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html,https://hail.is,https://hail.is/docs/batch/api/resource/hailtop.batch.resource.ResourceGroup.html,1,['Configurat'],['Configuration']
Deployability,". Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.; See the; MovieLens website; for more information about this dataset. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite existing files/directories at those locations. hail.utils.ANY_REGION; Built-in mutable sequence.; If no argument is given, the constructor creates a new empty list.; The argument must be an iterable if specified. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:12176,update,updated,12176,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,1,['update'],['updated']
Deployability,". RunningBatchType — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; RunningBatchType. Backend; LocalBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; RunningBatchType. View page source. RunningBatchType. class hailtop.batch.backend.RunningBatchType; The type of value returned by Backend._run(). The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; alias of TypeVar(‘RunningBatchType’). Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html:283,Configurat,Configuration,283,docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.RunningBatchType.html,1,['Configurat'],['Configuration']
Deployability,". Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'ensg': str; 'locus': locus<GRCh37>; 'symbol': str; 'Cells_Transformedfibroblasts': float64; 'Prostate': float64; 'Spleen': float64; 'Brain_FrontalCortex_BA9_': float64; 'SmallIntestine_TerminalIleum': float64; 'MinorSalivaryGland': float64; 'Artery_Coronary': float64; 'Skin_SunExposed_Lowerleg_': float64; 'Cells_EBV_transformedlymphocytes': float64; 'Brain_Hippocampus': float64; 'Esophagus_Muscularis': float64; 'Brain_Nucleusaccumbens_basalganglia_': float64; 'Artery_Tibial': float64; 'Brain_Hypothalamus': float64; 'Adipose_Visceral_Omentum_': float64; 'Cervix_Ectocervix': float64; 'Brain_Spinalcord_cervicalc_1_': float64; 'Brain_CerebellarHemisphere': float64; 'Nerve_Tibial': float64; 'Breast_MammaryTissue': float64; 'Liver': float64; 'Skin_NotSunExposed_Suprapubic_': float64; 'AdrenalGland': float64; 'Vagina': float64; 'Pancreas': float64; 'Lung': float64; 'FallopianTube': float64; 'Pituitary': float64; 'Muscle_Skeletal': float64; 'Colon_Transverse': float64; 'Artery_Aorta': float64; 'Heart_AtrialAppendage': float64; 'Adipose_Subcutaneous': float64; 'Esophagus_Mucosa': float64; 'Heart_LeftVentricle': float64; 'Brain_Cerebellum': float64; 'Brain_Cortex': float64; 'Thyroid': float64; 'Brain_Substantianigra': float64; 'Kidney_Cortex': float64; 'Uterus': float64; 'Stomach': float64; 'WholeBlood': float64; 'Bladder': float64; 'Brain_Anteriorcingulatecortex_BA24_': float64; 'Brain_Putamen_basalganglia_': float64; 'Brain_Caudate_basalganglia_': float64; 'Colon_Sigmoid': float64; 'Cervix_Endocervix': float64; 'Ovary': float64; 'Esophagus_GastroesophagealJunction': float64; 'Testis': float64; 'Brain_Amygdala': float64; 'mean_proportion': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html:10933,update,updated,10933,docs/0.2/datasets/schemas/gnomad_base_pext.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_base_pext.html,1,['update'],['updated']
Deployability,". ServiceBackend — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; RunningBatchType; Backend; LocalBackend; ServiceBackend; ServiceBackend. Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:279,Configurat,Configuration,279,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,1,['Configurat'],['Configuration']
Deployability,". Tutorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:598,install,installed,598,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['install'],['installed']
Deployability,". hailtop.batch.docker.build_python_image — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; build_python_image(). hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.docker.build_python_image. View page source. hailtop.batch.docker.build_python_image. hailtop.batch.docker.build_python_image(fullname, requirements=None, python_version=None, _tmp_dir='/tmp', *, show_docker_output=False); Build a new Python image with dill and the specified pip packages installed.; Notes; This function is used to build Python images for PythonJob.; Examples; >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) . Parameters:. fullname (str) – Full name of where to build the image including any repository prefix and tags; if desired (default tag is latest).; requirements (Optional[List[str]]) – List of pip packages to install.; python_version (Optional[str]) – String in the format of major_version.minor_version (ex: 3.9). Defaults to; current version of Python that is running.; _tmp_dir (str) – Location to place local temporary files used while building the image.; show_docker_output (bool) – Print the output from Docker when building / pushing the image. Return type:; str. Returns:; Full name where built image is located. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html:783,install,installed,783,docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.docker.build_python_image.html,5,"['Configurat', 'install']","['Configuration', 'install', 'installed']"
Deployability,". hailtop.batch.utils.concatenate — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; concatenate(). hailtop.batch.utils.plink_merge. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.utils.concatenate. View page source. hailtop.batch.utils.concatenate. hailtop.batch.utils.concatenate(b, files, image=None, branching_factor=100); Concatenate files using tree aggregation.; Examples; Create and execute a batch that concatenates output files:; >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'touch {j1.ofile}'); >>> j2 = b.new_job(); >>> j2.command(f'touch {j2.ofile}'); >>> j3 = b.new_job(); >>> j3.command(f'touch {j3.ofile}'); >>> files = [j1.ofile, j2.ofile, j3.ofile]; >>> ofile = concatenate(b, files, branching_factor=2); >>> b.run(). Parameters:. b (Batch) – Batch to add concatenation jobs to.; files (List[ResourceFile]) – List of files to concatenate.; branching_factor (int) – Grouping factor when concatenating files.; image (Optional[str]) – Image to use. Must have the cat command. Return type:; ResourceFile. Returns:; Concatenated output file. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.concatenate.html:345,Configurat,Configuration,345,docs/batch/api/utils/hailtop.batch.utils.concatenate.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.concatenate.html,1,['Configurat'],['Configuration']
Deployability,". hailtop.batch.utils.plink_merge — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; Backends; Utilities; hailtop.batch.docker.build_python_image; hailtop.batch.utils.concatenate; hailtop.batch.utils.plink_merge; plink_merge(). Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; hailtop.batch.utils.plink_merge. View page source. hailtop.batch.utils.plink_merge. hailtop.batch.utils.plink_merge(b, bfiles, image=None, branching_factor=100); Merge binary PLINK files using tree aggregation. Parameters:. b (Batch) – Batch to add merge jobs to.; bfiles (List[ResourceGroup]) – List of binary PLINK file roots to merge.; image (Optional[str]) – Image name that contains PLINK.; branching_factor (int) – Grouping factor when merging files. Return type:; ResourceGroup. Returns:; Merged binary PLINK file. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.plink_merge.html:345,Configurat,Configuration,345,docs/batch/api/utils/hailtop.batch.utils.plink_merge.html,https://hail.is,https://hail.is/docs/batch/api/utils/hailtop.batch.utils.plink_merge.html,1,['Configurat'],['Configuration']
Deployability,". note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length. original_determine_file_length = DataFileReader.determine_file_length. try:; DataFileReader.determine_file_length = patched_determine_file_length. with DataFileReader(avro_file, DatumReader()) as data_file_reader:; tr = ir.AvroTableReader(avro.schema.parse(data_file_reader.schema), paths, key, intervals). finally:; DataFileReader.determine_file_length = original_determine_file_length. return Table(ir.TableRead(tr)). @typecheck(; paths=oneof(str, sequenceof(str)),; key=table_key_type,; min_partitions=nullable(int),; impute=bool,; no_header=bool,; comment=oneof(str, sequenceof(str)),; missing=oneof(str, sequenceof(str)),; types=dictof(str, hail_type),; quote=nullable(char),; skip_blank_lines=bool,; f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112846,patch,patch,112846,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['patch'],['patch']
Deployability,".1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html:10970,update,updated,10970,docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_gene.html,1,['update'],['updated']
Deployability,".1.1, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'gene': str; 'transcript': str; 'obs_mis': int32; 'exp_mis': float64; 'oe_mis': float64; 'mu_mis': float64; 'possible_mis': int32; 'obs_mis_pphen': int32; 'exp_mis_pphen': float64; 'oe_mis_pphen': float64; 'possible_mis_pphen': int32; 'obs_syn': int32; 'exp_syn': float64; 'oe_syn': float64; 'mu_syn': float64; 'possible_syn': int32; 'obs_lof': int32; 'mu_lof': float64; 'possible_lof': int32; 'exp_lof': float64; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': str; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int32; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int32; 'classic_caf': float64; 'max_af': float64; 'no_lofs': int32; 'obs_het_lof': int32; 'obs_hom_lof': int32; 'defined': int32; 'p': float64; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': int32; 'cds_length': int32; 'num_coding_exons': int32; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': str; 'chromosome': str; 'start_position': int32; 'end_position': int32; ----------------------------------------; Key: ['gene']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html:10941,update,updated,10941,docs/0.2/datasets/schemas/gnomad_lof_metrics.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_lof_metrics.html,1,['update'],['updated']
Deployability,".131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12361,configurat,configuration,12361,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,".; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39166,install,installing,39166,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['install'],['installing']
Deployability,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111151,pipeline,pipelines,111151,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipelines']
Deployability,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21314,pipeline,pipelines,21314,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,".functions). UNKNOWN (hail.genetics.AlleleType attribute). unpersist() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). unphase() (hail.expr.CallExpression method). unphased_diploid_gt_index() (hail.expr.CallExpression method). (hail.genetics.Call method). unphased_diploid_gt_index_call() (in module hail.expr.functions). upper() (hail.expr.StringExpression method). V. validate() (hail.vds.VariantDataset method). values() (hail.expr.DictExpression method). (hail.expr.StructExpression method). variant_qc() (in module hail.methods). variant_str() (in module hail.expr.functions). VariantDataset (class in hail.vds). VariantDatasetCombiner (class in hail.vds.combiner). vars() (in module hail.ggplot). VDSMetadata (class in hail.vds.combiner). vep() (in module hail.methods). VEPConfig (class in hail.methods). VEPConfigGRCh37Version85 (class in hail.methods). VEPConfigGRCh38Version95 (class in hail.methods). version() (in module hail). visualize_missingness() (in module hail.plot). vstack() (in module hail.nd). W. when() (hail.expr.builders.CaseBuilder method). (hail.expr.builders.SwitchBuilder method). when_missing() (hail.expr.builders.SwitchBuilder method). window() (hail.expr.LocusExpression method). write() (hail.genetics.Pedigree method). (hail.genetics.ReferenceGenome method). (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). write_expression() (in module hail.experimental). write_from_entry_expr() (hail.linalg.BlockMatrix static method). write_image() (hail.ggplot.GGPlot method). write_many() (hail.Table method). X. x_contigs (hail.genetics.ReferenceGenome property). xlab() (in module hail.ggplot). Y. y_contigs (hail.genetics.ReferenceGenome property). ylab() (in module hail.ggplot). Z. zeros() (in module hail.nd). zip() (in module hail.expr.functions). zip_with_index() (in module hail.expr.functions). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:52812,update,updated,52812,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['update'],['updated']
Deployability,".locus.position + 1 > rbml, hl.agg.take((rd.row_key, rd.col_key), 5); ). res = rd.aggregate_entries(hl.struct(**end_exprs)). if res.missing_end:; error(; 'found records in reference data with missing END field\n '; + '\n '.join(str(x) for x in res.missing_end); ); if res.end_before_position:; error(; 'found records in reference data with END before locus position\n '; + '\n '.join(str(x) for x in res.end_before_position); ); blocks_too_long = res.get('blocks_too_long', []); if blocks_too_long:; error(; 'found records in reference data with blocks larger than `ref_block_max_length`\n '; + '\n '.join(str(x) for x in blocks_too_long); ). def _same(self, other: 'VariantDataset'):; return self.reference_data._same(other.reference_data) and self.variant_data._same(other.variant_data). [docs] def union_rows(*vdses):; """"""Combine many VDSes with the same samples but disjoint variants. **Examples**. If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:. >>> vds_paths = ['chr1.vds', 'chr2.vds'] # doctest: +SKIP; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) # doctest: +SKIP; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) # doctest: +SKIP. """""". fd = hl.vds.VariantDataset.ref_block_max_length_field; mts = [vds.reference_data for vds in vdses]; n_with_ref_max_len = len([mt for mt in mts if fd in mt.globals]); any_ref_max = n_with_ref_max_len > 0; all_ref_max = n_with_ref_max_len == len(mts). # if some mts have max ref len but not all, drop it; if all_ref_max:; new_ref_mt = hl.MatrixTable.union_rows(*mts).annotate_globals(**{; fd: hl.max([mt.index_globals()[fd] for mt in mts]); }); else:; if any_ref_max:; mts = [mt.drop(fd) if fd in mt.globals else mt for mt in mts]; new_ref_mt = hl.MatrixTable.union_rows(*mts). new_var_mt = hl.MatrixTable.union_rows(*(vds.variant_data for vds in vdses)); return hl.vds.VariantDataset(new_ref_mt, new_var_mt). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:12847,update,updated,12847,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['update'],['updated']
Deployability,".or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See also; coalesce(). Parameters:. a (Expression); b (Expression). Returns:; Expression. hail.expr.functions.or_missing(predicate, value)[source]; Returns value if predicate is True, otherwise returns missing.; Examples; >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters:. predicate (BooleanExpression); value (Expression) – Value to return if predicate is True. Returns:; Expression – This expression has the same type as b. hail.expr.functions.range(start, stop=None, step=1)[source]; Returns an array of integers from start to stop by step.; Examples; >>> hl.eval(hl.range(10)); [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(3, 10)); [3, 4, 5, 6, 7, 8, 9]. >>> hl.eval(hl.range(0, 10, step=3)); [0, 3, 6, 9]. Notes; The range includes start, but excludes stop.; If provided exactly one argument, the argument is interpreted as stop and; start is set to zero. This matches the behavior of Python’s range. Parameters:. start (int or Expression of type tint32) – Start of range.; stop (int or Expression of type tint32) – End of range.; step (int or Expression of type tint32) – Step of range. Returns:; ArrayNumericExpression. hail.expr.functions.query_table(path, point_or_interval)[source]; Query records from a table corresponding to a given point or range of keys.; Notes; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in Table.annotate(). Warning; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters:. path (str) – Table path.; point_or_interval – Point or interval to query. Returns:; ArrayExpression. CaseBuilder; Class for chaining multiple if-else statements. SwitchBuilder; Class for generating conditional trees based on value of an expression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:11338,update,updated,11338,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['update'],['updated']
Deployability,.rst.txt; scans.rst.txt; tutorials-landing.rst.txt; types.rst.txt. /_static; . /annotationdb; ; annotationdb.css; annotationdb.js. /cheatsheets; ; hail_matrix_tables_cheat_sheet.pdf; hail_tables_cheat_sheet.pdf. /css; . /fonts; ; fontawesome-webfont.eot; fontawesome-webfont-1.eot; fontawesome-webfont.svg; fontawesome-webfont.ttf; fontawesome-webfont.woff; fontawesome-webfont.woff2; lato-bold.woff; lato-bold.woff2; lato-bold-italic.woff; lato-bold-italic.woff2; lato-normal.woff; lato-normal.woff2; lato-normal-italic.woff; lato-normal-italic.woff2; Roboto-Slab-Bold.woff; Roboto-Slab-Bold.woff2; Roboto-Slab-Regular.woff; Roboto-Slab-Regular.woff2. theme.css. /datasets; ; datasets.js. /js; ; theme.js. _sphinx_javascript_frameworks_compat.js; auto-render.min.js; doctools.js; documentation_options.js; goto.js; hail_version.js; jquery.js; katex.min.js; katex_autorenderer.js; katex-math.css; LeveneHaldane.pdf; nbsphinx-code-cells.css; pygments.css; rtd_modifications.css; sphinx_highlight.js; toggle.js. Hail | Aggregators; Hail | Annotation Database; Hail | Hail Query Python API; Hail | hailtop.batch Python API; Hail | Change Log And Version Policy; Hail | Cheat Sheets. /cloud; ; Hail | Amazon Web Services; Hail | Microsoft Azure; Hail | Databricks; Hail | General Advice; Hail | Google Cloud Platform; Hail | Hail Query-on-Batch. Hail | Configuration Reference. /datasets; . /schemas; ; Hail | 1000_Genomes_autosomes; Hail | 1000_Genomes_chrMT; Hail | 1000_Genomes_chrX; Hail | 1000_Genomes_chrY; Hail | 1000_Genomes_HighCov_autosomes; Hail | 1000_Genomes_HighCov_chrX; Hail | 1000_Genomes_HighCov_chrY; Hail | 1000_Genomes_Retracted_autosomes; Hail | 1000_Genomes_Retracted_chrX; Hail | 1000_Genomes_Retracted_chrY; Hail | CADD; Hail | clinvar_gene_summary; Hail | clinvar_variant_summary; Hail | DANN; Hail | dbNSFP_genes; Hail | dbNSFP_variants; Hail | dbSNP; Hail | dbSNP_rsid; Hail | Ensembl_homo_sapiens_low_complexity_regions; Hail | Ensembl_homo_sapiens_reference_genome; Hail | ge,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:21628,toggle,toggle,21628,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['toggle'],['toggle']
Deployability,"0.; seed (int, optional) – Random seed.; size (int or tuple of int, optional). Returns:; Float64Expression. hail.expr.functions.rand_int32(a, b=None, *, seed=None)[source]; Samples from a uniform distribution of 32-bit integers.; If b is None, samples from the uniform distribution over [0, a). Otherwise, sample from the; uniform distribution over [a, b).; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int32(10)); 9. >>> hl.eval(hl.rand_int32(10, 15)); 14. >>> hl.eval(hl.rand_int32(10, 15)); 12. Parameters:. a (int or Int32Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int32Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int32Expression. hail.expr.functions.rand_int64(a=None, b=None, *, seed=None)[source]; Samples from a uniform distribution of 64-bit integers.; If a and b are both specified, samples from the uniform distribution over [a, b).; If b is None, samples from the uniform distribution over [0, a).; If both a and b are None samples from the uniform distribution over all; 64-bit integers.; Examples; >>> hl.reset_global_randomness(); >>> hl.eval(hl.rand_int64(10)); 9. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 33089740109. >>> hl.eval(hl.rand_int64(1 << 33, 1 << 35)); 18195458570. Parameters:. a (int or Int64Expression) – If b is None, the right boundary of the range; otherwise, the left boundary of range.; b (int or Int64Expression) – If specified, the right boundary of the range.; seed (int, optional) – Random seed. Returns:; Int64Expression. hail.expr.functions.shuffle(a, seed=None)[source]; Randomly permute an array; Example; >>> hl.reset_global_randomness(); >>> hl.eval(hl.shuffle(hl.range(5))); [4, 0, 2, 1, 3]. Parameters:. a (ArrayExpression) – Array to permute.; seed (int, optional) – Random seed. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/random.html:12833,update,updated,12833,docs/0.2/functions/random.html,https://hail.is,https://hail.is/docs/0.2/functions/random.html,1,['update'],['updated']
Deployability,"04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc jupyter notebooks to be compatible with Spark; 3.3, which have been broken since 0.2.110.; (#12855) In; Query-on-Batch, allow writing to requester pays buckets, which was; broken before this release. Version 0.2.112; Released 2023-03-15. Bug Fixes. (#12784) Removed an; internal caching mechanism in Query on Batch that caused stalls in; pipelines with large intermediates. Version 0.2.111; Released 2023-03-13. New Features. (#12581) In Query on; Batch, users can specify which regions to have jobs run in. Bug Fixes. (#12772) Fix; hailctl hdinsight submit to pass args to the files. Version 0.2.110; Released 2023-03-08. New Features. (#12643) In Query on; Batch, hl.skat(..., logistic=True) is now supported.; (#12643) In Query on; Batch, hl.liftover is now supported.; (#12629) In Query on; Batch, hl.ibd is now supported.; (#12722) Add; hl.simulate_random_mating to generate a population from founders; under the assumption of random mating.; (#12701) Query on; Spark now officially supports Spark 3.3.0 and Dataproc 2.1.x. Performance Improvements. (#12679) In Query on; Batch, hl.balding_nichols_model is slightly faster. Also ad",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:37163,release,release,37163,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26844,configurat,configuration,26844,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"1614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52499,release,release,52499,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a > b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a > b, a, b)). [docs]@typecheck(nd1=expr_ndarray(), nd2=oneof(expr_ndarray(), list)); def minimum(nd1, nd2):; """"""Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index. If an array element being compared has the value NaN,; the minimum for that index will be NaN. Examples; --------; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters; ----------; nd1 : :class:`.NDArrayExpression`; nd2 : class:`.NDArrayExpression`, `.ArrayExpression`, numpy ndarray, or nested python lists/tuples.; nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns; -------; min_array : :class:`.NDArrayExpression`; Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. """""". if (nd1.dtype.element_type or nd2.dtype.element_type) == (tfloat64 or tfloat32):; return nd1.map2(; nd2, lambda a, b: hl.if_else(hl.is_nan(a) | hl.is_nan(b), hl.float64(float(""NaN"")), hl.if_else(a < b, a, b)); ); return nd1.map2(nd2, lambda a, b: hl.if_else(a < b, a, b)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:22694,update,updated,22694,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['update'],['updated']
Deployability,"2.35e+017.53e+00; "">5%""False3.70e+017.65e+00; "">5%""True3.73e+017.70e+00. We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:. Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint; Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease. Epilogue; Congrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the Python API for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on Zulip chat or the discussion forum.; For reference, here’s the full workflow to all tutorial endpoints combined into one cell. [53]:. table = hl.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'). mt = hl.read_matrix_table('data/1kg.mt'); mt = mt.annotate_cols(pheno = table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt); mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01). eigenvalues, pcs, _ = hl.hwe_normalized_pca(mt.GT). mt = mt.annotate_cols(scores = pcs[mt.s].scores); gwas = hl.linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]]). [Stage 310:> (0 + 1) / 1]. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:27018,update,updated,27018,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['update'],['updated']
Deployability,"22-06-21. New Features. (#11833); hl.rand_unif now has default arguments of 0.0 and 1.0. Bug fixes. (#11905) Fix; erroneous FileNotFoundError in glob patterns; (#11921) and; (#11910) Fix file; clobbering during text export with speculative execution.; (#11920) Fix array; out of bounds error when tree aggregating a multiple of 50; partitions.; (#11937) Fixed; correctness bug in scan order for Table.annotate and; MatrixTable.annotate_rows in certain circumstances.; (#11887) Escape VCF; description strings when exporting.; (#11886) Fix an; error in an example in the docs for hl.split_multi. Version 0.2.95; Released 2022-05-13. New features. (#11809) Export; dtypes_from_pandas in expr.types; (#11807) Teach; smoothed_pdf to add a plot to an existing figure.; (#11746) The; ServiceBackend, in interactive mode, will print a link to the; currently executing driver batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49025,configurat,configuration,49025,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93331,pipeline,pipelines,93331,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"32,; AF: float64,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; END: int32,; ExcessHet: float64,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: int32,; MLEAF: float64,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; RAW_MQ: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; VariantType: str,; culprit: str,; AN_EAS: int32,; AN_AMR: int32,; AN_EUR: int32,; AN_AFR: int32,; AN_SAS: int32,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EAS: int32,; AC_AMR: int32,; AC_EUR: int32,; AC_AFR: int32,; AC_SAS: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AF_EAS: float64,; AF_AMR: float64,; AF_EUR: float64,; AF_AFR: float64,; AF_SAS: float64,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html:12118,update,updated,12118,docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_chrY.html,1,['update'],['updated']
Deployability,"32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/a').show(); +-------+-------+; | a | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 2 | 2 |; | 3 | 3 |; | 4 | 4 |; | 5 | 5 |; | 6 | 6 |; | 7 | 7 |; | 8 | 8 |; | 9 | 9 |; +-------+-------+; >>> hl.read_table('output-many/b').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'b': int32; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/b').show(); +-------+-------+; | b | idx |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 0 |; | 1 | 1 |; | 4 | 2 |; | 9 | 3 |; | 16 | 4 |; | 25 | 5 |; | 36 | 6 |; | 49 | 7 |; | 64 | 8 |; | 81 | 9 |; +-------+-------+; >>> hl.read_table('output-many/c').describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'c': str; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> hl.read_table('output-many/c').show(); +-----+-------+; | c | idx |; +-----+-------+; | str | int32 |; +-----+-------+; | ""0"" | 0 |; | ""1"" | 1 |; | ""2"" | 2 |; | ""3"" | 3 |; | ""4"" | 4 |; | ""5"" | 5 |; | ""6"" | 6 |; | ""7"" | 7 |; | ""8"" | 8 |; | ""9"" | 9 |; +-----+-------+. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_table(). Parameters:. output (str) – Path at which to write.; fields (list of str) – The fields to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output.; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:81823,update,updated,81823,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['update'],['updated']
Deployability,"4,; over_15: float64,; over_20: float64,; over_25: float64,; over_30: float64,; over_50: float64,; over_100: float64; }; }; 'gerp': float64; 'tx_annotation': array<struct {; ensg: str,; csq: str,; symbol: str,; lof: str,; lof_flag: str,; Cells_Transformedfibroblasts: float64,; Prostate: float64,; Spleen: float64,; Brain_FrontalCortex_BA9_: float64,; SmallIntestine_TerminalIleum: float64,; MinorSalivaryGland: float64,; Artery_Coronary: float64,; Skin_SunExposed_Lowerleg_: float64,; Cells_EBV_transformedlymphocytes: float64,; Brain_Hippocampus: float64,; Esophagus_Muscularis: float64,; Brain_Nucleusaccumbens_basalganglia_: float64,; Artery_Tibial: float64,; Brain_Hypothalamus: float64,; Adipose_Visceral_Omentum_: float64,; Cervix_Ectocervix: float64,; Brain_Spinalcord_cervicalc_1_: float64,; Brain_CerebellarHemisphere: float64,; Nerve_Tibial: float64,; Breast_MammaryTissue: float64,; Liver: float64,; Skin_NotSunExposed_Suprapubic_: float64,; AdrenalGland: float64,; Vagina: float64,; Pancreas: float64,; Lung: float64,; FallopianTube: float64,; Pituitary: float64,; Muscle_Skeletal: float64,; Colon_Transverse: float64,; Artery_Aorta: float64,; Heart_AtrialAppendage: float64,; Adipose_Subcutaneous: float64,; Esophagus_Mucosa: float64,; Heart_LeftVentricle: float64,; Brain_Cerebellum: float64,; Brain_Cortex: float64,; Thyroid: float64,; Brain_Substantianigra: float64,; Kidney_Cortex: float64,; Uterus: float64,; Stomach: float64,; WholeBlood: float64,; Bladder: float64,; Brain_Anteriorcingulatecortex_BA24_: float64,; Brain_Putamen_basalganglia_: float64,; Brain_Caudate_basalganglia_: float64,; Colon_Sigmoid: float64,; Cervix_Endocervix: float64,; Ovary: float64,; Esophagus_GastroesophagealJunction: float64,; Testis: float64,; Brain_Amygdala: float64,; mean_proportion: float64; }>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html:14370,update,updated,14370,docs/0.2/datasets/schemas/gnomad_annotation_pext.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_annotation_pext.html,1,['update'],['updated']
Deployability,"4.122; 0.192; 0. Groups larger than max_size appear with missing q_stat, p_value, and; fault. The hard limit on the number of rows in a group is 46340.; Note that the variance component score q_stat agrees with Q in the R; package skat, but both differ from \(Q\) in the paper by the factor; \(\frac{1}{2\sigma^2}\) in the linear case and \(\frac{1}{2}\) in; the logistic case, where \(\sigma^2\) is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; “small-sample adjustment” to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment.; The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of \(\chi^2(1)\) distributions. fault value; Description. 0; no issues. 1; accuracy NOT achieved. 2; round-off error possibly significant. 3; invalid parameters. 4; unable to locate integration parameters. 5; out of memory. Parameters:. key_expr (Expression) – Row-indexed expression for key associated to each row.; weight_expr (Float64Expression) – Row-indexed expression for row weights.; y (Float64Expression) – Column-indexed response expression.; If logistic is True, all non-missing values must evaluate to 0 or; 1. Note that a BooleanExpression will be implicitly converted; to a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; logistic (bool or tuple of int and float) – If false, use the linear test. If true, use the logistic test with no; more than 25 logistic iterations and a convergence tolerance of 1e-6. If; a tuple is given, use the logistic test with the tuple elements as the; maximum nubmer of iterations and convergence tolerance, respectively.; max_size (int) – Maximum size of group on which to run the test.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:81472,integrat,integration,81472,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"6938964441,; 0.5550464170615771]. In fact, in this case we are getting the tail of; >>> table = hl.utils.range_table(7, 1).annotate(x=hl.rand_unif(0, 1, seed=0)); >>> table.x.collect(); [0.5820244750020055,; 0.33150686392731943,; 0.20526631289173847,; 0.6964416913998893,; 0.6092952493383876,; 0.6404026938964441,; 0.5550464170615771]. Reproducibility across sessions; The values of a random function are fully determined by three things:. The seed set on the function itself. If not specified, these are simply; generated sequentially.; Some data uniquely identifying the current position within a larger context,; e.g. Table, MatrixTable, or array. For instance, in a range_table(),; this data is simply the row id, as suggested by the previous examples.; The global seed. This is fixed for the entire session, and can only be set; using the global_seed argument to init(). To ensure reproducibility within a single hail session, it suffices to either; manually set the seed on every random function call, or to call; reset_global_randomness() at the start of a pipeline, which resets the; counter used to generate seeds.; >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. >>> hl.reset_global_randomness(); >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])); [0.9828239225846387, 0.49094525115847415]. To ensure reproducibility across sessions, one must in addition specify the; global_seed in init(). If not specified, the global seed is chosen; randomly. All documentation examples were computed using global_seed=0.; >>> hl.stop() ; >>> hl.init(global_seed=0) ; >>> hl.eval(hl.array([hl.rand_unif(0, 1), hl.rand_unif(0, 1)])) ; [0.9828239225846387, 0.49094525115847415]. rand_bool(p[, seed]); Returns True with probability p. rand_beta(a, b[, lower, upper, seed]); Samples from a beta distribution with parameters a (alpha) and b (beta). rand_cat(prob[, seed]); Samples from a categorical ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/random.html:4778,pipeline,pipeline,4778,docs/0.2/functions/random.html,https://hail.is,https://hail.is/docs/0.2/functions/random.html,1,['pipeline'],['pipeline']
Deployability,"7). gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_est. View page source. gnomad_ld_scores_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_est.html,1,['update'],['updated']
Deployability,"7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87483,update,update-hail-version,87483,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update-hail-version']
Deployability,"7262169 | [""T"",""C""] | NA | NA | NA | NA |; +---------------+------------+-------+-------+----------+----------+. Export variants with p-values below 0.001:; >>> tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; The; transmission disequilibrium test; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. \[(t - u)^2 \over (t + u)\]; and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis.; transmission_disequilibrium_test() only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by in_autosome(), and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. Auto – in autosome or in PAR of X or female child; HemiX – in non-PAR of X and male child. Here PAR is the pseudoautosomal region; of X and Y defined by ReferenceGenome, which many variant callers; map to chromosome X. Kid; Dad; Mom; Copy State; t; u. HomRef; Het; Het; Auto; 0; 2. HomRef; HomRef; Het; Auto; 0; 1. HomRef; Het; HomRef; Auto; 0; 1. Het; Het; Het; Auto; 1; 1. Het; HomRef; Het; Auto; 1; 0. Het; Het; HomRef; Auto; 1; 0. Het; HomVar; Het; Auto; 0; 1. Het; Het; HomVar; Auto; 0; 1. HomVar; Het; Het; Auto; 2; 0. HomVar; Het; HomVar; Auto; 1; 0. HomVar; HomVar; Het; Auto; 1; 0. HomRef; HomRef; Het; HemiX; 0; 1. HomRef; HomVar; Het; HemiX; 0; 1. HomVar; HomRef; Het; HemiX; 1; 0. HomVar; HomVar; Het; HemiX; 1; 0. transmission_disequilibrium_test() produces a table with the following columns:. locus (tl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:95604,configurat,configurations,95604,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['configurat'],['configurations']
Deployability,": array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: array<str>,; RSID_REMOVED: array<str>,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html:11463,update,updated,11463,docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_autosomes.html,2,['update'],['updated']
Deployability,": int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; CIEND: int32,; CIPOS: int32,; CS: str,; END: int32,; IMPRECISE: bool,; MC: array<str>,; MEINFO: array<str>,; MEND: int32,; MLEN: int32,; MSTART: int32,; SVLEN: array<int32>,; SVTYPE: str,; TSD: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; DP: int32,; AA: str,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html:11127,update,updated,11127,docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_autosomes.html,2,['update'],['updated']
Deployability,"; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This sho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25901,install,install-completion,25901,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['install'],"['install', 'install-completion']"
Deployability,"; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invok",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37894,configurat,configuration,37894,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. window(before, after)[source]; Returns an interval of a specified number of bases around the locus.; Examples; Create a window of two megabases centered at a locus:; >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; The returned interval is inclusive of both the start and end; endpoints. Parameters:. before (Expression of type tint32) – Number of bases to include before the locus. Truncates at 1.; after (Expression of type tint32) – Number of bases to include after the locus. Truncates at; contig length. Returns:; IntervalExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:11883,update,updated,11883,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['update'],['updated']
Deployability,"; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'junction_id': str; 'junction_interval': interval<locus<GRCh37>>; 'gene_id': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'gene_symbol': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': int32; ----------------------------------------; Column key: ['s']; Row key: ['junction_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html:11193,update,updated,11193,docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_junction_read_counts.html,1,['update'],['updated']
Deployability,"; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.filtering_allele_frequency. Source code for hail.experimental.filtering_allele_frequency; from hail.expr.expressions import Float64Expression, expr_float64, expr_int32; from hail.expr.functions import _func; from hail.expr.types import tfloat64; from hail.typecheck import typecheck. [docs]@typecheck(ac=expr_int32, an=expr_int32, ci=expr_float64); def filtering_allele_frequency(ac, an, ci) -> Float64Expression:; """"""; Computes a filtering allele frequency (described below); for `ac` and `an` with confidence `ci`. The filtering allele frequency is the highest true population allele frequency; for which the upper bound of the `ci` (confidence interval) of allele count; under a Poisson distribution is still less than the variant's observed; `ac` (allele count) in the reference sample, given an `an` (allele number). This function defines a ""filtering AF"" that represents; the threshold disease-specific ""maximum credible AF"" at or below which; the disease could not plausibly be caused by that variant. A variant with; a filtering AF >= the maximum credible AF for the disease under consideration; should be filtered, while a variant with a filtering AF below the maximum; credible remains a candidate. This filtering AF is not disease-specific:; it can be applied to any disease of interest by comparing with a; user-defined disease-specific maximum credible AF. For more details, see: `Whiffin et al., 2017 <https://www.nature.com/articles/gim201726>`__. Parameters; ----------; ac : int or :class:`.Expression` of type :py:data:`.tint32`; an : int or :class:`.Expression` of type :py:data:`.tint32`; ci : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""filtering_allele_frequency"", tfloat64, ac, an, ci). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html:2325,update,updated,2325,docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,2,['update'],['updated']
Deployability,"; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') . Annotate a Table with clinvar_gene_summary, CADD,; and DANN:; >>> db = hl.experimental.DB(region=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1594,configurat,configuration,1594,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"; Next Steps. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Your First Hail Query. View page source. Your First Hail Query; We recommend using IPython, a super-powered Python terminal:; pip install ipython. Start an IPython session by copy-pasting the below into your Terminal.; ipython. Let’s randomly generate a dataset according to the Balding-Nichols; Model. The dataset has one-hundred variants and ten samples from three; populations.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=10,; n_variants=100); mt.show(). The last line, mt.show(), displays the dataset in a tabular form.; 2020-05-09 19:08:07 Hail: INFO: Coerced sorted dataset; +---------------+------------+------+------+------+------+; | locus | alleles | 0.GT | 1.GT | 2.GT | 3.GT |; +---------------+------------+------+------+------+------+; | locus<GRCh37> | array<str> | call | call | call | call |; +---------------+------------+------+------+------+------+; | 1:1 | [""A"",""C""] | 0/1 | 1/1 | 0/1 | 0/1 |; | 1:2 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:3 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 1/1 |; | 1:4 | [""A"",""C""] | 0/0 | 0/0 | 0/1 | 1/1 |; | 1:5 | [""A"",""C""] | 0/1 | 0/0 | 0/1 | 0/0 |; | 1:6 | [""A"",""C""] | 1/1 | 0/1 | 0/1 | 0/1 |; | 1:7 | [""A"",""C""] | 0/0 | 0/1 | 0/1 | 0/0 |; | 1:8 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 1/1 |; | 1:9 | [""A"",""C""] | 1/1 | 1/1 | 1/1 | 1/1 |; | 1:10 | [""A"",""C""] | 1/1 | 0/1 | 1/1 | 0/1 |; | 1:11 | [""A"",""C""] | 0/1 | 1/1 | 1/1 | 0/1 |; +---------------+------------+------+------+------+------+; showing top 11 rows; showing the first 4 of 10 columns</code></pre>. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial; Learn how to use Hail on Google Cloud. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/try.html:2263,update,updated,2263,docs/0.2/install/try.html,https://hail.is,https://hail.is/docs/0.2/install/try.html,1,['update'],['updated']
Deployability,"; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; BaseQRankSum: float64,; ClippingRankSum: float64,; DP: int32,; DS: bool,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MLEAC: array<int32>,; MLEAF: array<float64>,; MQ: float64,; MQ0: int32,; MQRankSum: float64,; QD: float64,; ReadPosRankSum: float64,; set: str; }; 'call_rate': float64; ----------------------------------------; Entry fields:; 'GT': call; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'PL': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. [12]:. p = hl.plot.histogram(mt2.call_rate, range=(0,1.0), bins=100,; title='Variant Call Rate Histogram', legend='Call Rate'); show(p). Exercise: GQ vs DP; In this exercise, you’ll use Hail to investigate a strange property of sequencing datasets.; The DP field is the sequencing depth (the number of reads).; Let’s first plot a histogram of DP:. [13]:. p = hl.plot.histogram(mt.DP, range=(0,40), bins=40, title='DP Histogram', legend='DP'); show(p). [Stage 9:> (0 + 1) / 1]. Now, let’s do the same thing for GQ.; The GQ field is the phred-scaled “genotype quality”. The formula to convert to a linear-scale confidence (0 to 1) is 10 ** -(mt.GQ / 10). GQ is truncated to lie between 0 and 99. [14]:. p = hl.plot.histogram(mt.GQ, range=(0,100), bins=100, title='GQ Histogram', legend='GQ'); show(p). [Stage 10:> (0 + 1) / 1]. Whoa! That’s a strange distribution! There’s a big spike at 100. The rest of the values have roughly the same shape as the DP distribution, but form a Dimetrodon. Use Hail to figure out what’s going on!. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/07-matrixtable.html:10599,update,updated,10599,docs/0.2/tutorials/07-matrixtable.html,https://hail.is,https://hail.is/docs/0.2/tutorials/07-matrixtable.html,1,['update'],['updated']
Deployability,"; Schema (2.1.1, GRCh37). gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_fin. View page source. gnomad_ld_scores_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_fin.html,1,['update'],['updated']
Deployability,"; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use by default if not specified by a job. Only; applicable for the ServiceBackend. See Job.storage().; default_regions (Optional[List[str]]) – Cloud regions in which jobs may run. When unspecified or None, use the regions attribute of; ServiceBackend. See ServiceBackend for details.; default_timeout (Union[int, float, None]) – Maximum time in seconds for a job to run before being killed. Only; applicable for the ServiceBackend. If None, there is no; timeout.; default_python_image (Optional[str]) – Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is latest). The image must have; the dill Python package installed and have the same version of Python installed that is; currently running. If None, a tag of the hailgenetics/hail image will be chosen; according to the current Hail and Python version.; default_spot (Optional[bool]) – If unspecified or True, jobs will run by default on spot instances. If False, jobs; will run by default on non-spot instances. Each job can override this setting with; Job.spot().; project (Optional[str]) – DEPRECATED: please specify google_project on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures (Optional[int]) – Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the ServiceBackend. Must be greater than 0. Methods. from_batch_id; Create a Batch from an existing batch id. ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:3750,install,installed,3750,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,4,['install'],['installed']
Deployability,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; DANN. View page source. DANN. Versions: None; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (None, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/DANN.html:9416,update,updated,9416,docs/0.2/datasets/schemas/DANN.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/DANN.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_elements. View page source. gerp_elements. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; 'S': float64; 'p_value': float64; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html:9425,update,updated,9425,docs/0.2/datasets/schemas/gerp_elements.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_elements.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; Schema (2.1.1, GRCh37). gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nfe. View page source. gnomad_ld_scores_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nfe.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; Schema (2.1.1, GRCh37). gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_nwe. View page source. gnomad_ld_scores_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_nwe.html,1,['update'],['updated']
Deployability,"; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; Schema (2.1.1, GRCh37). gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_seu. View page source. gnomad_ld_scores_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_seu.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_ALL. View page source. giant_bmi_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html:9563,update,updated,9563,docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_ALL.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9724,update,updated,9724,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9724,update,updated,9724,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_sQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html:9742,update,updated,9742,docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pituitary_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74290,update,update,74290,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return Env.hc()._tmpdir. class _TemporaryFilenameManager:; def __in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20829,release,releases,20829,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['release'],['releases']
Deployability,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:24481,update,updated,24481,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['update'],['updated']
Deployability,"; movie_cluster_readable = _copy_to_tmp(fs, local_path_uri(movie_table_path), 'txt'); ratings_cluster_readable = _copy_to_tmp(fs, local_path_uri(ratings_table_path), 'txt'). [movies_path, ratings_path, users_path] = paths. genres = [; 'Action',; 'Adventure',; 'Animation',; ""Children's"",; 'Comedy',; 'Crime',; 'Documentary',; 'Drama',; 'Fantasy',; 'Film-Noir',; 'Horror',; 'Musical',; 'Mystery',; 'Romance',; 'Sci-Fi',; 'Thriller',; 'War',; 'Western',; ]. # utility functions for importing movies; def field_to_array(ds, field):; return hl.if_else(ds[field] != 0, hl.array([field]), hl.empty_array(hl.tstr)). def fields_to_array(ds, fields):; return hl.flatten(hl.array([field_to_array(ds, f) for f in fields])). def rename_columns(ht, new_names):; return ht.rename({k: v for k, v in zip(ht.row, new_names)}). info(f'importing users table and writing to {users_path} ...'). users = rename_columns(; hl.import_table(user_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'),; ['id', 'age', 'sex', 'occupation', 'zipcode'],; ); users.write(users_path, overwrite=True). info(f'importing movies table and writing to {movies_path} ...'). movies = hl.import_table(movie_cluster_readable, key=['f0'], no_header=True, impute=True, delimiter='|'); movies = rename_columns(; movies, ['id', 'title', 'release date', 'video release date', 'IMDb URL', 'unknown', *genres]; ); movies = movies.drop('release date', 'video release date', 'unknown', 'IMDb URL'); movies = movies.transmute(genres=fields_to_array(movies, genres)); movies.write(movies_path, overwrite=True). info(f'importing ratings table and writing to {ratings_path} ...'). ratings = hl.import_table(ratings_cluster_readable, no_header=True, impute=True); ratings = rename_columns(ratings, ['user_id', 'movie_id', 'rating', 'timestamp']); ratings = ratings.drop('timestamp'); ratings.write(ratings_path, overwrite=True). else:; info('Movie Lens files found!'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:8872,release,release,8872,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,10,"['release', 'update']","['release', 'updated']"
Deployability,"; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Father column fields from `mt`.; - `mother` (``struct``) -- Mother column fields from `mt`.; - `proband_entry` (``struct``) -- Proband entry fields from `mt`.; - `father_entry` (``struct``) -- Father entry fields from `mt`.; - `proband_entry` (``struct``) -- Mother entry fields from `mt`.; - `is_female` (``bool``) -- ``True`` if proband is female.; - `p_de_novo` (``float64``) -- Unfiltered posterior probability; that the event is *de novo* rather than a missed heterozygous; event in a parent.; - `confidence` (``str``) Validation confidence. One of: ``'HIGH'``,; ``'MEDIUM'``, ``'LOW'``. The key of the table is ``['locus', 'alleles', 'id']``. The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration ``x = (AA, AA, AB)`` of calls; occurs, exactly one of the following is true:. - ``d``: a de novo mutation occurred in the proband and all calls are; accurate.; - ``m``: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. .. math::. \mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}. Applying Bayes rule to the numerator and denominator yields. .. math::. \frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}. The prior on de novo mutation is estimated from the rate in the literature:. .. math::. \mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}. The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. .. math::. \mathrm{P}(m) = 1 - (1 - AF)^4. The likelihoods :math:`\mathrm{P}(x \mid ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:22386,configurat,configuration,22386,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['configurat'],['configuration']
Deployability,"; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Locus):; raise TypeCheckError('TLocus expected type hail.representation.Locus, but found %s' %; type(annotation)). [docs]class TInterval(Type):; """"""; Hail type corresponding to :class:`hail.representation.Interval`. .. include:: hailType.rst. - `expression language documentation <types.html#interval>`__; - in Python, values are instances of :class:`hail.representation.Interval`. """"""; __metaclass__ = SingletonType. def __init__(self):; super(TInterval, self).__init__(scala_object(Env.hail().expr, 'TInterval')). def _convert_to_py(self, annotation):; if annotation:; return Interval._from_java(annotation); else:; return annotation. def _convert_to_j(self, annotation):; if annotation is not None:; return annotation._jrep; else:; return annotation. def _typecheck(self, annotation):; if annotation and not isinstance(annotation, Interval):; raise TypeCheckError('TInterval expected type hail.representation.Interval, but found %s' %; type(annotation)). __singletons__ = {'is.hail.expr.TInt$': TInt,; 'is.hail.expr.TLong$': TLong,; 'is.hail.expr.TFloat$': TFloat,; 'is.hail.expr.TDouble$': TDouble,; 'is.hail.expr.TBoolean$': TBoolean,; 'is.hail.expr.TString$': TString,; 'is.hail.expr.TVariant$': TVariant,; 'is.hail.expr.TAltAllele$': TAltAllele,; 'is.hail.expr.TLocus$': TLocus,; 'is.hail.expr.TGenotype$': TGenotype,; 'is.hail.expr.TCall$': TCall,; 'is.hail.expr.TInterval$': TInterval}. import pprint. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, Type):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/expr.html:20131,patch,patch,20131,docs/0.1/_modules/hail/expr.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/expr.html,1,['patch'],['patch']
Deployability,"; sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31501,update,update,31501,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. transpose(axes=None)[source]; Permute the dimensions of this ndarray according to the ordering of axes. Axis j in the ith index of; axes maps the jth dimension of the ndarray to the ith dimension of the output ndarray. Parameters:; axes (tuple of int, optional) – The new ordering of the ndarray’s dimensions. Notes; Does nothing on ndarrays of dimensionality 0 or 1. Returns:; NDArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html:8410,update,updated,8410,docs/0.2/hail.expr.NDArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayExpression.html,1,['update'],['updated']
Deployability,"=tint32,; transcript_consequences=tarray(; tstruct(; allele_num=tint32,; amino_acids=tstr,; biotype=tstr,; canonical=tint32,; ccds=tstr,; cdna_start=tint32,; cdna_end=tint32,; cds_end=tint32,; cds_start=tint32,; codons=tstr,; consequence_terms=tarray(tstr),; distance=tint32,; domains=tarray(tstruct(db=tstr, name=tstr)),; exon=tstr,; gene_id=tstr,; gene_pheno=tint32,; gene_symbol=tstr,; gene_symbol_source=tstr,; hgnc_id=tstr,; hgvsc=tstr,; hgvsp=tstr,; hgvs_offset=tint32,; impact=tstr,; intron=tstr,; lof=tstr,; lof_flags=tstr,; lof_filter=tstr,; lof_info=tstr,; minimised=tint32,; polyphen_prediction=tstr,; polyphen_score=tfloat,; protein_end=tint32,; protein_start=tint32,; protein_id=tstr,; sift_prediction=tstr,; sift_score=tfloat,; strand=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23077,configurat,configuration,23077,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {valid_clouds}.'; ). datasets = get_datasets_metadata(); names = set([dataset for dataset in datasets]); if name not in names:; raise ValueError(f'{name} is not a dataset available in the' f' repository.'). versions = set(dataset['version'] for dataset in datasets[name]['versions']); if version not in versions:; raise ValueError(; f'Version {version!r} not available for dataset' f' {name!r}.\n' f'Available versions: {versions}.'; ). reference_genomes = set(dataset['reference_genome'] for dataset in datasets[name]['versions']); if reference_genome not in reference_genomes:; raise ValueError(; f'Reference genome build {reference_genome!r} not'; f' available for dataset {name!r}.\n'; f'Available reference genome builds:'; f' {reference_genomes}.'; ). clouds = set(k for dataset in datasets[name]['versions'] for k in dataset['url'].keys()); if cloud not in clouds:; raise ValueError(f'Cloud platform {cloud!r} not available for dataset {name}.\nAvailable platforms: {clouds}.'). regions = set(k for dataset in datasets[name]['versions'] for k in dataset['url'][cloud].keys()); if region not in regions:; raise ValueError(; f'Region {region!r} not available for dataset'; f' {name!r} on cloud platform {cloud!r}.\n'; f'Available regions: {regions}.'; ). path = [; dataset['url'][cloud][region]; for dataset in datasets[name]['versions']; if all([dataset['version'] == version, dataset['reference_genome'] == reference_genome]); ]; assert len(path) == 1; path = path[0]; if path.startswith('s3://'):; try:; dataset = _read_dataset(path); except hl.utils.java.FatalError:; dataset = _read_dataset(path.replace('s3://', 's3a://')); else:; dataset = _read_dataset(path); return dataset. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:4542,update,updated,4542,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['update'],['updated']
Deployability,"={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes the expression specified in the fill aesthetic can be used as a fill color. hail.ggplot.scale_x_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous x scale. Parameters:. name (str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8983,continuous,continuous,8983,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"> hl.eval(hl.pgenchisq(40 , w=[-2, -1], k=[5, 2], lam=[3, 1], mu=-3, sigma=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; We follow Wikipedia’s notational conventions. Some texts refer to the weight vector (our w) as; \(\lambda\) or lb and the non-centrality vector (our lam) as nc.; We use the Davies’ algorithm which was published as:. Davies, Robert. “The distribution of a linear combination of chi-squared random variables.”; Applied Statistics 29 323-333. 1980. Davies included Fortran source code in the original publication. Davies also released a C; language port. Hail’s implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests.; Davies’ website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of ti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:18503,release,released,18503,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['release'],['released']
Deployability,"A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. index(value, start=0, stop=None)[source]; Do not use this method.; This only exists for compatibility with the Python Sequence abstract; base class. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.TupleExpression.html:6927,update,updated,6927,docs/0.2/hail.expr.TupleExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.TupleExpression.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_afr. View page source. gnomad_ld_variant_indices_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_afr.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_amr. View page source. gnomad_ld_variant_indices_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_amr.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_asj. View page source. gnomad_ld_variant_indices_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_asj.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_eas. View page source. gnomad_ld_variant_indices_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_eas.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_est. View page source. gnomad_ld_variant_indices_est. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_est.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_fin. View page source. gnomad_ld_variant_indices_fin. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_fin.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nwe. View page source. gnomad_ld_variant_indices_nwe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nwe.html,1,['update'],['updated']
Deployability,"AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_seu. View page source. gnomad_ld_variant_indices_seu. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html:10435,update,updated,10435,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_seu.html,1,['update'],['updated']
Deployability,"ATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, etc), include “CHANGELOG” in the commit message or PR title.; This helps identify what should be included in the change log when a new version is released. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:3369,release,released,3369,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,2,"['release', 'update']","['released', 'updated']"
Deployability,"Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes. (#10182) Fixed; serious memory leak in certain uses of filter_intervals.; (#10133) Fix bug; where some pipelines incorrectly infer missingness, leading to a type; error.; (#10134) Teach; hl.king to treat filtered entries as missing values.; (#10158) Fixes hail; usage in latest versions of jupyter that rely on asyncio.; (#10174) Fixed bad; error message when incorrect return type specified with hl.loop. Version 0.2.63; Released 2021-03-01. (#10105) Hail will; now return frozenset and hail.utils.frozendict instead of; normal sets and dicts. Bug fixes. (#10035) Fix; mishandling of NaN values in hl.agg.hist, where they were; unintentionally included in the first bin.; (#10007) Improve; error message from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:60931,pipeline,pipelines,60931,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_male. View page source. UK_Biobank_Rapid_GWAS_male. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html:10491,update,updated,10491,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_male.html,2,['update'],['updated']
Deployability,"C_nonTCGA_pRec': float64; 'ExAC_nonTCGA_pNull': float64; 'ExAC_nonpsych_pLI': float64; 'ExAC_nonpsych_pRec': float64; 'ExAC_nonpsych_pNull': float64; 'gnomAD_pLI': str; 'gnomAD_pRec': str; 'gnomAD_pNull': str; 'ExAC_del.score': float64; 'ExAC_dup.score': float64; 'ExAC_cnv.score': float64; 'ExAC_cnv_flag': str; 'GDI': float64; 'GDI-Phred': float64; 'Gene damage prediction (all disease-causing genes)': str; 'Gene damage prediction (all Mendelian disease-causing genes)': str; 'Gene damage prediction (Mendelian AD disease-causing genes)': str; 'Gene damage prediction (Mendelian AR disease-causing genes)': str; 'Gene damage prediction (all PID disease-causing genes)': str; 'Gene damage prediction (PID AD disease-causing genes)': str; 'Gene damage prediction (PID AR disease-causing genes)': str; 'Gene damage prediction (all cancer disease-causing genes)': str; 'Gene damage prediction (cancer recessive disease-causing genes)': str; 'Gene damage prediction (cancer dominant disease-causing genes)': str; 'LoFtool_score': float64; 'SORVA_LOF_MAF0.005_HetOrHom': float64; 'SORVA_LOF_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOF_MAF0.001_HetOrHom': float64; 'SORVA_LOF_MAF0.001_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.005_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.005_HomOrCompoundHet': float64; 'SORVA_LOForMissense_MAF0.001_HetOrHom': float64; 'SORVA_LOForMissense_MAF0.001_HomOrCompoundHet': float64; 'Essential_gene': str; 'Essential_gene_CRISPR': str; 'Essential_gene_CRISPR2': str; 'Essential_gene_gene-trap': str; 'Gene_indispensability_score': float64; 'Gene_indispensability_pred': str; 'MGI_mouse_gene': str; 'MGI_mouse_phenotype': str; 'ZFIN_zebrafish_gene': str; 'ZFIN_zebrafish_structure': str; 'ZFIN_zebrafish_phenotype_quality': str; 'ZFIN_zebrafish_phenotype_tag': str; ----------------------------------------; Key: ['Gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html:12380,update,updated,12380,docs/0.2/datasets/schemas/dbNSFP_genes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_genes.html,1,['update'],['updated']
Deployability,"Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The ou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3818,configurat,configuration,3818,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; 1000_Genomes_chrMT. View page source. 1000_Genomes_chrMT. Versions: phase_3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; AC: int32,; VT: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html:10725,update,updated,10725,docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrMT.html,1,['update'],['updated']
Deployability,"Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Aggregation; Annotation (Adding Fields); Genetics. Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. How-To Guides. View page source. How-To Guides. Note; Hail’s How-To Guides are in their early stages. We welcome suggestions; for additional guides, as well as feedback about our documentation. If; you’d like to add a guide to the documentation, make a pull request!. These guides are short, goal-oriented explanations of how to use Hail. Aggregation; Table Aggregations; Aggregate Over Rows Into A Local Value; One aggregation; Multiple aggregations. Aggregate Per Group. Matrix Table Aggregations; Aggregate Entries Per Row (Over Columns); Aggregate Entries Per Column (Over Rows); Aggregate Column Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Row Values Into a Local Value; One aggregation; Multiple aggregations. Aggregate Entry Values Into A Local Value; Aggregate Per Column Group; Aggregate Per Row Group. Annotation (Adding Fields); Create a nested annotation; Remove a nested annotation. Genetics; Formatting; Convert variants in string format to separate locus and allele fields; Liftover variants from one coordinate system to another. Filtering and Pruning; Remove related individuals from a dataset; Filter loci by a list of locus intervals; From a table of intervals; From a UCSC BED file; Using hl.filter_intervals; Declaring intervals with hl.parse_locus_interval. Pruning Variants in Linkage Disequilibrium. Analysis; Linear Regression; Single Phenotype; Multiple Phenotypes; Using Variants (SNPs) as Covariates; Stratified by Group. PLINK Conversions; Polygenic Score Calculation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides.html:2049,update,updated,2049,docs/0.2/guides.html,https://hail.is,https://hail.is/docs/0.2/guides.html,1,['update'],['updated']
Deployability,"E>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not have the appropriate permissions to check the default storage class of the bucket, the first object encountered in the bucket will have its storage class checked, and this will be assumed to be the default storage policy of the bucket. Shared between Query and Batch; Yes. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:2803,update,updated,2803,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['update'],['updated']
Deployability,"Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_fl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:70886,integrat,integration,70886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Deployability,"For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:31818,continuous,continuous,31818,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102953,configurat,configuration,102953,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,"['configurat', 'install', 'release']","['configuration', 'installed', 'release']"
Deployability,Installing Hail; Hail | For Software Developers. /ggplot; ; Hail | Plotting With hail.ggplot Overview. /guides; ; Hail | Aggregation; Hail | Annotation; Hail | Genetics. Hail | How-To Guides; Hail | Hadoop Glob Patterns; Hail | ArrayExpression; Hail | ArrayNumericExpression; Hail | BooleanExpression; Hail | CallExpression; Hail | CollectionExpression; Hail | DictExpression. Hail | Expression. Hail | Expression; Hail | Expression. Hail | Float32Expression; Hail | Float64Expression; Hail | Int32Expression; Hail | Int64Expression; Hail | IntervalExpression; Hail | LocusExpression; Hail | NDArrayExpression; Hail | NDArrayNumericExpression; Hail | NumericExpression; Hail | SetExpression; Hail | StringExpression; Hail | StructExpression; Hail | TupleExpression; Hail | GroupedMatrixTable; Hail | GroupedTable; Hail | MatrixTable; Hail | Table; Hail | Hail on the Cloud; Hail | Hail 0.2. /install; ; Hail | Use Hail on Azure HDInsight; Hail | Use Hail on Google Dataproc; Hail | Install Hail on GNU/Linux; Hail | Install Hail on Mac OS X; Hail | Install Hail on a Spark Cluster; Hail | Your First Hail Query. Hail | Libraries. /linalg; . /utils; ; Hail | linalg/utils. Hail | BlockMatrix; Hail | linalg. /methods; ; Hail | Genetics; Hail | Import / Export; Hail | Methods; Hail | Miscellaneous; Hail | Relatedness; Hail | Statistics. /nd; ; Hail | nd. Hail | Other Resources. /overview; ; Hail | Expressions Overview; Hail | Hail Overview. Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | MatrixTable Overview. Hail | MatrixTable Overview; Hail | Table Overview. Hail | Plot; Hail | Python API; Hail | Scans; Hail | Search. /stats; ; Hail | LinearMixedModel; Hail | stats. /tutorials; . /iframe_figures; ; figure_11.html; figure_12.html; figure_13.html; figure_15.html; figure_16.html; figure_17.html; figure_18.html; figure_3.html; figure_4.html; figure_5.html; figure_6.html; figure_7.html; figure_8.html. Hail | GWAS Tutorial; Hail | Table Tutorial; Hail | Aggregation Tutorial; ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:33391,install,install,33391,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['install'],['install']
Deployability,"K.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2137,install,install,2137,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['install']
Deployability,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23890,pipeline,pipelines,23890,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['pipeline'],['pipelines']
Deployability,"L2': float64; 'UTR_5_UCSC.flanking.500L2': float64; 'WeakEnhancer_HoffmanL2': float64; 'WeakEnhancer_Hoffman.flanking.500L2': float64; 'GERP.NSL2': float64; 'GERP.RSsup4L2': float64; 'MAFbin1L2': float64; 'MAFbin2L2': float64; 'MAFbin3L2': float64; 'MAFbin4L2': float64; 'MAFbin5L2': float64; 'MAFbin6L2': float64; 'MAFbin7L2': float64; 'MAFbin8L2': float64; 'MAFbin9L2': float64; 'MAFbin10L2': float64; 'MAF_Adj_Predicted_Allele_AgeL2': float64; 'MAF_Adj_LLD_AFRL2': float64; 'Recomb_Rate_10kbL2': float64; 'Nucleotide_Diversity_10kbL2': float64; 'Backgrd_Selection_StatL2': float64; 'CpG_Content_50kbL2': float64; 'MAF_Adj_ASMCL2': float64; 'GTEx_eQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K27acQTL_MaxCPPL2': float64; 'BLUEPRINT_H3K4me1QTL_MaxCPPL2': float64; 'BLUEPRINT_DNA_methylation_MaxCPPL2': float64; 'synonymousL2': float64; 'non_synonymousL2': float64; 'Conserved_Vertebrate_phastCons46wayL2': float64; 'Conserved_Vertebrate_phastCons46way.flanking.500L2': float64; 'Conserved_Mammal_phastCons46wayL2': float64; 'Conserved_Mammal_phastCons46way.flanking.500L2': float64; 'Conserved_Primate_phastCons46wayL2': float64; 'Conserved_Primate_phastCons46way.flanking.500L2': float64; 'BivFlnkL2': float64; 'BivFlnk.flanking.500L2': float64; 'Human_Promoter_VillarL2': float64; 'Human_Promoter_Villar.flanking.500L2': float64; 'Human_Enhancer_VillarL2': float64; 'Human_Enhancer_Villar.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_PromoterL2': float64; 'Ancient_Sequence_Age_Human_Promoter.flanking.500L2': float64; 'Ancient_Sequence_Age_Human_EnhancerL2': float64; 'Ancient_Sequence_Age_Human_Enhancer.flanking.500L2': float64; 'Human_Enhancer_Villar_Species_Enhancer_CountL2': float64; 'Human_Promoter_Villar_ExACL2': float64; 'Human_Promoter_Villar_ExAC.flanking.500L2': float64; 'locus': locus<GRCh37>; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html:12941,update,updated,12941,docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_annotations.html,2,['update'],['updated']
Deployability,"L_BACKEND_HEAP_SIZE', None); backend = LocalBackend(; tmpdir,; log,; quiet,; append,; branching_factor,; skip_logging_configuration,; optimizer_iterations,; jvm_heap_size,; gcs_requester_pays_configuration,; ). if not backend.fs.exists(tmpdir):; backend.fs.mkdir(tmpdir). HailContext.create(log, quiet, append, tmpdir, tmpdir, default_reference, global_seed, backend); if not quiet:; connect_logger(backend._utils_package_object, 'localhost', 12888). [docs]def version() -> str:; """"""Get the installed Hail version. Returns; -------; str; """"""; if hail.__version__ is None:; hail.__version__ = __resource_str('hail_version').strip(). return hail.__version__. def revision() -> str:; """"""Get the installed Hail git revision. Returns; -------; str; """"""; if hail.__revision__ is None:; hail.__revision__ = __resource_str('hail_revision').strip(). return hail.__revision__. def _hail_cite_url():; v = version(); [tag, sha_prefix] = v.split(""-""); if not local_jar_information().development_mode:; # pip installed; return f""https://github.com/hail-is/hail/releases/tag/{tag}""; return f""https://github.com/hail-is/hail/commit/{sha_prefix}"". [docs]def citation(*, bibtex=False):; """"""Generate a Hail citation. Parameters; ----------; bibtex : bool; Generate a citation in BibTeX form. Returns; -------; str; """"""; if bibtex:; return (; f""@misc{{Hail,""; f"" author = {{Hail Team}},""; f"" title = {{Hail}},""; f"" howpublished = {{\\url{{{_hail_cite_url()}}}}}""; f""}}""; ); return f""Hail Team. Hail {version()}. {_hail_cite_url()}."". def cite_hail():; return citation(bibtex=False). def cite_hail_bibtex():; return citation(bibtex=True). [docs]def stop():; """"""Stop the currently running Hail session.""""""; if Env._hc:; Env.hc().stop(). [docs]def spark_context():; """"""Returns the active Spark context. Returns; -------; :class:`pyspark.SparkContext`; """"""; return Env.spark_backend('spark_context').sc. [docs]def tmp_dir() -> str:; """"""Returns the Hail shared temporary directory. Returns; -------; :class:`str`; """"""; return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:20777,install,installed,20777,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['install'],['installed']
Deployability,"L_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Clouder",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2400,install,install,2400,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,3,['install'],['install']
Deployability,"MatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a column field and computing the number of non-reference calls; as an entry field:; >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.stats(dataset.pheno.height).mean); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). See also; aggregate(). Returns:; MatrixTable – Aggregated matrix table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:6747,update,updated,6747,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['update'],['updated']
Deployability,"N=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; hail-script.py. Next Steps. Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2893,configurat,configuration,2893,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"NPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is set to requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. hail.methods.balding_nichols_model(n_populations, n_samples, n_variants, n_partitions=None, pop_dist=None, fst=None, af_dist=None, reference_genome='default', mixture=False, *, phased=False)[source]; Generate a matrix table of variants, samples, and genotypes using the; Balding-Nichols or Pritchard-Stephens-Donnelly model.; Examples; Generate a matrix table of genotypes with 1000 variants and 100 samples; across 3 populations:; >>> hl.r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:7602,configurat,configuration,7602,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"PartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7458,install,installation,7458,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installation']
Deployability,"R; package ``skat``, but both differ from :math:`Q` in the paper by the factor; :math:`\frac{1}{2\sigma^2}` in the linear case and :math:`\frac{1}{2}` in; the logistic case, where :math:`\sigma^2` is the unbiased estimator of; residual variance for the linear null model. The R package also applies a; ""small-sample adjustment"" to the null distribution in the logistic case; when the sample size is less than 2000. Hail does not apply this; adjustment. The fault flag is an integer indicating whether any issues occurred when; running the Davies algorithm to compute the p-value as the right tail of a; weighted sum of :math:`\chi^2(1)` distributions. +-------------+-----------------------------------------+; | fault value | Description |; +=============+=========================================+; | 0 | no issues |; +------+------+-----------------------------------------+; | 1 | accuracy NOT achieved |; +------+------+-----------------------------------------+; | 2 | round-off error possibly significant |; +------+------+-----------------------------------------+; | 3 | invalid parameters |; +------+------+-----------------------------------------+; | 4 | unable to locate integration parameters |; +------+------+-----------------------------------------+; | 5 | out of memory |; +------+------+-----------------------------------------+. Parameters; ----------; key_expr : :class:`.Expression`; Row-indexed expression for key associated to each row.; weight_expr : :class:`.Float64Expression`; Row-indexed expression for row weights.; y : :class:`.Float64Expression`; Column-indexed response expression.; If `logistic` is ``True``, all non-missing values must evaluate to 0 or; 1. Note that a :class:`.BooleanExpression` will be implicitly converted; to a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:106025,integrat,integration,106025,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Deployability,"Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.truncate_reference_blocks. View page source. hail.vds.truncate_reference_blocks. hail.vds.truncate_reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None)[source]; Cap reference blocks at a maximum length in order to permit faster interval filtering.; Examples; Truncate reference blocks to 5 kilobases:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) . Truncate the longest 1% of reference blocks to the length of the 99th percentile block:; >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) . Notes; After this function has been run, the reference blocks have a known maximum length ref_block_max_length,; stored in the global fields, which permits vds.filter_intervals() to filter to intervals of the reference; data by reading ref_block_max_length bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work.; It is also possible to patch an existing VDS to store the max reference block length with vds.store_ref_block_max_length(). See also; vds.store_ref_block_max_length(). Parameters:. vds (VariantDataset or MatrixTable); max_ref_block_base_pairs – Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction – Fraction of reference block length distribution to truncate / winsorize. Returns:; VariantDataset or MatrixTable. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html:1714,patch,patch,1714,docs/0.2/vds/hail.vds.truncate_reference_blocks.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.truncate_reference_blocks.html,2,"['patch', 'update']","['patch', 'updated']"
Deployability,"Released 2022-10-04. New Features. (#12218) Support; missing values in primitive columns in Table.to_pandas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries muc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:45902,install,install-on-cluster,45902,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install-on-cluster']
Deployability,"Spark version!. Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1948,install,install,1948,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability,"Squared Distribution, we strongly recommend; the introduction of this paper:. Das, Abhranil; Geisler, Wilson (2020). “A method to integrate and classify normal; distributions”. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the cumulative distribution function (CDF).; w (list of float or Expression of type tarray of tfloat64) – A weight for each non-central chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20078,integrat,integration,20078,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"Table tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, you",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18150,configurat,configuration,18150,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"Union[Plot, Column]:; """"""Create an interactive scatter plot. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:30792,continuous,continuous,30792,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"Variant Effect Predictor (VEP). Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1233,install,install,1233,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['install'],['install']
Deployability,"[9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including the reference.; - ``G`` indicates one value per unique diploid genotype. Warning; -------; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample -- with 100,000 samples, nearly 50GB per variant!. See Also; --------; :func:`.lgt_to_gt`. Parameters; ----------; array : :class:`.ArrayExpression`; Array to reindex.; local_alleles : :class:`.ArrayExpression`; Local alleles array.; n_alleles : :class:`.Int32Expression`; Total number of alleles to reindex to.; fill_value; Value to fill in at global indices with no data.; number : :class:`str`; One of 'A', 'R', 'G'. Returns; -------; :class:`.ArrayExpression`; """"""; try:; fill_value = hl.coercer_from_dtype(array.dtype.element_type).coerce(fill_value); except Exception as e:; raise ValueError(f'fill_value type {fill_value.dtype} is incompatible with array type {array.dtype}') from e. if number == 'G':; return _func(""local_to_global_g"", array.dtype, array, local_alleles, n_alleles, fill_value); elif number == 'R':; omit_first = False; elif number == 'A':; omit_first = True; else:; raise ValueError(f'unrecognized number {number}'). return _func(""local_to_global_a_r"", array.dtype, array, local_alleles, n_alleles, fill_value, hl.bool(omit_first)). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/functions.html:3577,update,updated,3577,docs/0.2/_modules/hail/vds/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html,2,['update'],['updated']
Deployability,"[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42372,Configurat,Configuration,42372,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Configurat'],['Configuration']
Deployability,"\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = mayb",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30694,update,update,30694,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"__eq__(other)[source]; Check each field for equality. Parameters:; other (Expression) – An expression of the same type. __ge__(other); Return self>=value. __getitem__(item)[source]; Access a field of the struct by name or index.; Examples; >>> hl.eval(struct['a']); 5. >>> hl.eval(struct[1]); 'Foo'. Parameters:; item (str) – Field name. Returns:; Expression – Struct field. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __ne__(other)[source]; Return self!=value. annotate(**named_exprs)[source]; Add new fields or recompute existing fields.; Examples; >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; If an expression in named_exprs shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters:; named_exprs (keyword args of Expression) – Fields to add. Returns:; StructExpression – Struct with new or updated fields. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. describe(handler=<built-in function print>); Print information about type, index, and dependencies. drop(*fields)[source]; Drop fields from the struct.; Examples; >>> hl.eval(struct.drop('b')); Struct(a=5). Parameters:; fields (varargs of str) – Fields to drop. Returns:; StructExpression – Struct without certain fields. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:3071,update,updated,3071,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['update'],['updated']
Deployability,"_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; appris: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'vqsr': struct {; AS_VQSLOD: float64,; AS_culprit: str,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool; }; 'region_flag': struct {; lcr: bool,; segdup: bool; }; 'allele_info': struct {; variant_type: str,; allele_type: str,; n_alt_alleles: int32,; was_mixed: bool; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html:14673,update,updated,14673,docs/0.2/datasets/schemas/gnomad_genome_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_sites.html,1,['update'],['updated']
Deployability,"_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gerp_scores. View page source. gerp_scores. Versions: hg19; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (hg19, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'N': float64; 'S': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html:9397,update,updated,9397,docs/0.2/datasets/schemas/gerp_scores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gerp_scores.html,1,['update'],['updated']
Deployability,"_apply_map2(lambda _g, _mu: _dominance_encoding(_g, _mu), mu, sparsity_strategy='NeedsDense'); normalized_gd = gd - (variance * (1.0 + f_i)). # Compute IBD2 (k2) estimate; k2 = _gram(normalized_gd) / _gram(variance); ht = ht.annotate(k2=k2.entries()[ht.i, ht.j].entry). if statistics in ['kin20', 'all']:; # Get the numerator used in IBD0 (k0) computation (IBS0), compute indicator matrices for homozygotes; hom_alt = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 2.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); hom_ref = g._apply_map2(; lambda _g, _mu: hl.if_else((_g != 0.0) | hl.is_nan(_mu), 0.0, 1.0), mu, sparsity_strategy='NeedsDense'; ); ibs0 = _AtB_plus_BtA(hom_alt, hom_ref). # Get the denominator used in IBD0 (k0) computation; mu2 = _replace_nan(mu**2.0, 0.0); one_minus_mu2 = _replace_nan((1.0 - mu) ** 2.0, 0.0); k0_denom = _AtB_plus_BtA(mu2, one_minus_mu2). # Compute IBD0 (k0) estimates, correct the estimates where phi <= k0_cutoff; k0 = ibs0 / k0_denom; k0_cutoff = 2.0 ** (-5.0 / 2.0); ht = ht.annotate(k0=k0.entries()[ht.i, ht.j].entry); ht = ht.annotate(k0=hl.if_else(ht.kin <= k0_cutoff, 1.0 - (4.0 * ht.kin) + ht.k2, ht.k0)). if statistics == 'all':; # Finally, compute IBD1 (k1) estimate; ht = ht.annotate(k1=1.0 - (ht.k2 + ht.k0)). # Filter table to only have one row for each distinct pair of samples; ht = ht.filter(ht.i <= ht.j); ht = ht.rename({'k0': 'ibd0', 'k1': 'ibd1', 'k2': 'ibd2'}). if min_kinship is not None:; ht = ht.filter(ht.kin >= min_kinship). if statistics != 'all':; fields_to_drop = {'kin': ['ibd0', 'ibd1', 'ibd2'], 'kin2': ['ibd0', 'ibd1'], 'kin20': ['ibd1']}; ht = ht.drop(*fields_to_drop[statistics]). if not include_self_kinship:; ht = ht.filter(ht.i == ht.j, keep=False). col_keys = hl.literal(mt.select_cols().key_cols_by().cols().collect(), dtype=hl.tarray(mt.col_key.dtype)); return ht.key_by(i=col_keys[hl.int32(ht.i)], j=col_keys[hl.int32(ht.j)]). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:23163,update,updated,23163,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['update'],['updated']
Deployability,"_dir=None,; default_reference=None,; idempotent=False,; global_seed=None,; spark_conf=None,; skip_logging_configuration=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:6973,configurat,configuration,6973,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Muscularis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9718,update,updated,9718,docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Stomach_all_snp_gene_associations. View page source. GTEx_sQTL_Stomach_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Stomach_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Thyroid_all_snp_gene_associations. View page source. GTEx_sQTL_Thyroid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Thyroid_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"_exome_C_ALL_Rec; giant_whr_exome_C_EUR_Add; giant_whr_exome_C_EUR_Rec; giant_whr_exome_M_ALL_Add; giant_whr_exome_M_ALL_Rec; giant_whr_exome_M_EUR_Add; giant_whr_exome_M_EUR_Rec; giant_whr_exome_W_ALL_Add; giant_whr_exome_W_ALL_Rec; giant_whr_exome_W_EUR_Add; giant_whr_exome_W_EUR_Rec; gnomad_annotation_pext; gnomad_base_pext; gnomad_chrM_coverage; gnomad_chrM_sites; gnomad_exome_coverage; gnomad_exome_sites; gnomad_genome_coverage; gnomad_genome_sites; gnomad_hgdp_1kg_subset_dense; gnomad_hgdp_1kg_subset_sample_metadata; gnomad_hgdp_1kg_subset_sparse; gnomad_hgdp_1kg_subset_variant_annotations; gnomad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas.html:17268,update,updated,17268,docs/0.2/datasets/schemas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Transverse_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9706,update,updated,9706,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Mucosa_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; Schema (2.2, GRCh37). panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; ldsc_baselineLD_ldscores. View page source. ldsc_baselineLD_ldscores. Versions: 2.2, 1.1; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (2.2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'annotation': str; 'M_5_50': int32; 'M': int32; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'SNP': str; ----------------------------------------; Entry fields:; 'x': float64; ----------------------------------------; Column key: ['annotation']; Row key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html:9646,update,updated,9646,docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/ldsc_baselineLD_ldscores.html,2,['update'],['updated']
Deployability,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9736,update,updated,9736,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adrenal_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Sigmoid_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html:9754,update,updated,9754,docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Kidney_Cortex_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"_region_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. :param str config: Path to VEP configuration file. :param block_size: Number of variants to annotate per VEP invocation.; :type block_size: int. :param str root: Variant annotation path to store VEP output. :param bool csq: If ``True``, annotates VCF CSQ field as a String.; If ``False``, annotates with the full nested struct schema. :return: An annotated with variant annotations from VEP.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvds.vep(config, root, csq, block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; def variants_table(self):; """"""Convert variants and variant annotations to a KeyTable. The resulting KeyTable has schema:. .. code-block:: text. Struct {; v: Variant; va: variant annotations; }. with a single key ``v``. :return: Key table with variants and variant annotations.; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jvds.variantsKT()). [docs] @handle_py4j; def samples_table(self):; """"""Convert samples and sample annotations to KeyTable. The resulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:227116,configurat,configuration,227116,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Aorta_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html:9694,update,updated,9694,docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Nerve_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9805,update,updated,9805,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9805,update,updated,9805,docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; -------; :class:`.MatrixTable`. Notes; -----; Adds a new row field, `row_field`, and a new column field, `col_field`,; each of which are structs with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58960,pipeline,pipeline,58960,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,4,['pipeline'],['pipeline']
Deployability,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:6775,install,installing,6775,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installing']
Deployability,"a=0).value); 1.0. >>> hl.eval(hl.pgenchisq(-80, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.14284718767288906; >>> hl.eval(hl.pgenchisq(-20, w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.5950150356303258; >>> hl.eval(hl.pgenchisq(10 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.923219534175858; >>> hl.eval(hl.pgenchisq(40 , w=[1, -10, 2], k=[1, 2, 3], lam=[2, 3, 7], mu=-10, sigma=0).value); 0.9971746768781656. Notes; -----. We follow Wikipedia's notational conventions. Some texts refer to the weight vector (our `w`) as; :math:`\lambda` or `lb` and the non-centrality vector (our `lam`) as `nc`. We use the Davies' algorithm which was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:68494,release,released,68494,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['release'],['released']
Deployability,"abels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis to be vertically reversed. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_continuous()[source]; The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_discrete()[source]; The default discrete color scale. This maps each discrete value to a color. Equivalent to scale_color_hue. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_hue()[source]; Map discrete colors to evenly placed positions around the color wheel. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_manual(*, values)[source]; A color scale that assigns strings to colors using the pool of colors specified as values. Parameters:; values (list of str) – The colors to choose when assigning values to colors. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_color_identity()[source]; A color scale that assumes the expression specified in the color aesthetic can be used as a color. Returns:; FigureAttribute – The scale",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:12622,continuous,continuous,12622,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:1909,pipeline,pipeline,1909,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,3,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"ables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where li",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83904,pipeline,pipelines,83904,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223451,release,release-,223451,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['release'],['release-']
Deployability,"ad_ld_scores_afr; gnomad_ld_scores_amr; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_chrM_coverage. View page source. gnomad_chrM_coverage. Versions: 3.1; Reference genome builds: GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'mean': float64; 'median': int32; 'over_100': float64; 'over_1000': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html:9371,update,updated,9371,docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_coverage.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_ALL. View page source. giant_height_exome_ALL. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html:9572,update,updated,9572,docs/0.2/datasets/schemas/giant_height_exome_ALL.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_ALL.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. View page source. GTEx_sQTL_Artery_Aorta_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Artery_Aorta_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. View page source. GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Nerve_Tibial_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_hgdp_1kg_subset_sparse. View page source. gnomad_hgdp_1kg_subset_sparse. Versions: 3.1.2; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (3.1.2, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Entry fields:; 'DP': int32; 'END': int32; 'GQ': int32; 'LA': array<int32>; 'LAD': array<int32>; 'LGT': call; 'LPGT': call; 'LPL': array<int32>; 'MIN_DP': int32; 'PID': str; 'RGQ': int32; 'SB': array<int32>; 'gvcf_info': struct {; ClippingRankSum: float64,; BaseQRankSum: float64,; MQ: float64,; MQRankSum: float64,; MQ_DP: int32,; QUALapprox: int32,; RAW_MQ: float64,; ReadPosRankSum: float64,; VarDP: int32; }; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html:9925,update,updated,9925,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_sparse.html,1,['update'],['updated']
Deployability,"ader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1912,install,install,1912,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install']
Deployability,"ader when importing a VCF and written; to the VCF header when exporting a VCF:. - INFO fields attributes (attached to (`va.info.*`)):. - 'Number': The arity of the field. Can take values. - `0` (Boolean flag),; - `1` (single value),; - `R` (one value per allele, including the reference),; - `A` (one value per non-reference allele),; - `G` (one value per genotype), and; - `.` (any number of values). - When importing: The value in read from the VCF INFO field definition; - When exporting: The default value is `0` for **Boolean**, `.` for **Arrays** and 1 for all other types. - 'Description' (default is ''). - FILTER entries in the VCF header are generated based on the attributes; of `va.filters`. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. :param str ann_path: Variant annotation path starting with 'va', period-delimited. :param str attribute: The attribute to remove (key). :return: Annotated dataset with the updated variant annotation without the attribute.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(self.hc, self._jvds.deleteVaAttribute(ann_path, attribute)). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(propagate_gq=bool,; keep_star_alleles=bool,; max_shift=integral); def split_multi(self, propagate_gq=False, keep_star_alleles=False, max_shift=100):; """"""Split multiallelic variants. .. include:: requireTGenotype.rst. **Examples**. >>> vds.split_multi().write('output/split.vds'). **Notes**. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. split_multi will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic GT field is downcoded once for each; alternate allele. A call for an alternate allele maps to 1 in; the biallelic variant corresponding to itself and 0;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:207970,update,updated,207970,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['update'],['updated']
Deployability,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54215,pipeline,pipelines,54215,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipelines']
Deployability,"ailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:1418,update,update,1418,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49868,pipeline,pipelines,49868,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipelines']
Deployability,"al': array<int64>; 'exp_lof_afr': array<float64>; 'obs_lof_afr': array<int64>; 'exp_lof_amr': array<float64>; 'obs_lof_amr': array<int64>; 'exp_lof_eas': array<float64>; 'obs_lof_eas': array<int64>; 'exp_lof_nfe': array<float64>; 'obs_lof_nfe': array<int64>; 'exp_lof_sas': array<float64>; 'obs_lof_sas': array<int64>; 'pLI': float64; 'pNull': float64; 'pRec': float64; 'oe_lof': float64; 'oe_syn_lower': float64; 'oe_syn_upper': float64; 'oe_mis_lower': float64; 'oe_mis_upper': float64; 'oe_lof_lower': float64; 'oe_lof_upper': float64; 'constraint_flag': set<str>; 'syn_z': float64; 'mis_z': float64; 'lof_z': float64; 'oe_lof_upper_rank': int64; 'oe_lof_upper_bin': int32; 'oe_lof_upper_bin_6': int32; 'n_sites': int64; 'n_sites_array': array<int64>; 'classic_caf': float64; 'max_af': float64; 'classic_caf_array': array<float64>; 'no_lofs': int64; 'obs_het_lof': int64; 'obs_hom_lof': int64; 'defined': int64; 'pop_no_lofs': dict<str, int64>; 'pop_obs_het_lof': dict<str, int64>; 'pop_obs_hom_lof': dict<str, int64>; 'pop_defined': dict<str, int64>; 'p': float64; 'pop_p': dict<str, float64>; 'exp_hom_lof': float64; 'classic_caf_afr': float64; 'classic_caf_amr': float64; 'classic_caf_asj': float64; 'classic_caf_eas': float64; 'classic_caf_fin': float64; 'classic_caf_nfe': float64; 'classic_caf_oth': float64; 'classic_caf_sas': float64; 'p_afr': float64; 'p_amr': float64; 'p_asj': float64; 'p_eas': float64; 'p_fin': float64; 'p_nfe': float64; 'p_oth': float64; 'p_sas': float64; 'transcript_type': str; 'gene_id': str; 'transcript_level': str; 'cds_length': int64; 'num_coding_exons': int64; 'interval': interval<locus<GRCh37>>; 'gene_type': str; 'gene_length': int32; 'exac_pLI': float64; 'exac_obs_lof': int32; 'exac_exp_lof': float64; 'exac_oe_lof': float64; 'brain_expression': float64; ----------------------------------------; Key: ['gene', 'transcript']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html:13328,update,updated,13328,docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_plof_metrics_transcript.html,1,['update'],['updated']
Deployability,"al; language documentation search. Bug fixes. (#8981) Fix; BlockMatrix OOM triggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:69928,pipeline,pipelines,69928,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"alling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:59554,configurat,configuration,59554,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"alues to be downsampled.; label (StringExpression or ArrayExpression) – Additional data for each (x, y) coordinate. Can pass in multiple fields in an ArrayExpression.; n_divisions (int) – Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns:; ArrayExpression – Expression for downsampled coordinate points (x, y). The element type of the array is; ttuple of tfloat64, tfloat64, and tarray of tstr. hail.expr.aggregators.approx_cdf(expr, k=100, *, _raw=False)[source]; Produce a summary of the distribution of values.; Notes; This method returns a struct containing two arrays: values and ranks.; The values array contains an ordered sample of values seen. The ranks; array is one longer, and contains the approximate ranks for the; corresponding values.; These represent a summary of the CDF of the distribution of values. In; particular, for any value x = values(i) in the summary, we estimate that; there are ranks(i) values strictly less than x, and that there are; ranks(i+1) values less than or equal to x. For any value y (not; necessarily in the summary), we estimate CDF(y) to be ranks(i), where i; is such that values(i-1) < y ≤ values(i).; An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value values(i) occupying indices ranks(i) (inclusive) to; ranks(i+1) (exclusive).; The returned struct also contains an array _compaction_counts, which is; used internally to support downstream error estimation. Warning; This is an approximate and nondeterministic method. Parameters:. expr (Expression) – Expression to collect.; k (int) – Parameter controlling the accuracy vs. memory usage tradeoff. Returns:; StructExpression – Struct containing values and ranks arrays. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:35295,update,updated,35295,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['update'],['updated']
Deployability,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:54399,configurat,configuration,54399,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"andas.; (#12195) Add a; impute_sex_chr_ploidy_from_interval_coverage to impute sex ploidy; directly from a coverage MT.; (#12222); Query-on-Batch pipelines now add worker jobs to the same batch as the; driver job instead of producing a new batch per stage.; (#12244) Added; support for custom labels for per-group legends to; hail.ggplot.geom_point via the legend_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching hea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:46011,pipeline,pipelines,46011,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37508,Install,Installation,37508,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Install'],['Installation']
Deployability,"ant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_variant_indices_nfe. View page source. gnomad_ld_variant_indices_nfe. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'min_af': float64; 'rf': struct {; variants_by_type: dict<str, int32>,; feature_medians: dict<str, struct {; variant_type: str,; n_alt_alleles: int32,; qd: float64,; pab_max: float64,; info_MQRankSum: float64,; info_SOR: float64,; info_InbreedingCoeff: float64,; info_ReadPosRankSum: float64,; info_FS: float64,; info_QD: float64,; info_MQ: float64,; info_DP: int32; }>,; test_intervals: array<interval<locus<GRCh37>>>,; test_results: array<struct {; rf_prediction: str,; rf_label: str,; n: int32; }>,; features_importance: dict<str, float64>,; features: array<str>,; vqsr_training: bool,; no_transmitted_singletons: bool,; adj: bool,; rf_hash: str,; rf_snv_cutoff: struct {; bin: int32,; min_score: float64; },; rf_indel_cutoff: struct {; bin: int32,; min_score: float64; }; }; 'freq_meta': array<dict<str, str>>; 'freq_index_dict': dict<str, int32>; 'popmax_index_dict': dict<str, int32>; 'age_index_dict': dict<str, int32>; 'faf_index_dict': dict<str, int32>; 'age_distribution': array<int32>; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html:10454,update,updated,10454,docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_variant_indices_nfe.html,1,['update'],['updated']
Deployability,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Spinal_cord_cervical_c-1_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. View page source. GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Small_Intestine_Terminal_Ileum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html:9766,update,updated,9766,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hippocampus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()'); pprint(Counter(snp_counts).most_common()). [(AltAllele(ref=C, alt=T), 2436L),; (AltAllele(ref=G, alt=A), 2387L),; (AltAllele(ref=A, alt=G), 1944L),; (AltAllele(ref=T, alt=C), 1879L),; (AltAllele(ref=C, alt=A), 496L),; (AltAllele(ref=G, alt=T), 480L),; (AltAllele(ref=T, alt=G), 468L),; (AltAllele(ref=A, alt=C), 454L),; (AltAllele(ref=C, alt=G), 150L),; (AltAllele(ref=G, alt=C), 112L),; (AltAllele(ref=T, alt=A), 79L),; (AltAllele(ref=A, alt=T), 76L)]. It’s nice to see that we can actually uncover something biological from; this small dataset: we see that these frequencies come in pairs. C/T and; G/A are actually the same mutation, just viewed from from opposite; strands. Likewise, T/A and A/T are the same mutation on opposite; strands. There’s a 30x difference between the frequency of C/T and A/T; SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re; starting to hit a wall - the latest gnomAD; release publishes about 250; million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in; the dataset, and this is getting large even for our tiny dataset. Our; 1,000 samples and 10,000 variants produce 10 million unique genotypes.; The gnomAD dataset has about 5 trillion unique genotypes.; Here we will use the hist aggregator to produce and plot a histogram; of DP values for genotypes in our thousand genomes dataset. In [26]:. dp_hist = vds.query_genotypes('gs.map(g => g.dp).hist(0, 30, 30)'); plt.xlim(0, 31); plt.bar(dp_hist.binEdges[1:], dp_hist.binFrequencies); plt.show(). Quality Control¶; QC is where analysts spend most of their time with sequencing datasets.; QC is an iterative process, and is different for every project: there is; no “push-button” solution for QC. Each time the Broad collects a new; group of samples, it finds new batch effects. However, by practicing; open science an",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:12251,release,release,12251,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['release'],['release']
Deployability,"ape_str(s, backticked=True)). def parsable_strings(strs):; strs = ' '.join(f'""{escape_str(s)}""' for s in strs); return f""({strs})"". def _dumps_partitions(partitions, row_key_type):; parts_type = partitions.dtype; if not (isinstance(parts_type, hl.tarray) and isinstance(parts_type.element_type, hl.tinterval)):; raise ValueError(f'partitions type invalid: {parts_type} must be array of intervals'). point_type = parts_type.element_type.point_type. f1, t1 = next(iter(row_key_type.items())); if point_type == t1:; partitions = hl.map(; lambda x: hl.interval(; start=hl.struct(**{f1: x.start}),; end=hl.struct(**{f1: x.end}),; includes_start=x.includes_start,; includes_end=x.includes_end,; ),; partitions,; ); else:; if not isinstance(point_type, hl.tstruct):; raise ValueError(f'partitions has wrong type: {point_type} must be struct or type of first row key field'); if not point_type._is_prefix_of(row_key_type):; raise ValueError(f'partitions type invalid: {point_type} must be prefix of {row_key_type}'). s = json.dumps(partitions.dtype._convert_to_json(hl.eval(partitions))); return s, partitions.dtype. def default_handler():; try:; from IPython.display import display. return display; except ImportError:; return print. def guess_cloud_spark_provider() -> Optional[Literal['dataproc', 'hdinsight']]:; if 'HAIL_DATAPROC' in os.environ:; return 'dataproc'; if 'AZURE_SPARK' in os.environ or 'hdinsight' in os.getenv('CLASSPATH', ''):; return 'hdinsight'; return None. def no_service_backend(unsupported_feature):; from hail import current_backend; from hail.backend.service_backend import ServiceBackend. if isinstance(current_backend(), ServiceBackend):; raise NotImplementedError(; f'{unsupported_feature!r} is not yet supported on the service backend.'; f'\n If this is a pressing need, please alert the team on the discussion'; f'\n forum to aid in prioritization: https://discuss.hail.is'; ). ANY_REGION = ['any_region']. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:18317,update,updated,18317,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['update'],['updated']
Deployability,"ar = (ref.T @ var).checkpoint(hl.utils.new_temp_file()); # We need the count of times the pair is AA,aa and aa,AA. ref_var is only; # AA,aa. Transposing ref_var gives var_ref, i.e. aa,AA.; #; # n.b. (REF.T @ VAR).T == (VAR.T @ REF) by laws of matrix multiply; N_AA_aa = ref_var + ref_var.T; N_Aa_Aa = (het.T @ het).checkpoint(hl.utils.new_temp_file()); # We count the times the row individual has a heterozygous genotype and the; # column individual has any defined genotype at all.; N_Aa_defined = (het.T @ defined).checkpoint(hl.utils.new_temp_file()). het_hom_balance = N_Aa_Aa - (2 * N_AA_aa); het_hom_balance = het_hom_balance.to_matrix_table_row_major(); n_hets_for_rows = N_Aa_defined.to_matrix_table_row_major(); n_hets_for_cols = N_Aa_defined.T.to_matrix_table_row_major(). kinship_between = het_hom_balance.rename({'element': 'het_hom_balance'}); kinship_between = kinship_between.annotate_entries(; n_hets_row=n_hets_for_rows[kinship_between.row_key, kinship_between.col_key].element,; n_hets_col=n_hets_for_cols[kinship_between.row_key, kinship_between.col_key].element,; ). col_index_field = Env.get_uid(); col_key = mt.col_key; cols = mt.add_col_index(col_index_field).key_cols_by(col_index_field).cols(). kinship_between = kinship_between.key_cols_by(**cols[kinship_between.col_idx].select(*col_key)). renaming, _ = deduplicate(list(col_key), already_used=set(col_key)); assert len(renaming) == len(col_key). kinship_between = kinship_between.key_rows_by(; **cols[kinship_between.row_idx].select(*col_key).rename(dict(renaming)); ). kinship_between = kinship_between.annotate_entries(; min_n_hets=hl.min(kinship_between.n_hets_row, kinship_between.n_hets_col); ); return (; kinship_between.select_entries(; phi=(0.5); + (; (2 * kinship_between.het_hom_balance + -kinship_between.n_hets_row - kinship_between.n_hets_col); / (4 * kinship_between.min_n_hets); ); ); .select_rows(); .select_cols(); .select_globals(); ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:10801,update,updated,10801,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['update'],['updated']
Deployability,"aram log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:2453,configurat,configuration,2453,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['configurat'],['configuration']
Deployability,"are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of :math:`x_2`. Let :math:`y_1`, :math:`y_2`, and :math:`y_3` be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. .. math::. \\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\\frac{1}{2 \sigma^2}. The standard error :math:`\\hat{\sigma}` is then estimated by solving for :math:`\sigma`. Note that the mean and standard deviation of the (discretized or continuous) distribution held in ``global.lmmreg.fit.normLkhdH2`` will not coincide with :math:`\\hat{h}^2` and :math:`\\hat{\sigma}`, since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on :math:`h^2`. **Testing each variant for association**. Fixing a single variant, we define:. - :math:`v = n \\times 1` vector of genotypes, with missing genotypes imputed as the mean of called genotypes; - :math:`X_v = \\left[v | X \\right] = n \\times (1 + c)` matrix concatenating :math:`v` and :math:`X`; - :math:`\\beta_v = (\\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v) = (1 + c) \\times 1` vector of covariate coefficients. Fixing :math:`\delta` at the global REML estimate :math:`\\hat{\delta}`, we find the REML estimate :math:`(\\hat{\\beta}_v, \\hat{\si",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:133783,continuous,continuous,133783,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['continuous'],['continuous']
Deployability,ariant_indices_EAS.rst.txt; panukb_ld_variant_indices_EUR.rst.txt; panukb_ld_variant_indices_MID.rst.txt; panukb_meta_analysis_all_ancestries.rst.txt; panukb_meta_analysis_high_quality.rst.txt; panukb_summary_stats.rst.txt; UK_Biobank_Rapid_GWAS_both_sexes.rst.txt; UK_Biobank_Rapid_GWAS_female.rst.txt; UK_Biobank_Rapid_GWAS_male.rst.txt. schemas.rst.txt. /experimental; ; hail.experimental.DB.rst.txt; index.rst.txt; ldscsim.rst.txt. /functions; ; collections.rst.txt; constructors.rst.txt; core.rst.txt; genetics.rst.txt; hail.expr.builders.CaseBuilder.rst.txt; hail.expr.builders.SwitchBuilder.rst.txt; index.rst.txt; numeric.rst.txt; random.rst.txt; stats.rst.txt; string.rst.txt. /genetics; ; hail.genetics.AlleleType.rst.txt; hail.genetics.Call.rst.txt; hail.genetics.Locus.rst.txt; hail.genetics.Pedigree.rst.txt; hail.genetics.ReferenceGenome.rst.txt; hail.genetics.Trio.rst.txt; index.rst.txt. /ggplot; ; index.rst.txt. /guides; ; agg.rst.txt; annotation.rst.txt; genetics.rst.txt. /install; ; azure.rst.txt; dataproc.rst.txt; linux.rst.txt; macosx.rst.txt; other-cluster.rst.txt; try.rst.txt. /linalg; . /utils; ; index.rst.txt. hail.linalg.BlockMatrix.rst.txt; index.rst.txt. /methods; ; genetics.rst.txt; impex.rst.txt; index.rst.txt; misc.rst.txt; relatedness.rst.txt; stats.rst.txt. /nd; ; index.rst.txt. /overview; ; expressions.rst.txt; index.rst.txt; matrix_table.rst.txt; table.rst.txt. /stats; ; hail.stats.LinearMixedModel.rst.txt; index.rst.txt. /tutorials; ; 01-genome-wide-association-study.ipynb.txt; 03-tables.ipynb.txt; 04-aggregation.ipynb.txt; 05-filter-annotate.ipynb.txt; 06-joins.ipynb.txt; 07-matrixtable.ipynb.txt; 08-plotting.ipynb.txt; 09-ggplot.ipynb.txt. /utils; ; index.rst.txt. /vds; ; hail.vds.combiner.load_combiner.rst.txt; hail.vds.combiner.new_combiner.rst.txt; hail.vds.combiner.VariantDatasetCombiner.rst.txt; hail.vds.combiner.VDSMetadata.rst.txt; hail.vds.filter_chromosomes.rst.txt; hail.vds.filter_intervals.rst.txt; hail.vds.filter_samples.rst.txt; ,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index-wcopy.html:17786,install,install,17786,index-wcopy.html,https://hail.is,https://hail.is/index-wcopy.html,1,['install'],['install']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AFR. View page source. giant_height_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AFR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_AMR. View page source. giant_height_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_AMR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EAS. View page source. giant_height_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EAS.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_EUR. View page source. giant_height_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_EUR.html,1,['update'],['updated']
Deployability,"ariant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_height_exome_SAS. View page source. giant_height_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html:9579,update,updated,9579,docs/0.2/datasets/schemas/giant_height_exome_SAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_height_exome_SAS.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. View page source. GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adrenal_Gland_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Tibial_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Tibial_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. View page source. GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Colon_Sigmoid_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. View page source. GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html:9697,update,updated,9697,docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Kidney_Cortex_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ariant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9808,update,updated,9808,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ark 2.1.0. Unzip the distribution after you download it. Next, edit and copy the below bash commands to set up the Hail; environment variables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:1980,install,install,1980,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['install']
Deployability,"array and return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item); Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:7244,pipeline,pipeline,7244,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['pipeline'],['pipeline']
Deployability,"as; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9721,update,updated,9721,docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"as_bias_corrected),; __step2_jackknife_variance=(; hl.sum(mt.__step2_block_betas_bias_corrected**2); - hl.sum(mt.__step2_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ). # combine step 1 and step 2 block jackknifes; mt = mt.annotate_entries(; __step2_initial_w=1.0; / (mt.__w_initial_floor * 2.0 * (mt.__initial_betas[0] + +mt.__initial_betas[1] * mt.__x_floor) ** 2); ). mt = mt.annotate_cols(; __final_betas=[mt.__step1_betas[0], mt.__step2_betas[1]],; __c=(hl.agg.sum(mt.__step2_initial_w * mt.__x) / hl.agg.sum(mt.__step2_initial_w * mt.__x**2)),; ). mt = mt.annotate_cols(; __final_block_betas=hl.map(; lambda i: (mt.__step2_block_betas[i] - mt.__c * (mt.__step1_block_betas[i][0] - mt.__final_betas[0])),; hl.range(0, n_blocks),; ); ). mt = mt.annotate_cols(; __final_block_betas_bias_corrected=(n_blocks * mt.__final_betas[1] - (n_blocks - 1) * mt.__final_block_betas); ). mt = mt.annotate_cols(; __final_jackknife_mean=[mt.__step1_jackknife_mean[0], hl.mean(mt.__final_block_betas_bias_corrected)],; __final_jackknife_variance=[; mt.__step1_jackknife_variance[0],; (; hl.sum(mt.__final_block_betas_bias_corrected**2); - hl.sum(mt.__final_block_betas_bias_corrected) ** 2 / n_blocks; ); / (n_blocks - 1); / n_blocks,; ],; ). # convert coefficient to heritability estimate; mt = mt.annotate_cols(; phenotype=mt.__y_name,; mean_chi_sq=hl.agg.mean(mt.__y),; intercept=hl.struct(estimate=mt.__final_betas[0], standard_error=hl.sqrt(mt.__final_jackknife_variance[0])),; snp_heritability=hl.struct(; estimate=(M / hl.agg.mean(mt.__n)) * mt.__final_betas[1],; standard_error=hl.sqrt((M / hl.agg.mean(mt.__n)) ** 2 * mt.__final_jackknife_variance[1]),; ),; ). # format and return results; ht = mt.cols(); ht = ht.key_by(ht.phenotype); ht = ht.select(ht.mean_chi_sq, ht.intercept, ht.snp_heritability). ht_tmp_file = new_temp_file(); ht.write(ht_tmp_file); ht = hl.read_table(ht_tmp_file). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:18103,update,updated,18103,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,2,['update'],['updated']
Deployability,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Liver_all_snp_gene_associations. View page source. GTEx_eQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html:9673,update,updated,9673,docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Liver_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Ovary_all_snp_gene_associations. View page source. GTEx_eQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html:9673,update,updated,9673,docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Ovary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174466,configurat,configuration,174466,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"ataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2601,Patch,Patches,2601,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['Patch'],['Patches']
Deployability,"ate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. Miscellaneous. grep(regex, path[, max_count, show, force, ...]); Searches given paths for all lines containing regex matches. maximal_independent_set(i, j[, keep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. [1]; Purcell, Shaun et al. “PLINK: a tool set for whole-genome association and; population-based linkage analyses.” American journal of human genetics; vol. 81,3 (2007):; 559-75. doi:10.1086/519795. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838/. [2]; Manichaikul, Ani et al. “Robust relationship inference in genome-wide; association studies.” Bioinformatics (Oxford, England) vol. 26,22 (2010):; 2867-73. doi:10.1093/bioinformatics/btq559. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/. [3]; Conomos, Matthew P et al. “Model-free Estimation of Recent Genetic; Relatedness.” American journal of human genetics vol. 98,1 (2016):; 127-48. doi:10.1016/j.ajhg.2015.11.022. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4716688/. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:8850,update,updated,8850,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['update'],['updated']
Deployability,"ategorical label. Returns:; bokeh.models.Plot if no label or a single label was given, otherwise bokeh.models.layouts.Column. hail.plot.qq(pvals, label=None, title='Q-Q plot', xlabel='Expected -log10(p)', ylabel='Observed -log10(p)', size=6, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot); If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive qq plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:10279,continuous,continuous,10279,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"ation of the called alleles. unphased_diploid_gt_index; Return the genotype index for unphased, diploid calls. property alleles; Get the alleles of this call. Returns:; list of int. is_diploid()[source]; True if the ploidy == 2. Return type:; bool. is_haploid()[source]; True if the ploidy == 1. Return type:; bool. is_het()[source]; True if the call contains two different alleles. Return type:; bool. is_het_non_ref()[source]; True if the call contains two different alternate alleles. Return type:; bool. is_het_ref()[source]; True if the call contains one reference and one alternate allele. Return type:; bool. is_hom_ref()[source]; True if the call has no alternate alleles. Return type:; bool. is_hom_var()[source]; True if the call contains identical alternate alleles. Return type:; bool. is_non_ref()[source]; True if the call contains any non-reference alleles. Return type:; bool. n_alt_alleles()[source]; Returns the count of non-reference alleles. Return type:; int. one_hot_alleles(n_alleles)[source]; Returns a list containing the one-hot encoded representation of the; called alleles.; Examples; >>> n_alleles = 2; >>> hom_ref = hl.Call([0, 0]); >>> het = hl.Call([0, 1]); >>> hom_var = hl.Call([1, 1]). >>> het.one_hot_alleles(n_alleles); [1, 1]. >>> hom_var.one_hot_alleles(n_alleles); [0, 2]. Notes; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Parameters:; n_alleles (int) – Number of total alleles, including the reference. Returns:; list of int. property phased; True if the call is phased. Returns:; bool. property ploidy; The number of alleles for this call. Returns:; int. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls. Returns:; int. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html:3966,update,updated,3966,docs/0.2/genetics/hail.genetics.Call.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.Call.html,1,['update'],['updated']
Deployability,"ation=""log10""). [docs]def scale_y_log10(name=None):; """"""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""log10""). [docs]def scale_x_reverse(name=None):; """"""Transforms x-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, transformation=""reverse""). [docs]def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Sup",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:8936,continuous,continuous,8936,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['continuous'],['continuous']
Deployability,"ator can be used as hl.scan.count to add an; index along the rows of a table or the rows or columns of a matrix table; the; two statements below produce identical tables:; >>> ht_with_idx = ht.add_index(); >>> ht_with_idx = ht.annotate(idx=hl.scan.count()). For example, to compute a cumulative sum for a row field in a table:; >>> ht_scan = ht.select(ht.Z, cum_sum=hl.scan.sum(ht.Z)); >>> ht_scan.show(); +-------+-------+---------+; | ID | Z | cum_sum |; +-------+-------+---------+; | int32 | int32 | int64 |; +-------+-------+---------+; | 1 | 4 | 0 |; | 2 | 3 | 4 |; | 3 | 3 | 7 |; | 4 | 2 | 10 |; +-------+-------+---------+. Note that the cumulative sum is exclusive of the current row’s value. On a; matrix table, to compute the cumulative number of non-reference genotype calls; along the genome:; >>> ds_scan = ds.select_rows(ds.variant_qc.n_non_ref,; ... cum_n_non_ref=hl.scan.sum(ds.variant_qc.n_non_ref)); >>> ds_scan.rows().show(); +---------------+------------+-----------+---------------+; | locus | alleles | n_non_ref | cum_n_non_ref |; +---------------+------------+-----------+---------------+; | locus<GRCh37> | array<str> | int64 | int64 |; +---------------+------------+-----------+---------------+; | 20:10579373 | [""C"",""T""] | 1 | 0 |; | 20:10579398 | [""C"",""T""] | 1 | 1 |; | 20:10627772 | [""C"",""T""] | 2 | 2 |; | 20:10633237 | [""G"",""A""] | 69 | 4 |; | 20:10636995 | [""C"",""T""] | 2 | 73 |; | 20:10639222 | [""G"",""A""] | 22 | 75 |; | 20:13763601 | [""A"",""G""] | 2 | 97 |; | 20:16223922 | [""T"",""C""] | 66 | 99 |; | 20:17479617 | [""G"",""A""] | 9 | 165 |; +---------------+------------+-----------+---------------+. Scans over column fields can be done in a similar manner. Danger; Computing the result of certain aggregators, such as; hardy_weinberg_test(), can be very expensive when done; for every row in a scan.”. See the Aggregators module for documentation on the behavior; of specific aggregators. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/scans.html:3063,update,updated,3063,docs/0.2/scans.html,https://hail.is,https://hail.is/docs/0.2/scans.html,1,['update'],['updated']
Deployability,"atsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Azure HDInsight. View page source. Use Hail on Azure HDInsight; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl hdinsight which starts, stops, and; manipulates Hail-enabled HDInsight clusters.; Start an HDInsight cluster named “my-first-cluster”. Cluster names may only contain lowercase; letters, uppercase letter, and numbers. You must already have a storage account and resource; group.; hailctl hdinsight start MyFirstCluster MyStorageAccount MyResourceGroup. Be sure to record the generated http password so that you can access the cluster.; Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl hdinsight submit MyFirstCluster MyStorageAccount HTTP_PASSWORD MyResourceGroup hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also connect to a Jupyter Notebook running on the cluster at; https://MyFirstCluster.azurehdinisght.net/jupyter; When you are finished with the cluster stop it:; hailctl hdinsight stop MyFirstCluster MyStorageAccount MyResourceGroup. Next Steps. Read more about Hail on Azure HDInsight; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/azure.html:2390,update,updated,2390,docs/0.2/install/azure.html,https://hail.is,https://hail.is/docs/0.2/install/azure.html,1,['update'],['updated']
Deployability,"atten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95046,update,update,95046,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"ax_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_block_max_length=False). if VariantDataset.ref_block_max_length_field in vds.reference_data.globals:; warning(f""VDS at {vds_path} already contains a global annotation with the max reference block length""); return; rd = vds.reference_data; rd = rd.annotate_rows(__start_pos=rd.locus.position); fs = hl.current_backend().fs; ref_block_max_len = rd.aggregate_entries(hl.agg.max(rd.END - rd.__start_pos + 1))",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2832,patch,patch,2832,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['patch'],['patch']
Deployability,"ay of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; conseque",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60277,configurat,configuration,60277,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"bals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46039,configurat,configuration,46039,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"bic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coeffici",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97971,configurat,configuration,97971,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80169,pipeline,pipelines,80169,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipelines']
Deployability,"bles[0]; if any(head.key.dtype != t.key.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same key type:\n '; + '\n '.join(str(t.key.dtype) for t in tables); ); if any(head.row.dtype != t.row.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same row type\n '; + '\n '.join(str(t.row.dtype) for t in tables); ); if any(head.globals.dtype != t.globals.dtype for t in tables):; raise TypeError(; 'All input tables to multi_way_zip_join must have the same global type\n '; + '\n '.join(str(t.globals.dtype) for t in tables); ); return Table(ir.TableMultiWayZipJoin([t._tir for t in tables], data_field_name, global_field_name)). def _group_within_partitions(self, name, n):; def grouping_func(part):; groups = part.grouped(n); key_names = list(self.key); return groups.map(lambda group: group[0].select(*key_names, **{name: group})). return self._map_partitions(grouping_func). @typecheck_method(f=func_spec(1, expr_stream(expr_struct()))); def _map_partitions(self, f):; rows_uid = 'tmp_rows_' + Env.get_uid(); globals_uid = 'tmp_globals_' + Env.get_uid(); expr = construct_expr(; ir.Ref(rows_uid, hl.tstream(self.row.dtype)), hl.tstream(self.row.dtype), self._row_indices; ); body = f(expr); result_t = body.dtype; if any(k not in result_t.element_type for k in self.key):; raise ValueError('Table._map_partitions must preserve key fields'). body_ir = ir.Let('global', ir.Ref(globals_uid, self._global_type), body._ir); return Table(ir.TableMapPartitions(self._tir, globals_uid, rows_uid, body_ir, len(self.key), len(self.key))). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; return Env.backend().execute(; ir.TableToValueApply(; self.select().select_globals()._tir,; {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions},; ); ). table_type.set(Table). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:136644,update,updated,136644,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['update'],['updated']
Deployability,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10884,configurat,configuration,10884,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['configurat'],['configuration']
Deployability,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:8969,install,installed,8969,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,2,"['install', 'update']","['installed', 'updated']"
Deployability,"bytearray(f.read()))). Notes; The supported modes are:. 'r' – Readable text file (io.TextIOWrapper). Default behavior.; 'w' – Writable text file (io.TextIOWrapper).; 'x' – Exclusive writable text file (io.TextIOWrapper).; Throws an error if a file already exists at the path.; 'rb' – Readable binary file (io.BufferedReader).; 'wb' – Writable binary file (io.BufferedWriter).; 'xb' – Exclusive writable binary file (io.BufferedWriter).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters:. path (str) – Path to file.; mode (str) – File access mode.; buffer_size (int) – Buffer size, in bytes. Returns:; Readable or writable file handle. hailtop.fs.remove(path, *, requester_pays_config=None)[source]; Removes the file at path. If the file does not exist, this function does; nothing. path must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters:; path (str). hailtop.fs.rmtree(path, *, requester_pays_config=None)[source]; Recursively remove all files under the given path. On a local filesystem,; this removes the directory tree at path. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with path. As such,; path must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters:; path (str). hailtop.fs.stat(path, *, requester_pays_config=None)[source]; Returns information about the file or directory at a given path.; Notes; Raises an error if path does not exist.; The resulting dictionary contains the following data:. is_dir (bool) – Path is a directory.; size_bytes (int) – Size in bytes.; size (str) – Size as a readable string.; modification_time (str) – Time of last file modification.; owner (str) – Owner.; path (str) – Path. Parameters:; path (str). Returns:; dict. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/fs_api.html:7204,update,updated,7204,docs/0.2/fs_api.html,https://hail.is,https://hail.is/docs/0.2/fs_api.html,1,['update'],['updated']
Deployability,"call_fields:; warning(; ""Mismatch between 'call_fields' and VDS call fields. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30517,update,update,30517,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"ce. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1485,configurat,configuration,1485,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['configurat'],['configuration']
Deployability,"ce: str,; motif_feature_consequences: array<struct {; allele_num: int32,; consequence_terms: array<str>,; high_inf_pos: str,; impact: str,; minimised: int32,; motif_feature_id: str,; motif_name: str,; motif_pos: int32,; motif_score_change: float64,; strand: int32,; variant_allele: str; }>,; regulatory_feature_consequences: array<struct {; allele_num: int32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'allele_info': struct {; BaseQRankSum: float64,; ClippingRankSum: float64,; DB: bool,; DP: int32,; DS: bool,; END: int32,; FS: float64,; HaplotypeScore: float64,; InbreedingCoeff: float64,; MQ: float64,; MQRankSum: float64,; NEGATIVE_TRAIN_SITE: bool,; POSITIVE_TRAIN_SITE: bool,; QD: float64,; ReadPosRankSum: float64,; SOR: float64,; VQSLOD: float64,; culprit: str; }; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html:15398,update,updated,15398,docs/0.2/datasets/schemas/gnomad_exome_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_sites.html,1,['update'],['updated']
Deployability,"ces_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gencode. View page source. gencode. Versions: v19, v31, v35; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (v35, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh38>>; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; 'level': int32; 'gene_id': str; 'gene_type': str; 'ccdsid': str; 'exon_id': str; 'exon_number': int32; 'havana_gene': str; 'transcript_type': str; 'protein_id': str; 'gene_name': str; 'transcript_name': str; 'transcript_id': str; 'transcript_support_level': str; 'hgnc_id': str; 'ont': str; 'havana_transcript': str; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gencode.html:9698,update,updated,9698,docs/0.2/datasets/schemas/gencode.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gencode.html,1,['update'],['updated']
Deployability,"ces_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. View page source. GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html:9778,update,updated,9778,docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Breast_Mammary_Tissue_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"check_keys('drop', f, protected_key); row_fields = set(table.row); to_drop = [f for f in fields_to_drop if f in row_fields]; table = table._select('drop', table.row.drop(*to_drop)). return table. [docs] @typecheck_method(; output=str, types_file=nullable(str), header=bool, parallel=nullable(ir.ExportType.checker), delimiter=str; ); def export(self, output, types_file=None, header=True, parallel=None, delimiter='\t'):; """"""Export to a text file. Examples; --------; Export to a tab-separated file:. >>> table1.export('output/table1.tsv.bgz'). Note; ----; It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python's ``gzip.open``; and R's ``read.table``. Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use :meth:`flatten` first. Warning; -------; Do not export to a path that is being read from in the same pipeline. See Also; --------; :meth:`flatten`, :meth:`write`. Parameters; ----------; output : :class:`str`; URI at which to write exported file.; types_file : :class:`str`, optional; URI at which to write file containing field type information.; header : :obj:`bool`; Include a header in the file.; parallel : :class:`str`, optional; If None, a single file is produced, otherwise a; folder of file shards is produced. If 'separate_header',; the header file is output separately from the file shards. If; 'header_per_shard', each file shard has a header. If set to None; the export will be slower.; delimiter : :class:`str`; Field delimiter.; """"""; hl.current_backend().validate_file(output). parallel = ir.ExportType.default(parallel); Env.backend().execute(; ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:53565,pipeline,pipeline,53565,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability,"ckend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,; input_file=None,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ); env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env); b.create_job(; vep_config.image,; vep_config.batch_run_csq_header_command,; attributes={'name': 'csq-header'},; resources={'cpu': '1', 'memory': 'standard'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tol",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:33515,update,update,33515,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['update'],['update']
Deployability,"cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_comma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28802,configurat,configuration,28802,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_STRS[self]. @classmethod; def _missing_(cls, value):; if not isinstance(value, str):; return None; return cls.__members__.get(value.upper()). [docs] @staticmethod; def strings():; """"""Returns the names of the allele types, for use with; :func:`~hail.expr.functions.literal`; """"""; return list(_ALLELE_STRS). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:2400,update,updated,2400,docs/0.2/_modules/hail/genetics/allele_type.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html,2,['update'],['updated']
Deployability,"column of the matrix table to compute the; mean of the GQ field, and removes columns where the result is smaller than 20.; Annotate; MatrixTable has four methods to add new fields or update existing fields:. MatrixTable.annotate_globals(); MatrixTable.annotate_rows(); MatrixTable.annotate_cols(); MatrixTable.annotate_entries(). Annotate methods take keyword arguments where the key is the name of the new; field to add and the value is an expression specifying what should be added.; The simplest example is adding a new global field foo that just contains the constant; 5.; >>> mt_new = mt.annotate_globals(foo = 5); >>> print(mt_new.globals.dtype.pretty()); struct {; foo: int32; }. Another example is adding a new row field call_rate which computes the fraction; of non-missing entries GT per row:; >>> mt_new = mt.annotate_rows(call_rate = hl.agg.fraction(hl.is_defined(mt.GT))). Annotate methods are also useful for updating values. For example, to update the; GT entry field to be missing if GQ is less than 20, we can do the following:; >>> mt_new = mt.annotate_entries(GT = hl.or_missing(mt.GQ >= 20, mt.GT)). Select; Select is used to create a new schema for a dimension of the matrix table. Key; fields are always preserved even when not selected. For example, following the; matrix table schemas from importing a VCF file (shown above),; to create a hard calls dataset where each entry only contains the GT field; we can do the following:; >>> mt_new = mt.select_entries('GT'); >>> print(mt_new.entry.dtype.pretty()); struct {; GT: call; }. MatrixTable has four select methods that select and create new fields:. MatrixTable.select_globals(); MatrixTable.select_rows(); MatrixTable.select_cols(); MatrixTable.select_entries(). Each method can take either strings referring to top-level fields, an attribute; reference (useful for accessing nested fields), as well as keyword arguments; KEY=VALUE to compute new fields. The Python unpack operator ** can be; used to specify that all fields",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:6907,update,update,6907,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['update'],['update']
Deployability,"cores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_reference_genome. View page source. Ensembl_homo_sapiens_reference_genome. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'reference_allele': str; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html:9490,update,updated,9490,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_reference_genome.html,1,['update'],['updated']
Deployability,"ct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and MatrixTable; is a struct:. Table.row(); Table.globals(); MatrixTable.row(); MatrixTable.col(); MatrixTable.entry(); MatrixTable.globals(). Structs appear below the top-level component types as well. Consider the following join:; >>> new_table = table1.annotate(table2_fields = table2.index(table1.key)). This snippet adds a field to table1 called table2_fields. In the new table,; table2_fields will be a struct containing all the non-key fields from table2. Parameters:; field_types (keyword args of HailType) – Fields. See also; StructExpression, Struct. class hail.expr.types.ttuple(*types)[source]; Hail type for tuples.; In Python, these are represented as tuple. Parameters:; types (varargs of HailType) – Element types. See also; TupleExpression. hail.expr.types.tcall = dtype('call'); Hail type for a diploid genotype.; In Python, these are represented by Call. See also; CallExpression, Call, call(), parse_call(), unphased_diploid_gt_index_call(). class hail.expr.types.tlocus(reference_genome='default')[source]; Hail type for a genomic coordinate with a contig and a position.; In Python, these are represented by Locus. Parameters:; reference_genome (ReferenceGenome or str) – Reference genome to use. See also; LocusExpression, locus(), parse_locus(), Locus. reference_genome; Reference genome. Returns:; ReferenceGenome – Reference genome. class hail.expr.types.tinterval(point_type)[source]; Hail type for intervals of ordered values.; In Python, these are represented by Interval. Parameters:; point_type (HailType) – Interval point type. See also; IntervalExpression, Interval, interval(), parse_locus_interval(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:11849,update,updated,11849,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['update'],['updated']
Deployability,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/misc.html:7647,update,updated,7647,docs/0.2/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/methods/misc.html,1,['update'],['updated']
Deployability,"ction.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43054,configurat,configuration,43054,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"ctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an import_table or; import_vcf which used a glob expression. In such a case, only the; first 1000 files would have been included in the resulting Table or; MatrixTable.; (#13550); hl.utils.range_table(n) now supports all valid 32-bit signed; integer values of n.; (#13500) In; Query-on-Batch, the client-side Python code will not try to list; every job when a QoB batch fails. This could take hours for; long-running pipelines or pipelines with many partitions. Deprecations. (#13275) Hail no; longer officially supports Python 3.8.; (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:28934,pipeline,pipelines,28934,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['pipeline'],['pipelines']
Deployability,"cussed; below). Referencing Fields; Each Table object has all of its row fields and global fields as; attributes in its namespace. This means that the row field ID can be accessed; from table ht with ht.Sample or ht['Sample']. If ht also had a; global field G, then it could be accessed by either ht.G or ht['G'].; Both row fields and global fields are top level fields. Be aware that accessing; a field with the dot notation will not work if the field name has spaces or; special characters in it. The Python type of each attribute is an; Expression that also contains context about its type and source, in; this case a row field of table ht.; >>> ht ; <hail.table.Table at 0x110791a20>. >>> ht.ID ; <Int32Expression of type int32>. Updating Fields; Add or remove row fields from a Table with Table.select() and; Table.drop().; >>> ht.drop('C1', 'C2'); >>> ht.drop(*['C1', 'C2']). >>> ht.select(ht.ID, ht.SEX); >>> ht.select(*['ID', 'C3']). Use Table.annotate() to add new row fields or update the values of; existing row fields and use Table.filter() to either keep or remove; rows based on a condition:; >>> ht_new = ht.filter(ht['C1'] >= 10); >>> ht_new = ht_new.annotate(id_times_2 = ht_new.ID * 2). Aggregation; To compute an aggregate statistic over the rows of; a dataset, Hail provides an Table.aggregate() method which can be passed; a wide variety of aggregator functions (see Aggregators):; >>> ht.aggregate(hl.agg.fraction(ht.SEX == 'F')); 0.5. We also might want to compute the mean value of HT for each sex. This is; possible with a combination of Table.group_by() and; GroupedTable.aggregate():; >>> ht_agg = (ht.group_by(ht.SEX); ... .aggregate(mean = hl.agg.mean(ht.HT))); >>> ht_agg.show(); +-----+----------+; | SEX | mean |; +-----+----------+; | str | float64 |; +-----+----------+; | ""F"" | 6.50e+01 |; | ""M"" | 6.85e+01 |; +-----+----------+. Note that the result of ht.group_by(...).aggregate(...) is a new; Table while the result of ht.aggregate(...) is a Python value. Joi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:4439,update,update,4439,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['update'],['update']
Deployability,"cy'. x_span = data.bin_edges[-1] - data.bin_edges[0]; x_start = data.bin_edges[0] - 0.05 * x_span; x_end = data.bin_edges[-1] + 0.05 * x_span; p = figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; background_fill_color='#EEEEEE',; x_range=(x_start, x_end),; ); q = p.quad(; bottom=0,; top=data.bin_freq,; left=data.bin_edges[:-1],; right=data.bin_edges[1:],; legend_label=legend,; line_color='black',; ); if data.n_larger > 0:; p.quad(; bottom=0,; top=data.n_larger,; left=data.bin_edges[-1],; right=(data.bin_edges[-1] + (data.bin_edges[1] - data.bin_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13100,update,update,13100,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"d False is output as '1'. The; default and missing values are 'NA'.; varid (StringExpression, optional) – Expression for the variant ID (2nd column of the BIM file). The default; value is hl.delimit([dataset.locus.contig, hl.str(dataset.locus.position), dataset.alleles[0], dataset.alleles[1]], ':'); cm_position (Float64Expression, optional) – Expression for the 3rd column of the BIM file (position in centimorgans).; The default value is 0.0. The missing value is 0.0. hail.methods.get_vcf_metadata(path)[source]; Extract metadata from VCF header.; Examples; >>> hl.get_vcf_metadata('data/example2.vcf.bgz') ; {'filter': {'LowQual': {'Description': ''}, ...},; 'format': {'AD': {'Description': 'Allelic depths for the ref and alt alleles in the order listed',; 'Number': 'R',; 'Type': 'Integer'}, ...},; 'info': {'AC': {'Description': 'Allele count in genotypes, for each ALT allele, in the same order as listed',; 'Number': 'A',; 'Type': 'Integer'}, ...}}. Notes; This method parses the VCF header to extract the ID, Number,; Type, and Description fields from FORMAT and INFO lines as; well as ID and Description for FILTER lines. For example,; given the following header lines:; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read Depth"">; ##FILTER=<ID=LowQual,Description=""Low quality"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">. The resulting Python dictionary returned would be; metadata = {'filter': {'LowQual': {'Description': 'Low quality'}},; 'format': {'DP': {'Description': 'Read Depth',; 'Number': '1',; 'Type': 'Integer'}},; 'info': {'MQ': {'Description': 'RMS Mapping Quality',; 'Number': '1',; 'Type': 'Float'}}}. which can be used with export_vcf() to fill in the relevant fields in the header. Parameters:; path (str) – VCF file(s) to read. If more than one file is given, the first; file is used. Returns:; dict of str to (dict of str to (dict of str to str)). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:61482,update,updated,61482,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['update'],['updated']
Deployability,"d in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; max_iterations : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is ``1e5``.; min_accuracy : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is ``1e-5``. Returns; -------; :class:`.StructExpression`; This method returns a structure with the value as well as information about the numerical; integration. - value : :class:`.Float64Expression`. If converged is true, the value of the CDF evaluated; at `x`. Otherwise, this is the last value the integration evaluated before aborting. - n_iterations : :class:`.Int32Expression`. The number of iterations before stopping. - converged : :class:`.BooleanExpression`. True if the `min_accuracy` was achieved and round; off error is not likely significant. - fault : :class:`.Int32Expression`. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:71038,integrat,integration,71038,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integration']
Deployability,"d return the result.; Examples; >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding item. Parameters:; item (Expression) – Element to append, same type as the array element type. Returns:; ArrayExpression. collect(_localize=True); Collect all records of an expression into a local list.; Examples; Collect all the values from C1:; >>> table1.C1.collect(); [2, 2, 10, 11]. Warning; Extremely experimental. Warning; The list of records may be very large. Returns:; list. contains(item)[source]; Returns a boolean indicating whether item is found in the array.; Examples; >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters:; item (Expression) – Item for inclusion test. Warning; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (set()). Returns:; BooleanExpression – True if the element is found in the array, False otherwise. describe(handler=<built-in function print>); Print information about type, index, and dependencies. property dtype; The data type of the expression. Returns:; HailType. export(path, delimiter='\t', missing='NA', header=True); Export a field to a text file.; Examples; >>> small_mt.GT.export('output/gt.tsv'); >>> with open('output/gt.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles 0 1 2 3; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. >>> small_mt.GT.export('output/gt-no-header.tsv', header=False); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:5111,pipeline,pipeline,5111,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,1,['pipeline'],['pipeline']
Deployability,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39085,configurat,configuration,39085,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['configurat'],['configuration']
Deployability,"d'},; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; output_files=[(local_output_file, f'{vep_output_path}/csq-header')],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). for f in hl.hadoop_ls(vep_input_path):; path = f['path']; part_name = os.path.basename(path); if not part_name.startswith('part-'):; continue; part_id = int(part_name.split('-')[1]). local_input_file = '/io/input'; local_output_file = '/io/output.gz'. vep_command = vep_config.command(; consequence=csq,; part_id=part_id,; input_file=local_input_file,; output_file=local_output_file,; tolerate_parse_error=tolerate_parse_error,; ). env = {; 'VEP_BLOCK_SIZE': str(block_size),; 'VEP_DATA_MOUNT': shq(vep_config.data_mount),; 'VEP_CONSEQUENCE': str(int(csq)),; 'VEP_TOLERATE_PARSE_ERROR': str(int(tolerate_parse_error)),; 'VEP_PART_ID': str(-1),; 'VEP_INPUT_FILE': local_input_file,; 'VEP_OUTPUT_FILE': local_output_file,; 'VEP_COMMAND': vep_command,; }; env.update(vep_config.env). b.create_job(; vep_config.image,; vep_config.batch_run_command,; attributes={'name': f'vep-{part_id}'},; resources={'cpu': '1', 'memory': 'standard'},; input_files=[(path, local_input_file)],; output_files=[(local_output_file, f'{vep_output_path}/annotations/{part_name}.tsv.gz')],; cloudfuse=[(vep_config.data_bucket, vep_config.data_mount, True)],; regions=vep_config.regions,; requester_pays_project=requester_pays_project,; env=env,; ). hl.export_vcf(ht, temp_input_directory, parallel='header_per_shard'). starting_job_id = async_to_blocking(backend._batch.status())['n_jobs'] + 1. b = bc.client.Batch(backend._batch); build_vep_batch(b, temp_input_directory, temp_output_directory). b.submit(disable_progress_bar=True). try:; status = b.wait(; description='vep(...)',; disable_progress_bar=backend.disable_progress_bar,; progress=None,; starting_job=starting_job_id,; ); except BaseException as e:; if isinstance(e, KeyboardInterrupt):; print(""Received a keyboard interrupt, canc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:34683,update,update,34683,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['update'],['update']
Deployability,"d. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration in ServiceBackend. Methods. async_map; Aysncio compatible version of map(). async_submit; Aysncio compatible version of BatchPoolExecutor.submit(). map;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2646,install,installed,2646,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['install'],['installed']
Deployability,"d_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle argume",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1871,pipeline,pipeline,1871,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['pipeline'],['pipeline']
Deployability,"d_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_exome_coverage. View page source. gnomad_exome_coverage. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html:9527,update,updated,9527,docs/0.2/datasets/schemas/gnomad_exome_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_exome_coverage.html,1,['update'],['updated']
Deployability,"d_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; clinvar_gene_summary. View page source. clinvar_gene_summary. Versions: 2019-07; Reference genome builds: None; Type: hail.Table. Schema (2019-07, None); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'GeneID': int32; 'Total_submissions': int32; 'Total_alleles': int32; 'Submissions_reporting_this_gene': int32; 'Alleles_reported_Pathogenic_Likely_pathogenic': int32; 'Gene_MIM_number': int32; 'Number_uncertain': int32; 'Number_with_conflicts': int32; 'gene_name': str; ----------------------------------------; Key: ['gene_name']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html:9550,update,updated,9550,docs/0.2/datasets/schemas/clinvar_gene_summary.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/clinvar_gene_summary.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AFR. View page source. giant_bmi_exome_AFR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'afr_maf': dict<str, float64>; 'exac_afr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AFR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_AMR. View page source. giant_bmi_exome_AMR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'amr_maf': dict<str, float64>; 'exac_amr_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_AMR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EAS. View page source. giant_bmi_exome_EAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eas_maf': dict<str, float64>; 'exac_eas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EAS.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_EUR. View page source. giant_bmi_exome_EUR. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_EUR.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_bmi_exome_SAS. View page source. giant_bmi_exome_SAS. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'sas_maf': dict<str, float64>; 'exac_sas_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html:9570,update,updated,9570,docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_bmi_exome_SAS.html,1,['update'],['updated']
Deployability,"d_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_all_snp_gene_associations. View page source. GTEx_eQTL_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.MatrixTable. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'tissue': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'tss_distance': int32; ----------------------------------------; Entry fields:; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Column key: ['tissue']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html:9801,update,updated,9801,docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; Schema (2.1, GRCh37). gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d01. View page source. gnomad_mnv_genome_d01. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d01.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; Schema (2.1, GRCh37). gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d02. View page source. gnomad_mnv_genome_d02. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d02.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; Schema (2.1, GRCh37). gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d03. View page source. gnomad_mnv_genome_d03. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d03.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; Schema (2.1, GRCh37). gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d04. View page source. gnomad_mnv_genome_d04. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d04.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; Schema (2.1, GRCh37). gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d05. View page source. gnomad_mnv_genome_d05. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d05.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; Schema (2.1, GRCh37). gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d06. View page source. gnomad_mnv_genome_d06. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d06.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; Schema (2.1, GRCh37). gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d07. View page source. gnomad_mnv_genome_d07. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d07.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; Schema (2.1, GRCh37). gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d08. View page source. gnomad_mnv_genome_d08. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d08.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; Schema (2.1, GRCh37). gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d09. View page source. gnomad_mnv_genome_d09. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d09.html,1,['update'],['updated']
Deployability,"d_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; Schema (2.1, GRCh37). gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_mnv_genome_d10. View page source. gnomad_mnv_genome_d10. Versions: 2.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'refs': str; 'alts': str; 'distance': int32; 'snp1': str; 'snp2': str; 'ac1': int32; 'ac2': int32; 'ac_mnv': int32; 'ac1_adj': int32; 'ac2_adj': int32; 'ac_mnv_adj': int32; ----------------------------------------; Key: ['locus', 'refs', 'alts']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html:9486,update,updated,9486,docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_mnv_genome_d10.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Add. View page source. giant_whr_exome_C_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_EUR_Rec. View page source. giant_whr_exome_C_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Add. View page source. giant_whr_exome_M_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_EUR_Rec. View page source. giant_whr_exome_M_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Add. View page source. giant_whr_exome_W_EUR_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Add.html,1,['update'],['updated']
Deployability,"d_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_EUR_Rec. View page source. giant_whr_exome_W_EUR_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'eur_maf': dict<str, float64>; 'exac_nfe_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html:9610,update,updated,9610,docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_EUR_Rec.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"d_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html:9757,update,updated,9757,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Amygdala_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be identical, both in type, value, and ordering.; The row key schemas and row schemas must match.; The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match.; This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters:; datasets (varargs of MatrixTable) – Datasets to combine. Returns:; MatrixTable – Dataset with rows from each member of datasets. unpersist()[source]; Unpersists this dataset from memory/disk.; Notes; This function will have no effect on a dataset that was not previously; persisted. Returns:; MatrixTable – Unpersisted dataset. write(output, overwrite=False, stage_locally=False, _codec_spec=None, _partitions=None)[source]; Write to disk.; Examples; >>> dataset.write('output/dataset.mt'). Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. See also; read_matrix_table(). Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:69174,update,updated,69174,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['update'],['updated']
Deployability,"def scale_y_reverse(name=None):; """"""Transforms y-axis to be vertically reversed. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, transformation=""reverse""). [docs]def scale_x_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""x"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_y_continuous(name=None, breaks=None, labels=None, trans=""identity""):; """"""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionScaleContinuous(""y"", name=name, breaks=breaks, labels=labels, transformation=trans). [docs]def scale_x_discrete(name=None, breaks=None, labels=None):; """"""The default discrete x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`str`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return PositionS",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:9573,continuous,continuous,9573,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['continuous'],['continuous']
Deployability,"dices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9757,update,updated,9757,docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Subcutaneous_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Muscularis_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Left_Ventricle_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"dices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. View page source. GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html:9775,update,updated,9775,docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Minor_Salivary_Gland_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes import viridis. _palette = viridis(n). return CategoricalColorMapper(factors=factors, palette=_palette). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23678,update,update,23678,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by whi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:48076,continuous,continuous,48076,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"ds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5544,install,installed,5544,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"ds: varargs of :class:`str`; Fields to drop. Returns; -------; :class:`.Struct`; Struct without certain fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5, bar=2, apple=10). Drop one field from `s`. >>> s.drop('bar'); Struct(food=8, fruit=5, apple=10). Drop two fields from `s`. >>> s.drop('food', 'fruit'); Struct(bar=2, apple=10); """"""; d = OrderedDict((k, v) for k, v in self.items() if k not in args); return Struct(**d). @typecheck(struct=Struct); def to_dict(struct):; return dict(struct.items()). _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, stream, indent, allowance, context, level, *args, **kwargs):; if isinstance(obj, Struct):; rep = self._repr(obj, context, level); max_width = self._width - indent - allowance; if len(rep) <= max_width:; stream.write(rep); return. stream.write('Struct('); indent += len('Struct('); if all(k.isidentifier() for k in obj):; n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(k); stream.write('='); this_indent = indent + len(k) + len('='); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); else:; stream.write('**{'); indent += len('**{'); n = len(obj.items()); for i, (k, v) in enumerate(obj.items()):; is_first = i == 0; is_last = i == n - 1. if not is_first:; stream.write(' ' * indent); stream.write(repr(k)); stream.write(': '); this_indent = indent + len(repr(k)) + len(': '); self._format(v, stream, this_indent, allowance, context, level, *args, **kwargs); if not is_last:; stream.write(',\n'); stream.write('}'); stream.write(')'); else:; _old_printer._format(self, obj, stream, indent, allowance, context, level, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:7306,patch,patch,7306,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,4,"['patch', 'update']","['patch', 'updated']"
Deployability,"e 2: Left distinct join: ht[ht2.key] or ht[ht2.field1, ht2.field2]""; ) from e. @property; def key(self) -> StructExpression:; """"""Row key struct. Examples; --------. List of key field names:. >>> list(table1.key); ['ID']. Number of key fields:. >>> len(table1.key); 1. Returns; -------; :class:`.StructExpression`; """"""; return self._key. @property; def _value(self) -> 'StructExpression':; return self.row.drop(*self.key). [docs] def n_partitions(self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:12755,pipeline,pipeline,12755,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability,"e Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nich",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1468,install,install-on-cluster,1468,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install-on-cluster']
Deployability,"e ValueError(""""""ld_score: entry_expr, locus_expr, coord_expr; (if specified), and annotation_exprs (if; specified) must come from same MatrixTable.""""""). n = mt.count_cols(); r2 = hl.row_correlation(entry_expr, block_size) ** 2; r2_adj = ((n - 1.0) / (n - 2.0)) * r2 - (1.0 / (n - 2.0)). starts, stops = hl.linalg.utils.locus_windows(locus_expr, radius, coord_expr); r2_adj_sparse = r2_adj.sparsify_row_intervals(starts, stops). r2_adj_sparse_tmp = new_temp_file(); r2_adj_sparse.write(r2_adj_sparse_tmp); r2_adj_sparse = BlockMatrix.read(r2_adj_sparse_tmp). if not annotation_exprs:; cols = ['univariate']; col_idxs = {0: 'univariate'}; l2 = r2_adj_sparse.sum(axis=1); else:; ht = mt.select_rows(*wrap_to_list(annotation_exprs)).rows(); ht = ht.annotate(univariate=hl.literal(1.0)); names = [name for name in ht.row if name not in ht.key]. ht_union = Table.union(*[; (ht.annotate(name=hl.str(x), value=hl.float(ht[x])).select('name', 'value')) for x in names; ]); mt_annotations = ht_union.to_matrix_table(row_key=list(ht_union.key), col_key=['name']). cols = mt_annotations.key_cols_by()['name'].collect(); col_idxs = {i: cols[i] for i in range(len(cols))}. a_tmp = new_temp_file(); BlockMatrix.write_from_entry_expr(mt_annotations.value, a_tmp). a = BlockMatrix.read(a_tmp); l2 = r2_adj_sparse @ a. l2_bm_tmp = new_temp_file(); l2_tsv_tmp = new_temp_file(); l2.write(l2_bm_tmp, force_row_major=True); BlockMatrix.export(l2_bm_tmp, l2_tsv_tmp). ht_scores = hl.import_table(l2_tsv_tmp, no_header=True, impute=True); ht_scores = ht_scores.add_index(); ht_scores = ht_scores.key_by('idx'); ht_scores = ht_scores.rename({'f{:}'.format(i): col_idxs[i] for i in range(len(cols))}). ht = mt.select_rows(__locus=locus_expr).rows(); ht = ht.add_index(); ht = ht.annotate(**ht_scores[ht.idx]); ht = ht.key_by('__locus'); ht = ht.select(*[x for x in ht_scores.row if x not in ht_scores.key]); ht = ht.rename({'__locus': 'locus'}). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:6967,update,updated,6967,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,2,['update'],['updated']
Deployability,"e file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). ID; HT; SEX; X; Z. int32; int32; str; int32; int32. 2; 3; 72; 70; “M”; “F”; 6; 7; 3; 3. Remove rows where Z is 3:; >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-------+-------+; |",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:24302,pipeline,pipeline,24302,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"e following attributes are read from the VCF header when importing a VCF and written; to the VCF header when exporting a VCF:. INFO fields attributes (attached to (va.info.*)):; ‘Number’: The arity of the field. Can take values; 0 (Boolean flag),; 1 (single value),; R (one value per allele, including the reference),; A (one value per non-reference allele),; G (one value per genotype), and; . (any number of values); When importing: The value in read from the VCF INFO field definition; When exporting: The default value is 0 for Boolean, . for Arrays and 1 for all other types. ‘Description’ (default is ‘’). FILTER entries in the VCF header are generated based on the attributes; of va.filters. Each key/value pair in the attributes will generate a; FILTER entry in the VCF with ID = key and Description = value. Parameters:; ann_path (str) – Variant annotation path starting with ‘va’, period-delimited.; attribute (str) – The attribute to remove (key). Returns:Annotated dataset with the updated variant annotation without the attribute. Return type:VariantDataset. drop_samples()[source]¶; Removes all samples from variant dataset.; The variants, variant annotations, and global annnotations will remain,; producing a sites-only variant dataset. Returns:Sites-only variant dataset. Return type:VariantDataset. drop_variants()[source]¶; Discard all variants, variant annotations and genotypes.; Samples, sample annotations and global annotations are retained. This; is the same as filter_variants_expr('false'), but much faster.; Examples; >>> vds_result = vds.drop_variants(). Returns:Samples-only variant dataset. Return type:VariantDataset. export_gen(output, precision=4)[source]¶; Export variant dataset as GEN and SAMPLE file. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Import genotype probability data, filter variants based on INFO score, and export data to a GEN and SAMPLE file:; >>> vds3 = hc.import_bgen(""data/example3.bgen"", sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:35416,update,updated,35416,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['update'],['updated']
Deployability,"e full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#6378) Fixed problem; in how entry_float_type was being handled in import_vcf. Version 0.2.15; Released 2019-06-14; After some infrastructural changes to our development process, we should; be getting back to frequent releases. hailctl; Starting in 0.2.15, pip installations of Hail come bundled with a; command- line tool, hailctl. This tool subsumes the functionality of; cloudtools, which is now deprecated. See the release thread on the; forum; for more information. New features. (#5932)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly wor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:92287,release,release,92287,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"e is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. pvals (NumericExpression) – List of x-values to be plotted.; label (Expression or Dict[str, Expression]]) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title (str, optional) – Title of the scatterplot.; xlabel (str, optional) – X-axis label.; ylabel (str, optional) – Y-axis label.; size (int) – Size of markers in screen space units.; legend (bool) – Whether or not to show the legend in the resulting figure.; hover_fields (Dict[str, Expression], optional) – Extra fields to be displayed when hovering over a point on the plot.; colors (bokeh.models.mappers.ColorMapper or Dict[str, bokeh.models.mappers.ColorMapper], optional) – If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using label.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width (int) – Plot width; height (int) – Plot height; collect_all (bool) – Deprecated. Use n_divisions instead.; n_divisions (int, optional) – Factor by which to downsample (de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:11115,continuous,continuous,11115,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"e passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) which; caused incorrect results for identity by descent in Query-on-Batch.; (#14122) Ensure that; stack traces are transmitted from workers to the driver to the; client.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:19145,install,installation,19145,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['install'],['installation']
Deployability,"e run in parallel,; they are still run sequentially. However, if batches are executed by the ServiceBackend; using the Batch Service, then s and t can be run in parallel as; there exist no dependencies between them.; >>> b = hb.Batch(name='hello-parallel'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> b.run(). To create a dependency between s and t, we use the method; Job.depends_on() to explicitly state that t depends on s. In both the; LocalBackend and ServiceBackend, s will always run before; t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command('echo ""hello world 1""'); >>> t = b.new_job(name='j2'); >>> t.command('echo ""hello world 2""'); >>> t.depends_on(s); >>> b.run(). File Dependencies; So far we have created batches with two jobs where the dependencies between; them were declared explicitly. However, in many computational pipelines, we want to; have a file generated by one job be the input to a downstream job. Batch has a; mechanism for tracking file outputs and then inferring job dependencies from the usage of; those files.; In the example below, we have specified two jobs: s and t. s prints; “hello world” as in previous examples. However, instead of printing to stdout,; this time s redirects the output to a temporary file defined by s.ofile.; s.ofile is a Python object of type JobResourceFile that was created; on the fly when we accessed an attribute of a Job that does not already; exist. Any time we access the attribute again (in this example ofile), we get the; same JobResourceFile that was previously created. However, be aware that; you cannot use an existing method or property name of Job objects such; as BashJob.command() or BashJob.image().; Note the ‘f’ character before the string in the command for s! We placed s.ofile in curly braces so; when Python interpolates the f-string, it replaced the; JobResourceFile object with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:4118,pipeline,pipelines,4118,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['pipeline'],['pipelines']
Deployability,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:28031,update,updated,28031,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['update'],['updated']
Deployability,"e {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:31890,configurat,configuration,31890,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"e), all_quantiles=True). new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; plot = fig.step(x=[min_x, *x[keep], max_x], y=[*slopes, slopes[-1]], mode='after'); else:; plot = fig.quad(left=[min_x, *x[keep]], right=[*x[keep], max_x], bottom=0, top=slopes, legend_label=legend). if interactive:. def mk_interact(handle):; def update(confidence=confidence):; err = _error_from_cdf_python(data, 10 ** (-confidence), all_quantiles=True) / 1.8; new_y, keep = _max_entropy_cdf(min_x, max_x, x, y, err); slopes = np.diff([0, *new_y[keep], 1]) / np.diff([min_x, *x[keep], max_x]); if log:; new_data = {'x': [min_x, *x[keep], max_x], 'y': [*slopes, slopes[-1]]}; else:; new_data = {; 'left': [min_x, *x[keep]],; 'right': [*x[keep], max_x],; 'bottom': np.full(len(slopes), 0),; 'top': slopes,; }; plot.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, confidence=(1, 10, 0.01)). return fig, mk_interact; else:; return fig. def _max_entropy_cdf(min_x, max_x, x, y, e):; def compare(x1, y1, x2, y2):; return x1 * y2 - x2 * y1. new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). fx = min_x # fixed x; fy = 0 # fixed y; li = 0 # index of lower slope; ui = 0 # index of upper slope; ldx = x[li] - fx; udx = x[ui] - fx; ldy = y[li + 1] - e - fy; udy = y[ui] + e - fy; j = 1; while ui < len(x) and li < len(x):; if j == len(x):; ub = 1; lb = 1; xj = max_x; else:; ub = y[j] + e; lb = y[j + 1] - e; xj = x[j]; dx = xj - fx; judy = ub - fy; jldy = lb - fy; if compare(ldx, ldy, dx, judy) < 0:; # line must bend down at j; fx = x[li]; fy = y[li + 1] - e; new_y[li] = fy; keep[li] = True; j = li + 1; if j >= len(x):; break; li = j; ldx = x[li] - fx; ldy = y[li + 1] - e - fy; ui = j; udx = x[ui] - fx; udy = y[ui] + e - fy; j += 1; continue; elif compare(udx, udy, dx, jldy) > 0:; # line must bend up at j; fx = x[ui]; f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:5996,update,update,5996,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"e: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'read_count': int32; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html:11118,update,updated,11118,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_read_counts.html,1,['update'],['updated']
Deployability,"e_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=Tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31301,update,update,31301,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"eatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; genetics; AlleleType. View page source. AlleleType. class hail.genetics.AlleleType[source]; An enumeration for allele type.; Notes; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; Attributes. UNKNOWN; Unknown Allele Type. SNP; Single-nucleotide Polymorphism (SNP). MNP; Multi-nucleotide Polymorphism (MNP). INSERTION; Insertion. DELETION; Deletion. COMPLEX; Complex Polymorphism. STAR; Star Allele (alt=*). SYMBOLIC; Symbolic Allele. TRANSITION; Transition SNP. TRANSVERSION; Transversion SNP. pretty_name; A formatted (as opposed to uppercase) version of the member's name, to match allele_type(). Methods. strings; Returns the names of the allele types, for use with literal(). COMPLEX = 5; Complex Polymorphism. DELETION = 4; Deletion. INSERTION = 3; Insertion. MNP = 2; Multi-nucleotide Polymorphism (MNP). SNP = 1; Single-nucleotide Polymorphism (SNP). STAR = 6; Star Allele (alt=*). SYMBOLIC = 7; Symbolic Allele; e.g. alt=<INS>. TRANSITION = 8; Transition SNP; e.g. ref=A alt=G. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). TRANSVERSION = 9; Transversion SNP; e.g. ref=A alt=C. Note; This is only really used internally in hail.vds.sample_qc() and; hail.methods.sample_qc(). UNKNOWN = 0; Unknown Allele Type. property pretty_name; A formatted (as opposed to uppercase) version of the member’s name,; to match allele_type(); Examples; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True. static strings()[source]; Returns the names of the allele types, for use with; literal(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html:2441,update,updated,2441,docs/0.2/genetics/hail.genetics.AlleleType.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.AlleleType.html,1,['update'],['updated']
Deployability,"econd index is the state on the right.; For example, ``concordance[1][4]`` is the number of ""no call"" genotypes on the left that were called ; homozygous variant on the right. ; ; :param right: right hand variant dataset for concordance; :type right: :class:`.VariantDataset`. :return: The global concordance statistics, a key table with sample concordance; statistics, and a key table with variant concordance statistics.; :rtype: (list of list of int, :py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.concordance(right._jvds); j_global_concordance = r._1(); sample_kt = KeyTable(self.hc, r._2()); variant_kt = KeyTable(self.hc, r._3()); global_concordance = [[j_global_concordance.apply(j).apply(i) for i in xrange(5)] for j in xrange(5)]. return global_concordance, sample_kt, variant_kt. [docs] @handle_py4j; def count(self):; """"""Returns number of samples and variants in the dataset.; ; **Examples**; ; >>> samples, variants = vds.count(); ; **Notes**; ; This is also the fastest way to force evaluation of a Hail pipeline.; ; :returns: The sample and variant counts.; :rtype: (int, int); """""". r = self._jvds.count(). return r._1(), r._2(). [docs] @handle_py4j; def deduplicate(self):; """"""Remove duplicate variants. :return: Deduplicated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.deduplicate()). [docs] @handle_py4j; @typecheck_method(fraction=numeric,; seed=integral); def sample_variants(self, fraction, seed=1):; """"""Downsample variants to a given fraction of the dataset.; ; **Examples**; ; >>> small_vds = vds.sample_variants(0.01); ; **Notes**; ; This method may not sample exactly ``(fraction * n_variants)``; variants from the dataset. :param float fraction: (Expected) fraction of variants to keep. :param int seed: Random seed. :return: Downsampled variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.sampleVariants(fraction, seed)). [docs] @handle_py4j; @re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:45290,pipeline,pipeline,45290,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['pipeline'],['pipeline']
Deployability,"ed by Databricks. Please direct questions about them; to Databricks.; Hail can be installed on a Databricks Spark cluster on Microsoft Azure, Amazon Web Services, or; Google Cloud Platform via an open source Docker container located in the Project Glow Dockerhub. Docker; files to build your own Hail container on Databricks can be found in the Glow Github repository.; Install Hail via Docker with Databricks Container Services.; Use the Docker Image URL, projectglow/databricks-hail:<hail_version>, replacing the tag with an; available Hail version. Please match the Databricks Runtime Spark version to the Spark version Hail; is built with. Use Hail in a notebook; For the most part, Hail in Databricks works identically to the Hail documentation. However, there; are a few modifications that are necessary for the Databricks environment. Initialize Hail; When initializing Hail, pass in the pre-created SparkContext and mark the initialization as; idempotent. This setting enables multiple Databricks notebooks to use the same Hail context. note:. Enable skip_logging_configuration to save logs to the rolling driver log4j output. This; setting is supported only in Hail 0.2.39 and above.; Hail is not supported with Credential passthrough. code:; >>> import hail as hl; >>> hl.init(sc, idempotent=True, quiet=True, skip_logging_configuration=True) . Display Bokeh plots; Hail uses the Bokeh library to create plots. The show; function built into Bokeh does not work in Databricks. To display a Bokeh plot generated by Hail,; you can run a command like:; >>> from bokeh.embed import components, file_html; >>> from bokeh.resources import CDN; >>> plot = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); >>> html = file_html(plot, CDN, ""Chart""). And then call the Databricks function displayHTML with html as its argument.; See Databricks’ Bokeh docs for; more information. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/databricks.html:1780,rolling,rolling,1780,docs/0.2/cloud/databricks.html,https://hail.is,https://hail.is/docs/0.2/cloud/databricks.html,2,"['rolling', 'update']","['rolling', 'updated']"
Deployability,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1461,configurat,configuration,1461,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['configurat'],['configuration']
Deployability,"eight=height, width=width); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). if not legend:; assert sp_legend is not None; assert sp_color_bar is not None; sp_legend.visible = False; sp_color_bar.visible = False. # If multiple labels, create JS call back selector; if len(label_cols) > 1:; callback_args: Dict[str, Any]; callback_args = dict(color_mappers=sp_color_mappers, scatter_renderers=sp_scatter_renderers); callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); return Column(children=[select, sp]). return sp. @typecheck(; x=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; y=oneof(expr_numeric, sized_tupleof(str, expr_numeric)),; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:34929,update,update,34929,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"einberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101590,configurat,configuration,101590,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"el text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matm",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70838,pipeline,pipelines,70838,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"elds. ""; ""Overwriting with call fields from supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30583,update,update,30583,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"ele frequency.; >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size.; Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:74161,integrat,integration,74161,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['integrat'],['integration']
Deployability,"eles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.ve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102349,configurat,configuration,102349,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"emp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; -------; :func:`.ld_score` will fail if",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:1753,continuous,continuous,1753,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,2,['continuous'],['continuous']
Deployability,"ence genome of; the dataset.; The output VCF header will not contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; append_to_header parameter. Warning; INFO fields stored at VCF import are not automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating info, downstream; tools which may produce erroneous results. The solution is to create new; fields in info or overwrite existing fields. For example, in order to; produce an accurate AC field, one can run variant_qc() and copy; the variant_qc.AC field to info.AC as shown below.; >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) ; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; Do not export to a path that is being read from in the same pipeline. Parameters:. dataset (MatrixTable) – Dataset.; output (str) – Path of .vcf or .vcf.bgz file to write.; append_to_header (str, optional) – Path of file to append to VCF header.; parallel (str, optional) – If 'header_per_shard', return a set of VCF files (one per; partition) rather than serially concatenating these files. If; 'separate_header', return a separate VCF header file and a set of; VCF files (one per partition) without the header. If None,; concatenate the header and all partitions into one VCF file.; metadata (dict [str, dict [str, dict [str, str]]], optional) – Dictionary with information to fill in the VCF header. See; get_vcf_metadata() for how this; dictionary should be structured.; tabix (bool, optional) – If true, writes a tabix index for the output VCF.; Note: This feature is experimental, and the interface and defaults; may change in future versions. hail.methods.export_elasticsearch(t, host, port, index, index_type, block_size, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:50767,pipeline,pipeline,50767,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['pipeline'],['pipeline']
Deployability,"ene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',; }; if reference_genome is None:; reference_genome = hl.default_reference().name; else:; reference_genome = reference_genome.name; if gtf_file is None:; gtf_file = GTFS.get(reference_genome); if gtf_file is None:; raise ValueError(; 'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)'; ); ht = hl.experimental.import_gtf(; gtf_file, reference_genome=reference_genome, skip_invalid_contigs=True, min_partitions=12; ); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0], transcript_id=ht.transcript_id.split('\\.')[0]); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:10360,update,updated,10360,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['update'],['updated']
Deployability,"enomic interval of interest.; interval_size (int32): Size of interval, in bases. Computes the following entry fields:. bases_over_gq_threshold (tuple of int64): Number of bases in the interval; over each GQ threshold.; fraction_over_gq_threshold (tuple of float64): Fraction of interval (in bases); above each GQ threshold. Computed by dividing each member of bases_over_gq_threshold; by interval_size.; bases_over_dp_threshold (tuple of int64): Number of bases in the interval; over each DP threshold.; fraction_over_dp_threshold (tuple of float64): Fraction of interval (in bases); above each DP threshold. Computed by dividing each member of bases_over_dp_threshold; by interval_size.; sum_dp (int64): Sum of depth values by base across the interval.; mean_dp (float64): Mean depth of bases across the interval. Computed by dividing; sum_dp by interval_size. If the dp_field parameter is not specified, the DP is used for depth; if present. If no DP field is present, the MIN_DP field is used. If no DP; or MIN_DP field is present, no depth statistics will be calculated. Note; The metrics computed by this method are computed only from reference blocks. Most; variant callers produce data where non-reference calls interrupt reference blocks, and; so the metrics computed here are slight underestimates of the true values (which would; include the quality/depth of non-reference calls). This is likely a negligible difference,; but is something to be aware of, especially as it interacts with samples of; ancestral backgrounds with more or fewer non-reference calls. Parameters:. vds (VariantDataset); intervals (Table) – Table of intervals. Must be start-inclusive, and cannot span contigs.; gq_thresholds (tuple of int) – GQ thresholds.; dp_field (str, optional) – Field for depth calculation. Uses DP or MIN_DP by default (with priority for DP if present). Returns:; MatrixTable – Interval-by-sample matrix. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html:3031,update,updated,3031,docs/0.2/vds/hail.vds.interval_coverage.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.interval_coverage.html,1,['update'],['updated']
Deployability,"er batch.; (#11759); hl.logistic_regression_rows, hl.poisson_regression_rows, and; hl.skat all now support configuration of the maximum number of; iterations and the tolerance.; (#11835) Add; hl.ggplot.geom_density which renders a plot of an approximation; of the probability density function of its argument. Bug fixes. (#11815) Fix; incorrectly missing entries in to_dense_mt at the position of ref; block END.; (#11828) Fix; hl.init to not ignore its sc argument. This bug was; introduced in 0.2.94.; (#11830) Fix an; error and relax a timeout which caused hailtop.aiotools.copy to; hang.; (#11778) Fix a; (different) error which could cause hangs in; hailtop.aiotools.copy. Version 0.2.94; Released 2022-04-26. Deprecation. (#11765) Deprecated; and removed linear mixed model functionality. Beta features. (#11782); hl.import_table is up to twice as fast for small tables. New features. (#11428); hailtop.batch.build_python_image now accepts a; show_docker_output argument to toggle printing docker’s output to; the terminal while building container images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This versi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:49903,toggle,toggle,49903,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['toggle'],['toggle']
Deployability,"er images; (#11725); hl.ggplot now supports facet_wrap; (#11776); hailtop.aiotools.copy will always show a progress bar when; --verbose is passed. hailctl dataproc. (#11710) support; pass-through arguments to connect. Bug fixes. (#11792) Resolved; issue where corrupted tables could be created with whole-stage code; generation enabled. Version 0.2.93; Release 2022-03-27. Beta features. Several issues with the beta version of Hail Query on Hail Batch are; addressed in this release. Version 0.2.92; Release 2022-03-25. New features. (#11613) Add; hl.ggplot support for scale_fill_hue, scale_color_hue,; and scale_fill_manual, scale_color_manual. This allows for an; infinite number of discrete colors.; (#11608) Add all; remaining and all versions of extant public gnomAD datasets to the; Hail Annotation Database and Datasets API. Current as of March 23rd; 2022.; (#11662) Add the; weight aesthetic geom_bar. Beta features. This version of Hail includes all the necessary client-side; infrastructure to execute Hail Query pipelines on a Hail Batch; cluster. This effectively enables a “serverless” version of Hail; Query which is independent of Apache Spark. Broad affiliated users; should contact the Hail team for help using Hail Query on Hail Batch.; Unaffiliated users should also contact the Hail team to discuss the; feasibility of running your own Hail Batch cluster. The Hail team is; accessible at both https://hail.zulipchat.com and; https://discuss.hail.is . Version 0.2.91; Release 2022-03-18. Bug fixes. (#11614) Update; hail.utils.tutorial.get_movie_lens to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:51006,pipeline,pipelines,51006,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"er than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.n_alt_alleles(),; ... mt.GT.n_alt_alleles()), mt.prior))). dependencies:; import_plink(), variant_qc(), import_table(),; coalesce(), case(), cond(), Call.n_alt_alleles(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:14748,update,updated,14748,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['update'],['updated']
Deployability,"er/mother schema,; and the resulting entry schema is the same as the proband_entry/father_entry/mother_entry schema.; If the `keep_trio_cols` option is set, then an additional `source_trio` column is added with the trio column data.; If the `keep_trio_entries` option is set, then an additional `source_trio_entry` column is added with the trio entry data. Note; ----; This assumes that the input MatrixTable is a trio MatrixTable (similar to; the result of :func:`~.trio_matrix`) Its entry schema has to contain; 'proband_entry`, `father_entry` and `mother_entry` all with the same type.; Its column schema has to contain 'proband`, `father` and `mother` all with; the same type. Parameters; ----------; tm : :class:`.MatrixTable`; Trio MatrixTable (entries have to be a Struct with `proband_entry`, `mother_entry` and `father_entry` present); col_keys : :obj:`list` of str; Column key(s) for the resulting sample MatrixTable; keep_trio_cols: bool; Whether to add a `source_trio` column with the trio column data (default `True`); keep_trio_entries: bool; Whether to add a `source_trio_entries` column with the trio entry data (default `False`). Returns; -------; :class:`.MatrixTable`; Sample MatrixTable; """""". select_entries_expr = {'__trio_entries': hl.array([tm.proband_entry, tm.father_entry, tm.mother_entry])}; if keep_trio_entries:; select_entries_expr['source_trio_entry'] = hl.struct(**tm.entry); tm = tm.select_entries(**select_entries_expr). tm = tm.key_cols_by(); select_cols_expr = {'__trio_members': hl.enumerate(hl.array([tm.proband, tm.father, tm.mother]))}; if keep_trio_cols:; select_cols_expr['source_trio'] = hl.struct(**tm.col); tm = tm.select_cols(**select_cols_expr). mt = tm.explode_cols(tm.__trio_members). mt = mt.transmute_entries(**mt.__trio_entries[mt.__trio_members[0]]). mt = mt.key_cols_by(); mt = mt.transmute_cols(**mt.__trio_members[1]). if col_keys:; mt = mt.key_cols_by(*col_keys). return mt. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html:13533,update,updated,13533,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html,2,['update'],['updated']
Deployability,"erimental and subject to; change. This database contains a curated collection of variant annotations in an; accessible and Hail-friendly format, for use in Hail analysis pipelines.; To incorporate these annotations in your own Hail analysis pipeline, select; which annotations you would like to query from the table below and then; copy-and-paste the Hail generated code into your own analysis script.; Check out the DB class documentation for more detail on creating an; annotation database instance and annotating a MatrixTable or a; Table.; Google Cloud Storage; Note that these annotations are stored in Requester Pays buckets on Google Cloud Storage. Buckets are now available in both the; US-CENTRAL1 and EUROPE-WEST1 regions, so egress charges may apply if your; cluster is outside of the region specified when creating an annotation database; instance.; To access these buckets on a cluster started with hailctl dataproc, you; can use the additional argument --requester-pays-annotation-db as follows:; hailctl dataproc start my-cluster --requester-pays-allow-annotation-db. Amazon S3; Annotation datasets are now shared via Open Data on AWS as well, and can be accessed by users running Hail on; AWS. Note that on AWS the annotation datasets are currently only available in; a bucket in the US region. Database Query; Select annotations by clicking on the checkboxes in the table, and the; appropriate Hail command will be generated in the panel below.; In addition, a search bar is provided if looking for a specific annotation; within our curated collection.; Use the “Copy to Clipboard” button to copy the generated Hail code, and paste; the command into your own Hail script. Search. Database Query; . Copy to Clipboard; . Hail generated code:. db = hl.experimental.DB(region='us-central1', cloud='gcp'); mt = db.annotate_rows_db(mt); . name; description; version; reference genome; cloud: [regions]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/annotation_database_ui.html:2520,update,updated,2520,docs/0.2/annotation_database_ui.html,https://hail.is,https://hail.is/docs/0.2/annotation_database_ui.html,1,['update'],['updated']
Deployability,"ers using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#13960) for; details.; (#14057) Fix; (#13998) which; appeared in 0.2.58 and prevented reading from a networked filesystem; mounted within the filesystem of the worker node for certain; pipelines (those that did not trigger “lowering”).; (#14006) Fix; (#14000). Hail now; supports identity_by_descent on Apple M1 and M2 chips; however, your; Java installation must be an arm64 installation. Using x86_64 Java; with Hail on Apple M1 or M2 will cause SIGILL errors. If you have an; Apple M1 or Apple M2 and /usr/libexec/java_home -V does not; include (arm64), you must switch to an arm64 version of the JVM.; (#14022) Fix; (#13937) caused by; faulty library code in the Google Cloud Storage API Java client; library.; (#13812) Permit; hailctl batch submit to accept relative paths. Fix; (#13785).; (#13885) Hail; Query-on-Batch previously used Class A Operations for all interaction; with blobs. This change ensures that QoB only uses Class A Operations; when necessary.; (#14127); hailctl dataproc start ... --dry-run now uses shell escapes such; that, after copied and pasted into a shell, the gcloud command; works as expected.; (#14062) Fix; (#14052) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:18984,pipeline,pipelines,18984,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ers where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first inde",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94592,install,installs,94592,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installs']
Deployability,"ersion 0.2.114; Version 0.2.113; Version 0.2.112; Version 0.2.111; Version 0.2.110; Version 0.2.109; Version 0.2.108; Version 0.2.107; Version 0.2.106; Version 0.2.105; Version 0.2.104; Version 0.2.103; Version 0.2.102; Version 0.2.101; Version 0.2.100; Version 0.2.99; Version 0.2.98; Version 0.2.97; Version 0.2.96; Version 0.2.95; Version 0.2.94; Version 0.2.93; Version 0.2.92; Version 0.2.91; Version 0.2.90; Version 0.2.89; Version 0.2.88; Version 0.2.87; Version 0.2.86; Version 0.2.85; Version 0.2.84; Version 0.2.83; Version 0.2.82; Version 0.2.81; Version 0.2.80; Version 0.2.79; Version 0.2.78; Version 0.2.77; Version 0.2.76; Version 0.2.75; Version 0.2.74; Version 0.2.73; Version 0.2.72; Version 0.2.71; Version 0.2.70; Version 0.2.69; Version 0.2.68; Version 0.2.67; Version 0.2.66; Version 0.2.65; Version 0.2.64; Version 0.2.63; Version 0.2.62; Version 0.2.61; Version 0.2.60; Version 0.2.59; Version 0.2.58; Version 0.2.57; Version 0.2.56; Version 0.2.55; Version 0.2.54; Version 0.2.53; Version 0.2.52; Version 0.2.51; Version 0.2.50; Version 0.2.49; Version 0.2.48; Version 0.2.47; Version 0.2.46; Version 0.2.45; Version 0.2.44; Version 0.2.43; Version 0.2.42; Version 0.2.41; Version 0.2.40; Version 0.2.39; Version 0.2.38; Version 0.2.37; Version 0.2.36; Version 0.2.35; Version 0.2.34; Version 0.2.33; Version 0.2.32; Version 0.2.31; Version 0.2.30; Version 0.2.29; Version 0.2.28; Version 0.2.27; Version 0.2.26; Version 0.2.25; Version 0.2.24; Version 0.2.23; Version 0.2.22; Version 0.2.21; Version 0.2.20; Version 0.2.19; Version 0.2.18; Version 0.2.17; Version 0.2.16; Version 0.2.15; Version 0.2.14; Version 0.2.13; Version 0.2.12; Version 0.2.11; Version 0.2.10; Version 0.2.9; Version 0.2.8; Version 0.2.7; Version 0.2.6; Version 0.2.5; Version 0.2.4: Beginning of history!. Indices and tables. Index. If you would like to refer to our Hail v0.1 (deprecated) docs, please view Hail 0.1 docs. Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/index.html:4127,update,updated,4127,docs/0.2/index.html,https://hail.is,https://hail.is/docs/0.2/index.html,1,['update'],['updated']
Deployability,"ersion 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read from in a pipeline will now throw; an error instead of corrupting data. Version 0.2.36; Released 2020-04-06. Critical Memory Management Bug Fix. (#8463) Reverted a; change (separate to the bug in 0.2.34) that led to a memory leak in; version 0.2.35. Bug fixes. (#8371) Fix runtime; error in joins leading to “Cannot set required field missing” error; message.; (#8436) Fix compiler; bug leading to possibly-invalid generated code. Version 0.2.35; Released 2020-04-02. Critical Memory Management Bug Fix. (#8412) Fixed a; serious per-partition memory leak that causes certain pipelines to; run out of memory unexpectedly. Please update from 0.2.34. New features. (#8404) Added; “CanFam3” (a reference genome for dogs) as a bundled reference; genome. Bug fixes. (#8420) Fixed a bug; where hl.binom_test’s ""lower"" and ""upper"" alternative; options were reversed.; (#8377) Fixed; “inconsistent agg or scan environments” error.; (#8322) Fixed bug; where aggregate_rows did not interact with hl.agg.array_agg; correctly. Performance Improvements. (#8413) Improves; internal region memory management, decreasing JVM overhead.; (#8383) Significantly; improve GVCF import speed.; (#8358) Fixed memory; leak in hl.experimental.export_entries_by_col.; (#8326) Codegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:74237,pipeline,pipelines,74237,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:2369,update,updated,2369,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['update'],['updated']
Deployability,"er; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_ro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1570,install,install,1570,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,3,['install'],"['install', 'install-on-cluster', 'installed']"
Deployability,"es. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:7759,release,released,7759,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability,"es_POPMAX_AF': float64; 'gnomAD_genomes_POPMAX_nhomalt': int32; 'gnomAD_genomes_controls_AC': int32; 'gnomAD_genomes_controls_AN': int32; 'gnomAD_genomes_controls_AF': float64; 'gnomAD_genomes_controls_nhomalt': int32; 'gnomAD_genomes_controls_AFR_AC': int32; 'gnomAD_genomes_controls_AFR_AN': int32; 'gnomAD_genomes_controls_AFR_AF': float64; 'gnomAD_genomes_controls_AFR_nhomalt': int32; 'gnomAD_genomes_controls_AMR_AC': int32; 'gnomAD_genomes_controls_AMR_AN': int32; 'gnomAD_genomes_controls_AMR_AF': float64; 'gnomAD_genomes_controls_AMR_nhomalt': int32; 'gnomAD_genomes_controls_ASJ_AC': int32; 'gnomAD_genomes_controls_ASJ_AN': int32; 'gnomAD_genomes_controls_ASJ_AF': float64; 'gnomAD_genomes_controls_ASJ_nhomalt': int32; 'gnomAD_genomes_controls_EAS_AC': int32; 'gnomAD_genomes_controls_EAS_AN': int32; 'gnomAD_genomes_controls_EAS_AF': float64; 'gnomAD_genomes_controls_EAS_nhomalt': int32; 'gnomAD_genomes_controls_FIN_AC': int32; 'gnomAD_genomes_controls_FIN_AN': int32; 'gnomAD_genomes_controls_FIN_AF': float64; 'gnomAD_genomes_controls_FIN_nhomalt': int32; 'gnomAD_genomes_controls_NFE_AC': int32; 'gnomAD_genomes_controls_NFE_AN': int32; 'gnomAD_genomes_controls_NFE_AF': float64; 'gnomAD_genomes_controls_NFE_nhomalt': int32; 'gnomAD_genomes_controls_POPMAX_AC': int32; 'gnomAD_genomes_controls_POPMAX_AN': int32; 'gnomAD_genomes_controls_POPMAX_AF': float64; 'gnomAD_genomes_controls_POPMAX_nhomalt': int32; 'clinvar_id': int32; 'clinvar_clnsig': str; 'clinvar_trait': str; 'clinvar_review': str; 'clinvar_hgvs': str; 'clinvar_var_source': str; 'clinvar_MedGen_id': str; 'clinvar_OMIM_id': str; 'clinvar_Orphanet_id': str; 'Interpro_domain': str; 'GTEx_V7_gene': str; 'GTEx_V7_tissue': str; 'Geuvadis_eQTL_target_gene': str; 'locus': locus<GRCh37>; 'alleles': array<str>; 'chr': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html:20991,update,updated,20991,docs/0.2/datasets/schemas/dbNSFP_variants.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbNSFP_variants.html,1,['update'],['updated']
Deployability,"es_end == other._includes_end; ); if isinstance(other, Interval); else NotImplemented; ). def __hash__(self):; return hash(self._start) ^ hash(self._end) ^ hash(self._includes_start) ^ hash(self._includes_end). @property; def start(self):; """"""Start point of the interval. Examples; --------. >>> interval2.start; 3. Returns; -------; Object with type :meth:`.point_type`; """""". return self._start. @property; def end(self):; """"""End point of the interval. Examples; --------. >>> interval2.end; 6. Returns; -------; Object with type :meth:`.point_type`; """""". return self._end. @property; def includes_start(self):; """"""True if interval is inclusive of start. Examples; --------. >>> interval2.includes_start; True. Returns; -------; :obj:`bool`; """""". return self._includes_start. @property; def includes_end(self):; """"""True if interval is inclusive of end. Examples; --------. >>> interval2.includes_end; False. Returns; -------; :obj:`bool`; """""". return self._includes_end. @property; def point_type(self):; """"""Type of each element in the interval. Examples; --------. >>> interval2.point_type; dtype('int32'). Returns; -------; :class:`.Type`; """""". return self._point_type. def contains(self, value):; """"""True if `value` is contained within the interval. Examples; --------. >>> interval2.contains(5); True. >>> interval2.contains(6); False. Parameters; ----------; value :; Object with type :meth:`.point_type`. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).contains(value)). @typecheck_method(interval=interval_type); def overlaps(self, interval):; """"""True if the the supplied interval contains any value in common with this one. Parameters; ----------; interval : :class:`.Interval`; Interval object with the same point type. Returns; -------; :obj:`bool`; """""". return hl.eval(hl.literal(self, hl.tinterval(self._point_type)).overlaps(interval)). interval_type.set(Interval). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:4710,update,updated,4710,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,2,['update'],['updated']
Deployability,"es_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; Ensembl_homo_sapiens_low_complexity_regions. View page source. Ensembl_homo_sapiens_low_complexity_regions. Versions: release_95; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (release_95, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html:9499,update,updated,9499,docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/Ensembl_homo_sapiens_low_complexity_regions.html,1,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Spleen_all_snp_gene_associations. View page source. GTEx_sQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Spleen_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Testis_all_snp_gene_associations. View page source. GTEx_sQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Testis_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Uterus_all_snp_gene_associations. View page source. GTEx_sQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Uterus_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"es_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Vagina_all_snp_gene_associations. View page source. GTEx_sQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html:9733,update,updated,9733,docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Vagina_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"es_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9826,update,updated,9826,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ession refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size()[source]; Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f)[source]; Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html:11624,update,updated,11624,docs/0.2/hail.expr.CollectionExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CollectionExpression.html,1,['update'],['updated']
Deployability,"est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9766,update,updated,9766,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ethods work. See also; Table.transmute(), MatrixTable.select_globals(), MatrixTable.annotate_globals(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of row",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:64775,pipeline,pipeline,64775,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:15424,update,updated,15424,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['update'],['updated']
Deployability,"evels.find(lambda w: y >= w)); ).aggregate(c=hail.agg.count()); data = grouped_ht.filter(; hail.is_defined(grouped_ht.x); & (grouped_ht.x != str(x_range[1])); & hail.is_defined(grouped_ht.y); & (grouped_ht.y != str(y_range[1])); ); return data. def _collect_scatter_plot_data(; x: Tuple[str, NumericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23287,continuous,continuous,23287,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"ew” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; $ export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar. Running on a Spark cluster¶; Hail can run on any cluster that has Spark 2 installed. For instructions; specific to Google Cloud Dataproc clusters and Cloudera clusters, see below.; For all other Spark clusters, you will need to build Hail from the source code.; To build Hail, log onto the master node of the Spark cluster, and build a Hail JAR; and a zipfile of the Python code by running:. $ ./gradlew -Dspark.version=2.0.2 shadowJar archiveZip. You can then open an IPython shell which can run Hail backed by the cluster; with the ipyt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2834,install,installs,2834,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installs']
Deployability,"ey field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57155,pipeline,pipelines,57155,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"f either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2066,install,install,2066,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['install']
Deployability,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24809,pipeline,pipelines,24809,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"f the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175985,release,release-,175985,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['release'],['release-']
Deployability,"f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; rais",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11965,configurat,configurations,11965,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['configurat'],['configurations']
Deployability,"fault. Overrides fill aesthetic.; color – Color of line to draw outlining non x-axis facing side, none by default. Overrides color aesthetic. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_ribbon(mapping={}, fill=None, color=None)[source]; Creates filled in area between two lines specified by x, ymin, and ymax; Supported aesthetics: x, ymin, ymax, color, fill, tooltip. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; fill – Color of fill to draw, black by default. Overrides fill aesthetic.; color – Color of line to draw outlining both side, none by default. Overrides color aesthetic.; return:; :class:`FigureAttribute` – The geom to be applied. Scales. scale_x_continuous; The default continuous x scale. scale_x_discrete; The default discrete x scale. scale_x_genomic; The default genomic x scale. scale_x_log10; Transforms x axis to be log base 10 scaled. scale_x_reverse; Transforms x-axis to be vertically reversed. scale_y_continuous; The default continuous y scale. scale_y_discrete; The default discrete y scale. scale_y_log10; Transforms y-axis to be log base 10 scaled. scale_y_reverse; Transforms y-axis to be vertically reversed. scale_color_continuous; The default continuous color scale. scale_color_discrete; The default discrete color scale. scale_color_hue; Map discrete colors to evenly placed positions around the color wheel. scale_color_manual; A color scale that assigns strings to colors using the pool of colors specified as values. scale_color_identity; A color scale that assumes the expression specified in the color aesthetic can be used as a color. scale_fill_continuous; The default continuous fill scale. scale_fill_discrete; The default discrete fill scale. scale_fill_hue; Map discrete fill colors to evenly placed positions around the color wheel. scale_fill_manual; A color scale that assigns strings to fill colors using the pool of colors specified as values. scale_fill_identity; A color scale that assumes t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:8758,continuous,continuous,8758,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24634,pipeline,pipelines,24634,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76454,configurat,configuration,76454,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['configurat'],['configuration']
Deployability,"following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images; To create a Docker image, use; docker build -t us-docker.pkg.dev/<my-project>/<my-image>:<tag> -f Dockerfile . * `<dir>` is the context directory, `.` means the current w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1906,install,install,1906,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['install']
Deployability,"for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101682,install,installed,101682,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['install'],['installed']
Deployability,"from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix bug; where repeatedly annotating same field name could cause failure to; compile.; (#11342) Fix to; possible issues about having too many open file handles. New features. (#11300); geom_histogram ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52876,pipeline,pipelines,52876,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where larg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:3465,update,update,3465,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['update'],['update']
Deployability,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42449,configurat,configuration,42449,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['configurat'],['configuration']
Deployability,"g; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:42272,update,updated,42272,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['updated']
Deployability,"gate(is_defined=hail.agg.fraction(hail.is_defined(entry_field))); ); else:; mt = mt._select_all(; row_exprs={'_new_row_key': row_field}, entry_exprs={'is_defined': hail.is_defined(entry_field)}; ); ht = mt.localize_entries('entry_fields', 'phenos'); ht = ht.select(entry_fields=ht.entry_fields.map(lambda entry: entry.is_defined)); data = ht.entry_fields.collect(); if len(data) > 200:; warning(; f'Missingness dataset has {len(data)} rows. '; f'This may take {""a very long time"" if len(data) > 1000 else ""a few minutes""} to plot.'; ); rows = hail.str(ht._new_row_key).collect(). df = pd.DataFrame(data); df = df.rename(columns=dict(enumerate(columns))).rename(index=dict(enumerate(rows))); df.index.name = 'row'; df.columns.name = 'column'. df = pd.DataFrame(df.stack(), columns=['defined']).reset_index(). p = figure(; x_range=columns,; y_range=list(reversed(rows)),; x_axis_location=""above"",; width=plot_width,; height=plot_height,; toolbar_location='below',; tooltips=[('defined', '@defined'), ('row', '@row'), ('column', '@column')],; ). p.grid.grid_line_color = None; p.axis.axis_line_color = None; p.axis.major_tick_line_color = None; p.axis.major_label_text_font_size = ""5pt""; p.axis.major_label_standoff = 0; colors = [""#75968f"", ""#a5bab7"", ""#c9d9d3"", ""#e2e2e2"", ""#dfccce"", ""#ddb7b1"", ""#cc7878"", ""#933b41"", ""#550b1d""]; from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter. mapper = LinearColorMapper(palette=colors, low=df.defined.min(), high=df.defined.max()). p.rect(; x='column',; y='row',; width=1,; height=1,; source=df,; fill_color={'field': 'defined', 'transform': mapper},; line_color=None,; ). color_bar = ColorBar(; color_mapper=mapper,; major_label_text_font_size=""5pt"",; ticker=BasicTicker(desired_num_ticks=len(colors)),; formatter=PrintfTickFormatter(format=""%d""),; label_standoff=6,; border_line_color=None,; location=(0, 0),; ); p.add_layout(color_bar, 'right'); return p. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:59474,update,updated,59474,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['updated']
Deployability,"ge account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar files are located at; gs://hail-REGION-vep/loftee-beta/GRCh38.tar and; gs://hail-REGION-vep/homo-sapiens/95_GRCh38.tar.; A cluster started without the --vep argument is unable to run VEP and cannot be modified to run; VEP. You must start a new cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:3637,update,updated,3637,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['update'],['updated']
Deployability,"genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'subject_id': str; 'SMATSSCR': float64; 'SMCENTER': str; 'SMPTHNTS': str; 'SMRIN': float64; 'SMTS': str; 'SMTSD': str; 'SMUBRID': str; 'SMTSISCH': float64; 'SMTSPAX': float64; 'SMNABTCH': str; 'SMNABTCHT': str; 'SMNABTCHD': str; 'SMGEBTCH': str; 'SMGEBTCHD': str; 'SMGEBTCHT': str; 'SMAFRZE': str; 'SMGTC': str; 'SME2MPRT': float64; 'SMCHMPRS': float64; 'SMNTRART': float64; 'SMNUMGPS': str; 'SMMAPRT': float64; 'SMEXNCRT': float64; 'SM550NRM': str; 'SMGNSDTC': float64; 'SMUNMPRT': float64; 'SM350NRM': str; 'SMRDLGTH': float64; 'SMMNCPB': str; 'SME1MMRT': float64; 'SMSFLGTH': float64; 'SMESTLBS': float64; 'SMMPPD': float64; 'SMNTERRT': float64; 'SMRRNANM': float64; 'SMRDTTL': float64; 'SMVQCFL': float64; 'SMMNCV': str; 'SMTRSCPT': float64; 'SMMPPDPR': float64; 'SMCGLGTH': str; 'SMGAPPCT': str; 'SMUNPDRD': float64; 'SMNTRNRT': float64; 'SMMPUNRT': float64; 'SMEXPEFF': float64; 'SMMPPDUN': float64; 'SME2MMRT': float64; 'SME2ANTI': float64; 'SMALTALG': float64; 'SME2SNSE': float64; 'SMMFLGTH': float64; 'SME1ANTI': float64; 'SMSPLTRD': float64; 'SMBSMMRT': float64; 'SME1SNSE': float64; 'SME1PCTS': float64; 'SMRRNART': float64; 'SME1MPRT': float64; 'SMNUM5CD': str; 'SMDPMPRT': float64; 'SME2PCTS': float64; 'is_female': bool; 'age_range': str; 'death_classification_hardy_scale': str; ----------------------------------------; Row fields:; 'gene_id': str; 'gene_symbol': str; 'gene_interval': interval<locus<GRCh37>>; 'source': str; 'havana_gene_id': str; 'gene_type': str; 'gene_status': str; 'level': str; 'score': float64; 'strand': str; 'frame': int32; 'tag': str; ----------------------------------------; Entry fields:; 'TPM': float64; ----------------------------------------; Column key: ['s']; Row key: ['gene_id']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html:11092,update,updated,11092,docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_RNA_seq_gene_TPMs.html,1,['update'],['updated']
Deployability,"ggregate(; **{rv: hl.agg.take(ht[rv], 1)[0] for rv in ht.row_value if rv not in set([*key, field, value])},; **{; fv: hl.agg.filter(; ht[field] == fv,; hl.rbind(hl.agg.take(ht[value], 1), lambda take: hl.if_else(hl.len(take) > 0, take[0], 'NA')),; ); for fv in field_vals; },; ). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. [docs]@typecheck(ht=Table, field=str, into=sequenceof(str), delim=oneof(str, int)); def separate(ht, field, into, delim) -> Table:; """"""Separate a field into multiple fields by splitting on a delimiter; character or position. :func:`.separate` mimics the functionality of the `separate()` function in R's; ``tidyr`` package. This function will create a new table where ``field`` has been split into; multiple new fields, whose names are given by ``into``. If ``delim`` is a ``str`` (including regular expression strings), ``field``; will be separated into columns by that string. In this case, the length; of ``into`` must match the number of resulting fields. If ``delim`` is an ``int``, ``field`` will be separated into two row fields,; where the first field contains the first ``delim`` characters of ``field``; and the second field contains the remaining characters. Parameters; ----------; ht : :class:`.Table`; A Hail table.; field : :class:`str`; The name of the field to separate in ``ht``.; into : list of :class:`str`; The names of the fields to create by separating ``field``.; delimiter : :class:`str` or :obj:`int`; The character or position by which to separate ``field``. Returns; -------; :class:`.Table`; Table with original ``field`` split into fields whose names are defined; by `into`."""""". if isinstance(delim, int):; ht = ht.annotate(**{into[0]: ht[field][:delim], into[1]: ht[field][delim:]}); else:; split = ht[field].split(delim); ht = ht.annotate(**{into[i]: split[i] for i in range(len(into))}); ht = ht.drop(field). ht_tmp = new_temp_file(); ht.write(ht_tmp). return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html:5047,update,updated,5047,docs/0.2/_modules/hail/experimental/tidyr.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html,2,['update'],['updated']
Deployability,"gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_genome_coverage. View page source. gnomad_genome_coverage. Versions: 2.1, 3.0.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (2.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'row_id': int64; 'locus': locus<GRCh37>; 'mean': float64; 'median': int32; 'over_1': float64; 'over_5': float64; 'over_10': float64; 'over_15': float64; 'over_20': float64; 'over_25': float64; 'over_30': float64; 'over_50': float64; 'over_100': float64; ----------------------------------------; Key: ['locus']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html:9545,update,updated,9545,docs/0.2/datasets/schemas/gnomad_genome_coverage.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_genome_coverage.html,1,['update'],['updated']
Deployability,"gression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:4651,update,update,4651,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['update'],['update']
Deployability,"h more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""; if Env.hc()._warn_entries_order and len(self.col_key) > 0:; warning(; ""entries(): Resulting entries table is sorted by '(row_key, col_key)'.""; ""\n To preserve row-major matrix table order, ""; ""first unkey columns with 'key_cols_by()'""; ); Env.hc()._warn_entries_order = False. return Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:89787,pipeline,pipeline,89787,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipeline']
Deployability,"h was published as:. `Davies, Robert. ""The distribution of a linear combination of chi-squared random variables.""; Applied Statistics 29 323-333. 1980. <http://www.robertnz.net/pdf/lc_chisq.pdf>`__. Davies included Fortran source code in the original publication. Davies also released a `C; language port <http://www.robertnz.net/QF.htm>`__. Hail's implementation is a fairly direct port; of the C implementation to Scala. Davies provides 39 test cases with the source code. The Hail; tests include all 39 test cases as well as a few additional tests. Davies' website cautions:. The method works well in most situations if you want only modest accuracy, say 0.0001. But; problems may arise if the sum is dominated by one or two terms with a total of only one or; two degrees of freedom and x is small. For an accessible introduction the Generalized Chi-Squared Distribution, we strongly recommend; the introduction of this paper:. `Das, Abhranil; Geisler, Wilson (2020). ""A method to integrate and classify normal; distributions"". <https://arxiv.org/abs/2012.14331>`__. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the cumulative distribution function (CDF).; w : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A weight for each non-central chi-square term.; k : :obj:`list` of :obj:`int` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tint32`; A degrees of freedom parameter for each non-central chi-square term.; lam : :obj:`list` of :obj:`float` or :class:`.Expression` of type :py:class:`.tarray` of :py:data:`.tfloat64`; A non-centrality parameter for each non-central chi-square term. We use `lam` instead; of `lambda` because the latter is a reserved word in Python.; mu : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; The standard deviation of the normal term.; sigma : :obj:`float` or :class:`.Expression` of typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:69202,integrat,integrate,69202,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['integrat'],['integrate']
Deployability,"h(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; All expressions appearing as the then parameters to; when() or; default() method calls must be the; same type. See also; case(), cond(), switch(). Parameters:; expr (Expression) – Value to match against. Attributes. Methods. default; Finish the switch statement by adding a default case. or_error; Finish the switch statement by throwing an error with the given message. or_missing; Finish the switch statement by returning missing. when; Add a value test. when_missing; Add a test for missingness. default(then)[source]; Finish the switch statement by adding a default case.; Notes; If no value from a when() call is matched, then; then is returned. Parameters:; then (Expression). Returns:; Expression. or_error(message)[source]; Finish the switch statement by throwing an error with the given message.; Notes; If no value from a SwitchBuilder.when() call is matched, then an; error is thrown. Parameters:; message (Expression of type tstr). Returns:; Expression. or_missing()[source]; Finish the switch statement by returning missing.; Notes; If no value from a when() call is matched, then; the result is missing. Parameters:; then (Expression). Returns:; Expression. when(value, then)[source]; Add a value test. If the base expression is equal to value, then; returns then. Warning; Missingness always compares to missing. Both NA == NA and; NA != NA return NA. Use when_missing(); to test missingness. Parameters:. value (Expression); then (Expression). Returns:; SwitchBuilder – Mutates and returns self. when_missing(then)[source]; Add a test for missingness. If the base expression is missing,; returns then. Parameters:; then (Expression). Returns:; SwitchBuilder – Mutates and returns self. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html:2896,update,updated,2896,docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,https://hail.is,https://hail.is/docs/0.2/functions/hail.expr.builders.SwitchBuilder.html,1,['update'],['updated']
Deployability,"h; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11234,pipeline,pipelines,11234,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"hElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, is now gone. Please; use the standard; https://ACCOUNT.blob.core.windows.net/CONTAINER/PATH. Version 0.2.127; Released 2024-01-12; If you have an Apple M1 laptop, verify that; file $JAVA_HOME/bin/java. returns a message including the phrase “arm64”. If it instead includes; the phrase “x86_64” then you must upgrade to a new version of Java. You; may find such a version of Java; here. New Features. (#14093); hailctl dataproc now creates clusters using Dataproc version; 2.1.33. It previously used version 2.1.2.; (#13617); Query-on-Batch now supports joining two tables keyed by intervals.; (#13795)(#13567); Enable passing a requester pays configuration to hailtop.fs.open. Bug Fixes. (#14110) Fix; hailctl hdinsight start, which has been broken since 0.2.118.; (#14098)(#14090)(#14118); Fix (#14089), which; makes hailctl dataproc connect work in Windows Subsystem for; Linux.; (#14048) Fix; (#13979), affecting; Query-on-Batch and manifesting most frequently as; “com.github.luben.zstd.ZstdException: Corrupted block detected”.; (#14066) Since; 0.2.110, hailctl dataproc set the heap size of the driver JVM; dangerously high. It is now set to an appropriate level. This issue; manifests in a variety of inscrutable ways including; RemoteDisconnectedError and socket closed. See issue; (#1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:17814,upgrade,upgrade,17814,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multiple features should be split into multiple PRs.; Before submitting your PR, you should rebase onto the latest main.; PRs must pass all tests before being merged. See the section above on Running the tests locally.; PRs require a review before being merged. We will assign someone from our dev team to review your PR.; When you make a PR, include a short message that describes the purpose of the; PR and any necessary context for the changes you are making.; For user facing changes (new functions, e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:2212,install,install-dev-requirements,2212,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install-dev-requirements']
Deployability,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8988,configurat,configuration,8988,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"he facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; _base_scale_mappings: ClassVar = {; ""shared_xaxes"": ""all"",; ""shared_yaxes"": ""all"",; }. _scale_mappings: ClassVar = {; ""fixed"": _base_scale_mappings,; ""free_x"": {; **_base_scale_mappings,; ""shared_xaxes"": False,; },; ""free_y"": {; **_base_scale_mappings,; ""shared_yaxes"": False,; },; ""free"": {; ""shared_xaxes"": False,; ""shared_yaxes"": False,; },; }. def __init__(; self, facets: StructExpression, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ):; if nrow is not None and ncol is not None:; raise ValueError(""Both `nrow` and `ncol` were specified. "" ""Please specify only one of these values.""); if scales not in self._scale_mappings:; raise ValueError(; f""An unsupported value ({scales}) was provided for `scales`. ""; f""Supported values are: {[k for k in self._scale_mappings.keys()]}.""; ); self.nrow = nrow; self.ncol = ncol; self.facets = facets; self.scales = scales. def get_expr_to_group_by(self) -> StructExpression:; return self.facets. def get_facet_nrows_and_ncols(self, num_facet_values: int) -> Tuple[int, int]:; if self.ncol is not None:; return (n_partitions(num_facet_values, self.ncol), self.ncol); elif self.nrow is not None:; return (self.nrow, n_partitions(num_facet_values, self.nrow)); else:; ncol = int(math.ceil(math.sqrt(num_facet_values))); return (n_partitions(num_facet_values, ncol), ncol). def get_shared_axis_kwargs(self) -> Dict[str, str]:; return self._scale_mappings[self.scales]. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:3484,update,updated,3484,docs/0.2/_modules/hail/ggplot/facets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html,2,['update'],['updated']
Deployability,"hema (2.1.1, GRCh37). ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_variant_co-occurrence. View page source. gnomad_variant_co-occurrence. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; 'max_freq': float64; 'least_consequence': str; 'same_haplotype_em_probability_cutoff': float64; 'different_haplotypes_em_probability_cutoff': float64; 'global_annotation_descriptions': struct {; max_freq: str,; least_consequence: str,; same_haplotype_em_probability_cutoff: str,; different_haplotypes_em_probability_cutoff: str; }; 'row_annotation_descriptions': struct {; locus1: str,; alleles1: str,; locus2: str,; alleles2: str,; phase_info: struct {; description: str,; gt_counts: str,; em: struct {; hap_counts: str,; p_chet: str,; same_haplotype: str,; different_haplotype: str; }; }; }; ----------------------------------------; Row fields:; 'locus1': locus<GRCh37>; 'alleles1': array<str>; 'locus2': locus<GRCh37>; 'alleles2': array<str>; 'phase_info': dict<str, struct {; gt_counts: array<int32>,; em: struct {; hap_counts: array<float64>,; p_chet: float64,; same_haplotype: bool,; different_haplotype: bool; }; }>; ----------------------------------------; Key: ['locus1', 'alleles1', 'locus2', 'alleles2']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html:10199,update,updated,10199,docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_variant_co-occurrence.html,1,['update'],['updated']
Deployability,"hen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; tsl: int32,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'rsid': set<str>; 'common_low_heteroplasmy': bool; 'base_qual_hist': array<int64>; 'position_hist': array<int64>; 'strand_bias_hist': array<int64>; 'weak_evidence_hist': array<int64>; 'contamination_hist': array<int64>; 'heteroplasmy_below_min_het_threshold_hist': array<int64>; 'excluded_AC': int64; 'AN': int64; 'AC_hom': int64; 'AC_het': int64; 'hl_hist': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_all': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_hist_alt': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'dp_mean': float64; 'mq_mean': float64; 'tlod_mean': float64; 'AF_hom': float32; 'AF_het': float32; 'max_hl': float64; 'hap_AN': array<int64>; 'hap_AC_het': array<int64>; 'hap_AC_hom': array<int64>; 'hap_AF_hom': array<float32>; 'hap_AF_het': array<float32>; 'hap_hl_hist': array<array<int64>>; 'hap_faf_hom': array<float64>; 'hapmax_AF_hom': str; 'hapmax_AF_het': str; 'faf_hapmax_hom': float64; 'pop_AN': array<int64>; 'pop_AC_het': array<int64>; 'pop_AC_hom': array<int64>; 'pop_AF_hom': array<float32>; 'pop_AF_het': array<float32>; 'pop_hl_hist': array<array<int64>>; 'age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html:19008,update,updated,19008,docs/0.2/datasets/schemas/gnomad_chrM_sites.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_chrM_sites.html,1,['update'],['updated']
Deployability,"her bash jobs. Likewise, InputResourceFile,; JobResourceFile, and ResourceGroup can be passed to; PythonJob.call(). Batch automatically detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3673,install,installed,3673,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['install'],['installed']
Deployability,"hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_req",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:22519,update,update,22519,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), old_to_new_dict); mt = mt.annotate_rows(old_to_new=old_to_new, new_to_old=new_to_old); new_locus_alleles = hl.min_rep(mt.locus, mt.new_to_old.map(lambda i: mt.alleles[i])); mt = mt.annotate_rows(__new_locus=new_locus_alleles.locus, __new_alleles=new_locus_alleles.alleles); mt = mt.filter_rows(hl.len(mt.__new_alleles) > 1); left = mt.filter_rows((mt.locus == mt.__new_locus) & (mt.alleles == mt.__new_alleles)). right = mt.filter_rows((mt.locus != mt.__new_locus) | (mt.alleles != mt.__new_alleles)); right = right.key_rows_by(locus=right.__new_locus, alleles=right.__new_alleles); return left.union_rows(right, _check_cols=False).drop('__allele_inclusion', '__new_locus', '__new_alleles'). [docs]@typecheck(mt=MatrixTable, f=anytype, subset=bool); def filter_alleles_hts(mt: MatrixTable, f: Callable, subset: bool = False) -> MatrixTable:; """"""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, befo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:157652,update,update,157652,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['update']
Deployability,"hl.linalg.utils.locus_windows(ht.locus, 1); (array([0, 0, 2, 3, 3, 5]), array([2, 2, 3, 5, 5, 6])). Windows with 1cm radius:; >>> hl.linalg.utils.locus_windows(ht.locus, 1.0, coord_expr=ht.cm); (array([0, 1, 1, 3, 3, 5]), array([1, 3, 3, 5, 5, 6])). Notes; This function returns two 1-dimensional ndarrays of integers,; starts and stops, each of size equal to the number of rows.; By default, for all indices i, [starts[i], stops[i]) is the maximal; range of row indices j such that contig[i] == contig[j] and; position[i] - radius <= position[j] <= position[i] + radius.; If the global_position() on locus_expr is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that has; been ordered by locus_expr.; Set coord_expr to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-nan, on the; same source as locus_expr, and ascending with respect to locus; position for each contig; otherwise the function will fail.; The last example above uses centimorgan coordinates, so; [starts[i], stops[i]) is the maximal range of row indices j such; that contig[i] == contig[j] and; cm[i] - radius <= cm[j] <= cm[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. locus_expr (LocusExpression) – Row-indexed locus expression on a table or matrix table.; radius (int) – Radius of window for row values.; coord_expr (Float64Expression, optional) – Row-indexed numeric expression for the row value.; Must be on the same table or matrix table as locus_expr.; By default, the row value is given by the locus position. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop indices array. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/utils/index.html:4883,update,updated,4883,docs/0.2/linalg/utils/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html,2,['update'],['updated']
Deployability,"hood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the leading coefficient of the unique parabola through these points (as given by Lagrange interpolation) and the leading coefficient of the log of the normal distribution, we have:. \[\frac{x_3 (y_2 - y_1) + x_2 (y_1 - y_3) + x_1 (y_3 - y_2))}{(x_2 - x_1)(x_1 - x_3)(x_3 - x_2)} = -\frac{1}{2 \sigma^2}\]; The standard error \(\hat{\sigma}\) is then estimated by solving for \(\sigma\).; Note that the mean and standard deviation of the (discretized or continuous) distribution held in global.lmmreg.fit.normLkhdH2 will not coincide with \(\hat{h}^2\) and \(\hat{\sigma}\), since this distribution only becomes normal in the infinite sample limit. One can visually assess normality by plotting this distribution against a normal distribution with the same mean and standard deviation, or use this distribution to approximate credible intervals under a flat prior on \(h^2\).; Testing each variant for association; Fixing a single variant, we define:. \(v = n \times 1\) vector of genotypes, with missing genotypes imputed as the mean of called genotypes; \(X_v = \left[v | X \right] = n \times (1 + c)\) matrix concatenating \(v\) and \(X\); \(\beta_v = (\beta^0_v, \beta^1_v, \ldots, \beta^c_v) = (1 + c) \times 1\) vector of covariate coefficients. Fixing \(\delta\) at the global REML estimate \(\hat{\delta}\), we find the REML estimate \((\hat{\beta}_v, \hat{\sigma}_{g,v}^2)\) via rotation of the model. \[y \sim \mathrm{N}\left(X_v\b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:104394,continuous,continuous,104394,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['continuous'],['continuous']
Deployability,"ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see the first few rows; of a table.; Table.take() will collect the first n rows of a table into a local; Python list:; >>> first3 = ht.take(3); >>> first3; [Struct(ID=1, HT=65, SEX='M', X=5, Z=4, C1=2, C2=50, C3=5),; Struct(ID=2, HT=72, SEX='M', X=6, Z=3, C1=2, C2=61, C3=1),; Struct(ID=3, HT=70, SEX='F', X=7, Z=3, C1=10, C2=81, C3=-5)]. Note that each element of the list is a Struct whose elements can be; accessed using Python’s get attribute or get item notation:; >>> first3[0].ID; 1. >>> first3[0]['ID']; 1. The Table.head() method is helpful for testing pipelines. It subsets a; table to the first n rows, causing downstream operations to run much more; quickly.; Table.describe() is a useful method for showing all of the fields of the; table and their types. The types themselves can be accessed using the fields; (e.g. ht.ID.dtype), and the full row and global types can be accessed with; ht.row.dtype and ht.globals.dtype. The row fields that are part of the; key can be accessed with Table.key. The Table.count() method; returns the number of rows. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:8843,pipeline,pipelines,8843,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"iables. You may want to add these to the appropriate dot-file (we recommend ~/.profile); so that you don’t need to rerun these commands in each new session.; Here, fill in the path to the un-tarred Spark package.; export SPARK_HOME=???. Here, fill in the path to the unzipped Hail distribution.; export HAIL_HOME=???; export PATH=$PATH:$HAIL_HOME/bin/. Once you’ve set up Hail, we recommend that you run the Python tutorials to get an overview of Hail; functionality and learn about the powerful query language. To try Hail out, run the below commands; to start a Jupyter Notebook server in the tutorials directory.; cd $HAIL_HOME/tutorials; jhail. You can now click on the “hail-overview” notebook to get started!. Building Hail from source¶. On a Debian-based Linux OS like Ubuntu, run:; $ sudo apt-get install g++ cmake. On Mac OS X, install Xcode, available through the App Store, for the C++ compiler. CMake can be downloaded from the CMake website or through Homebrew. To install with Homebrew, run; $ brew install cmake. The Hail source code. To clone the Hail repository using Git, run; $ git clone --branch 0.1 https://github.com/broadinstitute/hail.git; $ cd hail. You can also download the source code directly from Github.; You may also want to install Seaborn, a Python library for statistical data visualization, using conda install seaborn or pip install seaborn. While not technically necessary, Seaborn is used in the tutorials to make prettier plots. The following commands are relative to the hail directory.; The single command. $ ./gradlew -Dspark.version=2.0.2 shadowJar. creates a Hail JAR file at build/libs/hail-all-spark.jar. The initial build takes time as Gradle installs all Hail dependencies.; Add the following environmental variables by filling in the paths to SPARK_HOME and HAIL_HOME below and exporting all four of them (consider adding them to your .bashrc):; $ export SPARK_HOME=/path/to/spark; $ export HAIL_HOME=/path/to/hail; $ export PYTHONPATH=""$PYTHONPATH:$H",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:2121,install,install,2121,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,2,['install'],['install']
Deployability,"iant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html:9700,update,updated,9700,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Amygdala_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices, aggregations=LinkedList); def construct_expr(; x: ir.IR, type: HailType, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation); ):; if type is None:; return Expression(x, None, indices, aggregations); x.assign_type(type); if isinstance(type, tarray) and is_numeric(type.element_type):; return ArrayNumericExpression(x, type, indices, aggregations); elif isinstance(type, tarray):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return ArrayStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tset):; etype = type.element_type; if isinstance(etype, (hl.tarray, hl.tset)):; while isinstance(etype, (hl.tarray, hl.tset)):; etype = etype.element_type; if isinstance(etype, hl.tstruct):; return SetStructExpression(x, type, indices, aggregations); else:; return typ_to_expr[type.__class__](x, type, indices, aggregations); elif isinstance(type, tndarray) and is_numeric(type.element_type):; return NDArrayNumericExpression(x, type, indices, aggregations); elif type in scalars:; return scalars[type](x, type, indices, aggregations); elif type.__class__ in typ_to_expr:; return typ_to_expr[type.__class__](x, type, indices, aggregations); else:; raise NotImplementedError(type). @typecheck(name=str, type=HailType, indices=Indices); def construct_reference(name, type, indices):; assert isinstance(type, hl.tstruct); x = ir.SelectedTopLevelReference(name, type); return construct_expr(x, type, indices). @typecheck(name=str, type=HailType, indices=Indices, aggregations=LinkedList); def construct_variable(name, type, indices: Indices = Indices(), aggregations: LinkedList = LinkedList(Aggregation)):; return construct_expr(ir.Ref(name, type), type, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:118675,update,updated,118675,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['update'],['updated']
Deployability,"ices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9712,update,updated,9712,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hypothalamus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Liver_all_snp_gene_associations. View page source. GTEx_sQTL_Liver_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Liver_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Ovary_all_snp_gene_associations. View page source. GTEx_sQTL_Ovary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Ovary_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html:9823,update,updated,9823,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Anterior_cingulate_cortex_BA24_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ich_missing_bit = i % 8; if which_missing_bit == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:34727,update,update,34727,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['update'],['update']
Deployability,"ide a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) Fixed a combinatorial explosion in cancellation calculation in the LocalBackend; (#12917) ABS blob URIs in the form of https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH> are now supported when running in Azure. The hail-az scheme for referencing ABS blobs is now deprecated and will be removed in a future release. Version 0.2.114. (#12780) PythonJobs now handle arguments with resources nested inside dicts and lists.; (#12900) Reading data from public blobs is now supported in Azure. Version 0.2.113. (#12780) The LocalBackend now supports always_run jobs. The LocalBackend will no longer immediately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called functio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:2826,release,release,2826,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['release'],['release']
Deployability,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178201,pipeline,pipelines,178201,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['pipeline'],['pipelines']
Deployability,"ields (Dict[str, Expression], optional) – Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all (bool, optional) – Deprecated - use n_divisions instead.; n_divisions (int, optional.) – Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use None to collect all points.; significance_line (float, optional) – p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If None, no line is added. Returns:; bokeh.models.Plot. hail.plot.output_notebook()[source]; Configure the Bokeh output state to generate output in notebook; cells when bokeh.io.show() is called. Calls; bokeh.io.output_notebook(). hail.plot.visualize_missingness(entry_field, row_field=None, column_field=None, window=6000000, plot_width=1800, plot_height=900)[source]; Visualize missingness in a MatrixTable.; Inspired by naniar.; Row field is windowed by default, and missingness is aggregated over this window to generate a proportion defined.; This windowing is set to 6,000,000 by default, so that the human genome is divided into ~500 rows.; With ~2,000 columns, this function returns a sensibly-sized plot with this windowing. Warning; Generating a plot with more than ~1M points takes a long time for Bokeh to render. Consider windowing carefully. Parameters:. entry_field (Expression) – Field for which to check missingness.; row_field (NumericExpression or LocusExpression) – Row field to use for y-axis (can be windowed). If not provided, the row key will be used.; column_field (StringExpression) – Column field to use for x-axis. If not provided, the column key will be used.; window (int, optional) – Size of window to summarize by row_field. If set to None, each field will be used individually.; plot_width (int) – Plot width in px.; plot_height (int) – Plot height in px. Returns:; bokeh.plotting.figure. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:14844,update,updated,14844,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['update'],['updated']
Deployability,"ies per continent, but the number of people living on each continent. We can do this with geom_bar as well by specifying a weight. [13]:. ggplot(gp_2007, aes(x=gp_2007.continent)) + geom_bar(aes(fill=gp_2007.continent, weight=gp_2007.pop)). [13]:. Histograms are similar to bar plots, except they break a continuous x axis into bins. Let’s import the iris dataset for this. [14]:. iris = hl.Table.from_pandas(plotly.data.iris()); iris.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'sepal_length': float64; 'sepal_width': float64; 'petal_length': float64; 'petal_width': float64; 'species': str; 'species_id': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s make a histogram:. [15]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(). [15]:. By default histogram plots groups stacked on top of each other, which is not always easy to interpret. We can specify the position argument to histogram to get different behavior. ""dodge"" puts the bars next to each other:. [16]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""dodge""). [16]:. And ""identity"" plots them over each other. It helps to set an alpha value to make them slightly transparent in these cases. [17]:. ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) + geom_histogram(position=""identity"", alpha=0.8). [17]:. Labels and Axes; It’s always a good idea to label your axes. This can be done most easily with xlab and ylab. We can also use ggtitle to add a title. Let’s pull in the same plot from above, and add labels. [18]:. (ggplot(iris, aes(x=iris.sepal_length, fill=iris.species)) +; geom_histogram(position=""identity"", alpha=0.8) +; xlab(""Sepal Length"") + ylab(""Number of samples"") + ggtitle(""Sepal length by flower type""); ). [18]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:7590,update,updated,7590,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['update'],['updated']
Deployability,"if figure is None:; p = bokeh.plotting.figure(; title=title,; x_axis_label=legend,; y_axis_label=y_axis_label,; y_axis_type=y_axis_type,; width=600,; height=400,; tools='xpan,xwheel_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Labe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9467,update,update,9467,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"il/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:126441,configurat,configuration,126441,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"iltered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""; if mt.entry.dtype != hl.hts_entry_schema:; raise FatalError(; ""'filter_alleles_hts': entry schema must be the HTS entry schema:\n""; "" found: {}\n""; "" expected: {}\n""; "" Use 'hl.filter_alleles' to split entries with non-HTS entry fields."".format(; mt.entry.dtype, hl.hts_entry_schema; ); ). mt = filter_alleles(mt, f). if subset:; newPL = hl.if_else(; hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:161986,update,update,161986,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['update']
Deployability,"il’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS envir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1740,configurat,configuration,1740,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['configurat'],['configuration']
Deployability,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:103042,integrat,integrate,103042,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['integrat'],['integrate']
Deployability,"in_edges[0])),; line_color='black',; fill_color='green',; legend_label='Outliers Above',; ); if data.n_smaller > 0:; p.quad(; bottom=0,; top=data.n_smaller,; left=data.bin_edges[0] - (data.bin_edges[1] - data.bin_edges[0]),; right=data.bin_edges[0],; line_color='black',; fill_color='red',; legend_label='Outliers Below',; ); if interactive:. def mk_interact(handle):; def update(bins=bins, phase=0):; if phase > 0 and phase < 1:; bins = bins + 1; delta = (cdf['values'][-1] - cdf['values'][0]) / bins; edges = np.linspace(cdf['values'][0] - (1 - phase) * delta, cdf['values'][-1] + phase * delta, bins); else:; edges = np.linspace(cdf['values'][0], cdf['values'][-1], bins); hist, edges = np.histogram(cdf['values'], bins=edges, weights=np.diff(cdf.ranks), density=True); new_data = {'top': hist, 'left': edges[:-1], 'right': edges[1:], 'bottom': np.full(len(hist), 0)}; q.data_source.data = new_data; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, bins=(0, 5 * bins), phase=(0, 1, 0.01)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; normalize=bool,; log=bool,; ); def cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False) -> figure:; """"""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(data, Expression):; if data._indices.source is not None:; agg_f = data._aggregation_metho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:13711,update,update,13711,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"ing Python’s Counter class. [23]:. from collections import Counter; counts = Counter(snp_counts); counts.most_common(). [23]:. [(Struct(ref='C', alt='T'), 2418),; (Struct(ref='G', alt='A'), 2367),; (Struct(ref='A', alt='G'), 1929),; (Struct(ref='T', alt='C'), 1864),; (Struct(ref='C', alt='A'), 494),; (Struct(ref='G', alt='T'), 477),; (Struct(ref='T', alt='G'), 466),; (Struct(ref='A', alt='C'), 451),; (Struct(ref='C', alt='G'), 150),; (Struct(ref='G', alt='C'), 111),; (Struct(ref='T', alt='A'), 77),; (Struct(ref='A', alt='T'), 75)]. It’s nice to see that we can actually uncover something biological from this small dataset: we see that these frequencies come in pairs. C/T and G/A are actually the same mutation, just viewed from from opposite strands. Likewise, T/A and A/T are the same mutation on opposite strands. There’s a 30x difference between the frequency of C/T and A/T SNPs. Why?; The same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest gnomAD release publishes about 250 million variants, and that won’t fit in memory on a single computer.; What about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 284 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about 5 trillion unique genotypes.; Hail plotting functions allow Hail fields as arguments, so we can pass in the DP field directly here. If the range and bins arguments are not set, this function will compute the range based on minimum and maximum values of the field and use the default 50 bins. [24]:. p = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p). [Stage 22:> (0 + 1) / 1]. Quality Control; QC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:11825,release,release,11825,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['release'],['release']
Deployability,"ing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """""". if alternative == 'two.sided':; warning(; '""two.sided"" is a deprecated and will be removed in a future '; 'release, please use ""two-sided"" for the `alternative` parameter '; 'to hl.binom_test'; ); alternative = 'two-sided'. alt_enum = {""two-sided"": 0, ""less"": 1, ""greater"": 2}[alternative]; return _func(""binomTest"", tfloat64, x, n, p, to_expr(alt_enum)). [docs]@typecheck(x=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def pchisqtail(x, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom. Examples; --------. >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:62490,release,release,62490,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['release'],['release']
Deployability,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39272,configurat,configuration-dependent,39272,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['configurat'],['configuration-dependent']
Deployability,"inux; Google Dataproc; Next Steps. Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Use Hail on Google Dataproc. View page source. Use Hail on Google Dataproc; First, install Hail on your Mac OS X or Linux laptop or; desktop. The Hail pip package includes a tool called hailctl dataproc which starts, stops, and; manipulates Hail-enabled Dataproc clusters.; Start a dataproc cluster named “my-first-cluster”. Cluster names may only; contain a mix lowercase letters and dashes. Starting a cluster can take as long; as two minutes.; hailctl dataproc start my-first-cluster. Create a file called “hail-script.py” and place the following analysis of a; randomly generated dataset with five-hundred samples and half-a-million; variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Submit the analysis to the cluster and wait for the results. You should not have; to wait more than a minute.; hailctl dataproc submit my-first-cluster hail-script.py. When the script is done running you’ll see 25 rows of variant association; results.; You can also start a Jupyter Notebook running on the cluster:; hailctl dataproc connect my-first-cluster notebook. When you are finished with the cluster stop it:; hailctl dataproc stop my-first-cluster. Next Steps. Read more about Hail on Google Cloud; Get the Hail cheatsheets; Follow the Hail GWAS Tutorial. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/dataproc.html:2162,update,updated,2162,docs/0.2/install/dataproc.html,https://hail.is,https://hail.is/docs/0.2/install/dataproc.html,1,['update'],['updated']
Deployability,"ion. The resulting VDS will be larger and slower in Hail but the genotypes will be accessible from other tools that support Parquet. """""". if self._is_generic_genotype:; self._jvdf.write(output, overwrite); else:; self._jvdf.write(output, overwrite, parquet_genotypes). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(expr=strlike,; annotation=strlike,; subset=bool,; keep=bool,; filter_altered_genotypes=bool,; max_shift=integral,; keep_star=bool); def filter_alleles(self, expr, annotation='va = va', subset=True, keep=True,; filter_altered_genotypes=False, max_shift=100, keep_star=False):; """"""Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. ``aIndex`` will never be zero). .. include:: requireTGenotype.rst. **Examples**. To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:. >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of ``aIndices`` because; we are mapping between the old and new *allele* indices, not; the *alternate allele* indices. **Notes**. If ``filter_altered_genotypes`` is true, genotypes that contain filtered-out alleles are set to missing. :py:meth:`~hail.VariantDataset.filter_alleles` implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. **Subset algorithm**. The subset algorithm (the default, ``subset=True``) subsets the; AD and PL",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:61168,update,update,61168,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['update'],['update']
Deployability,"ion` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""; self._unify_type(then.dtype); self._cases.append((condition, then)); return self. [docs] @typecheck_method(then=expr_any); def default(self, then):; """"""Finish the case statement by adding a default case. Notes; -----; If no condition from a :meth:`~.CaseBuilder.when` call is ``True``,; then `then` is returned. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; return then; self._unify_type(then.dtype); return self._finish(then). [docs] def or_missing(self):; """"""Finish the case statement by returning missing. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; the result is missing. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_missing' cannot be called without at least one 'when' call""); from hail.expr.functions import missing. return self._finish(missing(self._ret_type)). [docs] @typecheck_method(message=expr_str); def or_error(self, message):; """"""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""; if len(self._cases) == 0:; raise ExpressionException(""'or_error' cannot be called without at least one 'when' call""); error_expr = construct_expr(ir.Die(message._ir, self._ret_type), self._ret_type); return self._finish(error_expr). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:8658,update,updated,8658,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,2,['update'],['updated']
Deployability,"ions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:5352,continuous,continuous,5352,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['continuous'],['continuous']
Deployability,"ior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor() as bpe: ; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters:. name (Optional[str]) – A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend (Optional[ServiceBackend]) – Backend used to execute the jobs. Must be a ServiceBackend.; image (Optional[str]) – The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the dill Python package; installed. If you intend to use numpy, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; numpy, scipy, and sklearn installed is used.; cpus_per_job (Union[str, int, None]) – The number of CPU cores to allocate to each job. The default value is; 1. The parameter is passed unaltered to Job.cpu(). This; parameter’s value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit (bool) – If True or unspecified, wait for all jobs to complete when exiting a; context. If False, do not wait. This option has no effect if this; executor is not used with the with syntax.; cleanup_bucket (bool) – If True or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project (Optional[str]) – DEPRECATED. Please specify gcs_requester_pays_configuration ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:2479,install,installed,2479,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,2,['install'],['installed']
Deployability,"is batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2037,configurat,configurations,2037,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['configurat'],['configurations']
Deployability,"is facing side, none by default. Overrides ``color`` aesthetic. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomArea(mapping, fill=fill, color=color). class GeomRibbon(Geom):; aes_to_arg: ClassVar = {; ""fill"": (""fillcolor"", ""black""),; ""color"": (""line_color"", ""rgba(0, 0, 0, 0)""),; ""tooltip"": (""hovertext"", None),; ""fill_legend"": (""name"", None),; }. def __init__(self, aes, fill, color):; super().__init__(aes); self.fill = fill; self.color = color. def apply_to_fig(; self, grouped_data, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; def plot_group(df):; trace_args_bottom = {; ""x"": df.x,; ""y"": df.ymin,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""showlegend"": False,; }; self._add_aesthetics_to_trace_args(trace_args_bottom, df); self._update_legend_trace_args(trace_args_bottom, legend_cache). trace_args_top = {; ""x"": df.x,; ""y"": df.ymax,; ""row"": facet_row,; ""col"": facet_col,; ""mode"": ""lines"",; ""fill"": 'tonexty',; }; self._add_aesthetics_to_trace_args(trace_args_top, df); self._update_legend_trace_args(trace_args_top, legend_cache). fig_so_far.add_scatter(**trace_args_bottom); fig_so_far.add_scatter(**trace_args_top). for group_df in grouped_data:; plot_group(group_df). def get_stat(self):; return StatIdentity(). [docs]def geom_ribbon(mapping=aes(), fill=None, color=None):; """"""Creates filled in area between two lines specified by x, ymin, and ymax. Supported aesthetics: ``x``, ``ymin``, ``ymax``, ``color``, ``fill``, ``tooltip``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; fill:; Color of fill to draw, black by default. Overrides ``fill`` aesthetic.; color:; Color of line to draw outlining both side, none by default. Overrides ``color`` aesthetic. :return:; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomRibbon(mapping, fill=fill, color=color). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:28060,update,updated,28060,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['update'],['updated']
Deployability,"is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; ld_score() will fail if entry_expr results in any missing; va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:5619,continuous,continuous,5619,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['continuous'],['continuous']
Deployability,"it == 0:; current_missing_byte = missing_bytes[i // 8]. if lookup_bit(current_missing_byte, which_missing_bit):; kwargs[f] = None; else:; field_decoded = t._convert_from_encoding(byte_reader, _should_freeze); kwargs[f] = field_decoded. return Struct(**kwargs). def _convert_to_encoding(self, byte_writer: ByteWriter, value):; keys = list(self.keys()); length = len(keys); i = 0; while i < length:; missing_byte = 0; for j in range(min(8, length - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:34770,update,update,34770,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['update'],['update']
Deployability,"it multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102008,configurat,configuration,102008,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"iven_dn + p_data_given_missed_het). def solve(p_de_novo):; return (; hl.case(); .when(kid.GQ < min_gq, failure); .when((kid.DP / (parent.DP) < min_dp_ratio) | (kid_ad_ratio < min_child_ab), failure); .when((hl.sum(parent.AD) == 0), failure); .when(parent.AD[1] / hl.sum(parent.AD) > max_parent_ab, failure); .when(p_de_novo < min_p, failure); .when(; ~is_snp,; hl.case(); .when(; (p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles <= 5),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.3, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(),; ); .default(; hl.case(); .when(; ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (dp_ratio > 0.2)); | ((p_de_novo > 0.99) & (kid_ad_ratio > 0.3) & (n_alt_alleles == 1)); | ((p_de_novo > 0.5) & (kid_ad_ratio > 0.3) & (n_alt_alleles < 10) & (kid.DP > 10)),; hl.struct(p_de_novo=p_de_novo, confidence='HIGH'),; ); .when(; (p_de_novo > 0.5) & ((kid_ad_ratio > 0.3) | (n_alt_alleles == 1)),; hl.struct(p_de_novo=p_de_novo, confidence='MEDIUM'),; ); .when(kid_ad_ratio > 0.2, hl.struct(p_de_novo=p_de_novo, confidence='LOW')); .or_missing(); ); ). return hl.bind(solve, p_de_novo). de_novo_call = (; hl.case(); .when(~het_hom_hom | kid_ad_fail, failure); .when(autosomal, hl.bind(call_auto, kid_pp, dad_pp, mom_pp, kid_ad_ratio)); .when(hemi_x | hemi_mt, hl.bind(call_hemi, kid_pp, mom, mom_pp, kid_ad_ratio)); .when(hemi_y, hl.bind(call_hemi, kid_pp, dad, dad_pp, kid_ad_ratio)); .or_missing(); ). tm = tm.annotate_entries(__call=de_novo_call); tm = tm.filter_entries(hl.is_defined(tm.__call)); entries = tm.entries(); return entries.select(; '__site_freq',; 'proband',; 'father',; 'mother',; 'proband_entry',; 'father_entry',; 'mother_entry',; 'is_female',; **entries.__call,; ).rename({'__site_freq': 'prior'}). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:33740,update,updated,33740,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['update'],['updated']
Deployability,"ix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be unreadable.; (#11312) Fix; aggregator memory leak.; (#11340) Fix b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52702,release,release,52702,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"ixTable. Schema (phase_3, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 's': str; 'population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html:10939,update,updated,10939,docs/0.2/datasets/schemas/1000_Genomes_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_chrY.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Spleen_all_snp_gene_associations. View page source. GTEx_eQTL_Spleen_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Spleen_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Testis_all_snp_gene_associations. View page source. GTEx_eQTL_Testis_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Testis_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Uterus_all_snp_gene_associations. View page source. GTEx_eQTL_Uterus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Uterus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Vagina_all_snp_gene_associations. View page source. GTEx_eQTL_Vagina_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html:9676,update,updated,9676,docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Vagina_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"j}} := \\frac{\sum_{s \in S_{ij}}X_{is} X_{js}}{\sum_{s \in S_{ij}}\\widehat{\\sigma^2_{is}} \\widehat{\\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \\widehat{k^{(0)}_{ij}} :=; \\begin{cases}; \\frac{\\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The mini",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:174138,release,release,174138,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['release'],['release']
Deployability,"l be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.maximum(a, b)); array([2, 5, 4], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.maximum(a, b)); array([nan, 5., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – Nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; NDArrayExpression – Element-wise maximums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, the resulting; array will be of that shape. hail.nd.minimum(nd1, nd2)[source]; Compares elements at corresponding indexes in arrays; and returns an array of the minimum element found; at each compared index.; If an array element being compared has the value NaN,; the minimum for that index will be NaN.; Examples; >>> a = hl.nd.array([1, 5, 3]); >>> b = hl.nd.array([2, 3, 4]); >>> hl.eval(hl.nd.minimum(a, b)); array([1, 3, 3], dtype=int32); >>> a = hl.nd.array([hl.float64(float(""NaN"")), 5.0, 3.0]); >>> b = hl.nd.array([2.0, 3.0, hl.float64(float(""NaN""))]); >>> hl.eval(hl.nd.minimum(a, b)); array([nan, 3., nan]). Parameters:. nd1 (NDArrayExpression); nd2 (class:.NDArrayExpression, .ArrayExpression, numpy ndarray, or nested python lists/tuples.) – nd1 and nd2 must be the same shape or broadcastable into common shape. Nd1 and nd2 must; have elements of comparable types. Returns:; min_array (NDArrayExpression) – Element-wise minimums of nd1 and nd2. If nd1 has the same shape as nd2, the resulting array; will be of that shape. If nd1 and nd2 were broadcasted into a common shape, resulting array; will be of that shape. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/nd/index.html:15053,update,updated,15053,docs/0.2/nd/index.html,https://hail.is,https://hail.is/docs/0.2/nd/index.html,1,['update'],['updated']
Deployability,"l if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the x-axis label.; """"""; return Labels(xlabel=label). [docs]def ylab(label):; """"""Sets the y-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired y-axis label of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the y-axis label.; """"""; return Labels(ylabel=label). def labs(**group_labels):; """"""Sets the labels for the legend groups of a plot. Examples; --------. Create a scatterplot and label the legend groups according to their field names:. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared=ht.idx ** 2); >>> ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); >>> ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); >>> fig = (; ... hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)); ... + hl.ggplot.geom_point(hl.ggplot.aes(color=ht.even, shape=ht.threeven)); ... + hl.ggplot.labs(color=""Even"", shape=""Threeven""); ... ). Parameters; ----------; group_labels:; Map names of plotly ``legendgroup``s to the desired replacement labels. Returns; -------; :class:`.FigureAttribute`; Label object to change the legend group labels.; """"""; return Labels(group_labels=group_labels). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:3286,update,updated,3286,docs/0.2/_modules/hail/ggplot/labels.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html,2,['update'],['updated']
Deployability,"l.eval(call.ploidy); 2. Notes; Currently only ploidy 1 and 2 are supported. Returns:; Expression of type tint32. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. unphase()[source]; Returns an unphased version of this call. Returns:; CallExpression. unphased_diploid_gt_index()[source]; Return the genotype index for unphased, diploid calls.; Examples; >>> hl.eval(call.unphased_diploid_gt_index()); 1. Returns:; Expression of type tint32. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.CallExpression.html:11778,update,updated,11778,docs/0.2/hail.expr.CallExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.CallExpression.html,1,['update'],['updated']
Deployability,"l.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:1486,continuous,continuous,1486,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,2,['continuous'],['continuous']
Deployability,"l.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78474,pipeline,pipelines,78474,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"l_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping"": hl.struct(**self.aes)}; if self.facet is not None:; fields_to_select[""facet""] = self.facet.get_expr_to_group_by(). for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx); fields_to_select[geom_label] = hl.struct(**geom.aes.properties). name, ht = hl.struct(**fields_to_select)._to_table('__fallback'); return ht.select(**{field: ht[name][field] for field in fields_to_select}). def collect_mappings_and_precomputed(selected):; mapping_per_geom = []; precomputes = {}; for geom_idx, geom in enumerate(self.geoms):; geom_label = make_geom_label(geom_idx). combined_mapping = selected[""figure_mapping""].annotate(**selected[geom_label]). for key in combined_mapping:; if key in self.scales:; combined_mapping = combined_mapping.annotate(**{; key: self.scales[key].transform_data(combined_mapping[key]); }); mappin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:4387,update,updated,4387,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['update'],['updated']
Deployability,"l_n.take(5); [5, 5, 5, 5, 5]. >>> mt_filt = mt_filt.annotate_rows(row_n = hl.agg.count()); >>> mt_filt.row_n.take(5); [5, 5, 5, 5, 5]. 3. Annotating a new entry field will not annotate filtered entries. >>> mt_filt = mt_filt.annotate_entries(y = 1); >>> mt_filt.aggregate_entries(hl.agg.sum(mt_filt.y)); 50. 4. If all the entries in a row or column of a matrix table are; filtered, the row or column remains. >>> mt_filt.filter_entries(False).count(); (10, 10). See Also; --------; :meth:`unfilter_entries`, :meth:`compute_entry_filter_stats`; """"""; base, cleanup = self._process_joins(expr); analyze('MatrixTable.filter_entries', expr, self._entry_indices). m = MatrixTable(ir.MatrixFilterEntries(base._mir, ir.filter_predicate_with_keep(expr._ir, keep))); return cleanup(m). [docs] def unfilter_entries(self):; """"""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""; entry_ir = hl.if_else(; hl.is_defined(self.entry), self.entry, hl.struct(**{k: hl.missing(v.dtype) for k, v in self.entry.items()}); )._ir; return MatrixTable(ir.MatrixMapEntries(self._mir, entry_ir)). [docs] @typecheck_method(row_field=str, col_field=str); def compute_entry_filter_stats(self, row_field='entry_stats_row', col_field='entry_stats_col') -> 'MatrixTable':; """"""Compute statistics about the number and fraction of filtered entries. .. include:: _templates/experimental.rst. Parameters; ----------; row_field : :class:`str`; Name for computed row field (default: ``entry_stats_row``.; col_field : :class:`str`; Name for computed column field (default: ``entry_stats_col``. Returns; --",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:58811,pipeline,pipeline,58811,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipeline']
Deployability,"l_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == other._contig and self._position == other._position and self._rg == other._rg); if isinstance(other, Locus); else NotImplemented; ). def __hash__(self):; return hash(self._contig) ^ hash(self._position) ^ hash(self._rg). [docs] @classmethod; @typecheck_method(string=str, reference_genome=reference_genome_type); def parse(cls, string, reference_genome='default'):; """"""Parses a locus object from a CHR:POS string. **Examples**. >>> l1 = hl.Locus.parse('1:101230'); >>> l2 = hl.Locus.parse('X:4201230'). :param str string: String to parse.; :param reference_genome: Reference genome to use. Default is :func:`~hail.default_reference`.; :type reference_genome: :class:`str` or :class:`.ReferenceGenome`. :rtype: :class:`.Locus`; """"""; contig, pos = string.split(':'); if pos.lower() == 'end':; pos = reference_genome.contig_length(contig); else:; pos = int(pos); return Locus(contig, pos, reference_genome). @property; def contig(self):; """"""; Chromosome identifier.; :rtype: str; """"""; return self._contig. @property; def position(self):; """"""; Chromosomal position (1-based).; :rtype: int; """"""; return self._position. @property; def reference_genome(self):; """"""Reference genome. :return: :class:`.ReferenceGenome`; """"""; return self._rg. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:3272,update,updated,3272,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,2,['update'],['updated']
Deployability,"l_zoom,reset,save',; active_scroll='xwheel_zoom',; background_fill_color='#EEEEEE',; ); else:; p = figure. n = data['ranks'][-1]; weights = np.diff(data['ranks'][1:-1]); min = data['values'][0]; max = data['values'][-1]; values = np.array(data['values'][1:-1]); slope = 1 / (max - min). def f(x, prev, smoothing=smoothing):; inv_scale = (np.sqrt(n * slope) / smoothing) * np.sqrt(prev / weights); diff = x[:, np.newaxis] - values; grid = (3 / (4 * n)) * weights * np.maximum(0, inv_scale - np.power(diff, 2) * np.power(inv_scale, 3)); return np.sum(grid, axis=1). round1 = f(values, np.full(len(values), slope)); x_d = np.linspace(min, max, 1000); final = f(x_d, round1). line = p.line(x_d, final, line_width=2, line_color='black', legend_label=legend). if interactive:. def mk_interact(handle):; def update(smoothing=smoothing):; final = f(x_d, round1, smoothing); line.data_source.data = {'x': x_d, 'y': final}; bokeh.io.push_notebook(handle=handle). from ipywidgets import interact. interact(update, smoothing=(0.02, 0.8, 0.005)). return p, mk_interact; else:; return p. [docs]@typecheck(; data=oneof(Struct, expr_float64),; range=nullable(sized_tupleof(numeric, numeric)),; bins=int,; legend=nullable(str),; title=nullable(str),; log=bool,; interactive=bool,; ); def histogram(; data, range=None, bins=50, legend=None, title=None, log=False, interactive=False; ) -> Union[figure, Tuple[figure, Callable]]:; """"""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""; if isinstance(d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:9661,update,update,9661,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"label to show on x-axis; breaks (list of str) – The locations to draw ticks on the x-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_genomic(reference_genome, name=None)[source]; The default genomic x scale. This is used when the x aesthetic corresponds to a LocusExpression. Parameters:. reference_genome – The reference genome being used.; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_log10(name=None)[source]; Transforms x axis to be log base 10 scaled. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_x_reverse(name=None)[source]; Transforms x-axis to be vertically reversed. Parameters:; name (str) – The label to show on x-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_continuous(name=None, breaks=None, labels=None, trans='identity')[source]; The default continuous y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of float) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis.; trans (str) – The transformation to apply to the y-axis. Supports “identity”, “reverse”, “log10”. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_discrete(name=None, breaks=None, labels=None)[source]; The default discrete y scale. Parameters:. name (str) – The label to show on y-axis; breaks (list of str) – The locations to draw ticks on the y-axis.; labels (list of str) – The labels of the ticks on the axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_log10(name=None)[source]; Transforms y-axis to be log base 10 scaled. Parameters:; name (str) – The label to show on y-axis. Returns:; FigureAttribute – The scale to be applied. hail.ggplot.scale_y_reverse(name=None)[source]; Transforms y-axis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:11465,continuous,continuous,11465,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['continuous'],['continuous']
Deployability,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10003,configurat,configuration,10003,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84570,install,installed,84570,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; CADD. View page source. CADD. Versions: 1.4, 1.6; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (1.4, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int64,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'raw_score': float64; 'PHRED_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/CADD.html:9446,update,updated,9446,docs/0.2/datasets/schemas/CADD.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/CADD.html,1,['update'],['updated']
Deployability,"ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; UK_Biobank_Rapid_GWAS_both_sexes. View page source. UK_Biobank_Rapid_GWAS_both_sexes. Versions: v2; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (v2, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_cols: int32,; n_partitions: int32; }; ----------------------------------------; Column fields:; 'phenotype': str; 'description': str; 'variable_type': str; 'source': str; 'n_non_missing': int32; 'n_missing': int32; 'n_controls': int32; 'n_cases': int32; 'PHESANT_transformation': str; 'notes': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'variant': str; 'minor_allele': str; 'minor_AF': float64; 'rsid': str; 'varid': str; 'consequence': str; 'consequence_category': str; 'info': float64; 'call_rate': float64; 'alt_AC': int32; 'AF': float64; 'p_hwe': float64; 'n_called': int32; 'n_not_called': int32; 'n_hom_ref': int32; 'n_het': int32; 'n_hom_var': int32; 'n_non_ref': int32; 'r_heterozygosity': float64; 'r_het_hom_var': float64; 'r_expected_het_frequency': float64; ----------------------------------------; Entry fields:; 'expected_case_minor_AC': float64; 'expected_min_category_minor_AC': float64; 'low_confidence_variant': bool; 'n_complete_samples': int32; 'AC': float64; 'ytx': float64; 'beta': float64; 'se': float64; 'tstat': float64; 'pval': float64; ----------------------------------------; Column key: ['phenotype']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html:10509,update,updated,10509,docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/UK_Biobank_Rapid_GWAS_both_sexes.html,2,['update'],['updated']
Deployability,"ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_eQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html:9691,update,updated,9691,docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Whole_Blood_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72527,install,installation,72527,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"le to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-buc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:2286,install,install-gcs-connector,2286,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['install'],['install-gcs-connector']
Deployability,"le.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. size(); Returns the size of a collection.; Examples; >>> hl.eval(a.size()); 5. >>> hl.eval(s3.size()); 3. Returns:; Expression of type tint32 – The number of elements in the collection. starmap(f); Transform each element of a collection of tuples.; Examples; >>> hl.eval(hl.array([(1, 2), (2, 3)]).starmap(lambda x, y: x+y)); [3, 5]. Parameters:; f (function ( (*args) -> Expression)) – Function to transform each element of the collection. Returns:; CollectionExpression. – Collection where each element has been transformed according to f. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html:15408,update,updated,15408,docs/0.2/hail.expr.ArrayExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayExpression.html,2,['update'],['updated']
Deployability,"le_source(caller, expr):; from hail import MatrixTable. source = expr._indices.source; if not isinstance(source, MatrixTable):; raise ValueError(; ""{}: Expect an expression of 'MatrixTable', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def table_source(caller, expr):; from hail import Table. source = expr._indices.source; if not isinstance(source, Table):; raise ValueError(; ""{}: Expect an expression of 'Table', found {}"".format(; caller, ""expression of '{}'"".format(source.__class__) if source is not None else 'scalar expression'; ); ); return source. @typecheck(caller=str, expr=Expression); def raise_unless_entry_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be entry-indexed"" f"", found no indices (no source)""); if expr._indices != expr._indices.source._entry_indices:; raise ExpressionException(; f""{caller}: expression must be entry-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_row_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be row-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._row_indices:; raise ExpressionException(; f""{caller}: expression must be row-indexed"" f"", found indices {list(expr._indices.axes)}.""; ). @typecheck(caller=str, expr=Expression); def raise_unless_column_indexed(caller, expr):; if expr._indices.source is None:; raise ExpressionException(f""{caller}: expression must be column-indexed"" f"", found no indices (no source).""); if expr._indices != expr._indices.source._col_indices:; raise ExpressionException(; f""{caller}: expression must be column-indexed"" f"", found indices ({list(expr._indices.axes)}).""; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:9670,update,updated,9670,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,2,['update'],['updated']
Deployability,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:7191,configurat,configurations,7191,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['configurat'],['configurations']
Deployability,"les.; if aesthetic_str == ""x"":; if is_continuous:; self.scales[""x""] = scale_x_continuous(); elif is_genomic_type(dtype):; self.scales[""x""] = scale_x_genomic(reference_genome=dtype.reference_genome); else:; self.scales[""x""] = scale_x_discrete(); elif aesthetic_str == ""y"":; if is_continuous:; self.scales[""y""] = scale_y_continuous(); elif is_genomic_type(dtype):; raise ValueError(""Don't yet support y axis genomic""); else:; self.scales[""y""] = scale_y_discrete(); elif aesthetic_str == ""color"" and not is_continuous:; self.scales[""color""] = scale_color_discrete(); elif aesthetic_str == ""color"" and is_continuous:; self.scales[""color""] = scale_color_continuous(); elif aesthetic_str == ""fill"" and not is_continuous:; self.scales[""fill""] = scale_fill_discrete(); elif aesthetic_str == ""fill"" and is_continuous:; self.scales[""fill""] = scale_fill_continuous(); elif aesthetic_str == ""shape"" and not is_continuous:; self.scales[""shape""] = scale_shape_auto(); elif aesthetic_str == ""shape"" and is_continuous:; raise ValueError(; ""The 'shape' aesthetic does not support continuous ""; ""types. Specify values of a discrete type instead.""; ); elif is_continuous:; self.scales[aesthetic_str] = ScaleContinuous(aesthetic_str); else:; self.scales[aesthetic_str] = ScaleDiscrete(aesthetic_str). def copy(self):; return GGPlot(self.ht, self.aes, self.geoms[:], self.labels, self.coord_cartesian, self.scales, self.facet). def verify_scales(self):; for aes_key in self.aes.keys():; check_scale_continuity(self.scales[aes_key], self.aes[aes_key].dtype, aes_key); for geom in self.geoms:; aesthetic_dict = geom.aes.properties; for aes_key in aesthetic_dict.keys():; check_scale_continuity(self.scales[aes_key], aesthetic_dict[aes_key].dtype, aes_key). [docs] def to_plotly(self):; """"""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """""". def make_geom_label(geom_idx):; return f""geom{geom_idx}"". def select_table():; fields_to_select = {""figure_mapping",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:3599,continuous,continuous,3599,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['continuous'],['continuous']
Deployability,"lf), hl.agg.count()),; self._summary_aggs(),; )). def _summarize(self, agg_res=None, *, name=None, header=None, top=False):; src = self._indices.source; summary_header = None; if src is None or len(self._indices.axes) == 0:; raise ValueError(""Cannot summarize a scalar expression""); if agg_res is None:; count, agg_res = self._aggregation_method()(hl.tuple((hl.agg.count(), self._all_summary_aggs()))); summary_header = f'{count} records.'; sum_fields, nested = self._summary_fields(agg_res, top); summary = Summary(self._type, agg_res[0], sum_fields, nested, header=summary_header); if name is None and header is None:; return summary; else:; return NamedSummary(summary, name, header). [docs] def summarize(self, handler=None):; """"""Compute and print summary information about the expression. .. include:: _templates/experimental.rst; """""". src = self._indices.source; if self in src._fields:; field_name = src._fields_inverse[self]; prefix = field_name; elif self._ir.is_nested_field:; prefix = self._ir.name; else:; prefix = '<expr>'. if handler is None:; handler = hl.utils.default_handler(); handler(self._summarize(name=prefix)). def _selector_and_agg_method(self):; src = self._indices.source; assert src is not None; assert len(self._indices.axes) > 0; if isinstance(src, hl.MatrixTable):; if self._indices == src._row_indices:; return src.select_rows, lambda t: t.aggregate_rows; elif self._indices == src._col_indices:; return src.select_cols, lambda t: t.aggregate_cols; else:; return src.select_entries, lambda t: t.aggregate_entries; else:; return src.select, lambda t: t.aggregate. def _aggregation_method(self):; return self._selector_and_agg_method()[1](self._indices.source). def _persist(self):; src = self._indices.source; if src is not None:; raise ValueError(""Can only persist a scalar (no Table/MatrixTable source)""); expr = Env.backend().persist_expression(self); assert expr.dtype == self.dtype; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:35943,update,updated,35943,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,2,['update'],['updated']
Deployability,"lf):; return str(self). def __str__(self):; if all(k.isidentifier() for k in self._fields):; return 'Struct(' + ', '.join(f'{k}={v!r}' for k, v in self._fields.items()) + ')'; return 'Struct(**{' + ', '.join(f'{k!r}: {v!r}' for k, v in self._fields.items()) + '})'. def __eq__(self, other):; return self._fields == other._fields if isinstance(other, Struct) else NotImplemented. def __hash__(self):; return 37 + hash(tuple(sorted(self._fields.items()))). def __iter__(self):; return iter(self._fields). def __dir__(self):; super_dir = super().__dir__(); return super_dir + list(self._fields.keys()). def annotate(self, **kwargs):; """"""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""; d = OrderedDict(self.items()); for k, v in kwargs.items():; d[k] = v; return Struct(**d). @typecheck_method(fields=str, kwargs=anytype); def select(self, *fields, **kwargs):; """"""Select existing fields and compute new ones. Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `kwargs` arguments are new fields to add. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.; named_exprs : keyword args; New field. Returns; -------; :class:`.Struct`; Struct containing specified",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:3675,update,updated,3675,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,2,['update'],['updated']
Deployability,"lf, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:10483,install,installed,10483,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['install'],['installed']
Deployability,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5355,pipeline,pipeline,5355,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['pipeline'],['pipeline']
Deployability,"lf.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6938,configurat,configuration,6938,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"lineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats; Schema (0.3, GRCh37). Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_summary_stats. View page source. panukb_summary_stats. Versions: 0.3; Reference genome builds: GRCh37; Type: hail.MatrixTable. Schema (0.3, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'trait_type': str; 'phenocode': str; 'pheno_sex': str; 'coding': str; 'modifier': str; 'pheno_data': array<struct {; n_cases: int32,; n_controls: int32,; heritability: float64,; saige_version: str,; inv_normalized: bool,; pop: str; }>; 'description': str; 'description_more': str; 'coding_description': str; 'category': str; 'n_cases_full_cohort_both_sexes': int64; 'n_cases_full_cohort_females': int64; 'n_cases_full_cohort_males': int64; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'gene': str; 'annotation': str; ----------------------------------------; Entry fields:; 'summary_stats': array<struct {; AF_Allele2: float64,; imputationInfo: float64,; BETA: float64,; SE: float64,; `p.value.NA`: float64,; `AF.Cases`: float64,; `AF.Controls`: float64,; Pvalue: float64,; low_confidence: bool; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html:10227,update,updated,10227,docs/0.2/datasets/schemas/panukb_summary_stats.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_summary_stats.html,2,['update'],['updated']
Deployability,"linear_regression_rows(; y=mt.pheno.CaffeineConsumption,; x=mt.GT.n_alt_alleles(),; covariates=[1.0, mt.pheno.isFemale, mt.scores[0], mt.scores[1], mt.scores[2]],; ). gwas = gwas.select(SNP=hl.variant_str(gwas.locus, gwas.alleles), P=gwas.p_value); gwas = gwas.key_by(gwas.SNP); gwas = gwas.select(gwas.P); gwas.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:4946,install,installed,4946,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['install'],['installed']
Deployability,"ling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; errors.; (#10829) Fix a bug; where hl.missing and CaseBuilder.or_error failed if their; type was a struct containing a field starting with a number. New features. (#10768) Support; multiplying StringExpressions to repeat them, as with normal; python strings. Performance improvements. (#10625) Reduced; need to copy strings around, pipelines with many string operations; should get faster.; (#10775) Improved; performance of to_matrix_table_row_major on both BlockMatrix; and Table. Version 0.2.74; Released 2021-07-26. Bug fixes. (#10697) Fixed bug; in read_table when the table has missing keys and; _n_partitions is specified.; (#10695) Fixed bug; in hl.experimental.loop causing incorrect results when loop state; contained pointers. Version 0.2.73; Released 2021-07-22. Bug fixes. (#10684) Fixed a; rare bug reading arrays from disk where short arrays would have their; first elements corrupted and long arrays would cause segfaults.; (#10523) Fixed bug; where liftover would fail with “Could not initialize class” errors. Version 0.2.72; Released 2021-07-19. New Features. (#10655) Revamped; many hail error messages to give useful python stack traces.; (#10663) Added; DictExpression.items() to mirror python’s dict.items().; (#10657) hl.map; now supports mapping over multiple lists like Python’s built-in; map. Bug fixes.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:57643,pipeline,pipelines,57643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"ll be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus. If a dataset does not have unique rows for each key (consider the; ``gencode`` genes, which may overlap; and ``clinvar_variant_summary``,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters; ----------; rel : :class:`.MatrixTable` or :class:`.Table`; The relational object to which to add annotations.; names : varargs of :class:`str`; The names of the datasets with which to annotate `rel`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; The relational object `rel`, with the annotations from `names`; added.; """"""; rel = self._row_lens(rel); if len(set(names)) != len(names):; raise ValueError(f'cannot annotate same dataset twice,' f' please remove duplicates from: {names}'); self._check_availability(names); datasets = [self._dataset_by_name(name) for name in names]; if any(dataset.is_gene_keyed for dataset in datasets):; gene_field, rel = self._annotate_gene_name(rel); else:; gene_field = None; for dataset in datasets:; if dataset.is_gene_keyed:; genes = rel.select(gene_field).explode(gene_field); genes = genes.annotate(**{dataset.name: dataset.index_compatible_version(genes[gene_field])}); genes = genes.group_by(*genes.key).aggregate(**{; dataset.name: hl.dict(; hl.agg.filter(; hl.is_defined(genes[dataset.name]),; hl.agg.collect((genes[gene_field], genes[dataset.name])),; ); ); }); rel = rel.annotate(**{dataset.name: genes.index(rel.key)[dataset.name]}); else:; indexed_value = dataset.index_compatible_version(rel.key); if isinstance(indexed_value.dtype, hl.tstruct) and len(indexed_value.dtype) == 0:; indexed_value = hl.is_defined(indexed_value); rel = rel.annotate(**{dataset.name: indexed_value}); if gene_field:; rel = rel.drop(gene_field); return rel.unlens(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:17249,update,updated,17249,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['update'],['updated']
Deployability,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38882,configurat,configuration,38882,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,6,"['configurat', 'install', 'release']","['configuration', 'installed', 'release']"
Deployability,"ll not parse to python because it; begins with an integer, which is not an acceptable leading character; for an identifier. There are two ways to access this field:. >>> getattr(bar, '1kg'); >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). :param dict attributes: struct members.; """""". def __init__(self, attributes):. self._attrs = attributes. def __getattr__(self, item):; assert (self._attrs); if item not in self._attrs:; raise AttributeError(""Struct instance has no attribute '%s'"" % item); return self._attrs[item]. def __contains__(self, item):; return item in self._attrs. def __getitem__(self, item):; return self.__getattr__(item). def __len__(self):; return len(self._attrs). def __repr__(self):; return str(self). def __str__(self):; return 'Struct' + str(self._attrs). def __eq__(self, other):; if isinstance(other, Struct):; return self._attrs == other._attrs; else:; return False. def __hash__(self):; return 37 + hash(tuple(sorted(self._attrs.items()))). [docs] @typecheck_method(item=strlike,; default=anytype); def get(self, item, default=None):; """"""Get an item, or return a default value if the item is not found.; ; :param str item: Name of attribute.; ; :param default: Default value.; ; :returns: Value of item if found, or default value if not.; """"""; return self._attrs.get(item, default). @typecheck(struct=Struct); def to_dict(struct):; d = {}; for k, v in struct._attrs.iteritems():; if isinstance(v, Struct):; d[k] = to_dict(v); else:; d[k] = v; return d. import pprint. _old_printer = pprint.PrettyPrinter. class StructPrettyPrinter(pprint.PrettyPrinter):; def _format(self, obj, *args, **kwargs):; if isinstance(obj, Struct):; obj = to_dict(obj); return _old_printer._format(self, obj, *args, **kwargs). pprint.PrettyPrinter = StructPrettyPrinter # monkey-patch pprint. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html:2737,patch,patch,2737,docs/0.1/_modules/hail/representation/annotations.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/annotations.html,1,['patch'],['patch']
Deployability,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:3850,configurat,configuration,3850,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"llele frequency. >>> mt = hl.variant_qc(mt); >>> skat = hl._logistic_skat(; ... mt.gene,; ... hl.dbeta(mt.variant_qc.AF[0], 1, 25),; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | 8.04e+00 | 3.50e-01 | 0 |; | 1 | 9 | 1.22e+00 | 5.04e-01 | 0 |; +-------+-------+----------+----------+-------+. Our simulated data was unweighted, so the null hypothesis appears true. In real datasets, we; expect the allele frequency to correlate with effect size. Notice that, in the second group, the fault flag is set to 1. This indicates that the numerical; integration to calculate the p-value failed to achieve the required accuracy (by default,; 1e-6). In this particular case, the null hypothesis is likely true and the numerical integration; returned a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:93390,integrat,integration,93390,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['integrat'],['integration']
Deployability,"lleles of a variant are filtered out, the variant itself; is filtered out.; Using f; The f argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If f evaluates to True, the; allele is kept. If f evaluates to False or missing, the allele is; removed.; f is a function that takes two arguments: the allele string (of type; StringExpression) and the allele index (of type; Int32Expression), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:; (with a lambda); >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function); >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; filter_alleles() does not update any fields other than locus and; alleles. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with annotate_rows() and; annotate_entries(). See also; filter_alleles_hts(). Parameters:. mt (MatrixTable) – Dataset.; f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression. Returns:; MatrixTable. hail.methods.filter_alleles_hts(mt, f, subset=False)[source]; Filter alternate alleles and update standard GATK entry fields.; Examples; Filter to SNP alleles using the subset strategy:; >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; For usage of the f argument, see the filter_alleles(); documentation.; filter_alleles_hts() requires the dataset have",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:23545,update,updated,23545,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updated']
Deployability,"llo”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1869,configurat,configurations,1869,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['configurat'],['configurations']
Deployability,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90385,pipeline,pipelines,90385,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"lt PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: Stri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175825,configurat,configuration,175825,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"lt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (mt._af < 1)). gt_norm = (mt._call.n_alt_alleles() - 2 * mt._af) / hl.sqrt(n_variants * 2 * mt._af * (1 - mt._af)). return mt.select_cols(scores=hl.agg.array_sum(mt._loadings * gt_norm)).cols(). def _get_expr_or_join(expr, source, other_source, loc):; if source != other_source:; if isinstance(source, hl.MatrixTable):; source = source.annotate_rows(**{loc: expr}); else:; source = source.annotate(**{loc: expr}); expr = source[other_source.row_key][loc]; return expr. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:3303,update,updated,3303,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['update'],['updated']
Deployability,"lue in common with this one.; Examples; >>> hl.eval(interval.overlaps(hl.interval(5, 9))); True. >>> hl.eval(interval.overlaps(hl.interval(11, 20))); False. Parameters:; interval (Expression with type tinterval) – Interval object with the same point type. Returns:; BooleanExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. property start; Returns the start point.; Examples; >>> hl.eval(interval.start); 3. Returns:; Expression. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html:7841,update,updated,7841,docs/0.2/hail.expr.IntervalExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.IntervalExpression.html,1,['update'],['updated']
Deployability,"ly detects dependencies between jobs; including between python jobs and bash jobs.; When a ResourceFile is passed as an argument, it is passed to the; function as a string to the local file path. When a ResourceGroup; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original ResourceGroup; and the values are the local file paths.; Like JobResourceFile, all PythonResult are stored as; temporary files and must be written to a permanent location using; Batch.write_output() if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods PythonResult.as_str(), PythonResult.as_repr(),; or PythonResult.as_json() to convert a PythonResult to a; JobResourceFile with the desired output. Warning; You must have any non-builtin packages that are used by unapplied installed; in your image. You can use docker.build_python_image() to build a; Python image with additional Python packages installed that is compatible; with Python jobs.; Here are some tips to make sure your function can be used with Batch:. Only reference top-level modules in your functions: like numpy or pandas.; If you get a serialization error, try moving your imports into your function.; Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters:. unapplied (Callable) – A reference to a Python function to execute.; args (Union[PythonResult, ResourceFile, ResourceGroup, List[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Tuple[Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any], ...], Dict[str, Union[PythonResult, ResourceFile, ResourceGroup, List[UnpreparedArg], Tuple[UnpreparedArg, ...], Dict[str, UnpreparedArg], Any]], Any]) – Positional argum",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html:3796,install,installed,3796,docs/batch/api/batch/hailtop.batch.job.PythonJob.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.PythonJob.html,2,['install'],['installed']
Deployability,"m supplied VDS.\n""; f"" VDS call fields : {sorted(vds_call_fields)}\n""; f"" requested call fields: {sorted(call_fields)}\n""; ); call_fields = vds_call_fields. if gvcf_paths:; mt = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30627,update,update,30627,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"mad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP_rsid. View page source. dbSNP_rsid. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html:9426,update,updated,9426,docs/0.2/datasets/schemas/dbSNP_rsid.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP_rsid.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pituitary_all_snp_gene_associations. View page source. GTEx_eQTL_Pituitary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html:9685,update,updated,9685,docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pituitary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Caudate_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellar_Hemisphere_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html:9796,update,updated,9796,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Putamen_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38244,configurat,configuration,38244,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2269,configurat,configuration,2269,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"mericExpression],; y: Tuple[str, NumericExpression],; fields: Optional[Dict[str, Expression]] = None,; n_divisions: Optional[int] = None,; missing_label: str = 'NA',; ) -> pd.DataFrame:; expressions = dict(); if fields is not None:; expressions.update({; k: hail.or_else(v, missing_label) if isinstance(v, StringExpression) else v for k, v in fields.items(); }). if n_divisions is None:; collect_expr = hail.struct(**dict((k, v) for k, v in (x, y)), **expressions); plot_data = [point for point in collect_expr.collect() if point[x[0]] is not None and point[y[0]] is not None]; source_pd = pd.DataFrame(plot_data); else:; # FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame; numeric_expr = {k: 'int32' for k, v in expressions.items() if isinstance(v, Int32Expression)}; numeric_expr.update({k: 'int64' for k, v in expressions.items() if isinstance(v, Int64Expression)}); numeric_expr.update({k: 'float32' for k, v in expressions.items() if isinstance(v, Float32Expression)}); numeric_expr.update({k: 'float64' for k, v in expressions.items() if isinstance(v, Float64Expression)}). # Cast non-string types to string; expressions = {k: hail.str(v) if not isinstance(v, StringExpression) else v for k, v in expressions.items()}. agg_f = x[1]._aggregation_method(); res = agg_f(; hail.agg.downsample(; x[1], y[1], label=list(expressions.values()) if expressions else None, n_divisions=n_divisions; ); ); source_pd = pd.DataFrame([; dict(; **{x[0]: point[0], y[0]: point[1]},; **(dict(zip(expressions, point[2])) if point[2] is not None else {}),; ); for point in res; ]); source_pd = source_pd.astype(numeric_expr, copy=False). return source_pd. def _get_categorical_palette(factors: List[str]) -> ColorMapper:; n = max(3, len(factors)); _palette: Sequence[str]; if n < len(palette):; _palette = palette; elif n < 21:; from bokeh.palettes import Category20. _palette = Category20[n]; else:; from bokeh.palettes ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:23573,update,update,23573,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"mples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first elemen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:94668,upgrade,upgrade,94668,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2981,configurat,configuration,2981,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['configurat'],['configuration']
Deployability,"n API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1374,install,installs,1374,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['install'],['installs']
Deployability,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1539,configurat,configuration,1539,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['configurat'],['configuration']
Deployability,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42465,pipeline,pipelines,42465,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['pipeline'],['pipelines']
Deployability,"n the resulting struct in the order they appear in; fields.; The named_exprs arguments are new field expressions. Parameters:. fields (varargs of str) – Field names to keep.; named_exprs (keyword args of Expression) – New field expressions. Returns:; StructExpression – Struct containing specified existing fields and computed fields. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handler argument:; >>> ht.foo.show(handler=lambda x: logging.info(x)) . Parameters:. n (int) – Maximum number of rows to show.; width (int) – Horizontal width at which to break columns.; truncate (int, optional) – Truncate each field to the given number of characters. If; None, truncate fields to the given width.; types (bool) – Print an extra header line with the type of each field. summarize(handler=None); Compute and print summary information about the expression. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. take(n, _localize=True); Collect the first n records of an expression.; Examples; Take the first three rows:; >>> table1.X.take(3); [5, 6, 7]. Warning; Extremely experimental. Parameters:; n (int) – Number of records to take. Returns:; list. values()[source]; A list of expressions for each field. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.StructExpression.html:9200,update,updated,9200,docs/0.2/hail.expr.StructExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.StructExpression.html,1,['update'],['updated']
Deployability,"n(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37572,install,installed,37572,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['install'],['installed']
Deployability,"n-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12212,install,install-gcs-connector,12212,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install-gcs-connector']
Deployability,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9787,update,updated,9787,docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"n; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9787,update,updated,9787,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; hl.enumerate(hl.enumerate(mt.alleles).filter(lambda elt: mt.__allele_inclusion[elt[0]])).map(; lambda elt: (elt[1][1], elt[0]); ); ). old_to_new = hl.bind(lambda d: mt.alleles.map(lambda a: d.get(a)), ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155770,update,update,155770,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['update']
Deployability,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:65054,configurat,configuration,65054,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1330,configurat,configuration,1330,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['configurat'],['configuration']
Deployability,"ncol is set.; ncol (int) – The number of columns into which the facets will be spread.; scales (str) – Whether the scales are the same across facets. For more information and a list of supported options, see the ggplot documentation. Returns:; FigureAttribute – The faceter. hail.ggplot.vars(*args)[source]. Parameters:; *args (hail.expr.Expression) – Fields to facet by. Returns:; hail.expr.StructExpression – A struct to pass to a faceter. Labels. xlab(label); Sets the x-axis label of a plot. ylab(label); Sets the y-axis label of a plot. ggtitle(label); Sets the title of a plot. hail.ggplot.xlab(label)[source]; Sets the x-axis label of a plot. Parameters:; label (str) – The desired x-axis label of the plot. Returns:; FigureAttribute – Label object to change the x-axis label. hail.ggplot.ylab(label)[source]; Sets the y-axis label of a plot. Parameters:; label (str) – The desired y-axis label of the plot. Returns:; FigureAttribute – Label object to change the y-axis label. hail.ggplot.ggtitle(label)[source]; Sets the title of a plot. Parameters:; label (str) – The desired title of the plot. Returns:; FigureAttribute – Label object to change the title. Classes. class hail.ggplot.GGPlot(ht, aes, geoms=[], labels=<hail.ggplot.labels.Labels object>, coord_cartesian=None, scales=None, facet=None)[source]; The class representing a figure created using the hail.ggplot module.; Create one by using ggplot(). to_plotly()[source]; Turn the hail plot into a Plotly plot. Returns:; A Plotly figure that can be updated with plotly methods. show()[source]; Render and show the plot, either in a browser or notebook. write_image(path)[source]; Write out this plot as an image.; This requires you to have installed the python package kaleido from pypi. Parameters:; path (str) – The path to write the file to. class hail.ggplot.Aesthetic(properties)[source]. class hail.ggplot.FigureAttribute[source]. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:16683,update,updated,16683,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,3,"['install', 'update']","['installed', 'updated']"
Deployability,"nd a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For more information about Dockerfiles and directives that can be used see the following sources:. https://docs.docker.com/develop/develop-images/dockerfile_best-practices/; https://docs.docker.com/engine/reference/builder/. Building Images",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1708,install,installing,1708,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installing']
Deployability,"ndices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html:9709,update,updated,9709,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Hippocampus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ndices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Lung_all_snp_gene_associations. View page source. GTEx_sQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html:9727,update,updated,9727,docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Lung_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ndices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9820,update,updated,9820,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ndle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**flags):; Env.backend().set_flags(**flags). def _get_flags(*flags):; return Env.backend().get_flags(*flags). @contextmanager; def _with_flags(**flags):; before = _get_flags(*flags); try:; _set_flags(**flags); yield; finally:; _set_flags(**before). def debug_info():; from hail.backend.backend import local_jar_information; from hail.backend.spark_backend import SparkBackend. spark_conf = None; if isinstance(Env.backend(), SparkBackend):; spark_conf = spark_context()._conf.getAll(); return {'spark_conf': spark_conf, 'local_jar_information': local_jar_information(), 'version': version()}. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:27839,update,updated,27839,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['update'],['updated']
Deployability,"ndow); if hfs.exists(checkpoint):; result = b.read_input(checkpoint); results[i] = result; continue. inputs.append((window, i, checkpoint)). Then we have another for loop that uses the hailtop.grouped; function to group the inputs into groups of 10 and create a; job for each group. Then we create a PythonJob and; use PythonJob.call() to run the random forest function; for each window in that group. Lastly, we append the result; to the correct place in the results list.; for inputs in grouped(10, inputs):; j = b.new_python_job(); for window, i, checkpoint in inputs:; result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); tsv_result = tsv_result.as_str(). b.write_output(tsv_result, checkpoint); results[i] = tsv_result. Now we’ve only run the jobs in groups of 10 for jobs that have no; existing checkpoint file. The results will be concatenated in the correct; order. Synopsis; We have presented three different ways with increasing complexity to write; a pipeline that runs a random forest for various windows in the genome. The; complete code is provided here for your reference. run_rf_simple.py; from typing import Tuple. import pandas as pd; from sklearn.ensemble import RandomForestRegressor. import hailtop.batch as hb; import hailtop.fs as hfs. def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; max_features = 3 / 4; rf = RandomForestRegressor(n_estimators=100, n_jobs=cores, max_features=max_features, oob_score=True, verbose=False). rf.fit(x_train, y_train). # apply the trained random forest o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:11461,pipeline,pipeline,11461,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['pipeline'],['pipeline']
Deployability,"ne entry in some data_field_name array for every row in; the inputs.; The multi_way_zip_join() method assumes that inputs have distinct; keys. If any input has duplicate keys, the row value that is included; in the result array for that key is undefined. Parameters:. tables (list of Table) – A list of tables to combine; data_field_name (str) – The name of the resulting data field; global_field_name (str) – The name of the resulting global field. n_partitions()[source]; Returns the number of partitions in the table.; Examples; Range tables can be constructed with an explicit number of partitions:; >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The min_partitions argument to import_table() forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline.; >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns:; int – Number of partitions. naive_coalesce(max_partitions)[source]; Naively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> table_result = table1.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; Table – Table with at most max_partitions partitions. order_by(*exprs)[source]; Sort by the specified fields, defaulting",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:47332,pipeline,pipeline,47332,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['pipeline'],['pipeline']
Deployability,"ner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return [sr.to_legacy_dict() for sr in Env.fs().ls(path)]. [docs]def hadoop_scheme_supported(scheme: str) -> bool:; """"""Returns ``True`` if the Hadoop filesystem supports URLs with the given; scheme. Examples; --------. >>> hadoop_scheme_supported('gs') # doctest: +SKIP. Notes; -----; URLs with the `https` scheme are only supported if they are specifically; Azure Blob Storage URLs of the form `https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>`. Parameters; ----------; scheme : :class:`str`. Returns; -------; :obj:`.bool`; """"""; return Env.fs().supports_scheme(scheme). [docs]def copy_log(path: str) -> None:; """"""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""; from hail.utils import local_path_uri. log = os.path.realpath(Env.hc()._log); try:; if hadoop_is_dir(path):; _, tail = os.path.split(log); path = os.path.join(path, tail); info(f""copying log to {path!r}...""); hadoop_copy(local_path_uri(log), path); except Exception as e:; sys.stderr.write(f'Could not copy log: encountered error:\n {e}'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:8296,update,updated,8296,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,2,['update'],['updated']
Deployability,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6476,update,updated,6476,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['update'],['updated']
Deployability,"ngineer""""Charade (1963)""5.00e+00; ""entertainment""""American in Paris, An (1951)""5.00e+00; ""executive""""A Chef in Love (1996)""5.00e+00; ""healthcare""""39 Steps, The (1935)""5.00e+00; ""homemaker""""Beautiful Girls (1996)""5.00e+00; ""lawyer""""Anastasia (1997)""5.00e+00; showing top 10 rows. Let’s try to get a deeper understanding of this result. Notice that every movie displayed has an average rating of 5, which means that every person gave these movies the highest rating. Is that unlikely? We can determine how many people rated each of these movies by working backwards and filtering our original movie_data table by fields in highest_rated.; Note that in the second line below, we are taking advantage of the fact that Hail tables are keyed. [19]:. highest_rated = highest_rated.key_by(; highest_rated.occupation, highest_rated.movie). counts_temp = movie_data.filter(; hl.is_defined(highest_rated[movie_data.occupation, movie_data.movie])). counts = counts_temp.group_by(counts_temp.occupation, counts_temp.movie).aggregate(; counts = hl.agg.count()). counts.show(). [Stage 108:> (0 + 1) / 1]. occupationmoviecountsstrstrint64; ""administrator""""A Chef in Love (1996)""1; ""artist""""39 Steps, The (1935)""1; ""doctor""""Alien (1979)""1; ""educator""""Aparajito (1956)""2; ""engineer""""Charade (1963)""1; ""entertainment""""American in Paris, An (1951)""1; ""executive""""A Chef in Love (1996)""1; ""healthcare""""39 Steps, The (1935)""1; ""homemaker""""Beautiful Girls (1996)""1; ""lawyer""""Anastasia (1997)""1; showing top 10 rows. So it looks like the highest rated movies, when computed naively, mostly have a single viewer rating them. To get a better understanding of the data, we can recompute this list but only include movies which have more than 1 viewer (left as an exercise). Exercises. What is the favorite movie for each occupation, conditional on there being more than one viewer?; What genres are rated most differently by men and women?. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:14316,update,updated,14316,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['update'],['updated']
Deployability,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:132234,integrat,integrate,132234,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['integrat'],['integrate']
Deployability,"no standard representation; for this at current). A record from a component GVCF is included in the; reference_data if it defines the END INFO field (if the GT is not reference,; an error will be thrown by the Hail VDS combiner).; The variant_data matrix table is a sparse matrix of non-reference calls.; This table contains the complete schema from the component GVCFs, aside from; fields which are known to be defined only for reference blocks (e.g. END or; MIN_DP). A record from a component GVCF is included in the variant_data if; it does not define the END INFO field. This means that some records of the; variant_data can be no-call (./.) or reference, depending on the; semantics of the variant caller that produced the GVCFs. Building analyses on the VariantDataset; Analyses operating on sequencing data can be largely grouped into three categories; by functionality used. Analyses that use prebuilt methods. Some analyses can be supported by using; only the utility functions defined in the hl.vds module, like; vds.sample_qc().; Analyses that use variant data and/or reference data separately. Some; pipelines need to interrogate properties of the component tables; individually. Examples might include singleton analysis or burden tests; (which needs only to look at the variant data) or coverage analysis (which; looks only at reference data). These pipelines should explicitly extract and; manipulate the component tables with vds.variant_data and; vds.reference_data.; Analyses that use the full variant-by-sample matrix with variant and reference data.; Many pipelines require variant and reference data together. There are helper; functions provided for producing either the sparse (containing reference; blocks) or dense (reference information is filled in at each variant site); representations. For more information, see the documentation for; vds.to_dense_mt() and vds.to_merged_sparse_mt(). Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:7939,pipeline,pipelines,7939,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,4,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Add. View page source. giant_whr_exome_C_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_C_ALL_Rec. View page source. giant_whr_exome_C_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_C_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Add. View page source. giant_whr_exome_M_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_M_ALL_Rec. View page source. giant_whr_exome_M_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_M_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Add. View page source. giant_whr_exome_W_ALL_Add. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Add.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; giant_whr_exome_W_ALL_Rec. View page source. giant_whr_exome_W_ALL_Rec. Versions: 2018; Reference genome builds: GRCh37; Type: hail.Table. Schema (2018, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'snp_name': str; 'gmaf': dict<str, float64>; 'exac_maf': dict<str, float64>; 'beta': float64; 'se': float64; 'pvalue': float64; 'sample_size': int32; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html:9603,update,updated,9603,docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/giant_whr_exome_W_ALL_Rec.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_eQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html:9682,update,updated,9682,docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Pancreas_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Prostate_all_snp_gene_associations. View page source. GTEx_eQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html:9682,update,updated,9682,docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Prostate_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html:9793,update,updated,9793,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_Cultured_fibroblasts_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. View page source. GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html:9793,update,updated,9793,docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Skin_Sun_Exposed_Lower_leg_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:2844,configurat,configurations,2844,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['configurat'],['configurations']
Deployability,"np_dtype} could not be converted to a hail type.""). def dtypes_from_pandas(pd_dtype):; if type(pd_dtype) == pd.StringDtype:; return hl.tstr; elif pd_dtype == np.int64:; return hl.tint64; elif pd_dtype == np.uint64:; # Hail does *not* support unsigned integers but the next condition,; # pd.api.types.is_integer_dtype(pd_dtype) would return true on unsigned 64-bit ints; return None; # For some reason pandas doesn't have `is_int32_dtype`, so we use `is_integer_dtype` if first branch failed.; elif pd.api.types.is_integer_dtype(pd_dtype):; return hl.tint32; elif pd_dtype == np.float32:; return hl.tfloat32; elif pd_dtype == np.float64:; return hl.tfloat64; elif pd_dtype == bool:; return hl.tbool; return None. class tvariable(HailType):; _cond_map: ClassVar = {; 'numeric': is_numeric,; 'int32': lambda x: x == tint32,; 'int64': lambda x: x == tint64,; 'float32': lambda x: x == tfloat32,; 'float64': lambda x: x == tfloat64,; 'locus': lambda x: isinstance(x, tlocus),; 'struct': lambda x: isinstance(x, tstruct),; 'union': lambda x: isinstance(x, tunion),; 'tuple': lambda x: isinstance(x, ttuple),; }. def __init__(self, name, cond):; self.name = name; self.cond = cond; self.condf = tvariable._cond_map[cond] if cond else None; self.box = Box.from_name(name). def unify(self, t):; if self.condf and not self.condf(t):; return False; return self.box.unify(t). def clear(self):; self.box.clear(). def subst(self):; return self.box.get(). def __str__(self):; s = '?' + self.name; if self.cond:; s = s + ':' + self.cond; return s. _old_printer = pprint.PrettyPrinter. class TypePrettyPrinter(pprint.PrettyPrinter):; def _format(self, object, stream, indent, allowance, context, level):; if isinstance(object, HailType):; stream.write(object.pretty(self._indent_per_level)); else:; return _old_printer._format(self, object, stream, indent, allowance, context, level). pprint.PrettyPrinter = TypePrettyPrinter # monkey-patch pprint. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:56984,patch,patch,56984,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,4,"['patch', 'update']","['patch', 'updated']"
Deployability,"nsecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.; (#11331) Fix bug; that potentially caused files written to disk to be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52619,deploy,deploy,52619,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"nsight; Other Spark Clusters; Next Steps. After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on a Spark Cluster. View page source. Install Hail on a Spark Cluster; If you are using Google Dataproc, please see these simpler instructions. If you; are using Azure HDInsight please see these simpler instructions.; Hail should work with any Spark 3.5.x cluster built with Scala 2.12.; Hail needs to be built from source on the leader node. Building Hail from source; requires:. Java 11 JDK.; Python 3.9 or later.; A recent C and a C++ compiler, GCC 5.0, LLVM 3.4, or later versions of either; suffice.; The LZ4 library.; BLAS and LAPACK. On a Debian-like system, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:1195,update,update,1195,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['install', 'update']","['install', 'update']"
Deployability,"ntDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""; vds = read_vds(vds_path, _warn_no_ref_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2442,patch,patch,2442,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['patch'],['patch']
Deployability,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. View page source. GTEx_eQTL_Artery_Coronary_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html:9703,update,updated,9703,docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Artery_Coronary_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nt_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. View page source. GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html:9703,update,updated,9703,docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Muscle_Skeletal_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"nt_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. View page source. GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html:9814,update,updated,9814,docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Cells_EBV-transformed_lymphocytes_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"ntheses appropriately. A single ‘&’ denotes logical AND and a single ‘|’ denotes logical OR. [9]:. users.aggregate(hl.agg.filter((users.occupation == 'writer') | (users.occupation == 'executive'), hl.agg.count())). [9]:. 77. [10]:. users.aggregate(hl.agg.filter((users.sex == 'F') | (users.occupation == 'executive'), hl.agg.count())). [10]:. 302. hist; As we saw in the first tutorial, hist can be used to build a histogram over numeric data. [11]:. hist = users.aggregate(hl.agg.hist(users.age, 10, 70, 60)); hist. [11]:. Struct(bin_edges=[10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0], bin_freq=[1, 1, 0, 5, 3, 6, 5, 14, 18, 23, 32, 27, 37, 28, 33, 38, 34, 35, 36, 32, 39, 25, 28, 26, 17, 27, 21, 19, 17, 22, 21, 10, 21, 13, 23, 15, 12, 14, 20, 19, 20, 20, 6, 12, 4, 11, 6, 9, 3, 3, 9, 3, 2, 3, 2, 3, 1, 0, 2, 5], n_smaller=1, n_larger=1). [12]:. p = hl.plot.histogram(hist, legend='Age'); show(p). take and collect; There are a few aggregators for collecting values. take localizes a few values into an array. It has an optional ordering.; collect localizes all values into an array.; collect_as_set localizes all unique values into a set. [13]:. users.aggregate(hl.agg.take(users.occupation, 5)). [13]:. ['technician', 'other', 'writer', 'technician', 'other']. [14]:. users.aggregate(hl.agg.take(users.age, 5, ordering=-users.age)). [14]:. [73, 70, 70, 70, 69]. Warning! Aggregators like collect and counter return Python objects and can fail with out of memory errors if you apply them to collections that are too large (e.g. all 50 trillion genotypes in the UK Biobank dataset). [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:6535,update,updated,6535,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['update'],['updated']
Deployability,"o the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. Se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28151,update,updated,28151,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['updated']
Deployability,"oat64,; AF_EAS: float64,; AF_AMR: float64,; AF_SAS: float64,; AF_AFR: float64,; HWE_EUR: float64,; HWE_EAS: float64,; HWE_AMR: float64,; HWE_SAS: float64,; HWE_AFR: float64,; HWE: float64,; ExcHet_EUR: float64,; ExcHet_EAS: float64,; ExcHet_AMR: float64,; ExcHet_SAS: float64,; ExcHet_AFR: float64,; ExcHet: float64,; ME: float64,; AN_EUR_unrel: int32,; AN_EAS_unrel: int32,; AN_AMR_unrel: int32,; AN_SAS_unrel: int32,; AN_AFR_unrel: int32,; AC_EUR_unrel: int32,; AC_EAS_unrel: int32,; AC_AMR_unrel: int32,; AC_SAS_unrel: int32,; AC_AFR_unrel: int32,; AC_Hom_EUR_unrel: int32,; AC_Hom_EAS_unrel: int32,; AC_Hom_AMR_unrel: int32,; AC_Hom_SAS_unrel: int32,; AC_Hom_AFR_unrel: int32,; AC_Het_EUR_unrel: int32,; AC_Het_EAS_unrel: int32,; AC_Het_AMR_unrel: int32,; AC_Het_SAS_unrel: int32,; AC_Het_AFR_unrel: int32,; AF_EUR_unrel: float64,; AF_EAS_unrel: float64,; AF_AMR_unrel: float64,; AF_SAS_unrel: float64,; AF_AFR_unrel: float64,; HWE_EUR_unrel: float64,; HWE_EAS_unrel: float64,; HWE_AMR_unrel: float64,; HWE_SAS_unrel: float64,; HWE_AFR_unrel: float64; }; 'a_index': int32; 'was_split': bool; 'variant_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'AB': float64; 'AD': array<int32>; 'DP': int32; 'GQ': int32; 'GT': call; 'MIN_DP': int32; 'MQ0': int32; 'PGT': call; 'PID': str; 'PL': array<int32>; 'RGQ': int32; 'SB': array<int32>; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html:12998,update,updated,12998,docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_HighCov_autosomes.html,2,['update'],['updated']
Deployability,"odegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75900,install,installation,75900,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"odels genotypes of individuals from a structured; population comprising \(K\) homogeneous modern populations that have; each diverged from a single ancestral population (a star phylogeny). Each; sample is assigned a population by sampling from the categorical; distribution \(\pi\). Note that the actual size of each population is; random.; Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum \(P_0\). The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding \(F_{ST}\); parameter \(F_k\) (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots,; K\)). For each variant and population, allele frequencies are drawn from a; beta distribution; whose parameters are determined by the ancestral allele frequency and; \(F_{ST}\) parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by \(k_n\), ancestral allele frequencies by \(p_m\),; population allele frequencies by \(p_{k, m}\), and diploid, unphased; genotype calls by \(g_{n, m}\) (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively).; The generative model is then given by:. \[\begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. \]; The beta distribution by its mean and variance above; the usual parameters; are \(a = (1 - p) \frac{1 - F}{F}\) and \(b = p \frac{1 - F}{F}\) with; \(F = F_k\) and \(p = p_m\).; The resulting dataset has the following fields.; Global fields:. bn.n_populations (tint32) – Number of populations.; bn.n_samples (tint32) – Number of samples.; bn.n_variants (tint32) – Number of v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:12521,continuous,continuous,12521,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['continuous'],['continuous']
Deployability,"oerce_endpoint(point):; if point.dtype == key_typ[0]:; point = hl.struct(**{key_names[0]: point}); ts = point.dtype; if isinstance(ts, tstruct):; i = 0; while i < len(ts):; if i >= len(key_typ):; raise ValueError(; f""query_table: queried with {len(ts)} key field(s), but table only has {len(key_typ)} key field(s)""; ); if key_typ[i] != ts[i]:; raise ValueError(; f""query_table: key mismatch at key field {i} ({list(ts.keys())[i]!r}): query type is {ts[i]}, table key type is {key_typ[i]}""; ); i += 1. if i == 0:; raise ValueError(""query_table: cannot query with empty key""). point_size = builtins.len(point.dtype); return hl.tuple([; hl.struct(**{; key_names[i]: (point[i] if i < point_size else hl.missing(key_typ[i])); for i in builtins.range(builtins.len(key_typ)); }),; hl.int32(point_size),; ]); else:; raise ValueError(; f""query_table: key mismatch: cannot query a table with key ""; f""({', '.join(builtins.str(x) for x in key_typ.values())}) with query point type {point.dtype}""; ). if point_or_interval.dtype != key_typ[0] and isinstance(point_or_interval.dtype, hl.tinterval):; partition_interval = hl.interval(; start=coerce_endpoint(point_or_interval.start),; end=coerce_endpoint(point_or_interval.end),; includes_start=point_or_interval.includes_start,; includes_end=point_or_interval.includes_end,; ); else:; point = coerce_endpoint(point_or_interval); partition_interval = hl.interval(start=point, end=point, includes_start=True, includes_end=True); return construct_expr(; ir.ToArray(ir.ReadPartition(partition_interval._ir, reader=ir.PartitionNativeIntervalReader(path, row_typ))),; type=hl.tarray(row_typ),; indices=partition_interval._indices,; aggregations=partition_interval._aggregations,; ). @typecheck(msg=expr_str, result=expr_any); def _console_log(msg, result):; indices, aggregations = unify_all(msg, result); return construct_expr(ir.ConsoleLog(msg._ir, result._ir), result.dtype, indices, aggregations). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:188705,update,updated,188705,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['update'],['updated']
Deployability,"of .vcf file to write.; append_to_header (str or None) – Path of file to append to VCF header.; export_pp (bool) – If true, export linear-scaled probabilities (Hail’s pp field on genotype) as the VCF PP FORMAT field.; parallel (bool) – If true, return a set of VCF files (one per partition) rather than serially concatenating these files. file_version()[source]¶; File version of variant dataset. Return type:int. filter_alleles(expr, annotation='va = va', subset=True, keep=True, filter_altered_genotypes=False, max_shift=100, keep_star=False)[source]¶; Filter a user-defined set of alternate alleles for each variant.; If all alternate alleles of a variant are filtered, the; variant itself is filtered. The expr expression is; evaluated for each alternate allele, but not for; the reference allele (i.e. aIndex will never be zero). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To remove alternate alleles with zero allele count and; update the alternate allele count annotation with the new; indices:; >>> vds_result = vds.filter_alleles('va.info.AC[aIndex - 1] == 0',; ... annotation='va.info.AC = aIndices[1:].map(i => va.info.AC[i - 1])',; ... keep=False). Note that we skip the first element of aIndices because; we are mapping between the old and new allele indices, not; the alternate allele indices.; Notes; If filter_altered_genotypes is true, genotypes that contain filtered-out alleles are set to missing.; filter_alleles() implements two algorithms for filtering alleles: subset and downcode. We will illustrate their; behavior on the example genotype below when filtering the first alternate allele (allele 1) at a site with 1 reference; allele and 2 alternate alleles.; GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. Subset algorithm; The subset algorithm (the default, subset=True) subsets the; AD and PL arrays (i.e. removes entries corresponding to filtered alleles); and then sets",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:47865,update,update,47865,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['update'],['update']
Deployability,"of Expression) – Annotation expressions. Returns:; MatrixTable. transmute_rows(**named_exprs)[source]; Similar to MatrixTable.annotate_rows(), but drops referenced fields.; Notes; This method adds new row fields according to named_exprs, and drops; all row fields referenced in those expressions. See; Table.transmute() for full documentation on how transmute; methods work. Note; transmute_rows() will not drop key fields. Note; This method supports aggregation over columns. See also; Table.transmute(), MatrixTable.select_rows(), MatrixTable.annotate_rows(). Parameters:; named_exprs (keyword args of Expression) – Annotation expressions. Returns:; MatrixTable. unfilter_entries()[source]; Unfilters filtered entries, populating fields with missing values. Returns:; MatrixTable. Notes; This method is used in the case that a pipeline downstream of filter_entries(); requires a fully dense (no filtered entries) matrix table.; Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:64919,pipeline,pipeline,64919,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,['pipeline'],['pipeline']
Deployability,"of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.build_python_image; Build a new Python image with dill and the specified pip packages installed. utils.concatenate; Concatenate files using tree aggregation. utils.plink_merge; Merge binary PLINK files using tree aggregation. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:3766,install,installed,3766,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['install'],['installed']
Deployability,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10105,configurat,configuration,10105,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; Schema (0.2, GRCh37). panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AFR. View page source. panukb_ld_scores_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AFR.html,2,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; Schema (0.2, GRCh37). panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_AMR. View page source. panukb_ld_scores_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_AMR.html,2,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; Schema (0.2, GRCh37). panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_CSA. View page source. panukb_ld_scores_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_CSA.html,2,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; Schema (0.2, GRCh37). panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EAS. View page source. panukb_ld_scores_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EAS.html,2,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; Schema (0.2, GRCh37). panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_EUR. View page source. panukb_ld_scores_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_EUR.html,2,['update'],['updated']
Deployability,"omad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; Schema (0.2, GRCh37). panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_scores_MID. View page source. panukb_ld_scores_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; 'AF': float64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html:9391,update,updated,9391,docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_scores_MID.html,2,['update'],['updated']
Deployability,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. View page source. GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Adipose_Visceral_Omentum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"omad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html:9730,update,updated,9730,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Frontal_Cortex_BA9_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"omad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Whole_Blood_all_snp_gene_associations. View page source. GTEx_sQTL_Whole_Blood_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html:9748,update,updated,9748,docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Whole_Blood_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"omogeneous modern populations that have; each diverged from a single ancestral population (a `star phylogeny`). Each; sample is assigned a population by sampling from the categorical; distribution :math:`\pi`. Note that the actual size of each population is; random. Variants are modeled as biallelic and unlinked. Ancestral allele; frequencies are drawn independently for each variant from a frequency; spectrum :math:`P_0`. The extent of genetic drift of each modern population; from the ancestral population is defined by the corresponding :math:`F_{ST}`; parameter :math:`F_k` (here and below, lowercase indices run over a range; bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots,; K`). For each variant and population, allele frequencies are drawn from a; `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__; whose parameters are determined by the ancestral allele frequency and; :math:`F_{ST}` parameter. The beta distribution gives a continuous; approximation of the effect of genetic drift. We denote sample population; assignments by :math:`k_n`, ancestral allele frequencies by :math:`p_m`,; population allele frequencies by :math:`p_{k, m}`, and diploid, unphased; genotype calls by :math:`g_{n, m}` (0, 1, and 2 correspond to homozygous; reference, heterozygous, and homozygous variant, respectively). The generative model is then given by:. .. math::; \begin{aligned}; k_n \,&\sim\, \pi \\; p_m \,&\sim\, P_0 \\; p_{k,m} \mid p_m\,&\sim\, \mathrm{Beta}(\mu = p_m,\, \sigma^2 = F_k p_m (1 - p_m)) \\; g_{n,m} \mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}); \end{aligned}. The beta distribution by its mean and variance above; the usual parameters; are :math:`a = (1 - p) \frac{1 - F}{F}` and :math:`b = p \frac{1 - F}{F}` with; :math:`F = F_k` and :math:`p = p_m`. The resulting dataset has the following fields. Global fields:. - `bn.n_populations` (:py:data:`.tint32`) -- Number of populations.; - `bn.n_samples` (:py:data:`.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:146646,continuous,continuous,146646,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['continuous'],['continuous']
Deployability,"on Compatibility Policy; Change Log. Batch. Docker Resources. View page source. Docker Resources. What is Docker?; Docker is a tool for packaging up operating systems, scripts, and environments in order to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1325,install,installed,1325,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installed']
Deployability,"on-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101781,install,installing,101781,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['install'],['installing']
Deployability,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:179572,configurat,configuration,179572,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['configurat'],['configuration']
Deployability,"onal[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4297,configurat,configuration,4297,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['configurat'],['configuration']
Deployability,"onary keyed by results of f. hail.expr.functions.fold(f, zero, collection)[source]; Reduces a collection with the given function f, provided the initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.fold(lambda i, j: i + j, 0, a)); 3. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; collection (ArrayExpression or SetExpression). Returns:; Expression. hail.expr.functions.array_scan(f, zero, a)[source]; Map each element of a to cumulative value of function f, with initial value zero.; Examples; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters:. f (function ( (Expression, Expression) -> Expression)) – Function which takes the cumulative value and the next element, and; returns a new value.; zero (Expression) – Initial value to pass in as left argument of f.; a (ArrayExpression). Returns:; ArrayExpression. hail.expr.functions.reversed(x)[source]; Reverses the elements of a collection.; Examples; >>> a = ['The', 'quick', 'brown', 'fox']; >>> hl.eval(hl.reversed(a)); ['fox', 'brown', 'quick', 'The']. Parameters:; x (ArrayExpression or StringExpression) – Array or string expression. Returns:; Expression. hail.expr.functions.keyed_intersection(*arrays, key)[source]; Compute the intersection of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. arrays; key. Returns:; ArrayExpression. hail.expr.functions.keyed_union(*arrays, key)[source]; Compute the distinct union of sorted arrays on a given key.; Requires sorted arrays with distinct keys. Warning; Experimental. Does not support downstream randomness. Parameters:. exprs; key. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/collections.html:13994,update,updated,13994,docs/0.2/functions/collections.html,https://hail.is,https://hail.is/docs/0.2/functions/collections.html,1,['update'],['updated']
Deployability,"onding gcloud command. Version 0.2.58; Released 2020-10-08. New features. (#9524) Hail should; now be buildable using Spark 3.0.; (#9549) Add; ignore_in_sample_frequency flag to hl.de_novo.; (#9501) Configurable; cache size for BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9474) Add; ArrayExpression.first and ArrayExpression.last.; (#9459) Add; StringExpression.join, an analogue to Python’s str.join.; (#9398) Hail will now; throw HailUserErrors if the or_error branch of a; CaseBuilder is hit. Bug fixes. (#9503) NDArrays can; now hold arbitrary data types, though only ndarrays of primitives can; be collected to Python.; (#9501) Remove memory; leak in BlockMatrix.to_matrix_table_row_major and; BlockMatrix.to_table_row_major.; (#9424); hl.experimental.writeBlockMatrices didn’t correctly support; overwrite flag. Performance improvements. (#9506); hl.agg.ndarray_sum will now do a tree aggregation. hailctl dataproc. (#9502) Fix hailctl; dataproc modify to install dependencies of the wheel file.; (#9420) Add; --debug-mode flag to hailctl dataproc start. This will enable; heap dumps on OOM errors.; (#9520) Add support; for requester pays buckets to hailctl dataproc describe. Deprecations. (#9482); ArrayExpression.head has been deprecated in favor of; ArrayExpression.first. Version 0.2.57; Released 2020-09-03. New features. (#9343) Implement the; KING method for relationship inference as hl.methods.king. Version 0.2.56; Released 2020-08-31. New features. (#9308) Add; hl.enumerate in favor of hl.zip_with_index, which is now deprecated.; (#9278) Add; ArrayExpression.grouped, a function that groups hail arrays into; fixed size subarrays. Performance. (#9373)(#9374); Decrease amount of memory used when slicing or filtering along a; single BlockMatrix dimension. Bug fixes. (#9304) Fix crash in; run_combiner caused by inputs where VCF lines and BGZ blocks; align. hailctl dataproc. (#9263) Add support; for --expiration-time argument to h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:64617,install,install,64617,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['install']
Deployability,"ondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(path)[source]; Load reference genome from a JSON file.; Notes; The JSON file must have the following format:; {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. name must be unique and not overlap with Hail’s pre-instantiated; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'.; The contig names in xContigs, yContigs, and mtContigs must be; present in contigs. The intervals listed in par must have contigs in; either xContigs or yContigs and must have positions between 0 and; the contig length given in contigs. Parameters:; path (str) – Path to JSON file. Returns:; ReferenceGenome. remove_liftover(dest_reference_genome)[source]; Remove liftover to dest_reference_genome. Parameters:; dest_reference_genome (str or ReferenceGenome). remove_sequence()[source]; Remove the reference sequence. write(output)[source]; “Write this reference genome to a file in JSON format.; Examples; >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; Use read() to reimport the exported; reference genome in a new HailContext session. Parameters:; output (str) – Path of JSON file to write. property x_contigs; X contigs. Returns:; list of str. property y_contigs; Y contigs. Returns:; list of str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:10210,update,updated,10210,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['update'],['updated']
Deployability,"one, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2748,configurat,configuration,2748,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"onfiguration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Overview: module code. All modules for which code is available; hail.context; hail.experimental.datasets; hail.experimental.db; hail.experimental.export_entries_by_col; hail.experimental.expressions; hail.experimental.filtering_allele_frequency; hail.experimental.full_outer_join_mt; hail.experimental.import_gtf; hail.experimental.ld_score_regression; hail.experimental.ldscore; hail.experimental.ldscsim; hail.experimental.loop; hail.experimental.pca; hail.experimental.phase_by_transmission; hail.experimental.plots; hail.experimental.tidyr; hail.experimental.time; hail.expr.aggregators.aggregators; hail.expr.builders; hail.expr.expressions.base_expression; hail.expr.expressions.expression_utils; hail.expr.expressions.typed_expressions; hail.expr.functions; hail.expr.types; hail.genetics.allele_type; hail.genetics.call; hail.genetics.locus; hail.genetics.pedigree; hail.genetics.reference_genome; hail.ggplot.aes; hail.ggplot.coord_cartesian; hail.ggplot.facets; hail.ggplot.geoms; hail.ggplot.ggplot; hail.ggplot.labels; hail.ggplot.scale; hail.linalg.blockmatrix; hail.linalg.utils.misc; hail.matrixtable; hail.methods.family_methods; hail.methods.impex; hail.methods.misc; hail.methods.pca; hail.methods.qc; hail.methods.relatedness.identity_by_descent; hail.methods.relatedness.king; hail.methods.relatedness.mating_simulation; hail.methods.relatedness.pc_relate; hail.methods.statgen; hail.nd.nd; hail.plot.plots; hail.stats.linear_mixed_model; hail.table; hail.utils.hadoop_utils; hail.utils.interval; hail.utils.misc; hail.utils.struct; hail.utils.tutorial; hail.vds.combiner.variant_dataset_combiner; hail.vds.functions; hail.vds.methods; hail.vds.sample_qc; hail.vds.variant_dataset; hailtop.frozendict; hailtop.fs.fs_utils. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/index.html:2200,update,updated,2200,docs/0.2/_modules/index.html,https://hail.is,https://hail.is/docs/0.2/_modules/index.html,1,['update'],['updated']
Deployability,"onment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223269,configurat,configuration,223269,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"ons (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84199,update,update,84199,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"ons(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (:class:`.Expression`) -> :class:`.Expression`); The function used to combine the current aggregator state with the next element you're aggregating over. Type is; `A => A`; comb_op : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); The function used to combine two aggregator states together and produce final result. Type is `(A, A) => A`.; """""". return _agg_func._fold(initial_value, seq_op, comb_op). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:62023,update,updated,62023,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['update'],['updated']
Deployability,"option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Stor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5329,configurat,configuration,5329,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6890,configurat,configuration,6890,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['configurat'],['configuration']
Deployability,"ores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkCo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7160,configurat,configuration,7160,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"ort stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test). Our thanks to @Will-Tyler for; generously contributing this feature.; (#14393) hail; depends on protobuf no longer; users may choose their own version; of protobuf.; (#14360) Exposed; previous",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12460,upgrade,upgrade,12460,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['upgrade'],['upgrade']
Deployability,"ory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12140,update,update,12140,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'update']","['installing', 'update']"
Deployability,"ot exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""; return _fses[requester_pays_config].ls(path). [docs]def mkdir(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Ensure files can be created whose dirname is `path`. Warning; -------. On file systems without a notion of directories, this function will do nothing. For example,; on Google Cloud Storage, this operation does nothing. """"""; _fses[requester_pays_config].mkdir(path). [docs]def remove(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Removes the file at `path`. If the file does not exist, this function does; nothing. `path` must be a URI (uniform resource identifier) or a path on the; local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].remove(path). [docs]def rmtree(path: str, *, requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None):; """"""Recursively remove all files under the given `path`. On a local filesystem,; this removes the directory tree at `path`. On blob storage providers such as; GCS, S3 and ABS, this removes all files whose name starts with `path`. As such,; `path` must be a URI (uniform resource identifier) or a path on the local filesystem. Parameters; ----------; path : :class:`str`; """"""; _fses[requester_pays_config].rmtree(path). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:8117,update,updated,8117,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,2,['update'],['updated']
Deployability,"otate_fields[coords] = coord_expr; else:; coords = src._fields_inverse[coord_expr]. if isinstance(src, hl.MatrixTable):; new_src = src.annotate_rows(**annotate_fields); else:; new_src = src.annotate(**annotate_fields). locus_expr = new_src[locus]; if coord_expr is not None:; coord_expr = new_src[coords]. if coord_expr is None:; coord_expr = locus_expr.position. rg = locus_expr.dtype.reference_genome; contig_group_expr = hl.agg.group_by(hl.locus(locus_expr.contig, 1, reference_genome=rg), hl.agg.collect(coord_expr)). # check loci are in sorted order; last_pos = hl.fold(; lambda a, elt: (; hl.case(); .when(a <= elt, elt); .or_error(; hl.str(""locus_windows: 'locus_expr' global position must be in ascending order. ""); + hl.str(a); + hl.str("" was not less then or equal to ""); + hl.str(elt); ); ),; -1,; hl.agg.collect(; hl.case(); .when(hl.is_defined(locus_expr), locus_expr.global_position()); .or_error(""locus_windows: missing value for 'locus_expr'.""); ),; ); checked_contig_groups = (; hl.case().when(last_pos >= 0, contig_group_expr).or_error(""locus_windows: 'locus_expr' has length 0""); ). contig_groups = locus_expr._aggregation_method()(checked_contig_groups, _localize=False). coords = hl.sorted(hl.array(contig_groups)).map(lambda t: t[1]); starts_and_stops = hl._locus_windows_per_contig(coords, radius). if not _localize:; return starts_and_stops. starts, stops = hl.eval(starts_and_stops); return np.array(starts), np.array(stops). def _check_dims(a, name, ndim, min_size=1):; if len(a.shape) != ndim:; raise ValueError(f'{name} must be {ndim}-dimensional, ' f'found {a.ndim}'); for i in range(ndim):; if a.shape[i] < min_size:; raise ValueError(f'{name}.shape[{i}] must be at least ' f'{min_size}, found {a.shape[i]}'). def _ndarray_matmul_ndim(left, right):; if left == 1 and right == 1:; return 0; elif left == 1:; return right - 1; elif right == 1:; return left - 1; else:; assert left == right; return left. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:8635,update,updated,8635,docs/0.2/_modules/hail/linalg/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html,2,['update'],['updated']
Deployability,"otly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesthetic_name, name, None, None). if isinstance(reference_genome, str):; reference_genome = get_reference(reference_genome); self.reference_genome = reference_genome. def apply_to_fig(self, parent, fig_so_far):; contig_offsets = dict(list(self.reference_genome.global_positions_dict.items())[:24]); breaks = list(contig_offsets.values()); labels = list(contig_offsets.keys()); self.update_axis(fig_so_far)(tickvals=breaks, ticktext=labels). def transform_data(self, field_expr):; return field_expr.global_position(). def is_discrete(self):; r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:1509,continuous,continuous,1509,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['continuous'],['continuous']
Deployability,"ow(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89419,pipeline,pipeline,89419,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipeline']
Deployability,"own ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; """"""; n, m = self.shape. if n * m * min(n, m) <= complexity_bound*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76539,install,installing,76539,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['install'],['installing']
Deployability,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:104097,pipeline,pipelines,104097,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,"past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem implementations. File Format. The native file format version is now 1.7.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.118; Released 2023-06-13. New Features. (#13140) Enable; hail-az and Azure Blob Storage https URLs to contain SAS; tokens to enable bearer-auth style file access to Azure storage.; (#13129) Allow; subnet to be passed through to gcloud in hailctl. Bug Fixes. (#13126); Query-on-Batch pipelines with one partition are now retried when they; encounter transient errors.; (#13113); hail.ggplot.geom_point now displays a legend group for a column; even when it has only one value in it.; (#13075); (#13074) Add a new; transient error plaguing pipelines in Query-on-Batch in Google:; java.net.SocketTimeoutException: connect timed out.; (#12569) The; documentation for hail.ggplot.facets is now correctly included in; the API reference. Version 0.2.117; Released 2023-05-22. New Features. (#12875) Parallel; export modes now write a manifest file. These manifest files are text; files with one filename per line, containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:31856,pipeline,pipelines,31856,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"patible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Keep in mind the following principles when submitting a pull request:. A PR should focus on a single feature. Multi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1737,install,install,1737,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['install'],['install']
Deployability,"pe, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; refe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31419,update,update,31419,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"pects a table with interval keys""); point_type = ht.key[0].dtype.point_type; if isinstance(points, Table):; if len(points.key) != 1 or points.key[0].dtype != point_type:; raise ValueError(; ""'segment_intervals' expects points to be a table with a single""; "" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {list(points.key.dtype.values())}""; ); points = hl.array(hl.set(points.collect(_localize=False))); if points.dtype.element_type != point_type:; raise ValueError(; f""'segment_intervals' expects points to be a table with a single""; f"" key of the same type as the intervals in 'ht', or an array of those points:""; f""\n expect {point_type}, found {points.dtype.element_type}""; ). points = hl._sort_by(points, lambda l, r: hl._compare(l, r) < 0). ht = ht.annotate_globals(__points=points). interval = ht.key[0]; points = ht.__points; lower = hl.expr.functions._lower_bound(points, interval.start); higher = hl.expr.functions._lower_bound(points, interval.end); n_points = hl.len(points); lower = hl.if_else((lower < n_points) & (points[lower] == interval.start), lower + 1, lower); higher = hl.if_else((higher < n_points) & (points[higher] == interval.end), higher - 1, higher); interval_results = hl.rbind(; lower,; higher,; lambda lower, higher: hl.if_else(; lower >= higher,; [interval],; hl.flatten([; [; hl.interval(; interval.start, points[lower], includes_start=interval.includes_start, includes_end=False; ); ],; hl.range(lower, higher - 1).map(; lambda x: hl.interval(points[x], points[x + 1], includes_start=True, includes_end=False); ),; [; hl.interval(; points[higher - 1], interval.end, includes_start=True, includes_end=interval.includes_end; ); ],; ]),; ),; ); ht = ht.annotate(__new_intervals=interval_results, lower=lower, higher=higher).explode('__new_intervals'); return ht.key_by(**{next(iter(ht.key)): ht.__new_intervals}).drop('__new_intervals'). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:16992,update,updated,16992,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,['update'],['updated']
Deployability,"perimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2582,configurat,configuration,2582,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['configurat'],['configuration']
Deployability,"plot.scatter(x, y, label=None, title=None, xlabel=None, ylabel=None, size=4, legend=True, hover_fields=None, colors=None, width=800, height=800, collect_all=None, n_divisions=500, missing_label='NA')[source]; Create an interactive scatter plot.; x and y must both be either:; - a NumericExpression from the same Table.; - a tuple (str, NumericExpression) from the same Table. If passed as a tuple the first element is used as the hover label.; If no label or a single label is provided, then returns bokeh.plotting.figure; Otherwise returns a bokeh.models.layouts.Column containing:; - a bokeh.models.widgets.inputs.Select dropdown selection widget for labels; - a bokeh.plotting.figure containing the interactive scatter plot; Points will be colored by one of the labels defined in the label using the color scheme defined in; the corresponding entry of colors if provided (otherwise a default scheme is used). To specify your color; mapper, check the bokeh documentation; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions.; Hovering on points will display their coordinates, labels and any additional fields specified in hover_fields. Parameters:. x (NumericExpression or (str, NumericExpression)) – List of x-values to be plotted.; y (NumericExpression or (str, NumericExpression)) – List of y-values to be plotted.; label (Expression or Dict[str, Expression]], optional) – Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:7072,continuous,continuous,7072,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['continuous'],['continuous']
Deployability,"population': str; 'super_population': str; 'is_female': bool; 'family_id': str; 'relationship_role': str; 'maternal_id': str; 'paternal_id': str; 'children_ids': array<str>; 'sibling_ids': array<str>; 'second_order_relationship_ids': array<str>; 'third_order_relationship_ids': array<str>; 'sample_qc': struct {; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; DP: int32,; END: int32,; SVTYPE: str,; AA: str,; AC: int32,; AF: float64,; NS: int32,; AN: int32,; EAS_AF: float64,; EUR_AF: float64,; AFR_AF: float64,; AMR_AF: float64,; SAS_AF: float64,; VT: str,; EX_TARGET: bool,; MULTI_ALLELIC: bool,; STRAND_FLIP: bool,; REF_SWITCH: bool,; DEPRECATED_RSID: str,; RSID_REMOVED: str,; GRCH37_38_REF_STRING_MATCH: bool,; NOT_ALL_RSIDS_STRAND_CHANGE_OR_REF_SWITCH: bool,; GRCH37_POS: int32,; GRCH37_REF: str,; ALLELE_TRANSFORM: bool,; REF_NEW_ALLELE: bool,; CHROM_CHANGE_BETWEEN_ASSEMBLIES: str; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh38>; 'old_alleles': array<str>; 'variant_qc': struct {; AC: array<int32>,; AF: array<float64>,; AN: int32,; homozygote_count: array<int32>,; n_called: int64,; n_not_called: int64,; call_rate: float32,; n_het: int64,; n_non_ref: int64,; het_freq_hwe: float64,; p_value_hwe: float64; }; ----------------------------------------; Entry fields:; 'GT': call; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html:11261,update,updated,11261,docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/1000_Genomes_Retracted_chrY.html,1,['update'],['updated']
Deployability,"pruned_table.locus, bp_window_size). entries = r2_bm.sparsify_row_intervals(range(stops.size), stops, blocks_only=True).entries(keyed=False); entries = entries.filter((entries.entry >= r2) & (entries.i < entries.j)); entries = entries.select(i=hl.int32(entries.i), j=hl.int32(entries.j)). if keep_higher_maf:; fields = ['mean', 'locus']; else:; fields = ['locus']. info = locally_pruned_table.aggregate(; hl.agg.collect(locally_pruned_table.row.select('idx', *fields)), _localize=False; ); info = hl.sorted(info, key=lambda x: x.idx). entries = entries.annotate_globals(info=info). entries = entries.filter(; (entries.info[entries.i].locus.contig == entries.info[entries.j].locus.contig); & (entries.info[entries.j].locus.position - entries.info[entries.i].locus.position <= bp_window_size); ). if keep_higher_maf:; entries = entries.annotate(; i=hl.struct(; idx=entries.i, twice_maf=hl.min(entries.info[entries.i].mean, 2.0 - entries.info[entries.i].mean); ),; j=hl.struct(; idx=entries.j, twice_maf=hl.min(entries.info[entries.j].mean, 2.0 - entries.info[entries.j].mean); ),; ). def tie_breaker(left, right):; return hl.sign(right.twice_maf - left.twice_maf). else:; tie_breaker = None. variants_to_remove = hl.maximal_independent_set(; entries.i, entries.j, keep=False, tie_breaker=tie_breaker, keyed=False; ). locally_pruned_table = locally_pruned_table.annotate_globals(; variants_to_remove=variants_to_remove.aggregate(; hl.agg.collect_as_set(variants_to_remove.node.idx), _localize=False; ); ); return (; locally_pruned_table.filter(; locally_pruned_table.variants_to_remove.contains(hl.int32(locally_pruned_table.idx)), keep=False; ); .select(); .persist(); ). def _warn_if_no_intercept(caller, covariates):; if all([e._indices.axes for e in covariates]):; warning(; f'{caller}: model appears to have no intercept covariate.'; '\n To include an intercept, add 1.0 to the list of covariates.'; ); return True; return False. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:172830,update,updated,172830,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['updated']
Deployability,"pulation; 	 and national health register data. medRxiv 2022.03.03.22271360;; 	 doi: https://doi.org/10.1101/2022.03.03.22271360. https://www.medrxiv.org/content/10.1101/2022.03.03.22271360v1. 	 Akingbuwa, O. A. (2022). Polygenic analyses of childhood and adult psychopathology, and; 	 their overlap. [PhD- Thesis - Research and graduation internal, Vrije Universiteit; 	 Amsterdam]. https://research.vu.nl/ws/portalfiles/portal/149553301/O+A++Akingbuwa+-+thesis.pdf. 2021. Atkinson, E.G., et al. ""Tractor uses local ancestry to enable the inclusion of admixed individuals in GWAS and to boost power"", Nature Genetics (2021).; https://doi.org/10.1038/s41588-020-00766-y; https://www.nature.com/articles/s41588-020-00766-y. Maes, H.H. ""Notes on Three Decades of Methodology Workshops"", Behavior Genetics (2021). https://doi.org/10.1007/s10519-021-10049-9 https://link.springer.com/article/10.1007/s10519-021-10049-9; Malanchini, M., et al. ""Pathfinder: A gamified measure to integrate general cognitive ability into the biological, medical and behavioural sciences."", bioRxiv (2021). https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract https://www.biorxiv.org/content/10.1101/2021.02.10.430571v1.abstract. 2020. Zekavat, S.M., et al. ""Hematopoietic mosaic chromosomal alterations and risk for infection among 767,891 individuals without blood cancer"", medRxiv (2020). https://doi.org/10.1101/2020.11.12.20230821 https://europepmc.org/article/ppr/ppr238896; Kwong, A.K., et al. ""Exome Sequencing in Paediatric Patients with Movement Disorders with Treatment Possibilities"", Research Square (2020). https://doi.org/10.21203/rs.3.rs-101211/v1 https://europepmc.org/article/ppr/ppr235428; Krissaane, I, et al. “Scalability and cost-effectiveness analysis of whole genome-wide association studies on Google Cloud Platform and Amazon Web Services”, Journal of the American Medical Informatics Association (2020) ocaa068 https://doi.org/10.1093/jamia/ocaa068 https://academic.oup.com/jamia/ar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/references.html:9778,integrat,integrate,9778,references.html,https://hail.is,https://hail.is/references.html,1,['integrat'],['integrate']
Deployability,"p. description:; Group the columns of the matrix table by the column-indexed; field cohort and compute the call rate per cohort. code:; >>> result_mt = (mt.group_cols_by(mt.cohort); ... .aggregate(call_rate=hl.agg.fraction(hl.is_defined(mt.GT)))). dependencies:; MatrixTable.group_cols_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the columns of the matrix table by; the column-indexed field cohort using MatrixTable.group_cols_by(),; which returns a GroupedMatrixTable. Then use; GroupedMatrixTable.aggregate() to compute an aggregation per column; group.; The result is a matrix table with an entry field call_rate that contains; the result of the aggregation. The new matrix table has a row schema equal; to the original row schema, a column schema equal to the fields passed to; group_cols_by, and an entry schema determined by the expression passed to; aggregate. Other column fields and entry fields are dropped. Aggregate Per Row Group. description:; Compute the number of calls with one or more non-reference; alleles per gene group. code:; >>> result_mt = (mt.group_rows_by(mt.gene); ... .aggregate(n_non_ref=hl.agg.count_where(mt.GT.is_non_ref()))). dependencies:; MatrixTable.group_rows_by(), GroupedMatrixTable, GroupedMatrixTable.aggregate(). understanding:. Group the rows of the matrix table by the row-indexed field gene; using MatrixTable.group_rows_by(), which returns a; GroupedMatrixTable. Then use GroupedMatrixTable.aggregate(); to compute an aggregation per grouped row.; The result is a matrix table with an entry field n_non_ref that contains; the result of the aggregation. This new matrix table has a row schema; equal to the fields passed to group_rows_by, a column schema equal to the; column schema of the original matrix table, and an entry schema determined; by the expression passed to aggregate. Other row fields and entry fields; are dropped. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/agg.html:7072,update,updated,7072,docs/0.2/guides/agg.html,https://hail.is,https://hail.is/docs/0.2/guides/agg.html,1,['update'],['updated']
Deployability,"quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; dbSNP. View page source. dbSNP. Versions: 154; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (154, GRCh37); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'qual': float64; 'filters': set<str>; 'info': struct {; RS: int32,; GENEINFO: str,; PSEUDOGENEINFO: str,; dbSNPBuildID: int32,; SAO: int32,; SSR: int32,; VC: str,; PM: bool,; NSF: bool,; NSM: bool,; NSN: bool,; SYN: bool,; U3: bool,; U5: bool,; ASS: bool,; DSS: bool,; INT: bool,; R3: bool,; R5: bool,; GNO: bool,; PUB: bool,; FREQ: struct {; _GENOME_DK: float64,; _TWINSUK: float64,; _dbGaP_PopFreq: float64,; _Siberian: float64,; _Chileans: float64,; _FINRISK: float64,; _HapMap: float64,; _Estonian: float64,; _ALSPAC: float64,; _GoESP: float64,; _TOPMED: float64,; _PAGE_STUDY: float64,; _1000Genomes: float64,; _Korea1K: float64,; _ChromosomeY: float64,; _ExAC: float64,; _Qatari: float64,; _GoNL: float64,; _MGP: float64,; _GnomAD: float64,; _Vietnamese: float64,; _GnomAD_exomes: float64,; _PharmGKB: float64,; _KOREAN: float64,; _Daghestan: float64,; _HGDP_Stanford: float64,; _NorthernSweden: float64,; _SGDP_PRJ: float64; },; COMMON: bool,; CLNHGVS: array<str>,; CLNVI: array<str>,; CLNORIGIN: array<str>,; CLNSIG: array<str>,; CLNDISDB: array<str>,; CLNDN: array<str>,; CLNREVSTAT: array<str>,; CLNACC: array<str>; }; 'a_index': int32; 'was_split': bool; 'old_locus': locus<GRCh37>; 'old_alleles': array<str>; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html:10632,update,updated,10632,docs/0.2/datasets/schemas/dbSNP.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/dbSNP.html,1,['update'],['updated']
Deployability,"r -1, because the; discriminant, y, is missing. Switch Statements; Finally, Hail has the switch() function to build a conditional tree based; on the value of an expression. In the example below, csq is a; StringExpression representing the functional consequence of a; mutation. If csq does not match one of the cases specified by; when(), it is set to missing with; or_missing(). Other switch statements are documented in the; SwitchBuilder class.; >>> csq = hl.str('nonsense'). >>> (hl.switch(csq); ... .when(""synonymous"", False); ... .when(""intron"", False); ... .when(""nonsense"", True); ... .when(""indel"", True); ... .or_missing()); <BooleanExpression of type bool>. As with case statements, missingness will propagate up through a switch; statement. If we changed the value of csq to the missing value; hl.missing(hl.tstr), then the result of the switch statement above would also; be missing. Missingness; In Hail, all expressions can be missing. An expression representing a missing; value of a given type can be generated with the missing() function, which; takes the type as its single argument.; An example of generating a Float64Expression that is missing is:; >>> hl.missing('float64'); <Float64Expression of type float64>. These can be used with conditional statements to set values to missing if they; don’t satisfy a condition:; >>> hl.if_else(x > 2.0, x, hl.missing(hl.tfloat)); <Float64Expression of type float64>. The Python representation of a missing value is None. For example, if; we define cnull to be a missing value with type tcall, calling; the method is_het will return None and not False.; >>> cnull = hl.missing('call'); >>> hl.eval(cnull.is_het()); None. Functions; In addition to the methods exposed on each Expression, Hail also has; numerous functions that can be applied to expressions, which also return an; expression.; Take a look at the Functions page for full documentation. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/expressions.html:9046,update,updated,9046,docs/0.2/overview/expressions.html,https://hail.is,https://hail.is/docs/0.2/overview/expressions.html,1,['update'],['updated']
Deployability,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2580,configurat,configuration,2580,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['configurat'],['configuration']
Deployability,"r read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:81108,update,update,81108,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; Schema (0.2, GRCh37). panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AFR. View page source. panukb_ld_variant_indices_AFR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AFR.html,2,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; Schema (0.2, GRCh37). panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_AMR. View page source. panukb_ld_variant_indices_AMR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_AMR.html,2,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; Schema (0.2, GRCh37). panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_CSA. View page source. panukb_ld_variant_indices_CSA. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_CSA.html,2,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; Schema (0.2, GRCh37). panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EAS. View page source. panukb_ld_variant_indices_EAS. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EAS.html,2,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; Schema (0.2, GRCh37). panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_EUR. View page source. panukb_ld_variant_indices_EUR. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_EUR.html,2,['update'],['updated']
Deployability,"r; gnomad_ld_scores_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; Schema (0.2, GRCh37). panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; panukb_ld_variant_indices_MID. View page source. panukb_ld_variant_indices_MID. Versions: 0.2; Reference genome builds: GRCh37; Type: hail.Table. Schema (0.2, GRCh37); ----------------------------------------; Global fields:; 'n_samples': int32; 'pop': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'idx': int64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html:9395,update,updated,9395,docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_ld_variant_indices_MID.html,2,['update'],['updated']
Deployability,"r=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y_train). # apply the trained random forest on testing data; y_pred = rf.predict(x_test). # store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.Se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4033,install,installed,4033,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['install'],['installed']
Deployability,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22416,pipeline,pipelined,22416,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['pipeline'],['pipelined']
Deployability,"ral chi-square term.; k (list of int or Expression of type tarray of tint32) – A degrees of freedom parameter for each non-central chi-square term.; lam (list of float or Expression of type tarray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20495,integrat,integration,20495,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"range.end = y_max_densities[label_cols[0]]. callback_args: Dict[str, Any]; callback_args = dict(; scatter_renderers=sp_scatter_renderers,; color_mappers=sp_color_mappers,; density_renderers=x_renderers + y_renderers,; x_range=xp.y_range,; x_max_densities=x_max_densities,; y_range=yp.x_range,; y_max_densities=y_max_densities,; ). callback_code = """"""; for (var i = 0; i < scatter_renderers.length; i++){; scatter_renderers[i].glyph.fill_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].glyph.line_color = {field: cb_obj.value, transform: color_mappers[cb_obj.value]}; scatter_renderers[i].visible = true; }. for (var i = 0; i < density_renderers.length; i++){; density_renderers[i][2].visible = density_renderers[i][0] == cb_obj.value; }. x_range.start = 0; y_range.start = 0; x_range.end = x_max_densities[cb_obj.value]; y_range.end = y_max_densities[cb_obj.value]. """""". if legend:; callback_args.update(dict(legend_items=sp_legend_items, legend=sp_legend, color_bar=sp_color_bar)); callback_code += """"""; if (cb_obj.value in legend_items){; legend.items=legend_items[cb_obj.value]; legend.visible=true; color_bar.visible=false; }else{; legend.visible=false; color_bar.visible=true; }. """""". callback = CustomJS(args=callback_args, code=callback_code); select = Select(title=""Color by"", value=label_cols[0], options=label_cols); select.js_on_change('value', callback); first_row.append(select). return gridplot([first_row, [sp, yp]]). [docs]@typecheck(; pvals=expr_numeric,; label=nullable(oneof(dictof(str, expr_any), expr_any)),; title=nullable(str),; xlabel=nullable(str),; ylabel=nullable(str),; size=int,; legend=bool,; hover_fields=nullable(dictof(str, expr_any)),; colors=nullable(oneof(bokeh.models.mappers.ColorMapper, dictof(str, bokeh.models.mappers.ColorMapper))),; width=int,; height=int,; collect_all=nullable(bool),; n_divisions=nullable(int),; missing_label=str,; ); def qq(; pvals: NumericExpression,; label: Optional[Union[Expression, Dict[s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:44839,update,update,44839,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['update'],['update']
Deployability,"rays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud Storage SDK on which we; rely. The fix is to update to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#1375",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21687,pipeline,pipelines,21687,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45816,pipeline,pipelined,45816,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['pipeline'],['pipelined']
Deployability,"rce_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VD",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30872,update,update,30872,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"red_axis_kwargs(),; }; else:; n_facet_rows = 1; n_facet_cols = 1; subplot_args = {; ""rows"": 1,; ""cols"": 1,; }; fig = make_subplots(**subplot_args). # Need to know what I've added to legend already so we don't do it more than once.; legend_cache = {}. for geom, geom_label, facet_to_grouped_dfs in geoms_and_grouped_dfs_by_facet_idx:; for facet_idx, grouped_dfs in facet_to_grouped_dfs.items():; scaled_grouped_dfs = []; for df in grouped_dfs:; scales_to_consider = list(df.columns) + list(df.attrs); relevant_aesthetics = [scale_name for scale_name in scales_to_consider if scale_name in self.scales]; scaled_df = df; for relevant_aesthetic in relevant_aesthetics:; scaled_df = transformers[relevant_aesthetic](scaled_df); scaled_grouped_dfs.append(scaled_df). facet_row = facet_idx // n_facet_cols + 1; facet_col = facet_idx % n_facet_cols + 1; geom.apply_to_fig(; scaled_grouped_dfs, fig, precomputed[geom_label], facet_row, facet_col, legend_cache, is_faceted; ). # Important to update axes after labels, axes names take precedence.; self.labels.apply_to_fig(fig); if self.scales.get(""x"") is not None:; self.scales[""x""].apply_to_fig(self, fig); if self.scales.get(""y"") is not None:; self.scales[""y""].apply_to_fig(self, fig); if self.coord_cartesian is not None:; self.coord_cartesian.apply_to_fig(fig). fig = fig.update_xaxes(title_font_size=18, ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:9337,update,update,9337,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['update'],['update']
Deployability,"reference_blocks(ds, *, max_ref_block_base_pairs=None, ref_block_winsorize_fraction=None):; """"""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""; if isinstance(ds, VariantDataset):; rd = ds.reference_data; else:; rd = ds. fd_name = hl.vds.VariantDataset.ref_block_max_length_field; if fd_name in rd.globals:; rd = rd.drop(fd_name). if int(ref_block_winsorize_fraction is None) + int(max_ref_block_base_pairs is None) != 1:; raise ValueError(; 'truncate_reference_blocks: require exactly one of ""max_ref_block_base_pairs"", ""ref_block_winsorize_fraction""'; ). if ref_block_winsorize_fraction is not None:; assert (; ref_block_winsorize_fraction > 0 and ref_block_winsoriz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:33191,patch,patch,33191,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['patch'],['patch']
Deployability,"regate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection – the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using key_cols_by() with no arguments. Warning; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns:; Table – Table with all non-global fields from the matrix, with one row per entry of the matrix. property entry; Returns a struct expression including all row-and-column-indexed fields.; Examples; Get all entry field names:; >>> list(dataset.entry); ['GT', 'AD', 'DP', 'GQ', 'PL']. Returns:; StructExpression – Struct of all entry fields. explode_cols(field_expr)[source]; Explodes a column field of type array or set, copying the entire column for each element.; Examples; Explode columns by annotated cohorts:; >>> dataset_result = dataset.explode_cols(dataset.cohorts). Notes; The new matrix table will have N copies of each column, where N is the; number of elements that column contains for the field denoted by field_expr.; The field referenced in field_expr is replaced in the sequence of duplicated; columns by the sequence of elements in the array or set. All other fields remain; the same, includ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:28666,pipeline,pipeline,28666,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95258,pipeline,pipelines,95258,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27530,pipeline,pipelines,27530,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9409,update,updated,9409,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,1,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Cerebellum_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. View page source. GTEx_sQTL_Colon_Transverse_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Colon_Transverse_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"riant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. View page source. GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Esophagus_Mucosa_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:41977,configurat,configuration,41977,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEP",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106024,configurat,configuration,106024,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['configurat'],['configuration']
Deployability,"ritically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11095,pipeline,pipelines,11095,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"rk backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, opt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:3809,pipeline,pipeline,3809,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['pipeline'],['pipeline']
Deployability,"rn construct_expr(slice_ir, self.dtype, indices, aggregations). [docs] @typecheck_method(f=func_spec(1, expr_any)); def aggregate(self, f):; """"""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""; return hl.agg._aggregate_local_array(self, f). [docs] @typecheck_method(item=expr_any); def contains(self, item):; """"""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""; return self._method(""contains"", tbool, item). [docs] @deprecated(version=""0.2.58"", reason=""Replaced by first""); def head(self):; """"""Deprecated in favor of :meth:`~.ArrayExpression.first`. Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.head()); 'Alice'. If the array has no elements, then the result is missing:. >>> hl.eval(names.filter(lambda x: x.startswith('D')).head()); None; """"""; return self.first(). [docs] def first(self):; """"""Returns the first element of the array, or missing if empty. Returns; -------; :class:`.Expression`; Element. Examples; --------; >>> hl.eval(names.first()); 'Alice'. If the array has no elements, then the result is missing:; >>> hl.eval(na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:13923,pipeline,pipeline,13923,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['pipeline'],['pipeline']
Deployability,"rn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:5379,configurat,configuration,5379,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['configurat'],"['configuration', 'configurations']"
Deployability,"rocess. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; defa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11610,configurat,configuration,11610,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['configurat'],['configuration']
Deployability,"rray of tfloat64) – A non-centrality parameter for each non-central chi-square term. We use lam instead; of lambda because the latter is a reserved word in Python.; mu (float or Expression of type tfloat64) – The standard deviation of the normal term.; sigma (float or Expression of type tfloat64) – The standard deviation of the normal term.; max_iterations (int or Expression of type tint32) – The maximum number of iterations of the numerical integration before raising an error. The; default maximum number of iterations is 1e5.; min_accuracy (int or Expression of type tint32) – The minimum accuracy of the returned value. If the minimum accuracy is not achieved, this; function will raise an error. The default minimum accuracy is 1e-5. Returns:; StructExpression – This method returns a structure with the value as well as information about the numerical; integration. value : Float64Expression. If converged is true, the value of the CDF evaluated; at x. Otherwise, this is the last value the integration evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probabili",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:20633,integrat,integration,20633,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['integrat'],['integration']
Deployability,"rror caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89532,update,update,89532,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['update'],['update']
Deployability,"rs (keyword args of Expression) – Column-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix, can be used to call GroupedMatrixTable.aggregate(). group_rows_by(*exprs, **named_exprs)[source]; Group rows.; Examples; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:4978,pipeline,pipeline,4978,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"rs:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; BashJob. new_job(name=None, attributes=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello alice”:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. E",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7268,install,installed,7268,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['install'],['installed']
Deployability,"rt modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:221769,configurat,configuration,221769,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['configurat'],['configuration']
Deployability,"ruct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_qual_hists': struct {; gq_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_all: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; gq_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; dp_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; },; ab_hist_alt: struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; }; 'gnomad_age_hist_het': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'gnomad_age_hist_hom': struct {; bin_edges: array<float64>,; bin_freq: array<int64>,; n_smaller: int64,; n_larger: int64; }; 'cadd': struct {; phred: float32,; raw_score: float32,; has_duplicate: bool; }; 'revel': struct {; revel_score: float64,; has_duplicate: bool; }; 'splice_ai': struct {; splice_ai_score: float32,; splice_consequence: str,; has_duplicate: bool; }; 'primate_ai': struct {; primate_ai_score: float32,; has_duplicate: bool; }; 'AS_lowqual': bool; 'telomere_or_centromere': bool; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html:22976,update,updated,22976,docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_hgdp_1kg_subset_variant_annotations.html,1,['update'],['updated']
Deployability,"s 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the com",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:2034,install,installed,2034,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['install'],['installed']
Deployability,"s \in S_{ij}}; \widehat{\sigma^2_{is}} \widehat{\sigma^2_{js}}}. The estimator for identity-by-descent zero is given by:. .. math::. \widehat{k^{(0)}_{ij}} \coloneqq; \begin{cases}; \frac{\text{IBS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}}; \widehat{\mu_{is}}^2(1 - \widehat{\mu_{js}})^2; + (1 - \widehat{\mu_{is}})^2\widehat{\mu_{js}}^2}; & \widehat{\phi_{ij}} > 2^{-5/2} \\; 1 - 4 \widehat{\phi_{ij}} + k^{(2)}_{ij}; & \widehat{\phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:7764,release,release,7764,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['release'],['release']
Deployability,"s can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarrays into the equivalent list.; (#12851); hl.variant_qc no longer requires a locus field.; (#12816) In; Query-on-Batch, hl.logistic_regression('firth', ...) is now; supported.; (#12854) In; Query-on-Batch, simple pipelines with large numbers of partitions; should be substantially faster. Bug Fixes. (#12783) Fixed bug; where logs were not properly transmitted to Python.; (#12812) Fixed bug; where Table/MT._calculate_new_partitions returned unbalanced; intervals with whole-stage code generation runtime.; (#12839) Fixed; hailctl dataproc j",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:35961,patch,patch,35961,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['patch'],['patch']
Deployability,"s every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; library versions released after the file format. For example, Python; library version 0.2.119 introduces a new file format version: 1.7.0. All; library versions before 0.2.119, for example 0.2.118, cannot read file; format version 1.7.0. All library versions after and including 0.2.119; can read file format version 1.7.0.; Each version of the Hail Python library can only write files using the; latest file format version it supports.; The hl.experimental package and other methods marked experimental in; the docs are exempt from this policy. Their functionality or even; existence may change without notice. Please contact us if you critically; depend on experimental functionality. Version 0.2.133; Released 2024-09-25. New Features. (#14619) Teach; hailctl dataproc submit to use the --project argument as an; argument to gcloud dataproc rather than the submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:9485,release,released,9485,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['released']
Deployability,"s in ABS is now deprecated and; will be removed in an upcoming release. Bug Fixes. (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point.; (#12901); hl.Struct now has a correct and useful implementation of; pprint. Version 0.2.115; Released 2023-04-25. New Features. (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This can be used by; import hailtop.fs as hfs but has also replaced the underlying; implementation of the hl.hadoop_* methods. This means that the; hl.hadoop_* methods now support these additional blob storage; providers.; (#12917) ABS blob; URIs in the form of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported when running in Azure. Deprecations. (#12917) The; hail-az scheme for referencing ABS blobs in Azure is deprecated; in favor of the https scheme and will be removed in a future; release. Bug Fixes. (#12919) An; interactive hail session is no longer unusable after hitting CTRL-C; during a batch execution in Query-on-Batch; (#12913) Fixed bug; in hail.ggplot where all legend entries would have the same text; if one column had exactly one value for all rows and was mapped to; either the shape or the color aesthetic for geom_point. Version 0.2.114; Released 2023-04-19. New Features. (#12880) Added; hl.vds.store_ref_block_max_len to patch old VDSes to make; interval filtering faster. Bug Fixes. (#12860) Fixed; memory leak in shuffles in Query-on-Batch. Version 0.2.113; Released 2023-04-07. New Features. (#12798); Query-on-Batch now supports; BlockMatrix.write(..., stage_locally=True).; (#12793); Query-on-Batch now supports hl.poisson_regression_rows.; (#12801) Hitting; CTRL-C while interactively using Query-on-Batch cancels the; underlying batch.; (#12810); hl.array can now convert 1-d ndarray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:35500,release,release,35500,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"s in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28168,update,update,28168,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"s the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""; require_row_key_variant(mt, 'filter_alleles'); inclusion = hl.range(0, hl.len(mt.alleles)).map(lambda i: (i == 0) | hl.bind(lambda ii: f(mt.alleles[ii], ii), i)). # old locus, old alleles, new to old, old to new; mt = mt.annotate_rows(__allele_inclusion=inclusion, old_locus=mt.locus, old_alleles=mt.alleles); new_to_old = hl.enumerate(mt.__allele_inclusion).filter(lambda elt: elt[1]).map(lambda elt: elt[0]); old_to_new_dict = hl.dict(; h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:155564,update,update,155564,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['update']
Deployability,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77592,pipeline,pipelines,77592,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['pipeline'],['pipelines']
Deployability,"s); return self._copy(col_keys=col_key, computed_col_key=computed_key). def _check_bindings(self, caller, new_bindings, indices):; empty = []. def iter_option(o):; return o if o is not None else empty. if indices == self._parent._row_indices:; fixed_fields = [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:7871,pipeline,pipeline,7871,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['pipeline'],['pipeline']
Deployability,"s, mt.new_to_old.map(lambda i: mt.alleles[i])); mt = mt.annotate_rows(__new_locus=new_locus_alleles.locus, __new_alleles=new_locus_alleles.alleles); mt = mt.filter_rows(hl.len(mt.__new_alleles) > 1); left = mt.filter_rows((mt.locus == mt.__new_locus) & (mt.alleles == mt.__new_alleles)). right = mt.filter_rows((mt.locus != mt.__new_locus) | (mt.alleles != mt.__new_alleles)); right = right.key_rows_by(locus=right.__new_locus, alleles=right.__new_alleles); return left.union_rows(right, _check_cols=False).drop('__allele_inclusion', '__new_locus', '__new_alleles'). [docs]@typecheck(mt=MatrixTable, f=anytype, subset=bool); def filter_alleles_hts(mt: MatrixTable, f: Callable, subset: bool = False) -> MatrixTable:; """"""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:157881,Update,Update,157881,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Update'],['Update']
Deployability,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1981,integrat,integrated,1981,index.html,https://hail.is,https://hail.is/index.html,1,['integrat'],['integrated']
Deployability,"s.; The population distribution \(\pi\) defaults to uniform; The \(F_{ST}\) values default to 0.1; The number of partitions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising \(K\) homogeneous subpopulations; that have each diverged from a single ancestral population (a star phylogeny). We take \(N\) samples and \(M\) bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector \(\pi\); the ancestral allele frequencies are; drawn independently from a frequency spectrum \(P_0\); the subpopulations have diverged with possibly different \(F_{ST}\) parameters \(F_k\); (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. \(k = 1, \ldots, K\)).; For each variant, the subpopulation allele frequencies are drawn a beta distribution, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by \(k_n\), the ancestral allele frequences by \(p_{0, m}\),; the subpopulation allele frequencies by \(p_{k, m}\), and the genotypes by \(g_{n, m}\). The generative model in then given by:. \[ \begin{align}\begin{aligned}k_n \,&\sim\, \pi\\p_{0,m}\,&\sim\, P_0\\p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m}))\\g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m})\end{aligned}\end{align} \]; We have parametrized the beta distribution by its mean and variance; the usual parameters are \(a = (1 - p)(1 - F)/F,\; b = p(1-F)/F\) with \(F = F_k,\; p = p_{0,m}\).; Annotations; balding_nichols_model() adds the following global, sample, and variant annotations:. global.nPops (Int) – Number of populations; global.nSamples (Int) – Number of samples; global.nVariants (Int) – Number of variants; global.popDist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:5122,continuous,continuous,5122,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['continuous'],['continuous']
Deployability,"s.e10) / expectations.e11; Z2 = (ibs2 - Z0 * expectations.e20 - Z1 * expectations.e21) / expectations.e22. def convert_to_table(bm, annotation_name):; t = bm.entries(); t = t.rename({'entry': annotation_name}); return t. z0 = convert_to_table(Z0, 'Z0').checkpoint(hl.utils.new_temp_file()); z1 = convert_to_table(Z1, 'Z1').checkpoint(hl.utils.new_temp_file()); z2 = convert_to_table(Z2, 'Z2').checkpoint(hl.utils.new_temp_file()); ibs0 = convert_to_table(ibs0, 'ibs0').checkpoint(hl.utils.new_temp_file()); ibs1 = convert_to_table(ibs1, 'ibs1').checkpoint(hl.utils.new_temp_file()); ibs2 = convert_to_table(ibs2, 'ibs2').checkpoint(hl.utils.new_temp_file()). result = z0.join(z1.join(z2).join(ibs0).join(ibs1).join(ibs2)). def bound_result(_ibd):; return (; hl.case(); .when(_ibd.Z0 > 1, hl.struct(Z0=hl.float(1), Z1=hl.float(0), Z2=hl.float(0))); .when(_ibd.Z1 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(1), Z2=hl.float(0))); .when(_ibd.Z2 > 1, hl.struct(Z0=hl.float(0), Z1=hl.float(0), Z2=hl.float(1))); .when(; _ibd.Z0 < 0,; hl.struct(Z0=hl.float(0), Z1=_ibd.Z1 / (_ibd.Z1 + _ibd.Z2), Z2=_ibd.Z2 / (_ibd.Z1 + _ibd.Z2)),; ); .when(; _ibd.Z1 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z2), Z1=hl.float(0), Z2=_ibd.Z2 / (_ibd.Z0 + _ibd.Z2)),; ); .when(; _ibd.Z2 < 0,; hl.struct(Z0=_ibd.Z0 / (_ibd.Z0 + _ibd.Z1), Z1=_ibd.Z1 / (_ibd.Z0 + _ibd.Z1), Z2=hl.float(0)),; ); .default(_ibd); ). result = result.annotate(ibd=hl.struct(Z0=result.Z0, Z1=result.Z1, Z2=result.Z2)); result = result.drop('Z0', 'Z1', 'Z2'); if bounded:; result = result.annotate(ibd=bound_result(result.ibd)); result = result.annotate(ibd=result.ibd.annotate(PI_HAT=result.ibd.Z1 / 2 + result.ibd.Z2)); result = result.filter((result.i < result.j) & (min <= result.ibd.PI_HAT) & (result.ibd.PI_HAT <= max)). samples = hl.literal(dataset.s.collect()); result = result.key_by(i=samples[hl.int32(result.i)], j=samples[hl.int32(result.j)]). return result.persist(). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:9258,update,updated,9258,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['update'],['updated']
Deployability,"s.richUtils.RichDenseMatrixDouble.importFromDoubles(; Env.spark_backend('_breeze_fromfile').fs._jfs, uri, n_rows, n_cols, True; ). def _check_entries_size(n_rows, n_cols):; n_entries = n_rows * n_cols; if n_entries >= 1 << 31:; raise ValueError(f'number of entries must be less than 2^31, found {n_entries}'). def _breeze_from_ndarray(nd):; if any(i == 0 for i in nd.shape):; raise ValueError(f'from_numpy: ndarray dimensions must be non-zero, found shape {nd.shape}'). nd = _ndarray_as_2d(nd); nd = _ndarray_as_float64(nd); n_rows, n_cols = nd.shape. with with_local_temp_file() as path:; uri = local_path_uri(path); nd.tofile(path); return _breeze_fromfile(uri, n_rows, n_cols). def _svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True):; """"""; SciPy supports two Lapack algorithms:; DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesdd; GR: https://software.intel.com/en-us/mkl-developer-reference-fortran-gesvd; DC (gesdd) is faster but uses O(elements) memory; lwork may overflow int32; """"""; try:; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesdd',; ); except ValueError as e:; if 'Too large work array required' in str(e):; return spla.svd(; a,; full_matrices=full_matrices,; compute_uv=compute_uv,; overwrite_a=overwrite_a,; check_finite=check_finite,; lapack_driver='gesvd',; ); else:; raise. def _eigh(a):; """"""; Only the lower triangle is used. Returns eigenvalues, eigenvectors.; NumPy and SciPy apply different Lapack algorithms:; NumPy uses DC: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevd; SciPy uses RRR: https://software.intel.com/en-us/mkl-developer-reference-fortran-syevr; DC (syevd) is faster but uses O(elements) memory; lwork overflows int32 for dim_a > 32766; """"""; return np.linalg.eigh(a) if a.shape[0] <= 32766 else spla.eigh(a). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:83141,update,updated,83141,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['update'],['updated']
Deployability,"s: Optional[Union[ColorMapper, Dict[str, ColorMapper]]] = None,; width: int = 800,; height: int = 800,; collect_all: Optional[bool] = None,; n_divisions: Optional[int] = 500,; missing_label: str = 'NA',; ) -> Union[figure, Column]:; """"""Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:47198,continuous,continuous,47198,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"s; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return scale_fill_hue(). [docs]def scale_fill_continuous():; """"""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuous(""fill""). [docs]def scale_fill_identity():; """"""A color scale that assumes the expression specified in the ``fill`` aesthetic can be used as a fill color. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleColorContinuousIdentity(""fill""). [docs]def scale_fill_hue():; """"""Map discrete fill colors to evenly placed positions around the color wheel. Returns; -------; :class:`.FigureAttribute`; The scale to be applied. """"""; return ScaleColorHue(""fill""). [docs]def scale_fill_manual(*, values):; """"""A color scale that assigns strings to fill colors using the pool of colors specified as `values`. Parameters; ----------; values: :class:`list` of :class:`str`; The colors to choose when assigning values to colors. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""fill"", values=values). def scale_shape_manual(*, values):; """"""A scale that assigns shapes to discrete aesthetics. See `the plotly documentation <https://plotly.com/python-api-reference/generated/plotly.graph_objects.scatter.html#plotly.graph_objects.scatter.Marker.symbol>`__ for a list of supported shapes. Parameters; ----------; values: :class:`list` of :class:`str`; The shapes from which to choose. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleDiscreteManual(""shape"", values=values). def scale_shape_auto():; """"""A scale that automatically assigns shapes to discrete aesthetics. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""; return ScaleShapeAuto(""shape""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:14974,update,updated,14974,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,2,['update'],['updated']
Deployability,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5242,pipeline,pipeline,5242,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['pipeline'],['pipeline']
Deployability,"s=None, shell=None); Alias for Batch.new_bash_job(). Return type:; BashJob. new_python_job(name=None, attributes=None); Initialize a new PythonJob object with default; Python image, memory, storage, and CPU settings (defined in Batch); upon batch creation.; Examples; Create and execute a batch b with one job j that prints “hello alice”:; b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; The image to use for Python jobs can be specified by default_python_image; when constructing a Batch. The image specified must have the dill; package installed. If default_python_image is not specified, then a Docker; image will automatically be created for you with the base image; hailgenetics/python-dill:[major_version].[minor_version]-slim and the Python; packages specified by python_requirements will be installed. The default name; of the image is batch-python with a random string for the tag unless python_build_image_name; is specified. If the ServiceBackend is the backend, the locally built; image will be pushed to the repository specified by image_repository. Parameters:. name (Optional[str]) – Name of the job.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead. Return type:; PythonJob. read_input(path); Create a new input resource file object representing a single file. Warning; To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; Read the file hello.txt:; >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters:; path (str) – File path to read. Return type:; InputResourceFile. rea",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:7529,install,installed,7529,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['install'],['installed']
Deployability,"s_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Lung_all_snp_gene_associations. View page source. GTEx_eQTL_Lung_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html:9670,update,updated,9670,docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Lung_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. View page source. GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html:9763,update,updated,9763,docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Esophagus_Gastroesophageal_Junction_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html:9781,update,updated,9781,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Substantia_nigra_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"s_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. View page source. GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html:9781,update,updated,9781,docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Heart_Atrial_Appendage_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"sertions_dict.get(field); if e is None:; e = self._fields[field]; return e.dtype. new_type = hl.tstruct(**{f: get_type(f) for f in field_order}); indices, aggregations = unify_all(self, *insertions_dict.values()); return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, [(field, expr._ir) for field, expr in insertions_dict.items()], field_order; ),; new_type,; indices,; aggregations,; ). [docs] @typecheck_method(named_exprs=expr_any); def annotate(self, **named_exprs):; """"""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression in `named_exprs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Fields to add. Returns; -------; :class:`.StructExpression`; Struct with new or updated fields.; """"""; new_types = {n: t for (n, t) in self.dtype.items()}. for f, e in named_exprs.items():; new_types[f] = e.dtype. result_type = tstruct(**new_types); indices, aggregations = unify_all(self, *[x for (f, x) in named_exprs.items()]). return construct_expr(; ir.InsertFields.construct_with_deduplication(; self._ir, list(map(lambda x: (x[0], x[1]._ir), named_exprs.items())), None; ),; result_type,; indices,; aggregations,; ). [docs] @typecheck_method(fields=str, named_exprs=expr_any); def select(self, *fields, **named_exprs):; """"""Select existing fields and compute new ones. Examples; --------. >>> hl.eval(struct.select('a', c=['bar', 'baz'])); Struct(a=5, c=['bar', 'baz']). Notes; -----; The `fields` argument is a list of field names to keep. These fields; will appear in the resulting struct in the order they appear in; `fields`. The `named_exprs` arguments are new field expressions. Parameters; ----------; fields : varargs of :class:`str`; Field names to keep.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:47645,update,updated,47645,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['update'],['updated']
Deployability,"sion Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project,; and at minimum the two latest minor versions.; All minor versions of numpy released in the 24 months prior to the; project, and at minimum the last three minor versions. Frequently Asked Questions. With a version like 0.x, is Hail ready for use in publications?; Yes. The semantic versioning standard uses 0.x; (development) versions to refer to software that is either “buggy” or; “partial”. While we don’t view Hail as particularly buggy (especially; compared to one-off untested scripts pervasive in bioinformatics!), Hail; 0.2 is a partial realization of a larger vision. What is the difference between the Hail Python library version and the native file format version?; The Hail Python library version, the version you see on; PyPI, in pip, or in; hl.version() changes every time we release the Python library. The; Hail native file format version only changes when we change the format; of Hail Table and MatrixTable files. If a version of the Python library; introduces a new native file format version, we note that in the change; log. All subsequent versions of the Python library can read the new file; format version.; The native file format changes much slower than the Python library; version. It is not currently possible to view the file format version of; a Hail Table or MatrixTable. What stability is guaranteed?; The Hail file formats and Python API are backwards compatible. This; means that a script developed to run on Hail 0.2.5 should continue to; work in every subsequent release within the 0.2 major version. This also; means any file written by python library versions 0.2.1 through 0.2.5; can be read by 0.2.5.; Forward compatibility of file formats and the Python API is not; guaranteed. In particular, a new file format version is only readable by; librar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:8476,release,release,8476,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51732,configurat,configuration,51732,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"ss facilitates a sparse, split representation of genomic data in; which reference block data and variant data are contained in separate; MatrixTable objects. Parameters:. reference_data (MatrixTable) – MatrixTable containing only reference block data.; variant_data (MatrixTable) – MatrixTable containing only variant data. Attributes. ref_block_max_length_field; Name of global field that indicates max reference block length. reference_genome; Dataset reference genome. Methods. checkpoint; Write to path and then read from path. from_merged_representation; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples; The number of samples present. union_rows; Combine many VDSes with the same samples but disjoint variants. validate; Eagerly checks necessary representational properties of the VDS. write; Write to path. checkpoint(path, **kwargs)[source]; Write to path and then read from path. static from_merged_representation(mt, *, ref_block_fields=(), infer_ref_block_fields=True, is_split=False)[source]; Create a VariantDataset from a sparse MatrixTable containing variant and reference data. n_samples()[source]; The number of samples present. ref_block_max_length_field = 'ref_block_max_length'; Name of global field that indicates max reference block length. property reference_genome; Dataset reference genome. Returns:; ReferenceGenome. union_rows()[source]; Combine many VDSes with the same samples but disjoint variants.; Examples; If a dataset is imported as VDS in chromosome-chunks, the following will combine them into; one VDS:; >>> vds_paths = ['chr1.vds', 'chr2.vds'] ; ... vds_per_chrom = [hl.vds.read_vds(path) for path in vds_paths) ; ... hl.vds.VariantDataset.union_rows(*vds_per_chrom) . validate(*, check_data=True)[source]; Eagerly checks necessary representational properties of the VDS. write(path, **kwargs)[source]; Write to path. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html:2785,update,updated,2785,docs/0.2/vds/hail.vds.VariantDataset.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.VariantDataset.html,1,['update'],['updated']
Deployability,"ssing_occupations = hl.set(['other', 'none']). t = users.select(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idcleaned_occupationint32str; 1""technician""; 2NA; 3""writer""; 4""technician""; 5NA; 6""executive""; 7""administrator""; 8""administrator""; 9""student""; 10""lawyer""; showing top 10 rows. [11]:. missing_occupations = hl.set(['other', 'none']). t = users.transmute(; cleaned_occupation = hl.if_else(missing_occupations.contains(users.occupation),; hl.missing('str'),; users.occupation)); t.show(). idagesexzipcodecleaned_occupationint32int32strstrstr; 124""M""""85711""""technician""; 253""F""""94043""NA; 323""M""""32067""""writer""; 424""M""""43537""""technician""; 533""F""""15213""NA; 642""M""""98101""""executive""; 757""M""""91344""""administrator""; 836""M""""05201""""administrator""; 929""M""""01002""""student""; 1053""M""""90703""""lawyer""; showing top 10 rows. Global Fields; Finally, you can add global fields with annotate_globals. Globals are useful for storing metadata about a dataset or storing small data structures like sets and maps. [12]:. t = users.annotate_globals(cohort = 5, cloudable = hl.set(['sample1', 'sample10', 'sample15'])); t.describe(). ----------------------------------------; Global fields:; 'cohort': int32; 'cloudable': set<str>; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. [13]:. t.cloudable. [13]:. <SetExpression of type set<str>>. [14]:. hl.eval(t.cloudable). [14]:. {'sample1', 'sample10', 'sample15'}. Exercises. Z-score normalize the age field of users.; Convert zip to an integer. Hint: Not all zipcodes are US zipcodes! Use hl.int32 to convert a string to an integer. Use StringExpression.matches to see if a string matches a regular expression. [ ]:. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:7289,update,updated,7289,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['update'],['updated']
Deployability,"ssions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024-10-02; 0.2.129 contained test configuration artifacts that prevented users from; starting dataproc clusters with hailctl. Please upgrade to 0.2.130; if you use dataproc. New Features. (hail##14447) Added copy_spark_log_on_error initialization flag; that when set, copies the hail driver log to the remote tmpdir if; query execution raises an exception. Bug Fixes. (#14452) Fixes a bug; that prevents users from starting dataproc clusters with hailctl. Version 0.2.129; Released 2024-04-02. Documentation. (#14321) Removed; GOOGLE_APPLICATION_CREDENTIALS from batch docs. Metadata server; introduction means users no longer need to explicitly activate; service accounts with the gcloud command line tool.; (#14339) Added; citations since 2021. New Features. (#14406) Performance; improvements for reading structured data from (Matrix)Tables; (#14255) Added; Cochran-Hantel-Haenszel test for association; (cochran_mantel_haenszel_test",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:12260,update,update,12260,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['install', 'update']","['installing', 'update']"
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_afr. View page source. gnomad_ld_scores_afr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_afr.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_amr. View page source. gnomad_ld_scores_amr. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_amr.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_asj. View page source. gnomad_ld_scores_asj. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_asj.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_ld_scores_eas. View page source. gnomad_ld_scores_eas. Versions: 2.1.1; Reference genome builds: GRCh37; Type: hail.Table. Schema (2.1.1, GRCh37); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'pop_freq': struct {; AC: int32,; AF: float64,; AN: int32,; homozygote_count: int32; }; 'idx': int64; 'new_idx': int64; 'ld_score': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html:9475,update,updated,9475,docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_ld_scores_eas.html,1,['update'],['updated']
Deployability,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Pancreas_all_snp_gene_associations. View page source. GTEx_sQTL_Pancreas_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Pancreas_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"st; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Prostate_all_snp_gene_associations. View page source. GTEx_sQTL_Prostate_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html:9739,update,updated,9739,docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Prostate_all_snp_gene_associations.html,2,['update'],['updated']
Deployability,"stically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Releas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78542,configurat,configuration,78542,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"store obs and pred values for this window; obs = y_test[""oe""].to_list()[0]; pred = y_pred[0]. return (window_name, obs, pred). Format Result Function; The function below takes the expected output of the function random_forest; and returns a tab-delimited string that will be used later on when concatenating results.; def as_tsv(input: Tuple[str, float, float]) -> str:; return '\t'.join(str(i) for i in input). Build Python Image; In order to run a PythonJob, Batch needs an image that has the; same version of Python as the version of Python running on your computer; and the Python package dill installed. Batch will automatically; choose a suitable image for you if your Python version is 3.9 or newer.; You can supply your own image that meets the requirements listed above to the; method PythonJob.image() or as the argument default_python_image when; constructing a Batch . We also provide a convenience function docker.build_python_image(); for building an image that has the correct version of Python and dill installed; along with any desired Python packages.; For running the random forest, we need both the sklearn and pandas Python; packages installed in the image. We use docker.build_python_image() to build; an image and push it automatically to the location specified (ex: us-docker.pkg.dev/hail-vdc/random-forest).; image = hb.build_python_image('us-docker.pkg.dev/hail-vdc/random-forest',; requirements=['sklearn', 'pandas']). Control Code; We start by defining a backend.; backend = hb.ServiceBackend(). Second, we create a Batch and specify the default Python image to; use for Python jobs with default_python_image. image is the return value; from building the Python image above and is the full name of where the newly; built image was pushed to.; b = hb.Batch(name='rf',; default_python_image=image). Next, we read the y dataframe locally in order to get the list of windows; to run. The file path containing the dataframe could be stored on the cloud.; Therefore, we use t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:4454,install,installed,4454,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['install'],['installed']
Deployability,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11299,configurat,configuration,11299,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"t = hl.import_vcf(; gvcf_paths[0],; header_file=gvcf_external_header,; force_bgz=True,; array_elements_required=False,; reference_genome=reference_genome,; contig_recoding=contig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30807,update,update,30807,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:113156,update,updates,113156,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['update'],['updates']
Deployability,"t())))""; ). duplicates = [k for k, count in counts.items() if count > 1]; if duplicates:; raise ValueError(f""column keys must be unique, found duplicates: {', '.join(duplicates)}""). entries_uid = Env.get_uid(); cols_uid = Env.get_uid(). t = self; t = t._localize_entries(entries_uid, cols_uid). def fmt(f, col_key):; if f:; return col_key + separator + f; else:; return col_key. t = t.annotate(**{; fmt(f, col_keys[i]): t[entries_uid][i][j] for i in range(len(col_keys)) for j, f in enumerate(self.entry); }); t = t.drop(cols_uid, entries_uid). return t. [docs] @typecheck_method(rows=bool, cols=bool, entries=bool, handler=nullable(anyfunc)); def summarize(self, *, rows=True, cols=True, entries=True, handler=None):; """"""Compute and print summary information about the fields in the matrix table. .. include:: _templates/experimental.rst. Parameters; ----------; rows : :obj:`bool`; Compute summary for the row fields.; cols : :obj:`bool`; Compute summary for the column fields.; entries : :obj:`bool`; Compute summary for the entry fields.; """""". if handler is None:; handler = default_handler(); if cols:; handler(self.col._summarize(header='Columns', top=True)); if rows:; handler(self.row._summarize(header='Rows', top=True)); if entries:; handler(self.entry._summarize(header='Entries', top=True)). def _write_block_matrix(self, path, overwrite, entry_field, block_size):; mt = self; mt = mt._select_all(entry_exprs={entry_field: mt[entry_field]}). writer = ir.MatrixBlockMatrixWriter(path, overwrite, entry_field, block_size); Env.backend().execute(ir.MatrixWrite(self._mir, writer)). def _calculate_new_partitions(self, n_partitions):; """"""returns a set of range bounds that can be passed to write""""""; ht = self.rows(); ht = ht.select().select_globals(); return Env.backend().execute(; ir.TableToValueApply(ht._tir, {'name': 'TableCalculateNewPartitions', 'nPartitions': n_partitions}); ). matrix_table_type.set(MatrixTable). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:135577,update,updated,135577,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['update'],['updated']
Deployability,"t(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2218,integrat,integrate,2218,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['integrat'],['integrate']
Deployability,"t. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a contribution.; We can help you determine if your project is a good candidate for merging.; Ke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1569,install,install,1569,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,2,['install'],['install']
Deployability,"t32,; biotype: str,; consequence_terms: array<str>,; impact: str,; minimised: int32,; regulatory_feature_id: str,; variant_allele: str; }>,; seq_region_name: str,; start: int32,; strand: int32,; transcript_consequences: array<struct {; allele_num: int32,; amino_acids: str,; biotype: str,; canonical: int32,; ccds: str,; cdna_start: int32,; cdna_end: int32,; cds_end: int32,; cds_start: int32,; codons: str,; consequence_terms: array<str>,; distance: int32,; domains: array<struct {; db: str,; name: str; }>,; exon: str,; gene_id: str,; gene_pheno: int32,; gene_symbol: str,; gene_symbol_source: str,; hgnc_id: str,; hgvsc: str,; hgvsp: str,; hgvs_offset: int32,; impact: str,; intron: str,; lof: str,; lof_flags: str,; lof_filter: str,; lof_info: str,; minimised: int32,; polyphen_prediction: str,; polyphen_score: float64,; protein_end: int32,; protein_start: int32,; protein_id: str,; sift_prediction: str,; sift_score: float64,; strand: int32,; swissprot: str,; transcript_id: str,; trembl: str,; uniparc: str,; variant_allele: str; }>,; variant_class: str; }; 'freq': array<struct {; pop: str,; ac: float64,; af: float64,; an: int64,; gnomad_exomes_ac: int32,; gnomad_exomes_af: float64,; gnomad_exomes_an: int32,; gnomad_genomes_ac: int32,; gnomad_genomes_af: float64,; gnomad_genomes_an: int32; }>; 'pass_gnomad_exomes': bool; 'pass_gnomad_genomes': bool; 'n_passing_populations': int32; 'high_quality': bool; 'nearest_genes': str; 'info': float64; ----------------------------------------; Entry fields:; 'meta_analysis': array<struct {; BETA: float64,; SE: float64,; Pvalue: float64,; Q: float64,; Pvalue_het: float64,; N: int32,; N_pops: int32,; AF_Allele2: float64,; AF_Cases: float64,; AF_Controls: float64; }>; ----------------------------------------; Column key: ['trait_type', 'phenocode', 'pheno_sex', 'coding', 'modifier']; Row key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html:14753,update,updated,14753,docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/panukb_meta_analysis_all_ancestries.html,4,['update'],['updated']
Deployability,"t:; """"""Create an interactive scatter plot with marginal densities on the side. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:37647,continuous,continuous,37647,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"t; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Variant Dataset; hail.vds.local_to_global. View page source. hail.vds.local_to_global. hail.vds.local_to_global(array, local_alleles, n_alleles, fill_value, number)[source]; Reindex a locally-indexed array to globally-indexed.; Examples; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; The number parameter matches the VCF specification; number definitions:. A indicates one value per allele, excluding the reference.; R indicates one value per allele, including the reference.; G indicates one value per unique diploid genotype. Warning; Using this function can lead to an enormous explosion in data size, without increasing; information capacity. Its appropriate use is to conform to antiquated and badly-scaling; representations (e.g. pVCF), but even so, caution should be exercised. Reindexing local; PLs (or any G-numbered field) at a site with 1000 alleles will produce an array with; more than 5,000 values per sample – with 100,000 samples, nearly 50GB per variant!. See also; lgt_to_gt(). Parameters:. array (ArrayExpression) – Array to reindex.; local_alleles (ArrayExpression) – Local alleles array.; n_alleles (Int32Expression) – Total number of alleles to reindex to.; fill_value – Value to fill in at global indices with no data.; number (str) – One of ‘A’, ‘R’, ‘G’. Returns:; ArrayExpression. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html:2325,update,updated,2325,docs/0.2/vds/hail.vds.local_to_global.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.local_to_global.html,1,['update'],['updated']
Deployability,"t; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. View page source. GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html:9769,update,updated,9769,docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Brain_Nucleus_accumbens_basal_ganglia_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"t_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = []; for path in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_geno",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:31445,update,update,31445,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"t_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. View page source. GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'gene_id': str; 'variant_id': str; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html:9751,update,updated,9751,docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_eQTL_Skin_Not_Sun_Exposed_Suprapubic_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"t_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. View page source. GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations. Versions: v8; Reference genome builds: GRCh38; Type: hail.Table. Schema (v8, GRCh38); ----------------------------------------; Global fields:; 'metadata': struct {; name: str,; version: str,; reference_genome: str,; n_rows: int32,; n_partitions: int32; }; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'phenotype_id': struct {; intron: interval<locus<GRCh38>>,; cluster: str,; gene_id: str; }; 'tss_distance': int32; 'ma_samples': int32; 'ma_count': int32; 'maf': float64; 'pval_nominal': float64; 'slope': float64; 'slope_se': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html:9769,update,updated,9769,docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/GTEx_sQTL_Brain_Hypothalamus_all_snp_gene_associations.html,1,['update'],['updated']
Deployability,"tb, coef_dict=None, str_expr=None, axis='rows')[source]; Aggregates by linear combination fields matching either keys in coef_dict; or str_expr. Outputs the aggregation in a MatrixTable or Table; as a new row field “agg_annot” or a new column field “agg_cov”. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing fields to be aggregated.; coef_dict (dict, optional) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key.; If not included, coefficients are assumed to be 1.; str_expr (str, optional) – String expression to match against row (or col) field names.; axis (str) – Either ‘rows’ or ‘cols’. If ‘rows’, this aggregates across row fields.; If ‘cols’, this aggregates across col fields. If tb is a Table, axis = ‘rows’. Returns:; MatrixTable or Table – MatrixTable or Table containing aggregation field. hail.experimental.ldscsim.get_coef_dict(tb, str_expr=None, ref_coef_dict=None, axis='rows')[source]; Gets either col or row fields matching str_expr and take intersection; with keys in coefficient reference dict. Parameters:. tb (MatrixTable or Table) – MatrixTable or Table containing row (or col) for coef_dict.; str_expr (str, optional) – String expression pattern to match against row (or col) fields. If left; unspecified, the intersection of field names is only between existing; row (or col) fields in mt and keys of ref_coef_dict.; ref_coef_dict (dict, optional) – Reference coefficient dictionary with keys that are row (or col) field; names from which to subset. If not included, coefficients are assumed to be 1.; axis (str) – Field type in which to search for field names. Options: ‘rows’, ‘cols’. Returns:; coef_dict (dict) – Coefficients to multiply each field. The coefficients are specified by; coef_dict value, the row (or col) field name is specified by coef_dict key. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/ldscsim.html:17602,update,updated,17602,docs/0.2/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/experimental/ldscsim.html,1,['update'],['updated']
Deployability,"tch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25631,configurat,configuration,25631,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['configurat'],['configuration']
Deployability,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14866,configurat,configurations,14866,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,4,['configurat'],['configurations']
Deployability,"te |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}). def clear(self):; for f, t in self.items():; t.clear(). def _ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:35255,update,update,35255,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['update'],['update']
Deployability,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2110,install,installed,2110,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"tems in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:38721,continuous,continuous,38721,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['continuous'],['continuous']
Deployability,"ters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:5511,install,installed,5511,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['install'],['installed']
Deployability,"th - i)):; if HailType._missing(value[keys[i + j]]):; missing_byte |= 1 << j; byte_writer.write_byte(missing_byte); i += 8. for f, t in self.items():; if not HailType._missing(value[f]):; t._convert_to_encoding(byte_writer, value[f]). def _is_prefix_of(self, other):; return (; isinstance(other, tstruct); and len(self._fields) <= len(other._fields); and all(x == y for x, y in zip(self._field_types.values(), other._field_types.values())); ). def _concat(self, other):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(other._field_types); return tstruct(**new_field_types). def _insert(self, path, t):; if not path:; return t. key = path[0]; keyt = self.get(key); if not (keyt and isinstance(keyt, tstruct)):; keyt = tstruct(); return self._insert_fields(**{key: keyt._insert(path[1:], t)}). def _insert_field(self, field, typ):; return self._insert_fields(**{field: typ}). def _insert_fields(self, **new_fields):; new_field_types = {}; new_field_types.update(self._field_types); new_field_types.update(new_fields); return tstruct(**new_field_types). def _drop_fields(self, fields):; return tstruct(**{f: t for f, t in self.items() if f not in fields}). def _select_fields(self, fields):; return tstruct(**{f: self[f] for f in fields}). def _index_path(self, path):; t = self; for p in path:; t = t[p]; return t. def _rename(self, map):; seen = {}; new_field_types = {}. for f0, t in self.items():; f = map.get(f0, f0); if f in seen:; raise ValueError(; ""Cannot rename two fields to the same name: attempted to rename {} and {} both to {}"".format(; repr(seen[f]), repr(f0), repr(f); ); ); else:; seen[f] = f0; new_field_types[f] = t. return tstruct(**new_field_types). def unify(self, t):; if not (isinstance(t, tstruct) and len(self) == len(t)):; return False; for (f1, t1), (f2, t2) in zip(self.items(), t.items()):; if not (f1 == f2 and t1.unify(t2)):; return False; return True. def subst(self):; return tstruct(**{f: t.subst() for f, t in self.items()}).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:35212,update,update,35212,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['update'],['update']
Deployability,"th one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:2225,configurat,configuration,2225,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['configurat'],['configuration']
Deployability,"the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them.; The subset algorithm would produce the following:; GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; AD: The filtered alleles’ columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms 25,5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homoz",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28003,update,update,28003,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['update'],['update']
Deployability,"ticks=""outside""); fig = fig.update_yaxes(title_font_size=18, ticks=""outside""); fig.update_layout(; plot_bgcolor=""white"",; font_family='Arial, ""Open Sans"", verdana, sans-serif',; title_font_size=26,; xaxis=dict(linecolor=""black"", showticklabels=True),; yaxis=dict(linecolor=""black"", showticklabels=True),; # axes for plotly subplots are numbered following the pattern [xaxis, xaxis2, xaxis3, ...]; **{; f""{var}axis{idx}"": {""linecolor"": ""black"", ""showticklabels"": True}; for idx in range(2, n_facet_rows + n_facet_cols + 1); for var in [""x"", ""y""]; },; ). return fig. [docs] def show(self):; """"""Render and show the plot, either in a browser or notebook.""""""; self.to_plotly().show(). [docs] def write_image(self, path):; """"""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""; self.to_plotly().write_image(path). def _repr_html_(self):; return self.to_plotly()._repr_html_(). def _debug_print(self):; print(""Ggplot Object:""); print(""Aesthetics""); pprint(self.aes); pprint(""Scales:""); pprint(self.scales); print(""Geoms:""); pprint(self.geoms). [docs]def ggplot(table, mapping=aes()):; """"""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""; assert isinstance(mapping, Aesthetic); return GGPlot(table, mapping). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:11676,update,updated,11676,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,2,['update'],['updated']
Deployability,"tig_recoding,; ); gvcf_type = mt._type; if gvcf_reference_entry_fields_to_keep is None:; rmt = mt.filter_rows(hl.is_defined(mt.info.END)); gvcf_reference_entry_fields_to_keep = defined_entry_fields(rmt, 100_000) - {'PGT', 'PL'}; if vds is None:; vds = transform_gvcf(; mt._key_rows_by_assert_sorted('locus'), gvcf_reference_entry_fields_to_keep, gvcf_info_to_keep; ); dataset_type = CombinerOutType(reference_type=vds.reference_data._type, variant_type=vds.variant_data._type). if save_path is None:; sha = hashlib.sha256(); sha.update(output_path.encode()); sha.update(temp_path.encode()); sha.update(str(reference_genome).encode()); sha.update(str(dataset_type).encode()); if gvcf_type is not None:; sha.update(str(gvcf_type).encode()); for path in vds_paths:; sha.update(path.encode()); for path in gvcf_paths:; sha.update(path.encode()); if gvcf_external_header is not None:; sha.update(gvcf_external_header.encode()); if gvcf_sample_names is not None:; for name in gvcf_sample_names:; sha.update(name.encode()); if gvcf_info_to_keep is not None:; for kept_info in sorted(gvcf_info_to_keep):; sha.update(kept_info.encode()); if gvcf_reference_entry_fields_to_keep is not None:; for field in sorted(gvcf_reference_entry_fields_to_keep):; sha.update(field.encode()); for call_field in sorted(call_fields):; sha.update(call_field.encode()); if contig_recoding is not None:; for key, value in sorted(contig_recoding.items()):; sha.update(key.encode()); sha.update(value.encode()); for interval in intervals:; sha.update(str(interval).encode()); digest = sha.hexdigest(); name = f'vds-combiner-plan_{digest}_{hl.__pip_version__}.json'; save_path = os.path.join(temp_path, 'combiner-plans', name); saved_combiner = maybe_load_from_saved_path(save_path); if saved_combiner is not None:; return saved_combiner; warning(f'generated combiner save path of {save_path}'). if vds_sample_counts:; vdses = [VDSMetadata(path, n_samples) for path, n_samples in zip(vds_paths, vds_sample_counts)]; else:; vdses = [",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:30982,update,update,30982,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['update'],['update']
Deployability,"ting by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for; badly-formatted calls. New features. (#8327) Attempting to; write to the same file being read fro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72697,install,installation,72697,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['install'],['installation']
Deployability,"tions.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37673,install,installing,37673,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['install'],['installing']
Deployability,"tions.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37453,configurat,configuration,37453,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['configurat'],['configuration']
Deployability,"titions defaults to one partition per million genotypes (i.e., samples * variants / 10^6) or 8, whichever is larger. The Balding-Nichols model models genotypes of individuals from a structured population comprising :math:`K` homogeneous subpopulations; that have each diverged from a single ancestral population (a `star phylogeny`). We take :math:`N` samples and :math:`M` bi-allelic variants in perfect; linkage equilibrium. The relative sizes of the subpopulations are given by a probability vector :math:`\pi`; the ancestral allele frequencies are; drawn independently from a frequency spectrum :math:`P_0`; the subpopulations have diverged with possibly different :math:`F_{ST}` parameters :math:`F_k`; (here and below, lowercase indices run over a range bounded by the corresponding uppercase parameter, e.g. :math:`k = 1, \ldots, K`).; For each variant, the subpopulation allele frequencies are drawn a `beta distribution <https://en.wikipedia.org/wiki/Beta_distribution>`__, a useful continuous approximation of; the effect of genetic drift. We denote the individual subpopulation memberships by :math:`k_n`, the ancestral allele frequences by :math:`p_{0, m}`,; the subpopulation allele frequencies by :math:`p_{k, m}`, and the genotypes by :math:`g_{n, m}`. The generative model in then given by:. .. math::; k_n \,&\sim\, \pi. p_{0,m}\,&\sim\, P_0. p_{k,m}\mid p_{0,m}\,&\sim\, \mathrm{Beta}(\mu = p_{0,m},\, \sigma^2 = F_k p_{0,m}(1 - p_{0,m})). g_{n,m}\mid k_n, p_{k, m} \,&\sim\, \mathrm{Binomial}(2, p_{k_n, m}). We have parametrized the beta distribution by its mean and variance; the usual parameters are :math:`a = (1 - p)(1 - F)/F,\; b = p(1-F)/F` with :math:`F = F_k,\; p = p_{0,m}`. **Annotations**. :py:meth:`~hail.HailContext.balding_nichols_model` adds the following global, sample, and variant annotations:. - **global.nPops** (*Int*) -- Number of populations; - **global.nSamples** (*Int*) -- Number of samples; - **global.nVariants** (*Int*) -- Number of variants; - **glob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:29951,continuous,continuous,29951,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['continuous'],['continuous']
Deployability,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112139,configurat,configuration,112139,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['configurat'],['configuration']
Deployability,"to as_tsv is result and we assign the output to tsv_result.; Lastly in the for loop for each window, we append the tsv_result to the results list. However,; tsv_result is a Python object. We use the PythonResult.as_str() method to convert the; Python object to a text file containing the str() output of the Python object.; for window in local_df_y.index.to_list():; j = b.new_python_job(). result = j.call(random_forest, df_x_input, df_y_input, window); tsv_result = j.call(as_tsv, result); results.append(tsv_result.as_str()). Now that we have computed the random forest results for each window, we can concatenate; the outputs together into a single file using the concatenate() function and then; write the concatenated results file to a permanent output location.; output = hb.concatenate(b, results); b.write_output(output, results_path). Finally, we call Batch.run() to execute the batch and then close the backend.; b.run(wait=False); backend.close(). Add Checkpointing; The pipeline we wrote above is not resilient to failing jobs. Therefore, we can add; a way to checkpoint the results so we only run jobs that haven’t already succeeded; in future runs of the pipeline. The way we do this is by having Batch write the computed; result to a file and using the function hfs.exists to check whether the file already; exists before adding that job to the DAG.; First, we define the checkpoint path for each window.; def checkpoint_path(window):; return f'gs://my_bucket/checkpoints/random-forest/{window}'. Next, we define the list of results we’ll append to:; results = []. Now, we take our for loop over the windows from before, but now we check whether; the checkpointed already exists. If it does exist, then we read the checkpointed; file as a InputResourceFile using Batch.read_input() and append; the input to the results list. If the checkpoint doesn’t exist and we add the job; to the batch, then we need to write the results file to the checkpoint location; using Batch.write_output(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:8030,pipeline,pipeline,8030,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['pipeline'],['pipeline']
Deployability,"to use https instead of; http. Movie Lens has stopped serving data over insecure HTTP.; (#11563) Fix issue; hail-is/hail#11562.; (#11611) Fix a bug; that prevents the display of hl.ggplot.geom_hline and; hl.ggplot.geom_vline. Version 0.2.90; Release 2022-03-11. Critical BlockMatrix from_numpy correctness bug. (#11555); BlockMatrix.from_numpy did not work correctly. Version 1.0 of; org.scalanlp.breeze, a dependency of Apache Spark that hail also; depends on, has a correctness bug that results in BlockMatrices that; repeat the top left block of the block matrix for every block. This; affected anyone running Spark 3.0.x or 3.1.x. Bug fixes. (#11556) Fixed; assertion error ocassionally being thrown by valid joins where the; join key was a prefix of the left key. Versioning. (#11551) Support; Python 3.10. Version 0.2.89; Release 2022-03-04. (#11452) Fix; impute_sex_chromosome_ploidy docs. Version 0.2.88; Release 2022-03-01; This release addresses the deploy issues in the 0.2.87 release of Hail. Version 0.2.87; Release 2022-02-28; An error in the deploy process required us to yank this release from; PyPI. Please do not use this release. Bug fixes. (#11401) Fixed bug; where from_pandas didn’t support missing strings. Version 0.2.86; Release 2022-02-25. Bug fixes. (#11374) Fixed bug; where certain pipelines that read in PLINK files would give assertion; error.; (#11401) Fixed bug; where from_pandas didn’t support missing ints. Performance improvements. (#11306) Newly; written tables that have no duplicate keys will be faster to join; against. Version 0.2.85; Release 2022-02-14. Bug fixes. (#11355) Fixed; assertion errors being hit relating to RVDPartitioner.; (#11344) Fix error; where hail ggplot would mislabel points after more than 10 distinct; colors were used. New features. (#11332) Added; geom_ribbon and geom_area to hail ggplot. Version 0.2.84; Release 2022-02-10. Bug fixes. (#11328) Fix bug; where occasionally files written to disk would be unreadable.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:52549,release,release,52549,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['release'],['release']
Deployability,"to; be able to run the same code regardless of what machine the code is executing on. This packaged; code is called an image. There are three parts to Docker: a mechanism for building images,; an image repository called Docker Hub, and a way to execute code in an image; called a container. For using Batch effectively, we’re only going to focus on building images. Installation; You can install Docker by following the instructions for either Macs; or for Linux. Creating a Dockerfile; A Dockerfile contains the instructions for creating an image and is typically called Dockerfile.; The first directive at the top of each Dockerfile is FROM which states what image to create this; image on top of. For example, we can build off of ubuntu:22.04 which contains a complete Ubuntu; operating system, but does not have Python installed by default. You can use any image that already; exists to base your image on. An image that has Python preinstalled is python:3.6-slim-stretch and; one that has gcloud installed is google/cloud-sdk:slim. Be careful when choosing images from; unknown sources!; In the example below, we create a Dockerfile that is based on ubuntu:22.04. In this file, we show an; example of installing PLINK in the image with the RUN directive, which is an arbitrary bash command.; First, we download a bunch of utilities that do not come with Ubuntu using apt-get. Next, we; download and install PLINK from source. Finally, we can copy files from your local computer to the; docker image using the COPY directive.; FROM 'ubuntu:22.04'. RUN apt-get update && apt-get install -y \; python3 \; python3-pip \; tar \; wget \; unzip \; && \; rm -rf /var/lib/apt/lists/*. RUN mkdir plink && \; (cd plink && \; wget https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200217.zip && \; unzip plink_linux_x86_64_20200217.zip && \; rm -rf plink_linux_x86_64_20200217.zip). # copy single script; COPY my_script.py /scripts/. # copy entire directory recursively; COPY . /scripts/. For mor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/docker_resources.html:1503,install,installed,1503,docs/batch/docker_resources.html,https://hail.is,https://hail.is/docs/batch/docker_resources.html,1,['install'],['installed']
Deployability,"top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ). rows_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). if 'entry_file_sizes' in all_data:; title = f'Statistics for {data_type}: {t_path}'. msg = f""Rows: {sum(all_data['rows_per_partition']):,}<br/>Partitions: {len(all_data['rows_per_partition']):,}<br/>Size: {total_entry_file_size}<br/>""; if success_file[0]:; msg += success_file[0]. source = ColumnDataSource(pd.DataFrame(all_data)); p = figure(tools=tools, width=panel_size, height=panel_size); p.title.text = title; p.xaxis.axis_label = 'Number of rows'; p.yaxis.axis_label = f'File size ({entry_scale}B)'; color_map = factor_cmap('spans_chromosome', palette=Spectral8, factors=list(set(all_data['spans_chromosome']))); p.scatter('rows_per_partition', 'entry_file_sizes', color=color_map, legend='spans_chromosome', source=source); p.legend.location = 'bottom_right'; p.select_one(HoverTool).tooltips = [; (x, f'@{x}') for x in ('rows_per_partition', 'entry_file_sizes_human', 'partition_bounds', 'index'); ]. p_stats = Div(text=msg); p_rows_per_partition = figure(x_range=p.x_range, width=panel_size, height=subpanel_size); p_rows_per_partition.quad(; top=rows_per_partition_hist,; bottom=0,; left=rows_per_partition_edges[:-1],; right=rows_per_partition_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); p_file_size = figure(y_range=p.y_range, width=subpanel_size, height=panel_size). row_file_sizes_hist, row_file_sizes_edges = np.histogram(all_data['entry_file_sizes'], bins=50); p_file_size.quad(; right=row_file_sizes_hist,; left=0,; bottom=row_file_sizes_edges[:-1],; top=row_file_sizes_edges[1:],; fill_color=""#036564"",; line_color=""#033649"",; ); entries_grid = gridplot([[p_rows_per_partition, p_stats], [p, p_file_size]]). return Tabs(tabs=[TabPanel(child=entries_grid, title='Entries'), TabPanel(child=rows_grid, title='Rows')]); else:; return rows_grid. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:11564,update,updated,11564,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,2,['update'],['updated']
