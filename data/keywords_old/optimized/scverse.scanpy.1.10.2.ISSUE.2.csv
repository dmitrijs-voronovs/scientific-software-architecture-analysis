quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Energy Efficiency,"wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:1423,adapt,adapting,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,2,"['adapt', 'power']","['adapting', 'power']"
Integrability," 'expression'); ```. ```pytb; [/Users/dan/opt/anaconda3/lib/python3.9/site-packages/openpyxl/reader/workbook.py:88: UserWarning: File contains an invalid specification for expression. This will be removed; warn(msg). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); /var/folders/f1/7pq3lkn50cb7mtfkxrk9fnfh0000gn/T/ipykernel_3372/1224090119.py in <module>; ----> 1 adata = sc.read_excel(filename = '/Users/dan/Documents/Graduate_School/Bioinformatics_MS/BF550/Project/muscle_expression_.xlsx', sheet = 'expression'). ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_io/read.py in read_excel(filename, sheet, dtype); 73 from pandas import read_excel; 74 ; ---> 75 df = read_excel(fspath(filename), sheet); 76 X = df.values[:, 1:]; 77 row = dict(row_names=df.iloc[:, 0].values.astype(str)). ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 309 stacklevel=stacklevel,; 310 ); --> 311 return func(*args, **kwargs); 312 ; 313 return wrapper. ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in read_excel(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options); 463 ; 464 try:; --> 465 data = io.parse(; 466 sheet_name=sheet_name,; 467 header=header,. ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 1456 DataFrame from the passed in Excel file.; 1457 """"""; -> 1458 return self._reader.parse(; 1459 sheet_name=sheet_name,; 1460 header=header,. ~/opt/anaconda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:1576,wrap,wrapper,1576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,2,['wrap'],['wrapper']
Integrability," (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2408,wrap,wrapper,2408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability," 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-vi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15132,wrap,wrapt,15132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability," 20.2.0; backcall 0.2.0; brotli NA; cellrank 1.0.0-rc.10; certifi 2020.06.20; cffi 1.14.3; chardet 3.0.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; datacache 1.1.5; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; gtfparse 1.2.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; importlib_metadata 0.23; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.14.0; jsonschema 3.2.0; kiwisolver 1.1.0; legacy_api_wrap 1.2; leidenalg 0.8.1; libpetsc4py NA; llvmlite 0.34.0; louvain 0.6.1+2.g8073db7; markupsafe 1.1.1; matplotlib 3.3.2; memoized_property NA; more_itertools NA; mpl_toolkits NA; natsort 6.0.0; nbformat 5.0.7; networkx 2.3; numba 0.51.2; numexpr 2.7.0; numpy 1.19.1; packaging 20.4; palantir 1.0.0; pandas 1.1.2; parso 0.7.1; petsc4py 3.13.0; pexpect 4.8.0; phenograph 1.5.6; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.0; progressbar 3.53.1; prometheus_client NA; prompt_toolkit 3.0.7; psutil 5.7.2; ptyprocess 0.6.0; pvectorc NA; py 1.8.0; pyensembl 1.8.7; pygam 0.8.0; pygments 2.7.1; pyparsing 2.4.2; pyrsistent NA; pytest 5.2.1; python_utils NA; pytz 2019.2; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; scvelo 0.2.2; seaborn 0.11.0; send2trash NA; serializable 0.2.1; setuptools_scm NA; simplejson 3.17.2; sinfo 0.3.1; six 1.15.0; sklearn 0.21.3; slepc4py 3.13.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.1; storemagic NA; tables 3.5.2; terminado 0.9.1; texttable 1.6.3; tornado 6.0.4; traitlets 4.3.3; typechecks NA; typing_extensions NA; tzlocal NA; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.1.2; zipp NA; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) [GCC 7.5.0]; Linux-4.4.0-189-generic-x86_64-with-debian-buster-sid; 24 logical CPU cores, x86_64; -----; Session information updated at 2020-09-30 12:20. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:3710,wrap,wrapt,3710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['wrap'],['wrapt']
Integrability," I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; We ran across a case where score genes does not work for one specific gene, but works for others; the gene is in anndata. The same code works on other datasets preprocessed in the same way. ```pytb; genes=['INS']; score_name='ins'; 'INS' in adata_rawnorm.var_names; True; IndexError Traceback (most recent call last); Input In [109], in <module>; ----> 1 sc.tl.score_genes(adata_rawnorm, gene_list=genes, score_name=score_name+'_score', use_raw=False). File ~/miniconda3/envs/block/lib/python3.8/site-packages/scanpy/tools/_score_genes.py:167, in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 164 else:; 165 X_list = np.nanmean(X_list, axis=1, dtype='float64'); --> 167 X_control = _adata[:, control_genes].X; 168 if issparse(X_control):; 169 X_control = np.array(_sparse_nanmean(X_control, axis=1)).flatten(). File ~/miniconda3/envs/block/lib/python3.8/site-packages/anndata/_core/anndata.py:625, in AnnData.X(self); 622 X = _subset(X, (self._oidx, self._vidx)); 623 elif self.is_view:; 624 X = as_view(; --> 625 _subset(self._adata_ref.X, (self._oidx, self._vidx)),; 626 ElementRef(self, ""X""),; 627 ); 628 else:; 629 X = self._X. File ~/miniconda3/envs/block/lib/python3.8/functools.py:875, in singledispatch.<locals>.wrapper(*args, **kw); 871 if not args:; 872 raise TypeError(f'{funcname} requires at least '; 873 '1 positional argument'); --> 875 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/block/lib/python3.8/site-packages/anndata/_core/index.py:127, in _subset(a, subset_idx); 125 if all(isinstance(x, cabc.Iterable) for x in subset_idx):; 126 subset_idx = np.ix_(*subset_idx); --> 127 return a[subset_idx]. IndexError: arrays used as indices must be of integer (or boolean) type; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2153:1505,wrap,wrapper,1505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2153,1,['wrap'],['wrapper']
Integrability," brotli NA; celltypist 1.6.2; certifi 2023.11.17; cffi 1.16.0; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2022.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; dill 0.3.7; docrep 0.3.2; entrypoints 0.4; exceptiongroup 1.2.0; executing 0.8.3; fsspec 2023.10.0; h5py 3.7.0; idna 3.4; igraph 0.10.8; inflect NA; ipykernel 6.28.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.3; joblib 1.3.2; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.7; numpy 1.26.3; omnipath 1.0.8; packaging 23.1; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prompt_toolkit 3.0.43; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydantic 1.10.12; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pytz 2023.3.post1; requests 2.31.0; ruamel NA; scipy 1.11.4; seaborn 0.12.2; session_info 1.0.0; setuptools 65.6.3; six 1.16.0; sklearn 1.4.1.post1; snappy NA; socks 1.7.1; sparse 0.14.0; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.1; tblib 1.7.0; texttable 1.7.0; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.12.0; torch 1.12.1; tornado 6.1; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; urllib3 1.26.18; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0.1; zipp NA; zmq 25.1.2; zoneinfo NA; zope NA; zstandard 0.19.0; -----; IPython 8.20.0; jupyter_client 7.3.4; jupyter_core 5.5.0; jupyterlab 3.5.3; notebook 6.5.2; -----; Python 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]; macOS-14.1.2-arm64-arm-64bit; -----; Session information updated at 2024-03-12 14:52; ​. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906:2622,wrap,wrapt,2622,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906,1,['wrap'],['wrapt']
Integrability," conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Both https://github.com/scverse/scanpy/commit/23c20bc4dd3b1a1e5432ad4e23f56cf5efcf7ebe and https://dev.azure.com/scverse/scanpy/_build/results?buildId=6733&view=logs&j=cb4d9293-b492-5d67-02b0-e6a595893958&t=99aeec2e-a40e-57fc-1ab3-27c1a626c3e0&s=96ac2280-8cb4-5df5-99de-dd2da759617d show errors from scrublet which I cannot reproduce locally. ### Minimal code sample. ```python; See `test_scrublet_data` under `anndata_dev`; ```. ### Error output. ```pytb; E AssertionError: ; E Not equal to tolerance rtol=1e-15, atol=1e-15; E ; E Mismatched elements: 1 / 200 (0.5%); E Max absolute difference: 0.0126501664; E Max relative difference: 0.1823112224; E x: array([0.033079, 0.039326, 0.047022, 0.069388, 0.047022, 0.033079,; E 0.069388, 0.019841, 0.069388, 0.069388, 0.069388, 0.039326,; E 0.033079, 0.149254, 0.047022, 0.056738, 0.047022, 0.023555,...; E y: array([0.033079, 0.039326, 0.047022, 0.069388, 0.047022, 0.033079,; E 0.069388, 0.019841, 0.069388, 0.069388, 0.069388, 0.039326,; E 0.033079, 0.149254, 0.047022, 0.056738, 0.047022, 0.023555,...; ```. ### Versions. <details>. ```; Package Version; ----------------- -------------------------; anndata 0.11.0.dev114+g105f354; annoy 1.17.3; scipy 1.13.0; scprep 1.1.0; seaborn 0.13.2; session-info 1.0.0; setuptools 69.5.1; setuptools-scm 8.1.0; six 1.16.0; sniffio 1.3.1; sortedcontainers 2.4.0; sparse 0.16.0a6; statsmodels 0.14.2; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; textual 0.60.1; threadpoolctl 3.5.0; tifffile 2024.5.10; toolz 0.12.1; tornado 6.4; tqdm 4.66.4; typing-extensions 4.12.0rc1; tzdata 2024.1; uc-micro-py 1.0.3; umap-learn 0.5.6; urllib3 2.2.1; uv 0.1.44; virtualenv 20.26.2; wrapt 1.16.0; zarr 2.18.1; zict 3.0.0. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3068:1972,wrap,wrapt,1972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3068,1,['wrap'],['wrapt']
Integrability," dataset). At all times there seems to be enough free RAM / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1312,wrap,wrapper,1312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability," dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as ad; import dask.array as da; import scanpy as sc. # write data to zarr file; rel_zarr_path = 'data/pbmc3k_processed.zarr'; adata = sc.datasets.pbmc3k_processed(); adata.write_zarr(f'./{rel_zarr_path}', chunks=[adata.shape[0], 5]); zarr.consolidate_metadata(f'./{rel_zarr_path}'). # read data from zarr file with X as a dask array; def read_dask(store):; f = zarr.open(store, mode=""r""). def callback(func, elem_name: str, elem, iospec):; if iospec.encoding_type in (; ""dataframe"",; ""csr_matrix"",; ""csc_matrix"",; ""awkward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.fr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:1717,wrap,wrapped,1717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapped']
Integrability," filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20453,wrap,wraps,20453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wraps']
Integrability," igraph 0.10.4; importlib_resources NA; ipykernel 6.23.1; ipython_genutils 0.2.0; ipywidgets 8.0.6; jax 0.4.12; jaxlib 0.4.12; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; lightning 2.0.3; lightning_cloud NA; lightning_fabric 2.0.3; lightning_utilities 0.8.0; llvmlite 0.40.0; louvain 0.8.0; matplotlib 3.7.1; matplotlib_inline 0.1.6; ml_collections NA; ml_dtypes 0.2.0; mpl_toolkits NA; mpmath 1.3.0; msgpack 1.0.5; mudata 0.2.3; multidict 6.0.4; multipart 0.0.6; multipledispatch 0.6.0; natsort 8.3.1; numba 0.57.0; numpy 1.24.3; numpyro 0.12.1; nvfuser NA; opt_einsum v3.3.0; optax 0.1.5; ordered_set 4.1.0; packaging 23.1; pandas 2.0.2; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.3; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydantic 1.10.9; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.15.1; pyparsing 3.0.9; pyro 1.8.5; pytorch_lightning 2.0.3; pytz 2023.3; requests 2.31.0; rich NA; scipy 1.10.1; scvi 1.0.0; seaborn 0.12.2; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; sklearn 1.2.2; sniffio 1.3.0; socks 1.7.1; soupsieve 2.3.2.post1; sparse 0.14.0; sphinxcontrib NA; stack_data 0.6.2; starlette 0.22.0; statsmodels 0.14.0; sympy 1.12; texttable 1.6.7; threadpoolctl 3.1.0; toml 0.10.2; toolz 0.12.0; torch 2.0.1+cu117; torchmetrics 0.11.4; tornado 6.3.2; tqdm 4.65.0; traitlets 5.9.0; tree 0.1.8; typing_extensions NA; urllib3 2.0.3; uvicorn 0.22.0; wcwidth 0.2.6; websocket 1.5.3; websockets 11.0.3; wrapt 1.15.0; xarray 2023.5.0; yaml 6.0; yarl 1.9.2; zmq 25.1.0; zoneinfo NA; -----; IPython 8.14.0; jupyter_client 8.2.0; jupyter_core 5.3.1; notebook 6.5.4; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]; Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.27; -----; Session information updated at 2023-07-06 03:56; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547:7144,wrap,wrapt,7144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547,1,['wrap'],['wrapt']
Integrability," k in attributes if k in f}); File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pyd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:3503,wrap,wrapper,3503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['wrap'],['wrapper']
Integrability," more predictability at the expense of backwards compatibility*? Especially the “euclidean” condition makes not much sense IMHO; - Another piece of weird logic: *Where to allow`knn=False`*? It would make sense for e.g. the scipy knn transformer with `algorithm='kd_tree'` … ; https://github.com/scverse/scanpy/blob/73915b4bd0dc84108df08ad83ec28e627a9a0e0d/scanpy/neighbors/__init__.py#L614-L617; - I also changed `method` to only mean “connectivity method”. `transformer_cls` alone now determines how to calculate distances. ### TODO. - [x] split of sklearn-ann part into own issue/PR; - [x] figure out what the `_more_tags` methods are ; - [x] allow specifying algorithm and/or backend; - [x] revert 75c6670, move connectivities code out of backends; - [x] switch our stuff to KNeighborsTransformer; - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity); - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually tes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:2149,depend,depend,2149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['depend'],['depend']
Integrability," run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/cluster/software/lib/python3.6/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/cluster/software/lib/python3.6/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/cluster/software/lib/python3.6/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/scratch/tmp/pip-build-g14yf1xi/scanpy/versioneer.py"", line 1555, in run; _build_py.run(self); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/cluster/software/lib/python3.6/site-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: must be str, not list; ```. </details>. I have never had this error message before and Google can't find anything. . Debugging this a bit, it happens because in this line in build_py.py:. ```py; globs = (self.package_data.get('', []); + self.package_data.get(package, [])); ```. package is ""scanpy"" and the first part before the + is `""*.txt""` and the second part after the + is `[]`. This is Python 3.6.0a1 and pip 9.0.1 on Centos 6.8",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:8475,message,message,8475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['message'],['message']
Integrability," scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When I try to plot a scatter plot color coded by categorical data, it still output the image, but the legend does not include colors. Additionally, if i use continuous data as the key for plotting, the code executes as expected. Thanks!. ![image](https://github.com/scverse/scanpy/assets/43973217/8c1b0a03-3c0b-4452-b759-9cf588b45c53). ### Minimal code sample. ```python; sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1175,wrap,wrapper,1175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['wrap'],['wrapper']
Integrability," such a method in its. ValueError: provided out is the wrong size for the reduction; ```. #### Versions. <details>. ```; -----; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 7.2.0; anndata 0.7.4; appdirs 1.4.4; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; colorama 0.4.3; colorlog NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; fa2 NA; fcsparser 0.2.1; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; markupsafe 1.1.1; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; palantir 1.0.0; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.6; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; rpy2 3.4.2; sca NA; scanpy 1.7.0; scipy 1.4.1; scprep 1.0.5.post2; seaborn 0.10.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; texttable 1.6.2; threadpoolctl 2.1.0; tornado 6.0.4; tqdm 4.48.2; traitlets 4.3.3; tzlocal NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-19 11:23; ```; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1670:4337,wrap,wrapt,4337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670,1,['wrap'],['wrapt']
Integrability," the IPython notebooks), the scanpy ""set_figure_params(...)"" command just outputs an ""In :"", and I need to exit using [Ctrl][D]: . ### Minimal code sample (that we can copy&paste without having any data); ```; import scanpy as sc; sc.set_figure_params(figsize=(4, 4)); ```; The output in console is:; ```; In :. In :; ```; and need to press [Crtrl][D] to exit, as otherwise this just loops ""In :"" repeatedly. If I add ""ipython_format=None"":; ```; sc.set_figure_params(figsize=(4, 4), ipython_format=None); ```; then this continues without any ""In :"", and I can continue with script. . I think the reason for this is in the scanpy ""_settings.py"" file:; ```; def set_figure_params(; ......etc.....; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; from matplotlib import rcParams; .....etc......; ```; where the:; ```; IPython.display.set_matplotlib_formats(*ipython_format); ```; produces this ""In :"" message loop. I think this bug could be fixed by adding ""if self._is_run_from_ipython():"" before the ""try: import IPython"": ; ```; def set_figure_params(; ......etc.....; if self._is_run_from_ipython():; try:; import IPython; if isinstance(ipython_format, str):; ipython_format = [ipython_format]; IPython.display.set_matplotlib_formats(*ipython_format); except Exception:; pass; ```; The ""try:"" ; ```; >>> import IPython; ```; doesn't throw an exception, as IPython is installed:; ```; $ pip show IPython; Name: ipython; Version: 7.18.1; Summary: IPython: Productive Interactive Computing; Home-page: https://ipython.org; Author: The IPython Development Team; Author-email: ipython-dev@python.org; License: BSD; Location: /slipstream/home/sbridgett/miniconda3/lib/python3.8/site-packages; Requires: pygments, jedi, prompt-toolkit, backcall, pexpect, setuptools, decorator, traitlets, pickleshare; Required-by: ipywidgets, ipykernel; ```. whereas the _is_run_from_ipython()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477:1344,message,message,1344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477,1,['message'],['message']
Integrability," this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(; ncase,; n_top_genes=3000,; # subset=True, # to automatically subset to the 4000 genes; layer=""counts"",; flavor=""seurat""; ); ```. ```pytb; ValueError: cannot specify integer `bins` when input data contains infinity; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; marku",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:1115,message,message,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['message'],['message']
Integrability," with `sc.pp.pca` and default params.; computing PCA; with n_comps=50; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-65-c244be664e51> in <module>(); ----> 1 sc.tl.tsne(adata, n_pcs = 50); 2 # UMAP, first with neighbor calculation; 3 sc.pp.neighbors(adata, n_pcs = 50, n_neighbors = 20); 4 sc.tl.umap(adata). ~/.local/lib/python3.6/site-packages/scanpy/tools/_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 78 start = logg.info('computing tSNE'); 79 adata = adata.copy() if copy else adata; ---> 80 X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs); 81 # params for sklearn; 82 params_sklearn = dict(. ~/.local/lib/python3.6/site-packages/scanpy/tools/_utils.py in _choose_representation(adata, use_rep, n_pcs, silent); 41 'Falling back to preprocessing with `sc.pp.pca` and default params.'; 42 ); ---> 43 X = pca(adata.X); 44 adata.obsm['X_pca'] = X[:, :n_pcs]; 45 else:. ~/.local/lib/python3.6/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 199 ; 200 output = _pca_with_sparse(; --> 201 X, n_comps, solver=svd_solver, random_state=random_state; 202 ); 203 # this is just a wrapper for the results. ~/.local/lib/python3.6/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 298 shape=X.shape,; 299 rmatvec=rmatvec,; --> 300 rmatmat=rmatmat,; 301 ); 302 . /usr/local/anaconda/lib/python3.6/site-packages/scipy/sparse/linalg/interface.py in __new__(cls, *args, **kwargs); 131 if cls is LinearOperator:; 132 # Operate as _CustomLinearOperator factory.; --> 133 return _CustomLinearOperator(*args, **kwargs); 134 else:; 135 obj = super(LinearOperator, cls).__new__(cls). TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252:2336,wrap,wrapper,2336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252,2,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"""""""; 341 if isinstance(moduleOrReq, Requirement):; --> 342 return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]; 343 try:; 344 module = sys.modules[moduleOrReq]. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in require(self, *requirements); 884 included, even if they were already activated in this working set.; 885 """"""; --> 886 needed = self.resolve(parse_requirements(requirements)); 887 ; 888 for dist in needed:. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in resolve(self, requirements, env, installer, replace_conflicting, extras); 770 if dist is None:; 771 requirers = required_by.get(req, None); --> 772 raise DistributionNotFound(req, requirers); 773 to_activate.append(dist); 774 if dist not in req:. DistributionNotFound: The 'pynndescent' distribution was not found and is required by the application; ```. #### Versions; latest version of scipy as well as pynndescent. strangely enough, if I try to print the versions I get this error as well:; ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /tmp/955352.tmpdir/ipykernel_5496/3375110566.py in <module>; ----> 1 print(sc.logging.print_versions()). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/logging.py in print_versions(file); 167 try:; 168 buf = sys.stdout = io.StringIO(); --> 169 sinfo(; 170 dependencies=True,; 171 excludes=[. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/sinfo/main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 208 for mod_name in clean_modules:; 209 mod_names.append(mod_name); --> 210 mod = sys.modules[mod_name]; 211 # Since modules use different attribute names to store version info,; 212 # try the most common ones. KeyError: 'umap'; ```. I should note that I uninstalled and reinstalled the latest version of umap-learn.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:4822,depend,dependencies,4822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,2,['depend'],['dependencies']
Integrability,"## Description. Running the scanpy tests locally (see system information and package versions below), I noticed the test `scanpy/tests/test_plotting.py::test_paga` sometimes failing due to `AssertionError: Error: Image files did not match` but passing on a consecutive run. Did anyone encounter similar problems or does anyone know why this is happening?. ### Failure message. ```bash; >>> pytest scanpy/tests/test_plotting.py; scanpy/tests/test_plotting.py .............................................x............F.... [100%]. =========================================================================================================== FAILURES ============================================================================================================; ___________________________________________________________________________________________________________ test_paga ___________________________________________________________________________________________________________. image_comparer = <function make_comparer at 0x7fbce4ade8c0>. def test_paga(image_comparer):; # Sometimes things shift a pixel or so, resulting in diffs up to ~27; # The 1px-edges aren’t that good actually as they’re ignored at this tol …; save_and_compare_images = image_comparer(ROOT, FIGS, tol=30); ; pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.paga(pbmc, groups='bulk_labels'); ; common = dict(threshold=0.5, max_edge_width=1.0, random_state=0, show=False); ; # delete bulk_labels_colors to test the creation of color list by paga; del pbmc.uns['bulk_labels_colors']; sc.pl.paga(pbmc, **common); save_and_compare_images('master_paga'); ; sc.pl.paga(pbmc, color='CST3', **common); save_and_compare_images('master_paga_continuous'); ; pbmc.obs['cool_feature'] = pbmc[:, 'CST3'].X.squeeze(); sc.pl.paga(pbmc, color='cool_feature', **common); save_and_compare_images('master_paga_continuous_obs'); ; sc.pl.paga(pbmc, color=['CST3', 'GATA2'], **common); save_and_compare_images('master_paga_continuous_multiple'); ; sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1418:368,message,message,368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418,1,['message'],['message']
Integrability,"## Description. The documentation in `CONTRIBUTING.md` states that all dependencies required for testing can be installed via `pip install scanpy[test]`. It should, however, be `pip install ""scanpy[test]""`. ### Minimal code sample. ```zsh; pip install scanpy[test]; zsh: no matches found: scanpy[test]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441:71,depend,dependencies,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441,1,['depend'],['dependencies']
Integrability,"### Please describe your wishes and possible alternatives to achieve the desired result. No key inside 'use_rep' can mean 'X' or 'X_pca' was used for nearest neighbor calculation depending on the number of vars. It's storing things correctly if 'use_rep' is called explicitly. ```py; sc.pp.neighbors(combined_emb, n_neighbors=50); combined_emb.uns[""neighbors""][""params""][""use_rep""] -> KeyError. sc.pp.neighbors(combined_emb, n_neighbors=50, use_rep='X'); combined_emb.uns[""neighbors""][""params""][""use_rep""] -> 'X'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2736:179,depend,depending,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2736,1,['depend'],['depending']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Attempted label transfer using `ingest`, using the example code on the [docs page](https://scanpy.readthedocs.io/en/1.10.x/generated/scanpy.tl.ingest.html). The fix is trivial (you must run `sc.pp.pca(adata_ref)` as well), but the error message is cryptic so I got stuck for a bit! I'll create the PR too. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd. sc.settings.verbosity = 1 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:526,message,message,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I was running a basic workflow with 3k PBMC and at the step sc.pl.stacked_violin got Attribute error below. ### Minimal code sample. ```python; sc.pl.stacked_violin(adata, marker_genes, groupby='leiden', rotation=90); ```. ### Error output. ```pytb; --------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[51], line 1; ----> 1 sc.pl.stacked_violin(adata, marker_genes, groupby='leiden', rotation=90);. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/scanpy/plotting/_stacked_violin.py:825, in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, title, colorbar_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, standard_scale, var_group_rotation, layer, stripplot, jitter, size, scale, yticklabels, order, swap_axes, show, save, return_fig, row_palette, cmap, ax, vmin, vmax, vcenter, norm, **kwds); 823 if return_fig:; 824 return vp; --> 825 vp.make_figure(); 826 savefig_or_show(StackedViolin.DEFAULT_SAVE_PREFIX, show=show, save=save); 827 show = settings.autoshow if show is None else show. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/scanpy/plotting/_baseplot_class.py:792, in BasePlot.make_figure(self); 789 return_ax_dict[""gene_gr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:886,wrap,wrapper,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi all. I was having issues generating rank gene groups. The error is as below. When I used the ""Manuscript_Identity"" group, I got such error message, but when I used another group ""CellType_Category"", it worked. These two groups are in the same type. Could you tell me how to fix it?; Look forward to your response, thanks a lot!. ### Minimal code sample. ```python; sc.tl.rank_genes_groups(adata_sc, groupby=""Manuscript_Identity"", use_raw=False). adata_sc.obs['CellType_Category'].cat.categories; Index(['Endothelial', 'Epithelial', 'Lymphoid', 'Multiplet', 'Myeloid',; 'Stromal'],; dtype='object'); adata_sc.obs['Manuscript_Identity'].cat.categories; Index(['ATI', 'ATII', 'Aberrant_Basaloid', 'B', 'B_Plasma', 'Basal',; 'Ciliated', 'Club', 'DC_Langerhans', 'DC_Mature', 'Fibroblast',; 'Goblet', 'ILC_A', 'ILC_B', 'Ionocyte', 'Lymphatic', 'Macrophage',; 'Macrophage_Alveolar', 'Mast', 'Mesothelial', 'Multiplet',; 'Myofibroblast', 'NK', 'PNEC', 'Pericyte', 'SMC', 'T', 'T_Cytotoxic',; 'T_Regulatory', 'VE_Arterial', 'VE_Capillary_A', 'VE_Capillary_B',; 'VE_Peribronchial', 'VE_Venous', 'cDC1', 'cDC2', 'cMonocyte',; 'ncMonocyte', 'pDC'],; dtype='object'); ```. ### Error output. ```pytb; WARNING: Default of the method has been changed to 't-test' from 't-test_overestim_var'; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/udd/rekso/.conda/envs/rekso_tangram_env/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 592, in rank_genes_groups; test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, layer, pts); File ""/udd/rekso/.conda/envs/rekso_tangram_env/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 106, in __i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2821:433,message,message,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2821,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am trying to run ingest in order to transfer labels from a reference dataset to a query dataset. . I have subsetted both datasets to the same features and re-run basic processing on both datasets, including pca, umap, and generating neighbor graphs. . When I run ingest I receive the following error message:; `ValueError: all input arrays must have the same shape`. ### Minimal code sample. ```python; var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. sc.tl.pca(adata_ref, svd_solver='arpack'); sc.pp.neighbors(adata_ref, n_neighbors=10, n_pcs=40); sc.tl.paga(adata_ref, groups = 'cell_type'); sc.pl.paga(adata_ref, plot=False) # remove `plot=False` if you want to see the coarse-grained graph; sc.tl.umap(adata_ref, init_pos='paga'). adata.obs['seurat_clusters'] = adata.obs['seurat_clusters'].astype('category'); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.paga(adata, groups = 'seurat_clusters'); sc.pl.paga(adata, plot=False) # remove `plot=False` if you want to see the coarse-grained graph; sc.tl.umap(adata, init_pos='paga'). sc.tl.ingest(adata, adata_ref, obs='cell_type'); ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); <ipython-input-18-6b34a6250614> in <module>; 1 # we map our tabula sapiens cell type labels onto our data; ----> 2 sc.tl.ingest(adata, adata_ref, obs='cell_type'). ~/.local/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 124 labeling_method = labeling_method * len(obs); 125 ; --> 126 ing = Ingest(adata_ref, n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:593,message,message,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I'm encountering an error when running the sc.pl.rank_genes_groups_heatmap function in the scanpy package. The error message is ""Linkage 'Z' contains negative distances."" What could be causing this error and how can I fix it?. ### Minimal code sample. ```python; sc.pl.rank_genes_groups_heatmap(adata, n_genes=10, groupby='clusters',show_gene_labels=True,save='cluster.markers.heatmap.svg'); ```. ### Error output. ```pytb; sc.pl.rank_genes_groups_heatmap(adata, n_genes=10, groupby=cluster,show_gene_labels=True,save=(id+'_processed.top10.cluster.markers.heatmap.svg')); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 673, in rank_genes_groups_heatmap; return _rank_genes_groups_plot(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 592, in _rank_genes_groups_plot; return heatmap(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 1087, in heatmap; dendro_data = _reorder_categories_after_dendrogram(; File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 2134, in _reorder_categories_after_dendrogram; key = _get_dendrogram_key(adata, dendrogram, groupby); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 2236, in _get_dendrogram_key; dendrogram(adata, groupby, key_added=dendrogram_key); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scanpy/tools/_dendrogram.py"", line 143, in dendrogram; dendro_info = sch.dendrogram(z_var, labels=list(categories), no_plot=True); File ""/opt/conda/envs/st/lib/python3.8/site-packages/scipy/cluster/hierarchy.py"", line 3301, in dendrogram; is_valid_linkage(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2804:408,message,message,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2804,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistribut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:446,depend,dependencies,446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['depend'],['dependencies']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Since the most recent matplotlib release (3.8.1), the `_get_signature` function seems to have issues. I am not sure why this is the case. I am opening the issue for now and will dig a bit. Could be on my end. ### Minimal code sample. ```python; `pip install ehrapy`. `>>> import ehrapy as ep`; ```. ### Error output. ```pytb; tests/tools/causal/test_dowhy.py:10: in <module>; import ehrapy as ep; ehrapy/__init__.py:14: in <module>; from ehrapy import plot as pl; ehrapy/plot/__init__.py:3: in <module>; from ehrapy.plot._scanpy_pl_api import * # noqa: F403; ehrapy/plot/_scanpy_pl_api.py:1134: in <module>; @_wraps_plot_scatter; ../../miniconda3/envs/ehrapy/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:605: in _wraps_plot_scatter; wrapper_sig = _get_signature(wrapper, eval_str=True); ../../miniconda3/envs/ehrapy/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:590: in _get_signature; lambda: inspect.Signature.empty, get_annotations(obj, eval_str=eval_str); ../../miniconda3/envs/ehrapy/lib/python3.11/inspect.py:276: in get_annotations; return_value = {key:; ../../miniconda3/envs/ehrapy/lib/python3.11/inspect.py:277: in <dictcomp>; value if not isinstance(value, str) else eval(value, globals, locals); <string>:1: in <module>; ???; E NameError: name 'Axes' is not defined; ```. ### Versions. <details>. ```; matplotlib 3.8.1; scanpy 1.9.6; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2747:1076,wrap,wrapper,1076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2747,1,['wrap'],['wrapper']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. While I am using `sc.pp.calculate_qc_metrics(ad, inplace=True)` to get QC metrics, its reported that a error occoured. ; Error message as below. ; ﻿﻿; It might just be because there's something wrong with my data. Does anyone else have a similar situation?. ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; import anndata. # ad = anndata.read_h5ad('mypath'). def scrublet_by_sample(ad, key='samplename'):; """""" do doublet prediction by batch/sample """"""; """""" ad = anndata object """"""; """""" key = sample or batch in ad.obs""""""; sc.pp.calculate_qc_metrics(ad, inplace=True); ads = []; samplenames = ad.obs[key].unique(); for i in samplenames:; adx = ad[ad.obs[key].isin([i])].copy(); print(i,adx.n_obs); sc.external.pp.scrublet(adx,n_prin_comps=min(30,adx.shape[0]-1)); ads.append(adx); adata = ads[0].concatenate(tuple(ads[1:]), join='outer'); return adata. if np.array_equal(arr, np.round(arr)):; ad = scrublet_by_sample(ad, 'sample_ID'); ad.write(qc_h5); qc_md5 = generate_file_md5(qc_h5); print(""QC MD5 Hash:"", qc_md5); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""<stdin>"", line 5, in scrublet_by_sample; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 306, in calculate_qc_metrics; obs_metrics = describe_obs(; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 401, in top_segment_proportions; return top_segment_proportions_sparse_csr(mtx.d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2758:418,message,message,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2758,1,['message'],['message']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. aggregate throws error when aggregating `obsm` or `varm`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:866,wrap,wrapper,866,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['wrap'],['wrapper']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was trying to install `scanpy=1.9.6` using conda in a `python=3.9` environment that had 1.9.5 working with `seaborn=0.13`.; Conda raised a solving issue due to ; `package scanpy-1.9.6-pyhd8ed1ab_0 requires seaborn !=0.13.0, but none of the providers can be installed`; I tried the second build (1ab_1) and the error stayed:; ` package scanpy-1.9.6-pyhd8ed1ab_1 requires seaborn !=0.13.0, but none of the providers can be installed`. I checked github and saw that the dependency in `pyproject.toml` is `""seaborn>=0.13.0""` but when i checked the conda package's `index.json` i saw `""seaborn !=0.13.0""`. The discrepancy between the dependencies is unclear. the full `index.json`:; ```json; {; ""arch"": null,; ""build"": ""pyhd8ed1ab_1"",; ""build_number"": 1,; ""depends"": [; ""anndata >=0.7.4"",; ""get-annotations"",; ""h5py >=3"",; ""joblib"",; ""matplotlib-base >=3.6"",; ""natsort"",; ""networkx >=2.3"",; ""numba >=0.41"",; ""numpy >=1.17"",; ""packaging"",; ""pandas >=1.1.1,!=2.1.2"",; ""patsy"",; ""python >=3.8"",; ""scikit-learn >=0.24"",; ""scipy >=1.4"",; ""seaborn !=0.13.0"",; ""session-info"",; ""statsmodels >=0.11"",; ""tqdm"",; ""umap-learn >=0.3.10""; ],; ""license"": ""BSD-3-Clause"",; ""license_family"": ""BSD"",; ""name"": ""scanpy"",; ""noarch"": ""python"",; ""platform"": null,; ""subdir"": ""noarch"",; ""timestamp"": 1699376683854,; ""version"": ""1.9.6""; }; ```. ### Minimal code sample. ```python; conda create -n test ""python=3.9"" ""scanpy=1.9.6"" ""seaborn=0.13"" -c conda-forge; ```. ### Error output. _No response_. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2791:760,depend,dependency,760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2791,3,['depend'],"['dependencies', 'dependency', 'depends']"
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've noticed that `pyproject.toml` refers to the `python-igraph` package in the PyPI repository. This name is deprecated; the `igraph` package is currently called [`igraph`](https://pypi.org/project/igraph). The old package name currently works as a redirect (i.e. it brings in `igraph` as its own sub-dependency), but it will not be maintained in the future. Please switch to referring to `igraph` in `pyproject.toml` and not `python-igraph`. ### Minimal code sample. ```python; N/A; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.9.0; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; packaging 23.1; pandas 2.0.3; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.1; session_info 1.0.0; sitecustomize NA; six 1.16.0; sklearn 1.3.0; threadpoolctl 3.2.0; -----; Python 3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]; macOS-13.2.1-arm64-arm-64bit; -----; Session information updated at 2023-07-19 13:34; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2562:593,depend,dependency,593,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2562,1,['depend'],['dependency']
Integrability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Many different calls in scanpy emit warnings that are currently suppressed by our testing framework (I think). . ### Minimal code sample. I discovered this unrelatedly by editing the notebooks, see for example: https://github.com/scverse/scanpy-tutorials/blob/master/spatial/integration-scanorama.ipynb. @flying-sheep mentioned that the scanpy tests filter out warnings and indeed you can reproduce these by e.g.,:; ```sh; pytest -W error::FutureWarning -n auto scanpy/tests/test_plotting.py; ```. ### Error output. - [x] `…/scanpy/plotting/_tools/scatterplots.py:401:`. > UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored. - [x] `…/scanpy/plotting/_tools/__init__.py:1269:`. > FutureWarning: The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect. ; > `_ax = sns.violinplot(`. - [x] `…/scanpy/preprocessing/_simple.py:274:`. > ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; > `adata.var[""n_cells""] = number`. - [x] `…/scanpy/plotting/_stacked_violin.py:503: FutureWarning:`. > Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect. ; > `row_ax = sns.violinplot(`. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.10.0.dev191+gf7f5d5c6; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jedi 0.19.1; jinja2 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839:566,integrat,integration-scanorama,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839,1,['integrat'],['integration-scanorama']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:176,Integrat,Integration,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,1,['Integrat'],['Integration']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Currently sc.pp.subsample does not allow for sampling with replacement. When n_obs is provided, and it is larger than the size of the adata object, an error message from numpy.random.choice is given.; ""obs_indices = np.random.choice(old_n_obs, size=new_n_obs, replace=False)"". It seems like replace is automatically set to False. It would be great if sc.pp.subsample provided a paramater to change the np.random.choice's 'replace' parameter to True.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2854:319,message,message,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2854,1,['message'],['message']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hello,. I am trying to integrate some datasets that were generated with SmartSeq2 and 10X platforms. For the SmartSeq2 datasets, I only got the TPM data, while for the 10x data,I got raw counts matrix. How to merge and integrate SmartSeq2 and 10X Datasets? Can the TPM and raw counts be merged for analysis?. Thank you in advance for your assistance.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662:185,integrat,integrate,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662,2,['integrat'],['integrate']
Integrability,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. ; How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html; How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data?; Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3273:266,integrat,integrate,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273,1,['integrat'],['integrate']
Integrability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. It would be extremely helpful if the embedding manifold tools had scikit-learn style API. . For example, https://pydiffmap.readthedocs.io/en/master/reference/diffusion_map.html. Having the .fit, .transform, and .fit_transform would make the robust implementations in the backend of ScanPy a lot more accessible for users. Right now, the usage feels a bit restrictive and I'm having difficulty leveraging the power of the methods if it's not part of some similar workflow that is in the tutorials. . I'm trying to use the code in the backend of ScanPy implement this API myself but ScanPy is an extremely confusing package from an outside developer. There are nested functions and tests for even simple steps (many of which handle edge cases making the package robust). More specifically, I'm trying to use the ScanPy implementation of Diffusion Maps as I would use those from pyDiffMap or the spectral clustering from Sklearn. I would like to be able to fit a model with data. Pickle it. Then transform new samples based on the fitted model. This would provide a useful interface for users looking for a non linear alternative to pca.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054:1232,interface,interface,1232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054,1,['interface'],['interface']
Integrability,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. The array api is somewhat mature to the point of being using in scipy for interoperability between dense CPU/GPU functions (see https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html). It is coming with the new sparse format that may come into existence: https://github.com/pydata/sparse/discussions/618. So it would be good to start experimenting with this in scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3136:236,interoperab,interoperability,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3136,1,['interoperab'],['interoperability']
Integrability,### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I'm currently implementing a function that takes in an anndata and then subsamples a given representation using https://github.com/jmschrei/apricot. This generally serves the purpose of semi-optimally picking a reduced number of points that's still representative of the latent space. . Is this sth within the scope of scanpy? When it's done it wouldn't be too much effort to polish it up for a PR. The dependency load seems fairly low.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:583,depend,dependency,583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['depend'],['dependency']
Integrability,"### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I've thought for a while that we should have NMF in scanpy (https://github.com/scverse/scanpy/pull/941). But it's always been pretty trivial to implement, so not that much work for someone to cover. But now that we're increasing the amount of out of core support in scanpy I think we can offer a lot more value here with out-of-core NMF support. I would suggest we start with a simple [`sklearn.decompositions.NMF`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) wrapper for in memory datasets. . For out of core implementations, it'll be a bit more work. Some thoughts:. * `sklearn` offers `MiniBatchNMF` which allows updating by batch. While this is out of core, it's effectively serial and may not scale well with increasing compute; * But there are [many distributed NMF implementations out there](https://www.google.com/search?client=safari&rls=en&q=distributed+nmf&ie=UTF-8&oe=UTF-8) (including GPU specific ones, which is relevant for rapids-singlecell); * It would be nice to upstream whatever we do to dask-ml (https://github.com/dask/dask-ml/issues/96), maybe cuml",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2939:678,wrap,wrapper,678,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2939,1,['wrap'],['wrapper']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Another point that I'd like to throw into the scanpy 2.0 discussion: . Right now, many functions have the `inplace` argument, that determines if a function should write back to `adata` or return the result instead. . With this behavior it is hard to make correct type hints. While it is possible with `@overload`, it is cumbersome because it requires to type out the entire function signature twice. When I asked if it is possible to write these overloads in a more concise way [on stackoverflow](https://stackoverflow.com/questions/75757890/python-overload-single-argument?noredirect=1#comment133653817_75757890) several users argued that changing the return type based on an argument is an anti-pattern, and I think they convinced me. . ## Alternative approach; Have two API levels, e.g. ```py; def scanpy.tl.pca(adata: AnnData, **kwargs) -> None: ...; ```; and; ```py; def scanpy.lowlevel.tl.pca(data: np.ndarray | sp.spmatrix, n_pcs) -> np.ndarray: ...; ```. Where the former is a wrapper for the latter. This allows to separate the implementation of the actual method using only numpy/scipy data types from the scverse-specific behavior.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583:1079,wrap,wrapper,1079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583,1,['wrap'],['wrapper']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Depends on. - https://github.com/scientific-python/pytest-doctestplus/issues/229; - https://github.com/scientific-python/pytest-doctestplus/issues/231; - https://github.com/pytest-dev/pytest/issues/11475; - And after that one has been fixed, I’m sure doctestplus also needs to adjust to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2729:94,Depend,Depends,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2729,1,['Depend'],['Depends']
Integrability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Hi! I [tried](https://github.com/VPetukhov/ngschool2023-bayesian) running scanpy using [JupyterLite](https://jupyterlite.readthedocs.io/en/stable/) for teaching purposes. However, it fails, as only `noarch` packages are currently supported in [xeus](https://jupyterlite.readthedocs.io/en/stable/howto/xeus-python/preinstalled_packages.html). From my error log it seems the only non-`noarch` dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py), which is not really needed for teaching. . Would it be possible to make `h5py` an optional dependency on conda-forge somehow? It would be really helpful for many students to be able to try scanpy without the need to install anything locally.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667:485,depend,dependency,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667,2,['depend'],['dependency']
Integrability,"#### Problem; File `tissue_positions_list.csv` not found when running `sc.read_visium`; This issue was caused by the file name change by `spaceranger`. . #### Solution; Make a copy of the `outs/spatial/tissue_positions.csv` and name it as `outs/spatial/tissue_positions_list.csv`. #### Problem; The following error messages appeared when running `sc.pl.spatial`:; ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [4], line 1; ----> 1 sc.pl.spatial(adata, color=adata.var_names.tolist()[0], img_key='hires', scale_factor=None). File scanpy/plotting/_tools/scatterplots.py:1003, in spatial(adata, basis, img, img_key, library_id, crop_coord, alpha_img, bw, size, scale_factor, spot_size, na_color, show, return_fig, save, **kwargs); 1000 cmap_img = None; 1001 circle_radius = size * scale_factor * spot_size * 0.5; -> 1003 axs = embedding(; 1004 adata,; 1005 basis=basis,; 1006 scale_factor=scale_factor,; 1007 size=circle_radius,; 1008 na_color=na_color,; 1009 show=False,; 1010 save=False,; 1011 **kwargs,; 1012 ); 1013 if not isinstance(axs, list):; 1014 axs = [axs]. File scanpy/plotting/_tools/scatterplots.py:392, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 389 # if user did not set alpha, set alpha to 0.7; 390 kwargs['alpha'] = 0.7 if alpha is None else alpha; --> 392 cax = scatter(; 393 coords[:, 0],; 394 coords[:, 1],; 395 marker=""."",; 396 c=color_vector,; 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; 399 **kwargs,; 400 ); 402 # remo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2345:315,message,messages,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2345,1,['message'],['messages']
Integrability,"#### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; etils 0.7.1; executing 0.10.0; flatbuffers 22.11.23; fsspec 2022.7.1; gast NA; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.10.2; ipykernel 6.14.0; ipython_genutils 0.2.0; ipywidgets 7.7.1; jax 0.3.16; jaxlib 0.3.15; jedi 0.18.1; joblib 1.1.0; jupyter_server 1.18.1; keras 2.11.0; kiwisolver 1.4.3; leidenalg 0.9.0; llvmlite 0.38.1; louvain 0.8.0; lz4 4.0.2; matplotlib 3.5.3; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.3; numpy 1.22.4; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.30; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 9.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.2.1; requests 2.28.1; scipy 1.8.1; session_info 1.0.0; six 1.16.0; sklearn 1.1.1; socks 1.7.1; sphinxcontrib NA; stack_data 0.4.0; tensorboard 2.11.0; tensorflow 2.11.0; termcolor NA; texttable 1.6.7; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.0; traitlets 5.3.0; typing_extensions NA; umap 0.5.3; unicodedata2 NA; urllib3 1.26.11; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zmq 23.2.1; zope NA; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.5; notebook 6.4.12; -----; Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]; Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2022-12-15 16:32. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2381:3597,wrap,wrapt,3597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381,1,['wrap'],['wrapt']
Integrability,#2210 is triggered by not having dask installed. Our current CI setup can't test for this. This should be addressed with a CI run that uses minimal dependencies.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211:148,depend,dependencies,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211,1,['depend'],['dependencies']
Integrability,"'Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; Type",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:1790,wrap,wrapper,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"(optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; ncont = ncont[ncont.obs.pct_counts_mt < 5, :]; ncont.raw = ncont; ```. ```pytb; [TypeError: cannot unpack non-iterable NoneType object]; ```. #### Versions. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.21.5 scipy==1.7.1 pandas==1.3.2 scikit-learn==0.24.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. <details>. [WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2188:1172,message,message,1172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2188,1,['message'],['message']
Integrability,* fixed vmin/vmax for categorical data #800 ; * added error message when vmin is not valid to point out how to format it; * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/804:60,message,message,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804,1,['message'],['message']
Integrability,"**Set name for storing Umap coordinates explicitly in tl.umap and pl.umap**; tl.umap(..., x_umap = ""X_umap""); pl.umap(..., x_umap = ""X_umap""). Sometimes it would be helpful to specify the adata obsm field for storing the umap coordinates.; For example : ; -if I want to compute and plot the umap for the raw data and afterwards for the integrated or in any way modified data. The first x_umap is going to be overwritten and needs to be computed again, if I need to plot the first step again.; So it would be cool to enable a workflow like the following:. ```; tl.umap(adata, ..., x_umap = ""X_umap_raw""); # do some operations ...; tl.umap(adata, ..., x_umap = ""X_umap_mod). # now after computation I might need to take a look on both umaps again, or plot them in direct comparison; pl.umap(adata, ..., x_umap = ""X_umap_raw""); pl.umap(adata, ..., x_umap = ""X_umap_mod""). ```. I hope I was able to explain what I mean and did not oversee such feature or misunderstood the usage.; All the best ; maflot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2245:336,integrat,integrated,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2245,1,['integrat'],['integrated']
Integrability,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286:266,depend,depends,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286,1,['depend'],['depends']
Integrability,"**failed resolving arguments***); 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:; 376 logger.info(f""Running IncrementalPCA without downsampling""); --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,; 378 chunk_size=obs_chunk_size); 379 else: # downsample; 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***); 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):; 254 chunk = chunk.toarray() if issparse(chunk) else chunk; --> 255 pca_.partial_fit(chunk); 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):; 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs); 1466 estimator._validate_params(); 1468 with config_context(; 1469 skip_parameter_validation=(; 1470 prefer_skip_nested_validation or global_skip_validation; 1471 ); 1472 ):; -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input); 298 raise ValueError(; 299 ""n_components=%r invalid for n_features=%d, need ""; 300 ""more rows than columns for IncrementalPCA ""; 301 ""processing"" % (self.n_components, n_features); 302 ); 303 elif not self.n_components <= n_samples:; --> 304 raise ValueError(; 305 ""n_components=%r must be less or equal to ""; 306 ""the batch number of samples ""; 307 ""%d."" % (self.n_components, n_samples); 308 ); 309 else:; 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77; ```. To fix this bug, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3227:1301,wrap,wrapper,1301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227,1,['wrap'],['wrapper']
Integrability,", delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank 1.0.0; cffi 1.14.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jax 0.2.5; jaxlib 0.1.56; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; lapack NA; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.2; mpl_toolkits NA; natsort 7.0.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.4; opt_einsum v3.3.0; packaging 20.4; pandas 1.1.4; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 3.53.1; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pyparsing 2.4.7; python_utils NA; pytz 2020.4; scanpy 1.6.0; scipy 1.5.3; scvelo 0.2.2; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; threadpoolctl 2.1.0; tornado 6.1; traitlets 5.0.5; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; -----; IPython 7.19.0; jupyter_client 6.1.7; jupyter_core 4.6.3; notebook 6.1.4; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-3-amd64-x86_64-with-glibc2.10; 8 logical CPU cores; -----; Session information updated at 2020-11-03 13:36. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:3612,wrap,wrapt,3612,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['wrap'],['wrapt']
Integrability,", uns_merge, label, keys, index_unique, fill_value, pairwise); 813 # Annotation for other axis; 814 alt_annot = merge_dataframes(; --> 815 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 816 ); 817 . ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 322 @wraps(func); 323 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 324 return func(*args, **kwargs); 325 ; 326 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4770 kwargs.pop(""axis"", None); 4771 kwargs.pop(""labels"", None); -> 4772 return super().reindex(**kwargs); 4773 ; 4774 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""]). ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4817 # perform the reindex on the axes; 4818 return self._reindex_axes(; -> 4819 axes, level, limit, tolerance, method, fill_value, copy; 4820 ).__finalize__(self, method=""reindex""); 4821 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4596 if index is not None:; 4597 frame = frame._reindex_index(; -> 4598 index, method, copy, level, fill_value, limit, tolerance; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:3331,wrap,wrapper,3331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,3,['wrap'],"['wrapper', 'wraps']"
Integrability,", which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cut",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1366,Integrat,Integration,1366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['Integrat'],['Integration']
Integrability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy. ---; **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; ...; ...; test_data = sc.pp.regress_out(test_data,['n_count'], copy=True); sc.pp.scale(test_data); sc.tl.pca(test_data,n_comps=30, use_highly_variable=True); sc.pp.neighbors(test_data,n_neighbors=20). ```. ```output print. ...storing 'feature_types' as categorical; OMP: Info #276:omp_set_nested routine deprecated, please use omp_set_max_active_levels instead. ```. #### Versions. my python version is 3.8; and the versions of scanpy I have tried were 1.8.2 and 1.9.1. I guess that the reason inducing the errors was the version of python is too high. . How can I solve the problem?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2404:733,rout,routine,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2404,1,['rout'],['routine']
Integrability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). I am learning the example of [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html). When I run code ; ```python; sc.tl.umap(adata_ref); ```; I get; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-0e548e19df3a> in <module>; 1 sc.pp.pca(adata_ref); 2 sc.pp.neighbors(adata_ref); ----> 3 sc.tl.umap(adata_ref). ~/miniconda3/envs/tf/lib/python3.6/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 171 neigh_params.get('metric', 'euclidean'),; 172 neigh_params.get('metric_kwds', {}),; --> 173 verbose=settings.verbosity > 3,; 174 ); 175 elif method == 'rapids':. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.5 umap==0.5.0 numpy==1.19.5 scipy==1.5.4 pandas==1.1.5 scikit-learn==0.24.0 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579:518,Integrat,Integrating,518,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579,2,"['Integrat', 'integrat']","['Integrating', 'integrating-data-using-ingest']"
Integrability,"- [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [X ] (optional) I have confirmed this bug exists on the master branch of scanpy. I'm working on comit `63b42e4b` (latest master).; I'm not sure if intended or not but it seems like it would be usefull if one were able to ingest data that don't share 100% of all features. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.leiden(adata_ref); adata = adata_ref[:, :1000].copy() # assume adata_ref has more than 1000 genes.; sc.tl.ingest(adata, adata_ref, obs='leiden'); ```. Error message; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-37-b3cd11e67810> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='leiden'). ~/projects/scanpy/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 125 ; 126 ing = Ingest(adata_ref, neighbors_key); --> 127 ing.fit(adata); 128 ; 129 for method in embedding_method:. ~/projects/scanpy/scanpy/tools/_ingest.py in fit(self, adata_new); 437 ; 438 if not ref_var_names.equals(new_var_names):; --> 439 raise ValueError(; 440 'Variables in the new adata are different '; 441 'from variables in the reference adata'. ValueError: Variables in the new adata are different from variables in the reference adata; ```. --- . #### Versions. <details>. sc.logging.print_header(); scanpy==1.8.0.dev78+gc488909a anndata==0.7.6 umap==0.5.0 numpy==1.19.4 scipy==1.5.4 pandas==1.1.4 scikit-learn==0.23.2 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3 pynndescent==0.5.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001:746,message,message,746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001,1,['message'],['message']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, according to the documentation, the simulation of doublets with scrublet works best with un-normalized counts. The current wrapper implementation however uses a layer to hold the raw data, which then gets normalized anyway. . Please have in mind, that this is the first issue I have ever opened. If I made any grievous mistakes please inform me of such. . ---. ### Minimal code sample:. ```python; import scanpy; adata = scanpy.datasets.pbmc3k(); scanpy.external.pp.scrublet(adata); ```; and around line 177 in scanpy.external.pp.scrublet we need to 'observe' the values:; ```python; adata_obs.layers['raw'] = adata_obs.X; print(adata_obs.layers['raw']); pp.normalize_total(adata_obs) <--- currently normalizes all layers and X; print(adata_obs.layers['raw']); ```; ---; ### Impact; The 'raw' layer is later used in line 194 of scanpy.external.pp.scrublet to simulate doublets, however, it doesn't contain raw data anymore. ```python; adata_sim = scrublet_simulate_doublets(; adata_obs,; layer='raw',; sim_doublet_ratio=sim_doublet_ratio,; synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957:359,wrap,wrapper,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957,1,['wrap'],['wrapper']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; # Introduction. Hi,. so this is a weird one and I could not track it down yet.; The Schillerlab people have a workstation known as ""agando"". On this workstation the full environment is installed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:722,depend,dependencies,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependencies']
Integrability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. This is probably a bug in my thinking, but naively I thought that `sc.pp.normalize_total()` normalizes counts per cell, thus allowing comparison of different cells by correcting for variable sequencing depth. However, the log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:957,depend,depend,957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['depend'],['depend']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:479,integrat,integration,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['integrat'],['integration']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?. I recently wrote up a parallelized implementation of the Mann-Whitney U test, for my own use ([gist is here](https://gist.github.com/jamestwebber/38ab26d281f97feb8196b3d93edeeb7b)). For the types of tests we tend to do in scRNAseq (lots of different features, 2d arrays) it basically scales with the number of cores you can throw at it. When you're doing a lot of tests this is very nice!. Given that `scanpy` already has a dependency on `numba` this would be a pretty simple thing to add, if you want to do so. Thought I would just point it out!. - James",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060:506,depend,dependency,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060,1,['depend'],['dependency']
Integrability,"- [x] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. I'd really like a smarter d/e method to be accessible easily from Scanpy, one that allows proper treatment of covariates etc. MAST is obviously very popular, but fiddly to integrate from R. I see diffxpy mentioned about the place here, and see it's an in-house tool of yours. Is there a reason it's not been integrated already? If nobody's working on it, shall I take a crack at it?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955:538,integrat,integrate,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955,2,['integrat'],"['integrate', 'integrated']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample. RecursionError when using sc.tl.ingest with Python 3.10. Downgrading to Python 3.8 or 3.9 fixes the issue. . Recreate by just running scanpy's own [Integrating data using ingest and BBKNN](https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html#Integrating-data-using-ingest-and-BBKNN) tutorial. ![image](https://user-images.githubusercontent.com/4848896/160018184-7609a89a-adc5-4094-9bc3-7f2d894355f4.png). ```pytb; ---------------------------------------------------------------------------; RecursionError Traceback (most recent call last); Input In [6], in <cell line: 1>(); ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). File ~/miniconda3/envs/test/lib/python3.10/site-packages/scanpy/tools/_ingest.py:133, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 130 ing.map_embedding(method); 132 if obs is not None:; --> 133 ing.neighbors(**kwargs); 134 for i, col in enumerate(obs):; 135 ing.map_labels(col, labeling_method[i]). File ~/miniconda3/envs/test/lib/python3.10/site-packages/scanpy/tools/_ingest.py:472, in Ingest.neighbors(self, k, queue_size, epsilon, random_state); 469 if self._use_pynndescent:; 470 self._nnd_idx.search_rng_state = rng_state; --> 472 self._indices, self._distances = self._nnd_idx.query(test, k, epsilon); 474 else:; 475 from umap.utils import deheap_sort. File ~/miniconda3/envs/test/lib/python3.10/site-packages/pynndescent/pynndescent_.py:1595, in NNDescent.query(self, query_data, k, epsilon); 1564 """"""Query the training graph_data for the k nearest neighbors; 1565 ; 1566 Parameters; (...); 1592 training graph_data.; 1593 """"""; 1594 if not hasattr(self, ""_search_graph""):; -> 1595 self._init_search_graph(); 1597 if n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:402,Integrat,Integrating,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,3,"['Integrat', 'integrat']","['Integrating', 'Integrating-data-using-ingest-and-BBKNN', 'integrating-data-using-ingest']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Write any anndata with pearson residuals in uns; ```python; ad_all.write(filename='output/10x_h5/ad_all_2cello.h5ad'); ```; The pearson_residual_df looks like this, with 38291 rows (obs) and 5000 columns (features) :; ```python; {'theta': 100,; 'clip': None,; 'computed_on': 'adata.X',; 'pearson_residuals_df': gene_name A2M AADACL2-AS1 AAK1 ABCA1 \; barcode ; GAACGTTCACACCGAC-1-placenta_81 -1.125285 -1.159130 -3.921314 -2.533474 ; TATACCTGTTAGCTAC-1-placenta_81 -1.091364 3.267127 -1.806667 -2.109586 ; CTCAAGAGTGACTGTT-1-placenta_81 -1.074943 12.272920 -1.948798 -2.735791 ; TTCATTGTCACGAACT-1-placenta_81 -1.098699 -1.131765 3.481171 4.472371 ; TATCAGGCAGCTCATA-1-placenta_81 -1.107734 -1.141064 -0.571775 -2.813671 ; ... ... ... ... ... ; CACAACATCGGCGATC-1-placenta_314 -0.115585 -0.119107 -0.434686 -0.303945 ; AGCCAGCGTGCCCAGT-1-placenta_314 -0.097424 -0.100394 -0.366482 -0.256219 ; CCGGTGAGTGTTCGAT-1-placenta_314 -0.110334 -0.113696 -0.414971 -0.290148 ; AGGTCATAGCCTGACC-1-placenta_314 -0.115585 -0.119107 -0.434686 -0.303945 ; TTTATGCCAAAGGGTC-1-placenta_314 -0.112876 -0.116316 -0.424515 -0.296827 ; ```. ```pytb; Unable to create attribute (object header message is too large). Above error raised while writing key 'pearson_residuals_df' of <class 'h5py._hl.group.Group'> to /; ```. #### Versions. <details>. scanpy==1.9.1 anndata==0.8.0 umap==0.5.2 numpy==1.21.5 scipy==1.8.0 pandas==1.4.1 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.9.9 pynndescent==0.5.6. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2383:1659,message,message,1659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383,1,['message'],['message']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, . This may be a problem outside the realm of scanpy functionality, but I thought it best to bring up in case it is relevant or in case anyone here has seen something before while trying to use scanpy. It looks like I can having trouble importing a dependency of the sc.pp.regress() function. I don't think the data here is relevant, just something in my set up. I tried updating all the libraries so that everything is up to date. This problem just started occurring today (2/10/21) and had no issue yesterday, so I figure it was a change on the scanpy end that I didn't keep up with proprely. ```python; sc.pp.regress_out(merged_adata, ['pct_counts_mt', 'pct_counts_rp']); ```. yields the following error. ```pytb; /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/site-packages/statsmodels/tsa/filters/filtertools.py in <module>; 16 import scipy.fftpack as fft; 17 from scipy import signal; ---> 18 from scipy.signal.signaltools import _centered as trim_centered; 19 ; 20 from statsmodels.tools.validation import array_like, PandasWrapper. ImportError: cannot import name '_centered' from 'scipy.signal.signaltools' (/home/unix/jjeang/.local/lib/python3.8/site-packages/scipy/signal/signaltools.py); ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.8 umap==0.3.10 numpy==1.22.2 scipy==1.8.0 pandas==1.2.5 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.9.9 louvain==0.7.0 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2137:484,depend,dependency,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2137,1,['depend'],['dependency']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. In short `sc.pp.scale()` throws an error when `adata.X` is a dask array solely because `sc.pp._simple.scale` doesn't have the `sc.pp._simple.scale_array` function registered for a `dask.array.Array` type. Adding `@scale.register(da.Array)` [here](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#LL758C7-L758C7) would fix that, but considering `dask.array` is [only an optional dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:670,depend,dependency,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,2,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I clone the scanpy repository and I am on commit b69015e. I follow the instructions for a developmental install here:; https://scanpy.readthedocs.io/en/stable/installation.html#dev-install-instructions . ### Minimal code sample (that we can copy&paste without having any data). ```bash; pip install beni; beni pyproject.toml > environment.yml; conda env create -f environment.yml; ```. this is the error I get. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound:; - seaborn-split; ```. #### `environment.yml`. Here is the content of `environment.yml` which contains the strange package `seaborn-split`. So maybe the issue is upstream with beni?. <details>. ```; channels:; - conda-forge; dependencies:; - pip:; - flit; - bbknn; - scanpydoc>=0.7.4; - harmonypy; - magic-impute>=2.0; - cudf>=0.9; - cuml>=0.9; - cugraph>=0.9; - scanorama; - scrublet; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; - setuptools-scm; - black>=20.8b1; - docutils; - sphinx<4.2,>=4.1; - sphinx_rtd_theme>=0.3.1; - python-igraph; - leidenalg; - louvain!=0.6.2,>=0.6; - scikit-misc>=0.1.3; - pytest>=4.4; - pytest-nunit; - dask-core!=2.17.0; - fsspec; - zappy; - - zarr; - profimp; - flit-core; name: scanpy; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144:974,depend,dependencies,974,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144,1,['depend'],['dependencies']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. When scanpy gets installed with the latest version of `importlib_metadata` (2.0), the ; command `sc.logging.print_versions()` fails with the following error: . ```pytb; WARNING: If you miss a compact list, please try `print_header`!; Traceback (most recent call last):; File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 195, in sinfo; mod_version = _find_version(mod.__version__); AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/scanpy/logging.py"", line 161, in print_versions; sinfo(dependencies=True); File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 198, in sinfo; mod_version = _find_version(mod.version); File ""/home/sturm/anaconda3/envs/scanpy_test/lib/python3.7/site-packages/sinfo/main.py"", line 42, in _find_version; return mod_version_attr(); TypeError: version() missing 1 required positional argument: 'distribution_name'; ```. According to the `importlib_metadata` changelog, the `__version__` attribute has been removed from the package: . ```; =========================; importlib_metadata NEWS; =========================. v2.0.0; ======. * ``importlib_metadata`` no longer presents a; ``__version__`` attribute. Consumers wishing to; resolve the version of the package should query it; directly with; ``importlib_metadata.version('importlib-metadata')``.; Closes #71.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1437:1000,depend,dependencies,1000,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1437,1,['depend'],['dependencies']
Integrability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. When running the `Integrating spatial data with scRNA-seq using scanorama` [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration-and-label-transfer-from-scRNA-seq-dataset) with the provided sample data, the following error occurs. This may be related to the following warning I also see. `<string>:6: VisibleDeprecationWarning: Creating an ndarray from nested sequences exceeding the maximum number of dimensions of 32 is deprecated. If you mean to do this, you must specify 'dtype=object' when creating the ndarray.`. How can I overcome this issue?. Example code below has how I downloaded the data in the `Data integration and label transfer from scRNA-seq dataset` section of the tutorial and then just the code block where the error actually occurs. ---. ### Minimal code sample (that we can copy&paste without having any data). ![scanorama_error](https://user-images.githubusercontent.com/52245296/154322971-c45606d2-54d7-42da-8ac6-85f91359e3c8.png). ```python. import subprocess; from pathlib import Path. if Path('./downloaded_data/adata_processed.h5ad').is_file():; print(""Data previously downloaded, skipping to next step""); else:; subprocess.run(['wget', '-O', './downloaded_data/adata_processed.h5ad', 'https://hmgubox.helmholtz-muenchen.de/f/4ef254675e2a41f89835/?dl=1']). adata_cortex = sc.read(""./downloaded_data/adata_processed.h5ad""). embedding_anterior = np.concatenate(integrated_anterior, axis=0); adata_cortex_anterior.obsm[""scanorama_embedding""] = embedding_anterior. embedding_posterior = np.concatenate(integrated_posterior, axis=0); adata_cortex_posterior.obsm[""scanorama_embedding""] = embedding_posterior; ```. ```pytb. <string>:6: VisibleDeprecationWarning: Creating an ndarray from nested se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143:242,Integrat,Integrating,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143,4,"['Integrat', 'integrat']","['Integrating', 'integration', 'integration-and-label-transfer-from-scRNA-seq-dataset', 'integration-scanorama']"
Integrability,"------------------; AxisError Traceback (most recent call last); <ipython-input-55-c0d016811ded> in <module>; ----> 1 sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py in regress_out(adata, keys, n_jobs, copy); 817 # split the adata.X matrix by columns in chunks of size n_chunk; 818 # (the last chunk could be of smaller size than the others); --> 819 chunk_list = np.array_split(adata.X, n_chunks, axis=1); 820 if variable_is_categorical:; 821 regressors_chunk = np.array_split(regressors, n_chunks, axis=1). <__array_function__ internals> in array_split(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/lib/shape_base.py in array_split(ary, indices_or_sections, axis); 782 ; 783 sub_arys = []; --> 784 sary = _nx.swapaxes(ary, axis, 0); 785 for i in range(Nsections):; 786 st = div_points[i]. <__array_function__ internals> in swapaxes(*args, **kwargs). ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in swapaxes(a, axis1, axis2); 595 ; 596 """"""; --> 597 return _wrapfunc(a, 'swapaxes', axis1, axis2); 598 ; 599 . ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~/anaconda3/envs/scanpy/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. AxisError: axis1: axis 1 is out of bounds for array of dimension 0; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post3 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010:2089,wrap,wrap,2089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010,2,['wrap'],['wrap']
Integrability,-forge; debugpy 1.5.1 py39hc377ac9_0 anaconda; decorator 5.1.1 pyhd3eb1b0_0 anaconda; dunamai 1.18.0 pyhd8ed1ab_0 conda-forge; entrypoints 0.4 py39hca03da5_0 anaconda; executing 0.8.3 pyhd3eb1b0_0 anaconda; fonttools 4.41.0 py39h0f82c59_0 conda-forge; freetype 2.12.1 hd633e50_1 conda-forge; get_version 3.5.4 pyhd8ed1ab_0 conda-forge; gettext 0.21.1 h0186832_0 conda-forge; git 2.41.0 pl5321h46e2b6d_0 conda-forge; h5py 3.9.0 nompi_py39he9c2634_101 conda-forge; hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; ipykernel 6.9.1 py39hca03da5_0 anaconda; ipython 8.3.0 py39hca03da5_0 anaconda; jedi 0.18.1 py39hca03da5_1 anaconda; joblib 1.3.0 pyhd8ed1ab_1 conda-forge; jupyter_client 7.2.2 py39hca03da5_0 anaconda; jupyter_core 4.10.0 py39hca03da5_0 anaconda; kiwisolver 1.4.4 py39haaf3ac1_1 conda-forge; krb5 1.21.1 h92f50d5_0 conda-forge; lcms2 2.15 hd835a16_1 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; lerc 4.0.0 h9a09cb3_0 conda-forge; libaec 1.0.6 hb7217d7_1 conda-forge; libblas 3.9.0 17_osxarm64_openblas conda-forge; libbrotlicommon 1.0.9 h1a8c8d9_9 conda-forge; libbrotlidec 1.0.9 h1a8c8d9_9 conda-forge; libbrotlienc 1.0.9 h1a8c8d9_9 conda-forge; libcblas 3.9.0 17_osxarm64_openblas conda-forge; libcurl 8.1.2 hc52a3a8_1 conda-forge; libcxx 16.0.6 h4653b0c_0 conda-forge; libdeflate 1.18 h1a8c8d9_0 conda-forge; libedit 3.1.20191231 hc8eb9b7_2 conda-forge; libev 4.33 h642e427_1 conda-forge; libexpat 2.5.0 hb7217d7_1 conda-forge; libffi 3.4.2 h3422bc3_5 conda-forge; libgfortran 5.0.0 12_2_0_hd922786_31 conda-forge; libgfortran5 12.2.0 h0eea778_31 conda-forge; libiconv 1.17 he4db4b2_0 conda-forge; libjpeg-turbo 2.1.5.1 h1a8c8d9_0 conda-forge; liblapack 3.9.0 17_osxarm64_openblas conda-forge; libllvm14 14.0.6 hd1a9a77_3 conda-forge; libnghttp2 1.52.0 hae82a92_0 conda-forge; libopenblas 0.3.23 openmp_hc731615_0 conda-forge; libpng 1.6.39 h76d750c_,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:4016,wrap,wrap,4016,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['wrap'],['wrap']
Integrability,"-packages/setuptools/build_meta.py"", line 145, in run_setup; exec(compile(code, __file__, 'exec'), locals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2245,integrat,integration,2245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['integrat'],['integration']
Integrability,".0 | testpath | 0.5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15098,wrap,wrapt,15098,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability,".0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; dill 0.3.4; docrep 0.3.2; entrypoints 0.4; etils 0.8.0; flax 0.6.1; fsspec 2022.7.1; google NA; graphviz 0.20; h5py 3.7.0; idna 3.4; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.23; jaxlib 0.3.22; jedi 0.18.1; jinja2 2.11.3; jmespath 0.10.0; joblib 1.1.1; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.8.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; matplotlib_inline 0.1.6; ml_collections NA; mpl_toolkits NA; msgpack 1.0.3; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.56.3; numexpr 2.8.3; numpy 1.22.4; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pyro 1.8.2; pytorch_lightning 1.7.7; pytz 2022.1; regex 2.5.116; requests 2.28.1; rich NA; scipy 1.7.3; scvi 0.18.0; session_info 1.0.0; setuptools 63.4.1; simplejson 3.17.6; six 1.16.0; sklearn 1.1.2; snappy NA; socks 1.7.1; sphinxcontrib NA; storemagic NA; tblib 1.7.0; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.2; torch 1.12.1; torchmetrics 0.10.0; torchvision 0.13.1; tornado 6.1; tqdm 4.64.1; traitlets 5.1.1; tree 0.1.7; typing_extensions NA; urllib3 1.26.12; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 7.31.1; jupyter_client 7.3.4; jupyter_core 4.11.1; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Jun 1 2022, 06:36:29) [Clang 12.0.0 ]; macOS-10.16-x86_64-i386-64bit; -----; Session information updated at 2022-10-22 15:12. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:4347,wrap,wrapt,4347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['wrap'],['wrapt']
Integrability,".0; opt_einsum v3.3.0; optax 0.1.4; packaging 23.0; pandas 1.5.3; parso 0.8.3; paste NA; patsy 0.5.3; petsc4py 3.19.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.0.0; progressbar 4.2.0; prometheus_client NA; prompt_toolkit 3.0.38; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygam 0.8.0; pygments 2.14.0; pygpcca 1.0.4; pyparsing 3.0.9; pyro 1.8.4+9ed468d; pyrsistent NA; python_utils NA; pythonjsonlogger NA; pytorch_lightning 1.9.3; pytz 2022.7.1; pywt 1.4.1; requests 2.28.2; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; rich NA; scHPL NA; scarches 0.5.7; sccoda 0.1.9; scipy 1.10.1; scvelo 0.2.5; scvi 0.20.1; seaborn 0.12.2; send2trash NA; session_info 1.0.0; setuptools 67.4.0; six 1.16.0; skewnorm_ufunc NA; skimage 0.19.3; sklearn 1.2.1; slepc4py 3.19.0; sniffio 1.3.0; socks 1.7.1; squidpy 1.2.2; stack_data 0.6.2; statsmodels 0.13.5; tblib 1.7.0; tcr_embedding NA; tensorboard 2.11.2; tensorflow 2.11.0; tensorflow_probability 0.19.0; termcolor NA; texttable 1.6.7; threadpoolctl 3.1.0; tifffile 2023.2.28; tlz 0.12.0; toolz 0.12.0; torch 1.13.1; torch_cluster 1.6.0; torch_geometric 2.2.0; torch_scatter 2.1.0; torch_sparse 0.6.15; torchmetrics 0.11.3; torchvision 0.14.1; tornado 6.2; tqdm 4.64.1; traitlets 5.9.0; tree 0.1.7; typing_extensions NA; unicodedata2 NA; uri_template NA; urllib3 1.26.14; validators 0.20.0; wcwidth 0.2.6; webcolors 1.11.1; websocket 1.5.1; wrapt 1.15.0; xarray 2023.2.0; xarray_einstats 0.5.1; yaml 6.0; zarr 2.13.6; zipp NA; zmq 25.0.0; zoneinfo NA; zope NA; -----; IPython 8.11.0; jupyter_client 8.0.3; jupyter_core 5.2.0; jupyterlab 3.6.1; notebook 6.5.2; -----; Python 3.10.9 | packaged by conda-forge | (main, Feb 2 2023, 20:20:04) [GCC 11.3.0]; Linux-3.10.0-1160.83.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2023-05-26 01:06. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2493:3786,wrap,wrapt,3786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2493,1,['wrap'],['wrapt']
Integrability,.11.0 ; dataclasses 0.6 ; dataforest 0.0.2 /home/ubuntu/code/dataforest ; dbus-python 1.2.16 ; decorator 4.4.2 ; defusedxml 0.6.0 ; distro-info 0.23ubuntu1 ; entrypoints 0.3 ; fastcore 1.3.5 ; flake8 3.8.4 ; Flask 1.1.2 ; Flask-Compress 1.8.0 ; future 0.18.2 ; get-version 2.1 ; gitdb 4.0.5 ; GitPython 3.1.11 ; h5py 3.1.0 ; hyperopt 0.1.2 ; idna 2.8 ; importlib-metadata 2.0.0 ; iniconfig 1.1.1 ; ipdb 0.13.4 ; ipykernel 5.3.4 ; ipympl 0.5.8 ; ipython 7.19.0 ; ipython-genutils 0.2.0 ; ipywidgets 7.5.1 ; isort 5.6.4 ; itsdangerous 1.1.0 ; jedi 0.17.2 ; Jinja2 2.11.2 ; jmespath 0.10.0 ; joblib 0.17.0 ; json5 0.9.5 ; jsonschema 3.2.0 ; jupyter-client 6.1.7 ; jupyter-core 4.6.3 ; jupyter-dash 0.3.1 ; jupyter-lsp 0.9.2 ; jupyterlab 2.2.9 ; jupyterlab-code-formatter 1.3.6 ; jupyterlab-git 0.23.1 ; jupyterlab-latex 2.0.0 ; jupyterlab-pygments 0.1.2 ; jupyterlab-server 1.2.0 ; jupyterlab-sql 0.3.3 ; jupyterlab-templates 0.2.5 ; jupytext 1.6.0 ; kiwisolver 1.3.1 ; lazy-object-proxy 1.4.3 ; legacy-api-wrap 1.2 ; llvmlite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-ser,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:4982,wrap,wrap,4982,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['wrap'],['wrap']
Integrability,.2.0; - executing=2.0.1; - filelock=3.14.0; - fonttools=4.53.0; - fqdn=1.5.1; - freetype=2.12.1; - get-annotations=0.1.2; - gffpandas=1.2.2; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.5; - h11=0.14.0; - h2=4.1.0; - h5py=3.11.0; - hdf5=1.14.3; - hpack=4.0.0; - httpcore=1.0.5; - httpx=0.27.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.7; - igraph=0.10.12; - importlib-metadata=7.1.0; - importlib_metadata=7.1.0; - importlib_resources=6.4.0; - ipykernel=6.29.4; - ipython=8.25.0; - isoduration=20.11.0; - jedi=0.19.1; - jinja2=3.1.4; - joblib=1.4.2; - json5=0.9.25; - jsonpointer=2.4; - jsonschema=4.22.0; - jsonschema-specifications=2023.12.1; - jsonschema-with-format-nongpl=4.22.0; - jupyter-lsp=2.2.5; - jupyter_client=8.6.2; - jupyter_core=5.7.2; - jupyter_events=0.10.0; - jupyter_server=2.14.1; - jupyter_server_terminals=0.5.3; - jupyterlab=4.2.2; - jupyterlab_pygments=0.3.0; - jupyterlab_server=2.27.2; - kaleido-core=0.2.1; - kiwisolver=1.4.5; - krb5=1.21.2; - lcms2=2.16; - legacy-api-wrap=1.4; - leidenalg=0.10.2; - lerc=4.0.0; - libabseil=20240116.2; - libaec=1.1.3; - libblas=3.9.0; - libbrotlicommon=1.1.0; - libbrotlidec=1.1.0; - libbrotlienc=1.1.0; - libcblas=3.9.0; - libcurl=8.8.0; - libcxx=17.0.6; - libdeflate=1.20; - libedit=3.1.20191231; - libev=4.33; - libexpat=2.6.2; - libffi=3.4.2; - libgfortran=5.0.0; - libgfortran5=13.2.0; - libhwloc=2.10.0; - libiconv=1.17; - libjpeg-turbo=3.0.0; - liblapack=3.9.0; - liblapacke=3.9.0; - libleidenalg=0.11.1; - libllvm14=14.0.6; - libnghttp2=1.58.0; - libopenblas=0.3.27; - libpng=1.6.43; - libprotobuf=4.25.3; - libsodium=1.0.18; - libsqlite=3.46.0; - libssh2=1.11.0; - libtiff=4.6.0; - libwebp-base=1.4.0; - libxcb=1.15; - libxml2=2.12.7; - libzlib=1.3.1; - llvm-openmp=18.1.7; - llvmlite=0.42.0; - louvain=0.8.2; - lz4-c=1.9.4; - markupsafe=2.1.5; - mathjax=2.7.7; - matplotlib=3.8.4; - matplotlib-base=3.8.4; - matplotlib-inline=0.1.7; - mistune=3.0.2; - mkl=2023.2.0; - mkl-devel=2023.2.0; - mkl-include=2023.2.0; - mpc=1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:4535,wrap,wrap,4535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrap']
Integrability,".5.0 | testpath | 0.5.0; texttable | 1.6.4 | texttable | 1.6.4 | texttable | 1.6.4; threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0 | threadpoolctl | 3.0.0;   |   | tomli | 2.0.0 |   |  ; toolz | 0.11.1 | toolz | 0.11.1 | toolz | 0.11.1; tornado | 6.1 | tornado | 6.1 | tornado | 6.1; tqdm | 4.62.3 | tqdm | 4.62.3 | tqdm | 4.62.3; traitlets | 5.1.1 | traitlets | 5.1.1 | traitlets | 5.1.1; typing_extensions | 4.0.1 | typing_extensions | 4.0.1 | typing_extensions | 4.0.1; umap-learn | 0.5.2 | umap-learn | 0.5.2 | umap-learn | 0.5.2; urllib3 | 1.26.7 | urllib3 | 1.26.7 | urllib3 | 1.26.7; wcwidth | 0.2.5 | wcwidth | 0.2.5 | wcwidth | 0.2.5; webencodings | 0.5.1 | webencodings | 0.5.1 | webencodings | 0.5.1; wheel | 0.37.1 | wheel | 0.37.1 | wheel | 0.37.1; widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2 | widgetsnbextension | 3.5.2; win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0 | win-inet-pton | 1.1.0; wincertstore | 0.2 | wincertstore | 0.2 | wincertstore | 0.2; wrapt | 1.13.3 | wrapt | 1.13.3 | wrapt | 1.13.3; xlrd | 1.2.0 | xlrd | 1.2.0 | xlrd | 1.2.0; yarl | 1.7.2 | yarl | 1.7.2 | yarl | 1.7.2; zict | 2.0.0 | zict | 2.0.0 | zict | 2.0.0; zipp | 3.7.0 | zipp | 3.7.0 | zipp | 3.7.0. </body>. </html>. These packages are different among these 3 PCs :<html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/hyjfo/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:au",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:15115,wrap,wrapt,15115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['wrap'],['wrapt']
Integrability,".6.8/lib/python3.6/contextlib.py"", line 88, in __exit__; next(self.gen); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 219, in pending_warnings; memhandler.flushTo(logger); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 184, in flushTo; logger.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1454, in handle; self.callHandlers(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1516, in callHandlers; hdlr.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 861, in handle; rv = self.filter(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 720, in filter; result = f.filter(record); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/latest/lib/python3.6/site-packages/sphinx/util/logging.py"", line 404, in filter; raise SphinxWarning(location + "":"" + message); sphinx.errors.SphinxWarning: /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/preprocessing/_simple.py:docstring of scanpy.pp.downsample_counts:17:py:class reference target not found: numpy.random.RandomState. Warning, treated as error:; /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/preprocessing/_simple.py:docstring of scanpy.pp.downsample_counts:17:py:class reference target not found: numpy.random.RandomState; ```. </details>. It looks like readthedocs is failing due to `numpy.random.RandomState` not being rewritten to `numpy.random.mtrand.RandomState`. I think the transform isn't happening in the right order when the docs are built from scratch since I can reproduce the warnings locally with:. ```; make clean; make html; ```. But if I run `make html` again, I don't get the warnings.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1057:3055,message,message,3055,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1057,1,['message'],['message']
Integrability,".pyplot as plt; fig,ax=plt.subplots(1,1,figsize=(7,1)); path_data = sc.pl.paga_path(; adata,; [4, 5],; [""Elane""],; ax=ax,; show_node_names=False,; ytick_fontsize=12,; return_data=True,; #n_avg=1,; color_map=""Greys"",; groups_key=""leiden"",; color_maps_annotations={""dpt_pseudotime"": ""viridis""}; ); ```. ### Error output. ```pytb; TypeError Traceback (most recent call last); Cell In[1], line 15; 13 import matplotlib.pyplot as plt; 14 fig,ax=plt.subplots(1,1,figsize=(7,1)); ---> 15 path_data = sc.pl.paga_path(; 16 adata,; 17 [4, 5],; 18 [""Elane""],; 19 ax=ax,; 20 show_node_names=False,; 21 ytick_fontsize=12,; 22 return_data=True,; 23 #n_avg=1,; 24 color_map=""Greys"",; 25 groups_key=""leiden"",; 26 color_maps_annotations={""dpt_pseudotime"": ""viridis""}; 27 ). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:2137,wrap,wraps,2137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Integrability,"0 infer.propagate(raise_errors=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>); During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:; def array_sort_impl(arr):; <source elided>; # Note we clobber the return value; sort_func(arr); ^. During: lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8764,wrap,wrap,8764,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['wrap'],['wrap']
Integrability,0.2.0; bleach | 4.0.0 | 4.0.0; brotlipy | 0.7.0 | 0.7.0; cached-property | 1.5.2 | 1.5.2; certifi | 2021.5.30 | 2021.5.30; cffi | 1.14.6 | 1.14.6; charset-normalizer | 2.0.4 | 2.0.4; colorama | 0.4.4 | 0.4.4; contextvars | 2.4 | 2.4; **cryptography | 3.4.7 | 35.0.0**; cycler | 0.10.0 | 0.11.0; dataclasses | 0.8 | 0.8; decorator | 4.4.2 | 4.4.2; defusedxml | 0.7.1 | 0.7.1; entrypoints | 0.3 | 0.3; get-version | 2.1 | 2.1; h5py | 3.1.0 | 3.1.0; idna | 3.2 | 3.2; igraph | 0.9.8 | 0.9.8; immutables | 0.16 | 0.16; importlib-metadata | 4.8.1 | 4.8.1; ipykernel | 5.3.4 | 5.3.4; ipython | 7.16.1 | 7.16.1; ipython-genutils | 0.2.0 | 0.2.0; jedi | 0.17.0 | 0.17.0; **Jinja2 | 3.0.1 | 3.0.2**; joblib | 1.1.0 | 1.1.0; json5 | 0.9.6 | 0.9.6; jsonschema | 3.2.0 | 3.2.0; jupyter-client | 7.0.1 | 7.0.1; jupyter-core | 4.8.1 | 4.8.1; jupyter-server | 1.4.1 | 1.4.1; **jupyterlab | 3.1.7 | 3.2.1**; jupyterlab-pygments | 0.1.2 | 0.1.2; jupyterlab-server | 2.8.2 | 2.8.2; kiwisolver | 1.3.1 | 1.3.1; legacy-api-wrap | 1.2 | 1.2; leidenalg | 0.8.8 | 0.8.8; llvmlite | 0.36.0 | 0.36.0; MarkupSafe | 2.0.1 | 2.0.1; matplotlib | 3.3.4 | 3.3.4; mistune | 0.8.4 | 0.8.4; **natsort | 7.1.1 | 8.0.0**; nbclassic | 0.2.6 | 0.2.6; nbclient | 0.5.3 | 0.5.3; nbconvert | 6.0.7 | 6.0.7; nbformat | 5.1.3 | 5.1.3; nest-asyncio | 1.5.1 | 1.5.1; networkx | 2.5.1 | 2.5.1; notebook | 6.4.3 | 6.4.3; numba | 0.53.1 | 0.53.1; numexpr | 2.7.3 | 2.7.3; numpy | 1.19.5 | 1.19.5; packaging | 21 | 21; pandas | 1.1.5 | 1.1.5; pandocfilters | 1.4.3 | 1.4.3; parso | 0.8.2 | 0.8.2; patsy | 0.5.2 | 0.5.2; pickleshare | 0.7.5 | 0.7.5; Pillow | 8.4.0 | 8.4.0; pip | 21.0.1 | 21.2.2; prometheus-client | 0.11.0 | 0.11.0; prompt-toolkit | 3.0.20 | 3.0.20; pycparser | 2.2 | 2.2; Pygments | 2.10.0 | 2.10.0; pynndescent | 0.5.5 | 0.5.5; pyOpenSSL | 20.0.1 | 21.0.0; **pyparsing | 2.4.7 | 3.0.4**; pyrsistent | 0.17.3 | 0.17.3; PySocks | 1.7.1 | 1.7.1; python-dateutil | 2.8.2 | 2.8.2; python-igraph | 0.9.8 | 0.9.8; pytz | 2021.3 | 2021.3; ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045:4378,wrap,wrap,4378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045,1,['wrap'],['wrap']
Integrability,"0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-normalizer=3.3.2; - colorama=0.4.6; - colorcet=3.1.0; - colorful=0.5.6; - comm=0.2.2; - contourpy=1.2.1; - cycler=0.12.1; - debugpy=1.8.1; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.8; - dnspython=2.6.1; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.2.0; - executing=2.0.1; - ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2639,depend,dependencies,2639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependencies']
Integrability,"1 X,; 392 n_neighbors,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/umap/umap_.py in fuzzy_simplicial_set(X, n_neighbors, random_state, metric, metric_kwds, knn_indices, knn_dists, angular, set_op_mix_ratio, local_connectivity, apply_set_operations, verbose); 600 knn_dists = knn_dists.astype(np.float32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:4068,message,message,4068,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['message'],['message']
Integrability,1 he4413a7_1000 conda-forge; freetype 2.10.2 he06d7ca_0 conda-forge; get-version 2.1 pypi_0 pypi; glib 2.63.1 h3eb4bd4_1 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb31296c_0 ; h5py 2.10.0 pypi_0 pypi; hdf5 1.10.4 hb1b8bf9_0 ; icu 58.2 hf484d3e_1000 conda-forge; importlib-metadata 1.6.1 py36h9f0ad1d_0 conda-forge; importlib_metadata 1.6.1 0 conda-forge; intel-openmp 2020.1 217 ; ipyevents 0.7.1 py_0 conda-forge; ipykernel 5.3.0 py36h95af2a2_0 conda-forge; ipython 7.15.0 py36h9f0ad1d_0 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; ipywidgets 7.5.1 py_0 conda-forge; jedi 0.17.1 py36h9f0ad1d_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.15.1 py_0 ; jpeg 9d h516909a_0 conda-forge; jsonschema 3.2.0 py36h9f0ad1d_1 conda-forge; jupyter 1.0.0 py_2 conda-forge; jupyter_client 6.1.3 py_0 conda-forge; jupyter_console 6.1.0 py_1 conda-forge; jupyter_core 4.6.3 py36h9f0ad1d_1 conda-forge; kiwisolver 1.2.0 py36hfd86e86_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 pypi_0 pypi; leidenalg 0.8.0 pypi_0 pypi; libedit 3.1.20191231 h7b6447c_0 ; libffi 3.3 he6710b0_1 ; libgcc-ng 9.1.0 hdf63c60_0 ; libgfortran-ng 7.3.0 hdf63c60_0 ; libpng 1.6.37 hed695b0_1 conda-forge; libsodium 1.0.17 h516909a_0 conda-forge; libstdcxx-ng 9.1.0 hdf63c60_0 ; libuuid 2.32.1 h14c3975_1000 conda-forge; libxcb 1.13 h14c3975_1002 conda-forge; libxml2 2.9.10 he19cac6_1 ; llvmlite 0.33.0 pypi_0 pypi; lz4-c 1.9.2 he6710b0_0 ; lzo 2.10 h7b6447c_2 ; markupsafe 1.1.1 py36h8c4c3a4_1 conda-forge; matplotlib 3.2.2 0 ; matplotlib-base 3.2.2 py36hef1b27d_0 ; mistune 0.8.4 py36h8c4c3a4_1001 conda-forge; mkl 2020.1 217 ; mkl-service 2.3.0 py36he904b0f_0 ; mkl_fft 1.1.0 py36h23d657b_0 ; mkl_random 1.1.1 py36h0573a6f_0 ; mock 4.0.2 py_0 ; natsort 7.0.1 pypi_0 pypi; nbconvert 5.6.1 py36h9f0ad1d_1 conda-forge; nbformat 5.0.6 py_0 conda-forge; ncurses 6.2 he6710b0_1 ; networkx 2.4 pypi_0 pypi; notebook 6.0.3 py36h9f0ad1d_0 conda-forge; numba 0.50.0 pypi_0 pypi; numexpr 2.7.1 ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:7086,wrap,wrap,7086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['wrap'],['wrap']
Integrability,"1 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")); 322 def read_array(elem, _reader):; --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype); 767 try:; --> 768 return self._fast_reader.read(args); 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last); Cell In[2], line 4; 2 import pandas as pd; 3 import scanpy as sc; ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 240 return read_dataframe(elem); 241 return func(elem); --> 243 adata = read_dispatched(f, callback=callback); 245 # Backwards compat (should figure out which version); 246 i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:2234,wrap,wrapper,2234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['wrap'],['wrapper']
Integrability,"1.1; easydev 0.12.0; entrypoints 0.3; fsspec 2021.08.1; get_version 3.5.4; google NA; gridfs NA; gseapy 0.10.8; h5py 3.6.0; html5lib 1.1; idna 3.2; igraph 0.9.11; importlib_metadata NA; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; itsdangerous 2.0.1; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.9; llvmlite 0.36.0; lxml 4.6.3; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.53.0; numexpr 2.7.3; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.0; pandas 1.4.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 8.0.0; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.4.1; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyexpat NA; pygments 2.10.0; pylab NA; pymongo 4.1.0; pyparsing 3.0.4; pyro 1.8.1; pyrsistent NA; pytorch_lightning 1.5.10; pytz 2021.3; regex 2.5.97; requests 2.26.0; requests_cache 0.9.4; rich NA; scipy 1.7.1; scvelo 0.2.4; scvi 0.12.0; send2trash NA; setuptools_scm NA; six 1.16.0; sklearn 1.0.2; sniffio 1.2.0; socks 1.7.1; soupsieve 2.2.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.7.0; tensorboard 2.8.0; texttable 1.6.4; threadpoolctl 2.2.0; tlz 0.11.0; toolz 0.11.1; torch 1.11.0; torchmetrics 0.8.1; tornado 6.1; tqdm 4.62.3; traitlets 5.1.0; typing_extensions NA; ujson 4.0.2; url_normalize 1.4.3; urllib3 1.26.7; utils NA; wcwidth 0.2.5; webencodings 0.5.1; wrapt 1.12.1; yaml 5.4.1; zipp NA; zmq 22.2.1; zope NA; -----; IPython 7.29.0; jupyter_client 6.1.12; jupyter_core 4.8.1; jupyterlab 3.2.1; notebook 6.4.5; -----; Python 3.9.7 (default, Sep 16 2021, 08:50:36) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 10 logical CPU cores, i386. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:6689,wrap,wrapt,6689,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['wrap'],['wrapt']
Integrability,10X Visium-ScRNA-seq integrative analysis,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1386:21,integrat,integrative,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1386,1,['integrat'],['integrative']
Integrability,"1=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. ; If you need access to the data and the container please contact me and I will make it available to you.; The data is already at the ICB cluster. Code:. ```; from os import path; import numpy as np; import matplotlib.pyplot as plt; import scanpy as sc; import scanpy.external as sce; from os import listdir; import pandas as pd; import seaborn as sb; import datetime, time; import scvelo as scv. from matplotlib.colors import LinearSegmentedColormap. #Define a nice colour map for gene expression; from matplotlib import colors. def timestamp():; ts = time.time(); st = datetime.datetime.fromtimestamp(ts).strftime('%d-%m-%Y %H:%M:%S'); return st. # Exporting folder. folder = ""/output""; sc.settings.figdir = folder + ""Plots/""; sc.set_figure_params(vector_friendly = True, dpi=300). sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_version_and_date(); sc.logging.print_header(). ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:5400,depend,dependency,5400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependency']
Integrability,2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; - markdown==3.6; - markdown-it-py==3.0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1.6.8; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.7; - dnspython=2.4.2; - entrypoi,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:8667,depend,dependencies,8667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependencies']
Integrability,"3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 413 d[k] = read_attribute(f[k]); 414 ; 415 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/envs/rpy2_3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/rpy2_3/lib/python3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1995,message,message,1995,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['message'],['message']
Integrability,"3c4f42 NA; absl NA; astunparse 1.6.3; atomicwrites 1.4.1; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; bs4 4.11.1; certifi 2022.09.14; cffi 1.15.1; chardet 5.0.0; charset_normalizer 2.1.1; cloudpickle 2.2.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.5.1; entrypoints 0.4; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.11; import_all NA; ipykernel 6.15.3; jedi 0.18.1; joblib 1.2.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.38.1; louvain 0.7.1; lxml 4.9.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.2; numpy 1.22.4; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.4; params NA; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.10.0; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.2.1; requests 2.28.1; scipy 1.9.1; seaborn 0.12.0; session_info 1.0.0; setuptools 65.3.0; sip NA; six 1.16.0; sklearn 1.1.2; socks 1.7.1; soupsieve 2.3.2.post1; sphinxcontrib NA; spyder 5.3.3; spyder_kernels 2.3.3; spydercustomize NA; statsmodels 0.13.2; storemagic NA; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; tornado 6.2; tqdm 4.64.1; traitlets 5.4.0; typing_extensions NA; umap 0.5.3; unicodedata2 NA; urllib3 1.26.11; wcwidth 0.2.5; wrapt 1.14.1; wurlitzer 3.0.2; yaml 6.0; zipp NA; zmq 24.0.0; -----; IPython 7.33.0; jupyter_client 7.3.5; jupyter_core 4.11.1; -----; Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) [GCC 10.3.0]; Linux-5.4.0-124-generic-x86_64-with-glibc2.31; -----; Session information updated at 2022-09-16 10:24]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2330:4137,wrap,wrapt,4137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2330,1,['wrap'],['wrapt']
Integrability,"4, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2430,wrap,wrapper,2430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and ; even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:2337,message,message,2337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['message'],['message']
Integrability,"50 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 223 x_new[:, _hold:] = X[:, hold:]; 225 _, ax = plt.subplots(figsize=(1.5 * 4, 2 * 4)); 226 img = ax.imshow(; --> 227 np.array(X, dtype=np.float_),; 228 aspect=""auto"",; 229 interpolation=""nearest"",; 230 cmap=color_map,; 231 ); 232 plt.colorbar(img, shrink=0.5); 233 plt.yticks(range(X.shape[0]), var_names). ValueError: setting an array element with a sequence.; ```. Error in dpt_groups_pseudotime:. ```pytb; ValueError Traceback (most recent call last); Cell In[91], line 1; ----> 1 sc.pl.dpt_groups_pseudotime(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:276, in dpt_groups_pseudotime(adata, color_map, palette, show, save, marker); 274 """"""Plot groups and pseudotime.""""""; 275 _, (ax_grp, ax_ord) = plt.subplots(2, 1); --> 276 timeseries_subplot(; 277 adata.obs[""dpt_groups""].cat.codes,; 278 time=adata.obs[""dpt_order""].values,; 279 color=np.asarray(adata.obs[""dpt_groups""]),; 280 highlights_x=adata.uns[""dpt_changepoints""],; 281 ylabel=""dpt groups"",; 282 yticks=(; 283 np.arange(len(adata.obs[""dpt_groups""].cat.categories), dtype=int); 284 if len(adata.obs[""dpt_groups""].cat.categories) < 5; 285 else None; 286 ),; 287 palette=palette,; 288 ax=ax_grp,; 289 marker=marker,; 290 ); 291 timeseries_subplot(; 292 adata.obs[""dpt_pseudotime""].values,; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:2928,wrap,wraps,2928,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wraps']
Integrability,"7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; keras 2.9.0; kiwisolver 1.4.3; llvmlite 0.38.1; magic 3.0.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.1; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pcurve NA; pexpect 4.8.0; phate 1.0.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pygsp 0.5.1; pyparsing 3.0.9; pytz 2022.1; requests 2.28.0; s_gd2 1.8; scipy 1.8.1; scprep 1.2.0; seaborn 0.11.2; session_info 1.0.0; setuptools 62.6.0; six 1.16.0; sklearn 1.1.1; slingshot NA; sparse 0.13.0; stack_data 0.3.0; statsmodels 0.13.2; swig_runtime_data4 NA; tasklogger 1.1.2; tensorboard 2.9.1; tensorflow 2.9.1; termcolor 1.1.0; threadpoolctl 3.1.0; tlz 0.11.2; toolz 0.11.2; torch 1.11.0; tornado 6.1; tqdm 4.64.0; traitlets 5.3.0; typing_extensions NA; unicodedata2 NA; urllib3 1.26.9; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 23.1.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; notebook 6.4.12; -----; Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:59) [GCC 10.3.0]; Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31; -----; Session information updated at 2022-06-28 16:19; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:12324,wrap,wrapt,12324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wrapt']
Integrability,"72 | 2.049016 | 0.0 | 0.0; NaN | 66.620399 | 1.637823 | 0.0 | 0.0; NaN | 66.236443 | 1.807056 | 0.0 | 0.0; NaN | 64.112152 | 2.446921 | 0.0 | 0.0; NaN | 64.083160 | 2.495992 | 0.0 | 0.0; HBA1 | 63.376114 | 4.161097 | 0.0 | 0.0; NaN | 63.009491 | 2.059168 | 0.0 | 0.0; NaN | 58.142750 | 2.201216 | 0.0 | 0. ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appdirs 1.4.4; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; colorama 0.4.3; colorlog NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; sca NA; scanpy 1.7.0; scipy 1.6.1; scprep 1.0.5.post2; seaborn 0.10.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; texttable 1.6.2; threadpoolctl 2.1.0; tornado 6.0.4; traitlets 4.3.3; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1748:2766,wrap,wrapt,2766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1748,1,['wrap'],['wrapt']
Integrability,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:2076,integrat,integrated,2076,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['integrat'],['integrated']
Integrability,"92 reverse=True)):. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:471, in Axes3D.draw.<locals>.do_3d_projection(artist); 458 """"""; 459 Call `do_3d_projection` on an *artist*, and warn if passing; 460 *renderer*.; (...); 464 *renderer* and raise a warning.; 465 """"""; 467 if artist.__module__ == 'mpl_toolkits.mplot3d.art3d':; 468 # Our 3D Artists have deprecated the renderer parameter, so; 469 # avoid passing it to them; call this directly once the; 470 # deprecation has expired.; --> 471 return artist.do_3d_projection(); 473 _api.warn_deprecated(; 474 ""3.4"",; 475 message=""The 'renderer' parameter of ""; 476 ""do_3d_projection() was deprecated in Matplotlib ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; an",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:9429,wrap,wrapper,9429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wrapper']
Integrability,"99 dendro_info = self.adata.uns[key]; 900 if self.groupby != dendro_info[""groupby""]:. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_anndata.py:2384, in _get_dendrogram_key(adata, dendrogram_key, groupby); 2377 from ..tools._dendrogram import dendrogram; 2379 logg.warning(; 2380 f""dendrogram data not found (using key={dendrogram_key}). ""; 2381 ""Running `sc.tl.dendrogram` with default parameters. For fine ""; 2382 ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; 2383 ); -> 2384 dendrogram(adata, groupby, key_added=dendrogram_key); 2386 if ""dendrogram_info"" not in adata.uns[dendrogram_key]:; 2387 raise ValueError(; 2388 f""The given dendrogram key ({dendrogram_key!r}) does not contain ""; 2389 ""valid dendrogram information.""; 2390 ). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/tools/_dendrogram.py:121, in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 25 @old_positionals(; 26 ""n_pcs"",; 27 ""use_rep"",; (...); 49 inplace: bool = True,; 50 ) -> dict[str, Any] | None:; 51 """"""\; 52 Computes a hierarchical clustering for the given `groupby` categories.; 53 ; (...); 118 >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True); 119 """"""; --> 121 raise_not_implemented_error_if_backed_type(adata.X, ""dendrogram""); 122 if isinstance(groupby, str):; 123 # if not a list, turn i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:4322,wrap,wrapper,4322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wrapper']
Integrability,::test_sparse_nanmean - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_sparse_vs_dense - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_score_genes_deplete - AttributeError: 'csr_matrix' object has no attribute 'A'; FAILED scanpy/tests/test_score_genes.py::test_npnanmean_vs_sparsemean - AttributeError: 'csr_matrix' object has no attribute 'A'; ```. ### Minimal code sample. ```python; pip install scipy==1.14.0rc1; pytest; ```. ### Error output. _No response_. ### Versions. <details>. ```; + anndata==0.11.0.dev116+g1bff5fb (from git+https://github.com/scverse/anndata@1bff5fbf0894185c0759b61d78c6df66d6dfeeba); + annoy==1.17.3; + anyio==4.4.0; + array-api-compat==1.7.1; + pillow==10.3.0; + platformdirs==4.2.2; + pluggy==1.5.0; + pre-commit==3.7.1; + profimp==0.1.0; + psutil==5.9.8; + pyarrow==16.1.0; + pygments==2.18.0; + pygsp==0.5.1; + pynndescent==0.5.12; + pyparsing==3.1.2; + pytest==8.2.1; + pytest-cov==5.0.0; + pytest-memray==1.6.0; + pytest-mock==3.14.0; + pytest-nunit==1.0.7; + pytest-xdist==3.6.1; + python-dateutil==2.9.0.post0; + pytz==2024.1; + pyyaml==6.0.1; + rich==13.7.1; + scanorama==1.7.4; + scanpy==1.10.2.dev25+gf5a62eee (from file:///home/vsts/work/1/s); + scikit-image==0.23.2; + scikit-learn==1.5.0; + scikit-misc==0.3.1; + scipy==1.14.0rc1; + scprep==1.1.0; + seaborn==0.13.2; + session-info==1.0.0; + setuptools==70.0.0; + setuptools-scm==8.1.0; + six==1.16.0; + sniffio==1.3.1; + sortedcontainers==2.4.0; + sparse==0.16.0a7; + statsmodels==0.14.2; + stdlib-list==0.10.0; + tasklogger==1.2.0; + tblib==3.0.0; + texttable==1.7.0; + textual==0.63.6; + threadpoolctl==3.5.0; + tifffile==2024.5.22; + toolz==0.12.1; + tornado==6.4; + tqdm==4.66.4; + typing-extensions==4.12.0; + tzdata==2024.1; + uc-micro-py==1.0.3; + umap-learn==0.5.6; + urllib3==2.2.1; + virtualenv==20.26.2; + wrapt==1.16.0; + zarr==2.18.2; + zict==3.0.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3083:3104,wrap,wrapt,3104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3083,1,['wrap'],['wrapt']
Integrability,":; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); 270 ; 271 if swmr_support:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 97 if swmr and swmr_support:; 98 flags |= h5f.ACC_SWMR_READ; ---> 99 fid = h5f.open(name, flags, fapl=fapl); 100 elif mode == 'r+':; 101 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5f.pyx in h5py.h5f.open(). OSError: Unable to open file (truncated file: eof = 1241513984, sblock->base_addr = 0, stored_eof = 14011376022); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:2839,wrap,wrapper,2839,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,2,['wrap'],['wrapper']
Integrability,; - freetype=2.12.1; - fribidi=1.0.10; - get-annotations=0.1.2; - gettext=0.21.1; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.2; - gnutls=3.7.8; - graphite2=1.3.13; - h11=0.14.0; - h2=4.1.0; - h5py=3.9.0; - harfbuzz=7.3.0; - hdf5=1.14.1; - hpack=4.0.0; - httpcore=0.18.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.4; - igraph=0.10.8; - importlib-metadata=6.8.0; - importlib_metadata=6.8.0; - importlib_resources=6.0.1; - ipykernel=6.25.1; - ipython=8.14.0; - isoduration=20.11.0; - jedi=0.19.0; - jinja2=3.1.2; - joblib=1.3.2; - jpeg=9e; - json5=0.9.14; - jsonpointer=2.0; - jsonschema=4.19.0; - jsonschema-specifications=2023.7.1; - jsonschema-with-format-nongpl=4.19.0; - jupyter-lsp=2.2.0; - jupyter_client=8.3.0; - jupyter_core=5.3.1; - jupyter_events=0.7.0; - jupyter_server=2.7.1; - jupyter_server_terminals=0.4.4; - jupyterlab=4.0.5; - jupyterlab_pygments=0.2.2; - jupyterlab_server=2.24.0; - kaleido-core=0.2.1; - kiwisolver=1.4.4; - krb5=1.21.2; - lame=3.100; - lcms2=2.15; - legacy-api-wrap=1.4; - leidenalg=0.10.2; - lerc=4.0.0; - libabseil=20240116.2; - libaec=1.0.6; - libass=0.17.1; - libblas=3.9.0; - libbrotlicommon=1.0.9; - libbrotlidec=1.0.9; - libbrotlienc=1.0.9; - libcblas=3.9.0; - libcurl=8.2.1; - libcxx=16.0.6; - libdeflate=1.17; - libedit=3.1.20191231; - libev=4.33; - libexpat=2.5.0; - libffi=3.4.2; - libgfortran=5.0.0; - libgfortran5=12.3.0; - libglib=2.80.0; - libhwloc=2.9.3; - libiconv=1.17; - libidn2=2.3.4; - libintl=0.22.5; - libjpeg-turbo=2.1.4; - liblapack=3.9.0; - libleidenalg=0.11.1; - libllvm14=14.0.6; - libnghttp2=1.52.0; - libopenblas=0.3.23; - libopus=1.3.1; - libpng=1.6.39; - libprotobuf=4.25.3; - libsodium=1.0.18; - libsqlite=3.42.0; - libssh2=1.11.0; - libtasn1=4.19.0; - libtiff=4.5.0; - libunistring=0.9.10; - libuv=1.48.0; - libvpx=1.13.0; - libwebp-base=1.3.1; - libxcb=1.13; - libxml2=2.11.6; - libzlib=1.2.13; - llvm-openmp=16.0.6; - llvmlite=0.40.1; - lz4-c=1.9.4; - markupsafe=2.1.3; - mathjax=2.7.7; - matplotlib=3.7.2; - matplotlib-,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:10978,wrap,wrap,10978,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrap']
Integrability,; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess 0.7.0; pure-eval 0.2.2; pyarrow 14.0.2; pyarrow-hotfix 0.6; pycparser 2.21; pyct 0.5.0; pydantic 1.10.14; pyee 8.1.0; Pygments 2.17.2; pylibcugraph 24.2.0; pylibraft 24.2.0; pynndescent 0.5.11; pynvml 11.4.1; pyparsing 3.1.2; pyppeteer 1.0.2; pyproj 3.6.1; PySocks 1.7.1; python-dateutil 2.9.0; python-json-logger 2.0.7; pytometry 0.1.4; pytz 2024.1; pyviz_comms 3.0.1; PyWavelets 1.4.1; PyYAML 6.0.1; pyzmq 25.1.2; raft-dask 24.2.0; rapids_singlecell 0.9.6; readfcs 1.1.7; referencing 0.33.0; requests 2.31.0; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.7.1; rmm 24.2.0; rpds-py 0.18.0; Rtree 1.2.0; scanpy 1.10.0rc2; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; seaborn 0.13.2; Send2Trash 1.8.2; session-info 1.0.0; setuptools 69.1.1; shapely 2.0.3; simpervisor 1.0.0; single_cell_helper 0.0.1 ; six 1.16.0; sniffio 1.3.1; sortedcontainers 2.4.0; soupsieve 2.5; stack-data 0.6.2; statsmodels 0.14.1; stdlib-list 0.10.0; streamz 0.6.4; tblib 3.0.0; terminado 0.18.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tinycss2 1.2.1; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; traitlets 5.14.1; treelite 4.0.0; types-python-dateutil 2.8.19.20240311; typing_extensions 4.10.0; typing-utils 0.1.0; uc-micro-py 1.0.3; ucx-py 0.36.0; umap-learn 0.5.5; unicodedata2 15.1.0; uri-template 1.3.0; urllib3 1.26.18; wcwidth 0.2.13; webcolors 1.13; webencodings 0.5.1; websocket-client 1.7.0; websockets 10.4; wget 3.2; wheel 0.42.0; widgetsnbextension 4.0.10; wrapt 1.16.0; xarray 2024.2.0; xgboost 2.0.3; xyzservices 2023.10.1; yarl 1.9.4; zarr 2.17.1; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:6366,wrap,wrapt,6366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['wrap'],['wrapt']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->. After I use function. ```py; adata = sc.read_visium(; './', count_file='V1_Human_Lymph_Node_filtered_feature_bc_matrix.h5',; genome=None, library_id=None, load_images=True,; ); ```. I use `sc.pp.calculate_qc_metrics(adata, qc_vars=[""mt""], inplace=True)` and got an error message:. ```pytb; RuntimeError Traceback (most recent call last); ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 744 try:; --> 745 yield; 746 except NumbaError as e:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 485 if isinstance(inst, _class):; --> 486 func(self, inst); 487 return. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _lower_parfor_parallel(lowerer, parfor); 239 lowerer, parfor, typemap, typingctx, targetctx, flags, {},; --> 240 bool(alias_map), index_var_typ, parfor.races); 241 finally:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\parfors\parfor_lowering.py in _create_gufunc_for_parfor_body(lowerer, parfor, typemap, typingctx, targetctx, flags, locals, has_aliases, index_var_typ, races); 1326 flags,; -> 1327 locals); 1328 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(typingctx, targetctx, func_ir, args, return_type, flags, locals, lifted, lifted_from, is_lifted_loop, library, pipeline_class); 666 return pipeline.compile_ir(func_ir=func_ir, lifted=lifted,; --> 667 lifted_from=lifted_from); 668 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler.py in compile_ir(self, func_ir, lifted, lifted_from); 348 FixupArgs().run_pass(self.state); --> 349 return self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:345,message,message,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['message'],['message']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; Hi, I was trying to apply the SAM integration on a merged dataset of three example dataset from 10x homepage, in an attempt to compare the result to the PAGA and bbknn integrated umap.; I got a successful run on one of the PBMC dataset with no reprocessing. However in any other case I kept bump into an error:. TypeError: some keyword arguments unexpected. Here is the record. On the other hand, if I want to integrate bbknn with SAM, do I just apply bbknn after the run of SAM like this?. ////; import scanpy.external as sce. sam_obj = sce.tl.sam(adata); sc.pl.umap(sam_obj, color='Sample') . bbknn.bbknn(adata,batch_key='Sample'); #does this change the umap? or do I need to make another call of tl.umap?. sc.pl.umap(sam_obj, color='Sample') ; ////; i. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. import scanpy.external as sce; for adata in adatalist:; sam_obj = sce.tl.sam(adata); sce.pl.sam(adata,projection='X_umap'). <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ... Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.016564277393631113; Iteration: 1, Convergence: 0.01278454723440345; Computing the UMAP embedding...; Elapsed time: 50.534051179885864 seconds; Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.022868878389371346. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-17-4514ae92b370> in <module>; 1 import scanpy.external as sce; 2 for adata in adatalist:; ----> 3 sam_obj = sce.tl.sam(adata); 4 sce.pl.sam(adata,projection='X_umap'). ~/.local/lib/python3.7/site-packages/scanpy/external/tl/_sam.py in sam(adata, max_iter, num_norm_avg, k, distance, standardization, weight_pcs, sparse_pca, n_pcs, n_genes, projection, inplace, verbo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157:108,integrat,integration,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157,3,['integrat'],"['integrate', 'integrated', 'integration']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I am trying to integrate data using ingest. I followed exactly the same steps and scripts as described in the scanpy tutorial. However, I got the error message that ‘UMAP’ object has no attribute ‘_input_distance_func’ every time when I ran the command of sc.tl.ingest(adata, adata_ref, obs=‘louvain’). I have the same problem even with using the example pbmc3k_processed and pbmc3k_processed as in the tutorial. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(); var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]; sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref). # problem occurs here; sc.tl.ingest(adata, adata_ref, obs='louvain'); ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; AttributeError Traceback (most recent call last); <ipython-input-12-27e22cc8f823> in <module>(); ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). 3 frames; /usr/local/lib/python3.6/dist-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.2 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.0 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181:89,integrat,integrate,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181,2,"['integrat', 'message']","['integrate', 'message']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I'm using the ""scvelo"" https://scvelo.readthedocs.io/getting_started.html for scRNA data analysis. It underlying called ""scanpy"" function ""umap"" for calculating the coordinates. I tried the release version ""scanpy-1.4.4.post1-py_0"". It can not be imported to Python. Error message: ""ImportError: cannot import name '_Metric' from 'scanpy.neighbors' (/Users/shuzhe/anaconda3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py)"". I finally switch to the developing version for ""scanpy"". When I run ""umap"", it gives me the error message below. I'm wondering what the "".obsp"" is and how it is generated.; ; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; scv.tl.umap(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-391fc8667646> in <module>; ----> 1 scv.tl.umap(adata). ~/scanpy/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 125 start = logg.info('computing UMAP'); 126 ; --> 127 neighbors = NeighborsView(adata, neighbors_key); 128 ; 129 if ('params' not in neighbors. ~/scanpy/scanpy/_utils.py in __init__(self, adata, key); 667 self._dists_key = self._neighbors_dict['distances_key']; 668 ; --> 669 if self._conns_key in adata.obsp:; 670 self._connectivities = adata.obsp[self._conns_key]; 671 if self._dists_key in adata.obsp:. AttributeError: 'AnnData' object has no attribute 'obsp'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.7.dev26+gc255fa10 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.21.2 statsmodels==0.11.0 python-igraph==0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125:347,message,message,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125,2,['message'],['message']
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; I've had hard time in figuring this out. This is not a problem of scanpy directly but apparently is related to [scikit-learn 0.21 series](https://github.com/scikit-learn/scikit-learn/issues/14485) which is a dependency of latest scanpy version (1.4.6). Also related to [this comment in pytorch](https://github.com/pytorch/pytorch/issues/2575#issuecomment-523657178). My issue is that I'm using, in addition to scanpy, another library performing a dl_import with static TLS. ; So if I issue; ```python; import scanpy as sc; import graph_tool.all as gt; ```; I get. ```python; ImportError: dlopen: cannot load any more object with static TLS ; ```; error and I'm not able to use the second library. Reversing the order of the imports raises the same error and I'm not able to use `scanpy`. The issue is solved installing scikit-learn 0.20.4 (the last of 0.20 series). What are the exact scikit-learn 0.21.2 dependecies in scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:282,depend,dependency,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,2,['depend'],"['dependecies', 'dependency']"
Integrability,"<!-- Please give a clear and concise description of what the bug is: -->; `wx` appears to be a missing scanpy dependancy linked to matplotlib when installing on macOS. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; >>> import scanpy as sc; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:110,depend,dependancy,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['depend'],['dependancy']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:577,wrap,wrap,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['wrap'],['wrap']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:474,integrat,integrating,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,2,['integrat'],"['integrated', 'integrating']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. PyMDE is a nice visualization method that to me seems to effectively serve the same purpose as UMAP in analyses (discussion about appropriateness of these methods can be in another issue :) ). It's super fast because after running pynndescent it puts the graph on the GPU optionally (using pytorch). I would love to see this in scanpy. There might be a way to use the scanpy neighbors graph from `sc.pp.neighbors` directly in pymde as the function below is a wrapper of some internal classes. <img width=""2431"" alt=""Screen Shot 2022-02-24 at 12 38 25 PM"" src=""https://user-images.githubusercontent.com/10859440/155603698-7f0e975e-2f4b-4a95-97eb-f119522c2510.png"">",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154:928,wrap,wrapper,928,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154,1,['wrap'],['wrapper']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Adding multiplex community detection from Leiden: https://leidenalg.readthedocs.io/en/stable/multiplex.html#layer-multiplex. It seems very straightforward and would be the most simple way to integrate two modalities on the graph. We would make great use of it in Squidpy (rna counts+image), but I think it should live in Scanpy becasue it could be useful for other multi-modal data. This is a duplicate of #1107 and it has been extensively discussed in #1117 . In the latter however, lots of thought went into normalization/processing which is superfluous for this case as it is only specific for CITE-seq data. Here we'd just want to allow users to get partitions out of multiple graphs. This could be done in two ways:; - adding arguments to existing `tl.leiden`, so that it accepts multiple graphs and multiple resolutions params per graph.; - creating a separate function `sc.tl.leiden_multiplex`.; Any thoughts on this @ivirshup @Koncopd ?. I think @WeilerP also had some thoughts along these lines. Have you ever tried this out? is there any other analysis tool you explored with a simlar purpose? Would be interested to hear your thoughts!; worth mentioning that another approach, the WNN from seurat, was also mentioned here: https://github.com/theislab/scanpy/pull/1117#issuecomment-777020580; although am not sure how much work that requries.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818:660,integrat,integrate,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818,1,['integrat'],['integrate']
Integrability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; We are increasingly using ProjectR as a transfer learning technique from one dataset onto others. It would be great if this were part of the scanpy package, since most of the rest of what we do with single-cell uses scanpy. I'm going to start working on integration of the two but doubt I have the data science experience currently to submit a PR to this project for it. Are there any plans already in the works to pull in transfer-learning techniques such as this?. Relevant links:. - [Article in Bioinformatics](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btaa183/5804979?redirectedFrom=PDF); - [Article in Cell](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30146-2.pdf); - [GitHub](https://github.com/genesofeve/projectR)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1205:723,integrat,integration,723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1205,1,['integrat'],['integration']
Integrability,<!-- What kind of feature would you like to request? -->; - [X ] Additional function parameters / changed functionality / changed defaults?; <!-- Please describe your wishes below: -->; Could the function add a boolean parameter to make it work for non-log transformed data?. if [boolean depending on whether data is log transformed or not]:; foldchanges = (self.expm1_func(mean_group) + 1e-9) / (self.expm1_func(mean_rest) + 1e-9); else:; foldchanges = (mean_group+ 1e-9) / (mean_rest + 1e-9),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1454:288,depend,depending,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1454,1,['depend'],['depending']
Integrability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?; - [X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [X] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; t-SNE is an interative algorithm, and takes numerous iterations to converge, particularly on larger datasets. For example, if MulticoreTSNE is installed, it accepts `n_iter=30000` as opposed to the default `n_iter=1000`. It would be nice to have this parameter exposed. For larger datasets of say 200K cells, 1000 iterations isn't enough to fully converge to its final compact cluster shapes. . Alternatively, is it possible to pass in a kwargs to scanpy tools that wrap other algorithms, so that the advanced user can flexibly look up [additional MulticoreTSNE parameters](https://github.com/DmitryUlyanov/Multicore-TSNE/blob/62dedde52469f3a0aeb22fdd7bce2538f17f77ef/MulticoreTSNE/__init__.py#L55) to modify, without needing to exhaustively enumerate all parameters in the scanpy wrapper?. Finally, it would be even better to have the faster FFT-based tsne to generalize to millions of cells, the most recent re-implementation being https://github.com/pavlin-policar/openTSNE. In the mean time, one has to overwrite the `.X_tsne` attribute after running these other tools separately.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1150:935,wrap,wrap,935,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1150,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. When we visualize gene expression, range of colorbar can be different. ![image](https://user-images.githubusercontent.com/30337666/89102749-a6d10b00-d43e-11ea-99f7-0867f3c31d13.png). Usually, it's fine. But if we want to compare gene expression in control-treat experiment in single-cell level, sometimes this happens. ![image](https://user-images.githubusercontent.com/30337666/89102998-08927480-d441-11ea-9068-3d7f09ab26a3.png)![image](https://user-images.githubusercontent.com/30337666/89103003-1942ea80-d441-11ea-8a6f-091a6cd2f19b.png). It's hard for us to estimate up or down regulation of this gene in different group of cells because colors with same value in the two figures are not consistent. Plotting the two figures with same colorbar range can solve it. So, I want to know that are there parameters solving it in scanpy.pl.umap or some matplotlib methods that can be integrated with Scanpy?. Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1352:1349,integrat,integrated,1349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1352,1,['integrat'],['integrated']
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ... Hi!. We're considering implementing some of the t-SNE recommendations in https://www.nature.com/articles/s41467-019-13056-x for our single-cell analysis. They use a different t-SNE implementation (https://github.com/KlugerLab/FIt-SNE), and before I ran off doing my own wrapping and plumbing to integrate with Scanpy I thought I'd check: have you considered integrating this yourselves?. Thanks!. Jon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/996:743,wrap,wrapping,743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/996,3,"['integrat', 'wrap']","['integrate', 'integrating', 'wrapping']"
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; Hi scanpy develovepers,. A rotation student asked me what is `sc.pl.umap` showing if `sc.tl.umap` was not computed beforehand. To which I don't have the answer since I have never done it. If you know the answer I'd like to know it, but most importantly, I think it would be nice to have an error message in the UMAP plotting function if UMAP has not been computed. Unless there were meaning and a reason to use `sc.pl.umap` without running `sc.tl.umap` previously, and it was designed that way purposely. I assume this would apply to other plotting functions too. Thanks!; Alejandro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1460:770,message,message,770,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460,1,['message'],['message']
Integrability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:653,integrat,integrative,653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,1,['integrat'],['integrative']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. - Depends on #2814. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2777, closes #2807; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] batched. <table>; <thead>; <tr>; <th scope=row>. `flavor=`. <th>. `""seurat""`. <th>. `""cell_ranger""`. <tbody>; <tr>; <th scope=row>. `n_top_genes=n`. <td>. - [x] &zwnj;. <td>. - [x] (https://github.com/dask/dask/issues/10853). <tr>; <th scope=row>. `{min,max}_{disp,mean}`. <td>. - [x] &zwnj;. <td>. - [x] &zwnj;. </table>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809:236,Depend,Depends,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809,1,['Depend'],['Depends']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Only check the following box if you did not include release notes -->. ## TODO:. - [x] Fix tests; - [x] Figure out PCA test case with anndata 0.8.0; - [x] Add CI job; - [x] Rename CI job to be less similar to minimal dependencies, this will probably be `MinVer`; - [x] Bump anndata requirement back down to 0.7.3 (breaks dask tests); - Maybe 0.8 is low enough?; - [x] Bump pandas requirement back down to 1.5 (breaks grouped plots ordering). ## Some thoughts. * Sibling PR to: https://github.com/scverse/anndata/pull/1314; * Not completley sure what to do about plotting tests yet. Possible we just ignore any comparison failures, but ideally we could still know if these are broken.; * Metric consistency test failure is from https://github.com/scverse/scanpy/issues/2688; * Test updates in https://github.com/scverse/scanpy/pull/2705 (plus bumping one test a little lower) fixes it. <!-- Please check (“- [x]”) and fill in the following boxes -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816:456,depend,dependencies,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816,1,['depend'],['dependencies']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Adresses #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:540,depend,depending,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['depend'],['depending']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3226; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263:652,integrat,integrated,652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263,1,['integrat'],['integrated']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because: dev process; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process. Filt resulted in the release workflow failing, as it tries to install the package’s runtime dependencies. Backporting the switch to hatch fixes that, now building only needs build deps",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2727:600,depend,dependencies,600,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2727,1,['depend'],['dependencies']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: docs change. Also bump scanpydoc so the `[source]` links for wrapped functions work. E.g. `sc.pp.filter_cells` now links to the correct code lines):. ```diff; -<a href=""https://github.com/scverse/scanpy/tree/419c1a45aef26b5531a5b9cf1ec430e5ae67ce97/python3.11/site-packages/legacy_api_wrap/__init__.py#L49-L193"">[source]</a>; +<a href=""https://github.com/scverse/scanpy/tree/2d5bda1e45525354b9b751aa572c0b08175450cf/scanpy/preprocessing/_simple.py#L49-L193"">[source]</a>; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2800:544,wrap,wrapped,544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2800,1,['wrap'],['wrapped']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes N/A; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Since today, there are some test breakages:. - `adata[:, [True, True]]` now behaves like `adata[:, np.array([1, 1])]` instead of `adata[:, np.array([True, True])]` (exposed through `read_10x_mtx`); - `sc.pl.violin` now creates slightly wider plots through some dependency change. This fixes everything that broke.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2801:720,depend,dependency,720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2801,1,['depend'],['dependency']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes https://github.com/scverse/scanpy/issues/2763; - [x] Tests included or not required because: manually checked using rtd PR build; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Adds [readthedocs-sphinx-search](https://readthedocs-sphinx-search.readthedocs.io/en/latest/) support via the scanpydoc theme, which contains JS and CSS customizations to make the search extension integrate with the theme. See. - https://github.com/theislab/scanpydoc/pull/121; - https://github.com/theislab/scanpydoc/pull/125. ### [rendered](https://icb-scanpy--2805.com.readthedocs.build/en/2805/). An alternative that looks nicer would be https://github.com/readthedocs/addons, but it’s still in alpha. PS: I didn’t add the same hack as in scanpydoc that makes the search work in PR builds, so you’ll only see “No results found” in the above. Check out https://icb-scanpydoc.readthedocs-hosted.com/en/latest/?rtd_search=scanpydoc to see rendered search results. You can see that the API works for scanpy:. ```console; $ http get https://icb-scanpy.readthedocs-hosted.com/_/api/v3/search/?q=project%3Aicb-scanpy%2Flatest+filter_cells; ╭──────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ count │ 2 │; │ next │ │; │ previous │ │; │ │ ╭───┬────────────┬────────────────╮ │; │ projects │ │ # │ slug │ versions │ │; │ │ ├───┼────────────┼────────────────┤ │; │ │ │ 0 │ icb-scanpy │ ╭───┬────────╮ │ │;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2805:759,integrat,integrate,759,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2805,1,['integrat'],['integrate']
Integrability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; This PR fixes the case when `use_raw=None` in `scanpy.tl.score_genes`. It causes to first fetch `var_names` from `adata.var_names`, but later a subset on `adata.raw` can happen, which can have different gene names.; Also fixes the type of `use_raw` and adds a `ValueError` if `gene_pool` is empty (otherwise, crashes with non-informative error message). related issue: https://github.com/theislab/cellrank/issues/746",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1999:578,message,message,578,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1999,1,['message'],['message']
Integrability,"@dkobak @ivirshup @Koncopd This is a first stab #1233. Features. - [X] Construct t-SNE embeddings; - [ ] Recipes; - [ ] Ingest functionality. As discussed, this PR currently implements t-SNE with uniform affinity kernels, making it fit in nicely with the existing `sc.pp.neighbors` architecture. While this isn't technically t-SNE per se, it's visually almost impossible to tell them apart. It would also make sense to add a `tsne` option to `sc.pp.neighbors`, but from what I can tell, there's no direct way to change the existing code to do this. It looks like `sc.pp.neighbors` calls UMAP to calculate the nearest neighbors directly, calculating the UMAP weights. We'd probably have to do something similar to the `gauss` option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. I like the API of calling `sc.tl.tsne.recipe_X(adata)`. Adding the recipes would be simple here; we can just add a simple class wrapper around `_tsne` which which would `__call__` tsne, and a bunch of static methods to the wrapper for recipes. This is kind-of messy and probably not something you guys do anywhere else throughout the code base, so I'd appreciate your feedback on this. Do you like this, or should we do it in a different way?. The `ingest` functionality should be fairly straightforward as well, just adding a `tsne` option to `embedding_method`. All of these things should be pretty easy to do, but I'd like to see that this is moving in a direction you like first.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561:941,wrap,wrapper,941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561,2,['wrap'],['wrapper']
Integrability,@falexwolf ; I'm adding a wrapper for Palantir by [Setty et al. (2018)](https://doi.org/10.1101/385328); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493:26,wrap,wrapper,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493,1,['wrap'],['wrapper']
Integrability,@falexwolf; I'm adding a wrapper for Harmony by [Setty et al. (2018)](https://doi.org/10.1101/471078); Please let me know if you have any comments,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/503:25,wrap,wrapper,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/503,1,['wrap'],['wrapper']
Integrability,"@fidelram . As noted in #1632, dotplot labels can often extend outside of the plotted area. Whether the full labels can be seen is dependent on how the plots are being output. It would be great if this always worked. <details>; <summary> Example from the docs: </summary>. ![](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). </details>. Could possibly be solved by using `matplotlib`'s `constrained_layout` or `tight_layout`. I think these would require modifying how the grid spec and axes are created.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:131,depend,dependent,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['depend'],['dependent']
Integrability,"AC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1441,wrap,wrapper,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wrapper']
Integrability,"AFAIK networkx and python-igraph do the same thing, only that python-igraph is faster. We also need python-igraph anyway for louvain and so on, so maybe it would be good to get rid of networkx. Downside: python-igraph and louvain-igraph is currently deliberately an optional dependency since it’s hard to install on windows. People need to build it themselves (A task that even I didn’t manage by now, and I got *many* things to compile!) or use Christoph Grohlke’s unofficial builds ([here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph) and [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#louvain-igraph))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97:275,depend,dependency,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97,1,['depend'],['dependency']
Integrability,"According to [scanorama MemoryError](https://github.com/brianhie/scanorama/blob/master/scanorama/scanorama.py#L768-L775), I think `batch_size` parameter doesn't work for `sc.external.pp.scanorama_integrate`. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.external.pp.scanorama_integrate(adata, key=""datasetID"", batch_size=500); ```. ```pytb; WARNING: Out of memory, consider turning on batched computation with batch_size parameter.; Traceback (most recent call last):; File ""/lustre1/shiq//02_igt/py/1_igt_scanorama.py"", line 26, in <module>; sc.external.pp.scanorama_integrate(adata, key=""datasetID"", batch_size=500); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/external/pp/_scanorama_integrate.py"", line 121, in scanorama_integrate; integrated = scanorama.assemble(; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 933, in assemble; bias = transform(curr_ds, curr_ref, ds_ind, ref_ind, sigma=sigma,; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 762, in transform; avg_bias = batch_bias(curr_ds, match_ds, bias, sigma=sigma,; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanorama/scanorama.py"", line 723, in batch_bias; weights = rbf_kernel(curr_ds, match_ds, gamma=0.5*sigma); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/metrics/pairwise.py"", line 1294, in rbf_kernel; X, Y = check_pairwise_arrays(X, Y); File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/metrics/pairwise.py"", line 155, in check_pairwise_arrays; X = check_array(; File ""/lustre1/shiq/app/miniconda3/envs/scanpy/lib/python3.10/site-packages/sklearn/utils/validation.py"", line 727, in check_array; warnings.warn(; FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2319:816,integrat,integrated,816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2319,1,['integrat'],['integrated']
Integrability,Add CI job with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2222:24,depend,dependencies,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2222,1,['depend'],['dependencies']
Integrability,Add CI run with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211:24,depend,dependencies,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211,1,['depend'],['dependencies']
Integrability,Add DCA integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/186:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186,1,['integrat'],['integration']
Integrability,Add PHATE integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:10,integrat,integration,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,1,['integrat'],['integration']
Integrability,Add Scanorama integration to external API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332:14,integrat,integration,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332,1,['integrat'],['integration']
Integrability,Add fsspec as a test dependency due to dask,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/765:21,depend,dependency,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765,1,['depend'],['dependency']
Integrability,Add sparsificiation step before sparse-dependent Scrublet calls,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1707:39,depend,dependent,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1707,1,['depend'],['dependent']
Integrability,Add support for Harmony integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306:24,integrat,integration,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306,1,['integrat'],['integration']
Integrability,"Added DCA integration, as discussed in #142 . For denoising, uses can call:. `sc.pp.dca(adata)`. which replaces adata.X inplace. For latent representations:. `sc.pp.dca(adata, mode='latent')`. can be called, which adds 'X_dca' to adata.obsm. Fixes #142 . This integration uses new DCA API (>= 0.2.1). All DCA API arguments are exposed and usable in scanpy integration.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/186:10,integrat,integration,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/186,3,['integrat'],['integration']
Integrability,Added wrapper for mnnpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/131:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/131,1,['wrap'],['wrapper']
Integrability,Added wrapper for mnnpy in pp,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/131:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/131,1,['wrap'],['wrapper']
Integrability,Adding cell2location to the list of scRNA->spatial integration methods,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1574:51,integrat,integration,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574,1,['integrat'],['integration']
Integrability,"After updating to the most recent version of scanpy, I had to separately update anndata, louvain, and leidenalg packages that are dependancies. Any dependent package updates of these types should just happen when the newest version is installed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518:130,depend,dependancies,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518,2,['depend'],"['dependancies', 'dependent']"
Integrability,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194:157,message,message,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194,1,['message'],['message']
Integrability,"As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlib’s 3.1.2 milestone, so maybe we can just set the dependency to “matplotlib == 3.0.0 or matplotlib >= 3.1.2”. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/849:226,depend,dependency,226,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849,1,['depend'],['dependency']
Integrability,As it's scheduled to be removed in anndata 0.10.0. This will require a quick bug fix release. It may also require bumping the anndata dependency to >0.8.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2658:134,depend,dependency,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2658,1,['depend'],['dependency']
Integrability,"At the moment scanpy seems to be compatible with only python >=3.7,<3.11, and it took me quite long to realise that the installation problem was due to having python 3.11 which is not yet compatible with scanpy. This appears as an error when running `pip install scanpy`, but not when running `conda install -c conda-forge scanpy`. Maybe compatibility with python versions could be mentioned on the installation manual, or be displayed as an error message when installing through conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369:448,message,message,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369,1,['message'],['message']
Integrability,"At the stage of finding neighbors, my jupyter kept showing this error:; <img width=""1103"" alt=""Screen Shot 2022-10-22 at 2 51 46 PM"" src=""https://user-images.githubusercontent.com/99854950/197325988-9a22e635-43df-4461-9d22-81f160fa652b.png"">. the error:; ```; OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.; And it killed the kernel entirely. ; ```. I try to make this work by running this in Linux but it got killed again. ; <img width=""281"" alt=""Screen Shot 2022-10-22 at 3 13 47 PM"" src=""https://user-images.githubusercontent.com/99854950/197326001-b8dbd92d-332a-40c6-a9b1-e6c3c0f68a6f.png"">. Below is my basic workflow:; ```python; def pp(adata):; sc.pp.filter_cells(adata, min_genes=200) #get rid of cells with fewer than 200 genes; sc.pp.filter_genes(adata, min_cells=3) #get rid of genes that are found in fewer than 3 cells; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98); lower_lim = np.quantile(adata.obs.n_genes_by_counts.values, .02); adata = adata[(adata.obs.n_genes_by_counts < upper_lim) & (adata.obs.n_genes_by_counts > lower_lim)]; adata = adata[adata.obs.pct_counts_mt < 25]; sc.pp.normalize_total(adata, target_sum=1e4) #normalize every cell to 10,000 UMI; sc.pp.log1p(adata) #change to log counts; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:291,rout,routine,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['rout'],['routine']
Integrability,Backport PR #1608 on branch 1.7.x (Remove dependency on scvelo for doc builds),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1609:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609,1,['depend'],['dependency']
Integrability,Backport PR #1608: Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1609:26,depend,dependency,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1609,1,['depend'],['dependency']
Integrability,Backport PR #1659 on branch 1.7.x (Fix passing of arguments between scrublet routines),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1674:77,rout,routines,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1674,1,['rout'],['routines']
Integrability,Backport PR #1659: Fix passing of arguments between scrublet routines,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1674:61,rout,routines,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1674,1,['rout'],['routines']
Integrability,Backport PR #1707 on branch 1.7.x (Add sparsificiation step before sparse-dependent Scrublet calls),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1711:74,depend,dependent,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1711,1,['depend'],['dependent']
Integrability,Backport PR #1707: Add sparsificiation step before sparse-dependent Scrublet calls,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1711:58,depend,dependent,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1711,1,['depend'],['dependent']
Integrability,Backport PR #2222 on branch 1.9.x (Add CI job with minimal dependencies),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226:59,depend,dependencies,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226,1,['depend'],['dependencies']
Integrability,Backport PR #2222: Add CI job with minimal dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2226:43,depend,dependencies,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2226,1,['depend'],['dependencies']
Integrability,"Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1828,wrap,wrapper,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['wrap'],['wrapper']
Integrability,Building the docs fails ever since the version of `sphinx-autodoc-typehints` increased from v1.12.0 to v1.13.0. This PR temporarily pins this dependency's version to v1.12.0. See https://github.com/theislab/scanpy/pull/1828#issuecomment-1005072811 and cc @Zethson,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2100:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2100,1,['depend'],['dependency']
Integrability,"Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)?. **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` tak",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:693,wrap,wrapper,693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,2,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"Checks if current axis is colorbar before trying to set the name, see #2681.; This might not be the best solution and does not yet integrate a unit test. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2682:131,integrat,integrate,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2682,1,['integrat'],['integrate']
Integrability,Cluster content plot for integrated data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:25,integrat,integrated,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['integrat'],['integrated']
Integrability,"Companion PR to https://github.com/theislab/anndata/pull/434. Basically, I would like to deprecate the `dtype` argument of `AnnData._init_as_actual`, since it mostly just makes unexpected copies of `X`. Since other elements of an AnnData object are passed by reference, it makes sense for this to happen with `X` as well. Right now, this PR will fail CI. What I've done so far is remove all uses of that argument from the scanpy code base, while keeping the tests passing. I'm trying to figure out how to best preserve compatibility with older versions of `anndata`, without throwing too many warnings. I think the thing to do will be make code work with both (`AnnData(X.astype(dtype), dtype=dtype))` should only make one copy) and catch warnings. This can be removed once scanpy depends on `anndata 0.8`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1430:781,depend,depends,781,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1430,1,['depend'],['depends']
Integrability,"CompilerBase._compile_core(self); 461 res = None; 462 try:; --> 463 pm.run(self.state); 464 if self.state.cr is not None:; 465 break. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:353, in PassManager.run(self, state); 350 msg = ""Failed in %s mode pipeline (step: %s)"" % \; 351 (self.pipeline_name, pass_desc); 352 patched_exception = self._patch_error(msg, e); --> 353 raise patched_exception. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341, in PassManager.run(self, state); 339 pass_inst = _pass_registry.get(pss).pass_inst; 340 if isinstance(pass_inst, CompilerPass):; --> 341 self._runPass(idx, pass_inst, state); 342 else:; 343 raise BaseException(""Legacy pass in use""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); 32 @functools.wraps(func); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); 294 mutated |= check(pss.run_initialization, internal_state); 295 with SimpleTimer() as pass_time:; --> 296 mutated |= check(pss.run_pass, internal_state); 297 with SimpleTimer() as finalize_time:; 298 mutated |= check(pss.run_finalizer, internal_state). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/compiler_machinery.py:269, in PassManager._runPass.<locals>.check(func, compiler_state); 268 def check(func, compiler_state):; --> 269 mangled = func(compiler_state); 270 if mangled not in (True, False):; 271 msg = (""CompilerPass implementations should return True/False. ""; 272 ""CompilerPass with name '%s' did not.""). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/typed_passes.py:306, in ParforPass.run_pass(self, state); 295 assert state.fun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:7734,wrap,wraps,7734,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['wrap'],['wraps']
Integrability,Critical updates to Palantir external tool wrapper. Includes updated DocString.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1245:43,wrap,wrapper,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1245,1,['wrap'],['wrapper']
Integrability,"Currently there's only a wrapper for `scanorama.integrate_scanpy`. From https://github.com/brianhie/scanorama: . > The function integrate_scanpy() will simply add an entry into adata.obsm called 'X_scanorama' for each adata in adatas. obsm['X_scanorama'] contains the low dimensional embeddings as a result of integration, which can be used for KNN graph construction, visualization, and other downstream analysis. ; > The function correct_scanpy() is a little more involved -- it will create new AnnData objects and replace adata.X with the Scanorama-transformed cell-by-gene matrix, while keeping the other metadata in adata as well.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2323:25,wrap,wrapper,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2323,2,"['integrat', 'wrap']","['integration', 'wrapper']"
Integrability,"Currently, `sc.tl.louvain` etc return cluster assignments as a Categorical with dtype `str` resulting in incompatibility with matplotlib color sequences. For example, the following code raises a ValueError:. ```python; import numpy as np; import scanpy as sc; import matplotlib.pyplot as plt. adata = sc.AnnData(np.random.normal(size=(100,2))); sc.pp.neighbors(adata); sc.tl.louvain(adata); plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain']); ```. The error is: `ValueError: RGBA values should be within 0-1 range`. Funnily enough, this used to work due to a bug in matplotlib that was fixed in https://github.com/matplotlib/matplotlib/pull/13913. Note, the following code works as intended:. ```python; plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'].astype(int)); ```. I would have submitted a PR changing this behavior had I not noticed that returning cluster assignments as `str` is explicitly checked here:. https://github.com/theislab/scanpy/blob/78125e6355c0cd2c4ae930495829282eea6f4a52/scanpy/tools/_utils_clustering.py#L11-L23. This brings up a larger design question in scanpy / anndata: *Why are arrays of numerics routinely converted to strings representing numbers?*. In `https://github.com/theislab/anndata/issues/311` I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData `obsm` with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the `.loc` and `.iloc` conventions. Why not carry that forward?. In this case, why not just return cluster assignments as arrays of numerics as is done in `sklearn.cluster`? . I think following these conventions will make both tools much more accessible to the general Python data science community.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030:1154,rout,routinely,1154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030,1,['rout'],['routinely']
Integrability,DCA integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/142:4,integrat,integration,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142,1,['integrat'],['integration']
Integrability,Dask no longer having fsspec as a required dependency means the dask tests fail. This should fix that.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/765:43,depend,dependency,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/765,1,['depend'],['dependency']
Integrability,"Data object for each sample; 2 for sample in sample_list:; ----> 3 sample_adata = adata[:, adata.obs['sample_name'] == sample]. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1113, in AnnData.__getitem__(self, index); 1111 def __getitem__(self, index: Index) -> ""AnnData"":; 1112 """"""Returns a sliced view of the object.""""""; -> 1113 oidx, vidx = self._normalize_indices(index); 1114 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1094, in AnnData._normalize_indices(self, index); 1093 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1094 return _normalize_indices(index, self.obs_names, self.var_names). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/index.py:36, in _normalize_indices(index, names0, names1); 34 ax0, ax1 = unpack_index(index); 35 ax0 = _normalize_index(ax0, names0); ---> 36 ax1 = _normalize_index(ax1, names1); 37 return ax0, ax1. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/index.py:90, in _normalize_index(indexer, index); 88 elif issubclass(indexer.dtype.type, np.bool_):; 89 if indexer.shape != index.shape:; ---> 90 raise IndexError(; 91 f""Boolean index does not match AnnData’s shape along this ""; 92 f""dimension. Boolean index has shape {indexer.shape} while ""; 93 f""AnnData index has shape {index.shape}.""; 94 ); 95 positions = np.where(indexer)[0]; 96 return positions # np.ndarray[int]. IndexError: Boolean index does not match AnnData’s shape along this dimension. Boolean index has shape (5258,) while AnnData index has shape (17143,). ```; I would appreciate any insights. Thank you so much! ; #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. this code had a error message because scanpy wasn't defined, but when I ran . from importlib.metadata import version; version('scanpy'). I got an output: '1.9.1'. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2402:3241,message,message,3241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402,1,['message'],['message']
Integrability,"Dear author,. Can bbknn integrate multiple variables？such as Platform and Individual. Looking forward your reply; Siyu. >>> sc.external.pp.bbknn(mydata, batch_key=[""Platform"",""Individual_2""],n_pcs=50,set_op_mix_ratio=ratio). computing batch balanced neighbors; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/scanpy/external/pp/_bbknn.py"", line 134, in bbknn; return bbknn(; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/bbknn/__init__.py"", line 110, in bbknn; if batch_key not in adata.obs:; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 1721, in __contains__; return key in self._info_axis; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4071, in __contains__; hash(key); TypeError: unhashable type: 'list",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:24,integrat,integrate,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['integrat'],['integrate']
Integrability,"Dear, . Is it possible to integrate scanpy with CCA and pyscenic?; CCA (canonical correlation analysis to alignment different datasets and batch effect correction):; https://satijalab.org/seurat/immune_alignment.html. pyscenic (single-cell regulatory network inference and clustering):; https://github.com/aertslab/pySCENIC",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265:26,integrat,integrate,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265,1,['integrat'],['integrate']
Integrability,Depend on session-info2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3202:0,Depend,Depend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3202,1,['Depend'],['Depend']
Integrability,Dependencies missing in bioconda package,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:0,Depend,Dependencies,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['Depend'],['Dependencies']
Integrability,Dependency on `legacy-api-wrap` prevents 1.10 conda release,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966:0,Depend,Dependency,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966,2,"['Depend', 'wrap']","['Dependency', 'wrap']"
Integrability,"E.g. matplotlib is only necessary when plotting, and for e.g. Docker images, it would be useful to have a slim scanpy core. An idea would be to do it like Jupyter:. - A `scanpy-core` PyPI package with just the essentials.; - A `scanpy` metapackage, which depends on `scanpy-core` and most (or all) of the optional dependencies. Users doing `pip install scanpy` will get the full package, with no annoying runtime errors, and packagers needing flexibility get `scanpy-core` and can slim everything down as needed. cc @hensing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59:255,depend,depends,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59,2,['depend'],"['dependencies', 'depends']"
Integrability,Error message in tl.diffmap / why n_comps must be > 2,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/668:6,message,message,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668,1,['message'],['message']
Integrability,"External has outlived its usefulness with the emergence of the scverse ecosystem, and should be removed. ^ This text should be expanded. # TODO. - [ ] Finalize methods to keep/ merge into main namespace; - Currently: `scanorama`, `hashsolo`, `scrublet` or equivalent, maybe `bbknn`; - [ ] Make removal plan (probably deprecation message + maybe email to authors) for; - [ ] `harmony`; - [ ] `dca`; - [ ] `magic`; - [ ] `phate`; - [ ] `palantir`; - [ ] `trimap`; - [ ] `sam`; - [ ] `phenograph`; - [ ] `wishbone`; - [ ] `sandbag`; - [ ] `cyclone`; - [ ] Export functionality for; - [ ] `spring_project`; - [ ] `cellbrowser`. ```[tasklist]; ### Sub-issues; - [ ] #173; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2717:329,message,message,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2717,1,['message'],['message']
Integrability,FIt-SNE integration?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/996:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/996,1,['integrat'],['integration']
Integrability,"File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, in write_dataframe; write_elem(group, colname, series._values, dataset_kwargs=dataset_kwargs); File ""/home/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:3048,Integrat,Integrated,3048,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['Integrat'],['Integrated']
Integrability,Fix passing of arguments between scrublet routines,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1659:42,rout,routines,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659,1,['rout'],['routines']
Integrability,"Fix plotting and harmony, depend on anndata 0.7 for obsp (#1004, #1007)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1004:26,depend,depend,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1004,1,['depend'],['depend']
Integrability,"Fixes #1697. I'll leave any major backwards compatibility changes (i.e. updating to use python 3.7+ features) to the future. CI took a really long time to install the dependencies on the first run. Hopefully that will be cached? Not sure what triggers a saved cache. At the moment it looks like building `louvain` is taking a while, which we should actually just get around to deprecating.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1897:167,depend,dependencies,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1897,1,['depend'],['dependencies']
Integrability,"Fixes #435. Breaking case:. ```python; import scanpy as sc, numpy as np; sc.pp.log1p(; sc.AnnData(np.ones((100, 100)), dtype=int); ); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-f3ae2b50ac50> in <module>; ----> 1 sc.pp.log1p(a). /usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/scanpy/scanpy/preprocessing/_simple.py in log1p_anndata(adata, base, copy, chunked, chunk_size, layer, obsm); 348 else:; 349 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 350 X = log1p(X, copy=False, base=base); 351 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 352 . /usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/scanpy/scanpy/preprocessing/_simple.py in log1p_array(X, base, copy); 316 else:; 317 X = X.copy(); --> 318 np.log1p(X, out=X); 319 if base is not None:; 320 np.divide(X, np.log(base), out=X). TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1400:444,wrap,wrapper,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1400,2,['wrap'],['wrapper']
Integrability,"Fixes #993. This is an approximate implementation of the Seurat v3 hvg method. The only difference should be the use of lowess instead of loess (which is not available in python as far as I know). This method takes the UMI counts as input. The way HVGs from batches are merged is also different from the other flavors. As such, I didn't see a straightforward way to integrate this in the existing HVG code.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182:366,integrat,integrate,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182,1,['integrat'],['integrate']
Integrability,"Follow up to #2420, which let the tests run, but didn't actually install anndata-dev. Current version was quoted due to my error in reading a `pre-commit` complaint about yaml syntax. Tbf, yaml syntax is complicated, and the pre-commit check gives bad error messages. ----. Additionally apply other fixes from yesterdays bug fix release to main branch",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2421:258,message,messages,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2421,1,['message'],['messages']
Integrability,"Follow up to #703, this should take import times down another second. It does add a dependency, but that dependency will be in the stdlib from 3.8+. After this it looks like the next biggest contributors to import times are caused by `numba` (which would be hard to factor out) and `sklearn.metrics`, but these are on a smaller scale. Evidence of speedup:. ```sh; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭master ; python3 -c ""import scanpy as sc"" 3.10s user 0.41s system 116% cpu 3.010 total; isaac@Mimir ~/github/scanpy$ git checkout defer-umap ✹ ✭master ; Switched to branch 'defer-umap'; Your branch is up to date with 'origin/defer-umap'.; isaac@Mimir ~/github/scanpy$ time python3 -c ""import scanpy as sc"" ✹ ✭defer-umap ; python3 -c ""import scanpy as sc"" 2.18s user 0.42s system 131% cpu 1.981 total; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704:84,depend,dependency,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704,2,['depend'],['dependency']
Integrability,"Following up on #242. Here's my solution to the current queries being pretty unreliable for me (due to issue with bioservices module). It's all a pretty thin wrapper around `pybiomart`, which has a nice API and is well tested but has maintenance issues. . Currently I've replaced the `gene_coordinates` query with a more generic `biomart_annotations` – the example covers the functionality of `gene_coordinates`. I'm debating how to add tests given that they're network based (could fail when nothing is wrong with the code) and can take a while.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467:158,wrap,wrapper,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467,1,['wrap'],['wrapper']
Integrability,"Hej all,. I am using a backed dataset because, when I run the umap scatterplot, the RAM go pretty much crazy on our server. When I use not-backed data, I can do UMAP scatterplots without any problem apart from the memory usage. But now that the data is backed, when running the following:; ```; sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); ```. I get an error message that seems related to the `h5py` package. Here is the whole trace back. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-a78575d924b7> in <module>; 24 if len(markers) > 0:; 25 print(""Expression plots of "", names, "" markers: "", markers); ---> 26 sc.plotting.tools.scatterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selectio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:409,message,message,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['message'],['message']
Integrability,"Hej all. it seems there is a problem on the batch correction with bbknn. It gives an error at the pca step of bbknn, but I have problem understanding if this is due to the bbknn package itself or the wrapper of scanpy around it, or if it is due to my data, even though it worked when I used it previously. ```python; sc.external.pp.bbknn(all_data_flt, batch_key='batch', n_pcs=15,); ```. gives the error. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-a9dd619ada2e> in <module>; 1 #sc.neighbors.neighbors(all_data_flt, n_neighbors=40, n_pcs=15); ----> 2 sc.external.pp.bbknn(all_data_flt, n_pcs=15); 3 #sc.tools.umap(all_data_flt). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/preprocessing/_bbknn.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 82 params = locals(); 83 kwargs = params.pop('kwargs'); ---> 84 return bbknn(**params, **kwargs). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/bbknn/__init__.py in bbknn(adata, batch_key, save_knn, copy, **kwargs); 215 batch_list = adata.obs[batch_key].values; 216 #call BBKNN proper; --> 217 bbknn_out = bbknn_pca_matrix(pca=pca,batch_list=batch_list,save_knn=save_knn,**kwargs); 218 #optionally save knn_indices; 219 if save_knn:. TypeError: bbknn_pca_matrix() got an unexpected keyword argument 'bbknn'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/514:200,wrap,wrapper,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/514,1,['wrap'],['wrapper']
Integrability,"Hello everyone,; I have faced the below warning messages during running `sc.tl.rank_genes_groups` with parameters t-test and t-test overestimated. > RuntimeWarning: invalid value encountered in greater; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less; return (self.a < x) & (x < self.b). > RuntimeWarning: invalid value encountered in less_equal; cond2 = cond0 & (x <= self.a). I found exactly the same warning message in a forum but there the warning message was coming from auto-sklearn package and since there is no auto-sklearn used in scanpy and instead we got scikit-learn I am wondering is it a problem with the version that I am using or is it a bug from scikit or scanpy it self. my versions are:. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/674:48,message,messages,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674,3,['message'],"['message', 'messages']"
Integrability,"Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not.; It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me?; Maybe there are discussions recorded somewhere?. Best,; Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3272:199,depend,dependencies,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272,2,['depend'],['dependencies']
Integrability,"Hello world!; I've read in many papers that when performing a re-clustering of some populations, like T cells or B cells, prior to the step of integration and so on, they re-calculate the HVGs but excluding the TCR- or BCR-related genes, because they are donor-specific, especially when talking about BCR. Can you help me how to remove the TCR- or BCR-related genes before computing the HVGs selection, but without removing them from the .var of the anndata, since I want to evaluate their expression during the step of cell annotation?. The code that I use to calculate the HVGs is the following:; sc.pp.highly_variable_genes(adata,; n_top_genes = 4000, flavor = ""seurat_v3"",; layer = ""raw"", batch_key = 'sample_id',; subset = False). Thanks a lot!; Paolo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2895:143,integrat,integration,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2895,1,['integrat'],['integration']
Integrability,Hello! I thought it might be useful to add [Scanorama ](https://github.com/brianhie/scanorama) to the set of integration methods in scanpy.external.pp. I've followed the example laid out in https://github.com/theislab/scanpy/pull/1306 pretty closely. I've also done some small scale integration testing on a local machine just to verify that batch effect correction indeed works when called with this API. Let me know what I can do to help get this merged!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332:109,integrat,integration,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332,2,['integrat'],['integration']
Integrability,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680:264,integrat,integrating,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680,1,['integrat'],['integrating']
Integrability,"Hello, I am trying to use the visualize marker genes tutorial to make some plots. I am importing scanpy in the new way (import scanpy as sc) as suggested in the tutorial but I am getting an error message:. AttributeError Traceback (most recent call last); <ipython-input-5-dfc1e4d9ed06> in <module>(); ----> 1 ax = sc.pl.correlation_matrix(adata, 'cell_types'). AttributeError: module 'scanpy.plotting' has no attribute 'correlation_matrix'. Here are the versions of all the packages I am using:; scanpy==1.4 anndata==0.6.17 numpy==1.16.0 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . Am I missing something ?. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544:196,message,message,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544,1,['message'],['message']
Integrability,"Hello, I am unable to import scanpy and the error message shows below:. ```python; import scanpy as sc; ```. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/__init__.py"", line 6, in <module>; from ._utils import check_versions; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 29, in <module>; from .compute.is_constant import is_constant; File ""/opt/anaconda3/lib/python3.9/site-packages/scanpy/_utils/compute/is_constant.py"", line 5, in <module>; from numba import njit; ImportError: cannot import name 'njit' from 'numba' (unknown location)```; ```. Scanpy has been working well, but today it reports an issue. I have tried to uninstall and reinstall numba and scanpy, but it still did not work. Can you help? Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2438:50,message,message,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2438,1,['message'],['message']
Integrability,"Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853:436,message,message,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853,2,['message'],['message']
Integrability,"Hello,. I am having the same issue as issue #1246 but my version of scipy being used with scanpy is not updating. I don't know if this is related to my using an ubuntu server or what's causing this but I was wondering if there is a workaround to make scanpy use a more updated version? I have scipy 1.4.1 installed when I check the version but for some reason scanpy is using 1.01 and I don't know how to change this. I'm a bit new to python so I'm sorry if this is a novice question. I appreciate any help you can offer. I am using an ubuntu server running python 3.6 with the following versions:; sc.logging.print_versions() ; scanpy==1.5.1 anndata==0.7.3 umap==0.4-dev numpy==1.15.0 scipy==1.0.1 pandas==0.23.3 scikit-learn==0.23.1 statsmodels==0.11.1. This is the error message:. ```pytb; computing tSNE; WARNING: You’re trying to run this on 16872 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; computing PCA; with n_comps=50; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-65-c244be664e51> in <module>(); ----> 1 sc.tl.tsne(adata, n_pcs = 50); 2 # UMAP, first with neighbor calculation; 3 sc.pp.neighbors(adata, n_pcs = 50, n_neighbors = 20); 4 sc.tl.umap(adata). ~/.local/lib/python3.6/site-packages/scanpy/tools/_tsne.py in tsne(adata, n_pcs, use_rep, perplexity, early_exaggeration, learning_rate, random_state, use_fast_tsne, n_jobs, copy); 78 start = logg.info('computing tSNE'); 79 adata = adata.copy() if copy else adata; ---> 80 X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs); 81 # params for sklearn; 82 params_sklearn = dict(. ~/.local/lib/python3.6/site-packages/scanpy/tools/_utils.py in _choose_representation(adata, use_rep, n_pcs, silent); 41 'Falling back to preprocessing with `sc.pp.pca` and default params.'; 42 ); ---> 43 X = pca(adata.X); 44 adata.obsm['X_pca'] = X[:, :n_pcs]; 45 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252:774,message,message,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252,1,['message'],['message']
Integrability,"Hello,. I am trying to use the SAM algorithm in my single-cell analysis. I run the SAM function like so:. ```py; sam_obj = sce.tl.sam(adata,inplace=True); ```. The function runs fine and appears to finish training however it crashes when computing the UMAP with the following error:. ```pytb; TypeError: a bytes-like object is required, not 'list'; ```. I'm not sure where this problem is coming from and I have spent the past day installing different versions of python and other dependencies to see if that solves the issue. Maybe naive but I know conda can sometimes be behind in their updates. I installed scanpy following the anaconda instructions here: https://scanpy.readthedocs.io/en/stable/installation.html; And I installed sam-algorithm using pip. Below is the entire output from the function call above. Below this I have included the output of ""conda list"" in case this information is helpful. . Any help would be greatly appreciated. Thank you, Hasan. ```pytb; Self-assembling manifold; Running SAM; RUNNING SAM; Iteration: 0, Convergence: 0.6008695832027542; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.; self.adata.uns[""neighbors""][""connectivities""] = EDM; Iteration: 1, Convergence: 0.3743130193917588; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities' is deprecated. It has been moved to .obsp[connectivities], and will not be accesible here in a future version of anndata.; self.adata.uns[""neighbors""][""connectivities""] = EDM; Iteration: 2, Convergence: 0.029142717058066172; /wynton/home/state/alkhairohr/miniconda3/envs/python_env/lib/python3.6/site-packages/samalg/__init__.py:1165: FutureWarning: This location for 'connectivities'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:481,depend,dependencies,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['depend'],['dependencies']
Integrability,"Hello,. Thank you for developing and maintaining such a useful tool!; I'm trying to integrate two data sets, they're replicates of the same condition. . ```; var_names = adata_002.var_names.intersection(adata_003.var_names); adata_002 = adata_002[:, var_names]; adata_003 = adata_003[:, var_names]. sc.pp.pca(adata_002); sc.pp.neighbors(adata_002); sc.tl.umap(adata_002). sc.tl.ingest(adata_003, adata_002, obs='louvain'); ```. And I got the following error:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-42-b3f5427509ba> in <module>; ----> 1 sc.tl.ingest(adata_003, adata_002, obs='louvain'). AttributeError: module 'scanpy.tools' has no attribute 'ingest'; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1. I've already tried installing the package both with conda and pip and I continue having the same issue. . I would really appreciate your comments and suggestions. . Sara",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092:84,integrat,integrate,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092,1,['integrat'],['integrate']
Integrability,"Here we go. ~~It’s not done yet~~:. - [x] The timing stuff isn’t yet implemented. I think we just do timing stuff when doing “info” level logging, correct?. If yes, we probably just need to override `RootLogger.info` so it sets `self.handlers[0].formatter.passed_time = kwargs.get('t', False)`. - [x] All the places hackily using `_settings_verbosity_greater_or_equal_than(2|3)` have to work differently. I propose that we just add a kwarg `deepinfo: str` or so which only adds the passed string to the message if the active verbosity is higher than the function’s (i.e. calling `logging.warn('foo', deepinfo='bar')` and the loglevel/verbosity is 'INFO' or noisier adds 'bar'). - [x] We now use vanilla log function syntax, so no more using it like `print`. We switched to Python 3.6+ though, so I propose f-strings everywhere! That looks better and works. Fixes #256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676:503,message,message,503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676,1,['message'],['message']
Integrability,"Hey! I thought plotting `.var` columns in `sc.pl.violin()` worked previously (and the error message seems to suggest this as well). I am doing the following:; ```; adata_counts.var['dropout_per_gene'] = (adata_counts.X > 0).mean(0); adata_counts.obs['dropout_per_cell'] = (adata_counts.X > 0).mean(1). sc.pl.violin(adata_counts, keys='dropout_per_gene'); ```. and I get this error:; ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375:92,message,message,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375,1,['message'],['message']
Integrability,"Hey!. I hope you are doing well. I run into the error AttributeError: module 'umap' has no attribute '__version__' when running this code:. ```; import pandas as pd; #pd.set_option(""display.max_columns"", None); import numpy as np; import anndata; import scanpy as sc. %store -r df. adata = anndata.AnnData(df); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.paga(adata); sc.pl.paga(adata, plot=False); sc.tl.umap(adata, init_pos='paga'); ```. I am using Python 3.8 on Jupyter Notebooks on Mac M1. Please find below the error in a more detailed fashion:. ```; AttributeError Traceback (most recent call last); <ipython-input-7-7cfb2fb3103e> in <module>; ----> 1 sc.tl.umap(adata, init_pos='paga'); 2 sc.pl.umap(adata). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 141 import umap; 142 ; --> 143 if version.parse(umap.__version__) >= version.parse(""0.5.0""):; 144 ; 145 def simplicial_set_embedding(*args, **kwargs):. AttributeError: module 'umap' has no attribute '__version__'; ```. Would you have any idea on how to solve this? I think it is a package dependency problem. Thank you in advance for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978:1234,depend,dependency,1234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978,1,['depend'],['dependency']
Integrability,"Hey,; while writing tests for #1715 I noted the following behavior:. `output = sc.pp.highly_variable(adata,inplace=True,subset=False,n_top_genes=100)`; --> Returns nothing :heavy_check_mark: ; --> `adata.var` fields are updated but shape stays the same :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=True,subset=True,n_top_genes=100)`; --> Returns nothing :heavy_check_mark: ; --> `adata.var` fields are updated and shape is subsetted correctly :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=False,subset=False,n_top_genes=100)`; --> output is a dataframe with the original number of genes as rows :heavy_check_mark: ; --> `adata` is unchanged :heavy_check_mark: . `output = sc.pp.highly_variable(adata,inplace=False,subset=True,n_top_genes=100)`; --> Returns nothing :x: ; --> `adata` shape is changed an `var` fields are updated :x: . I think the last case is unexpected, assuming that `inplace=False` should protect `adata` from changes and results in an output. I think these parts of code are the cause (depending on the flavor):. https://github.com/theislab/scanpy/blob/4dd8de9e355ce8d59009c15522670a3d0970462c/scanpy/preprocessing/_highly_variable_genes.py#L148-L174. https://github.com/theislab/scanpy/blob/4dd8de9e355ce8d59009c15522670a3d0970462c/scanpy/preprocessing/_highly_variable_genes.py#L528-L553. To fix this, one could change `if subset or inplace:` to `if inplace:`. Then one would need to add another `if subet:` in the `else` block, like so (half-pseudocode):. ```; if inplace: ; ; #update adata; ; if batch_key is not None:; #drop batch related keys; if subset:; adata._inplace_subset_var(df['highly_variable'].values); else:; if batch_key is None:; #drop batch related keys; if subset: ; df=df.iloc[df.highly_variable.values,:]; ; return df; ```. I can make a quick PR if this is the intended behavior. :slightly_smiling_face: ; best, jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1867:1049,depend,depending,1049,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1867,1,['depend'],['depending']
Integrability,"Hi @all,; Thanks to develop the great tools,; I encounter a pecular problem on bbknn integrated data.; i follow the workflow code to run the data integrated,the code showing below,; #; sc.pp.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; sc.tl.pca(adata, svd_solver='arpack'); sc.tl.tsne(adata); ///data integrated; sc.external.pp.bbknn(adata, batch_key='orig.ident'). sc.tl.umap(adata); adata; sc.pl.umap(adata, color=['orig.ident']); showing the well integrated, picture below,; ![image](https://user-images.githubusercontent.com/41668708/90249921-df74d980-de6d-11ea-8283-3830dd3c6ad2.png); But,when i want to see the tsne picture, the batch from different sample showing up on the tsne but umap like above picture; ,i runing ,; sc.tl.tsne(adata); sc.pl.tsne(adata, color=['orig.ident']); the picture show below, indicating that the integrated can not be worked on tsne.; ![image](https://user-images.githubusercontent.com/41668708/90250154-3c708f80-de6e-11ea-8d43-7a597c5693a2.png); So, why this tsne showing significantly different with the object just running over the integrated process.; any advice would be appreciated; Best,; hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370:85,integrat,integrated,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370,6,['integrat'],['integrated']
Integrability,"Hi @falexwolf . Added a small wrapper for phenograph clustering, similar to phate.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292:30,wrap,wrapper,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292,1,['wrap'],['wrapper']
Integrability,"Hi @falexwolf,. 10X is releasing a new version of CellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/334:475,wrap,wraps,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334,1,['wrap'],['wraps']
Integrability,"Hi @giovp @ivirshup, (also CCing @dkobak). as we discussed at #1715 and in https://github.com/theislab/scanpy-tutorials/pull/43, I prepared a function `pearson_residuals_hvg_scatter()` that wraps `sc.pl.scatter()` to reproduce the ""gene selection plot"" from the tutorial. It can be used as a sanity check for both the HVG selection and the appropriateness of the used Pearson residual null model (as explained also in the tutorial notebook). I also added a feature to show known marker genes on the plot, change plot aesthetics and which fields are used for plotting. Looking forward to your thoughts on this one :); Best,; Jan. PS: I prepared this PR on an independent branch from #1715 - hope that is the correct way in this situation!; ```; import scanpy as sc; sc.settings.set_figure_params(dpi=80, facecolor=""white""). #run pearson residuals gene selection; adata=sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_cells=1); sc.experimental.pp.highly_variable_genes(adata, flavor=""pearson_residuals"", n_top_genes=2000). #basic plot; sc.experimental.pl.pearson_residuals_hvg_scatter(adata); ```; ![image](https://user-images.githubusercontent.com/34481813/158408471-5d661d60-11f3-482d-981b-da3a507b64b4.png). ```; #modify some aesthetics; sc.experimental.pl.pearson_residuals_hvg_scatter(adata,kwargs_sc_pl_scatter=dict(size=30)); ```. ![image](https://user-images.githubusercontent.com/34481813/158408564-9ca8b476-c0e6-4159-8c4e-cc4b542f43c4.png). ```; #highlight some marker genes; markers = [""IL7R"", ""LYZ"", ""CD14"", ""MS4A1"", ""CD8A"", ""GNLY""]; sc.experimental.pl.pearson_residuals_hvg_scatter(adata,marker_names=markers,kwargs_sc_pl_scatter=dict(size=30)); ```; ![image](https://user-images.githubusercontent.com/34481813/158408630-f973cb16-0165-42ee-95e6-55e9e6de676e.png). ```; #use custom fields in `adata` for x and y; #(there is also a similar option to use a different field for where HVG flag is stored); sc.experimental.pl.pearson_residuals_hvg_scatter(adata,x='means',y='variances',return",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2176:190,wrap,wraps,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2176,1,['wrap'],['wraps']
Integrability,"Hi @ivirshup!. We've discussed this in Aptos a couple of months ago. Adding an `interactive` parameter to all the scatter plots would be really useful for working with notebooks. Would you consider adding that functionality as you have a lot of experience with it? Importantly, it should be based on the restructured plotting code that @fidelram is currently working on in https://github.com/theislab/scanpy/pull/244 (we could move that branch to the scanpy repo?). Hence, this would be for post-Scanpy 1.3 and there is no great hurry. A solution that takes an `AnnData` and creates an interactive plot but totally ignores the current way scatter plots are generated and Fidel's restructured way would be what follows below (due to @NDKoehler). Hence, the task is to think about a good way of integrating this with how scatter plots are done in Scanpy (after Fidel's changes).; ```; from bokeh.plotting import figure, show, output_notebook, save#, output_file; from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource; from bokeh.palettes import viridis; output_notebook(). import matplotlib as mpl. def plot_interactive(data):. colors = [; ""#%02x%02x%02x"" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.viridis(mpl.colors.Normalize()(data.obs['CCS'].values)); ]. source = ColumnDataSource(dict(; x=data.obsm['X_umap'][:,0],; y=data.obsm['X_umap'][:,1],; color=colors,#data.obs['CCS'],; label=data.obs['Charge'],; #msize= p_df['marker_size'],; #topic_key= p_df['clusters'],; #title= p_df[u'Title'],; #content = p_df['Text_Rep']; seq=data.obs['seq'],; ccs=data.obs['CCS'],; charge=data.obs['Charge'],; )); #ax = sc.pl.umap(data, color=['Charge','CCS']); #sc.pl.umap(data, color=['CCS'], save='ccs'). title = 'T-SNE visualization of sequences'. plot_lda = figure(plot_width=800, plot_height=600,; title=title, tools=""pan,wheel_zoom,box_zoom,reset,hover,previewsave"",; x_axis_type=None, y_axis_type=None, min_border=1). plot_lda.scatter(x='x', y='y', legend='label', source=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253:793,integrat,integrating,793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253,1,['integrat'],['integrating']
Integrability,"Hi Scanpy team!. After facing the issue with duplicated gene symbols again for the n-th time, I realised that one of the best solutions for renaming duplicates would likely be to do the following `'DuplicatedName-ENSEMBL_ID'` rather than just adding an order-dependent number `'DuplicatedName-1'` that can differ between dataset from different papers - preventing correct matching when integrating datasets which in turn essentially requires deleting duplicated genes. . What do you think in general?. Would it be possible to add support for this with interface like `var_names_make_unique(unique_column='ENSEMBL')`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1719:259,depend,dependent,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719,3,"['depend', 'integrat', 'interface']","['dependent', 'integrating', 'interface']"
Integrability,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:881,wrap,wraps,881,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,1,['wrap'],['wraps']
Integrability,"Hi all,. I am trying to use `ingest` to integrate different datasets.; I found a couple of issues. - `ingest` requires that the `var_names` are the same in the reference and the new object. I can select the intersection between the datasets; however, it requires that the genes are in the same order `if not ref_var_names.equals(new_var_names)`. I think this `if` could be modified using `set` (e.g., `len(set(ref_var_names).difference(set(new_var_names))) == 0`). I tried to order the `.var` dataframe, but the `.X` remains the same. In such a way, the expression of the genes does not correspond to the correct one. I can generate a dataframe and recreate the `.X`, but it could be very nice that the `.X` will be modified according to `.var` or `.obs` modifications (i.e., ordering). . - although it is possible to set `embedding_method=umap`, `ingest` requires the PCA components. I used autoencoders instead of PCA, and I cannot run `ingest` only considering the UMAP. Can you fix it? . Thank you in advance.; Best,; Andrea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1128:40,integrat,integrate,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128,1,['integrat'],['integrate']
Integrability,"Hi everyone,. Seeing how many new single cell and spatial tools are being developed in Python, and how we are increasingly using it in general and scanpy in particular, at saezlab we decided to re-implement our tools to estimate pathways and Transcription factor (TF) activity ([Dorothea](https://saezlab.github.io/dorothea/) and [Progeny](https://saezlab.github.io/progeny/)) in it. Here's a first draft in Python of our tools:; https://github.com/saezlab/dorothea-py; https://github.com/saezlab/progeny-py. Our tools take gene expression as input and generate matrices of TF and pathway activities. They can be understood as: ; 1) Prior-knowledge dimensionality reduction methods (`obsm`). Examples of usage:; 	* Used as input for NN; 	* Used as input for integration methods; 2) New data assays (`X`). Examples of usage:; 	* Plot feature activities in projections such as PCA or UMAP; 	* Plot feature activities in heat-maps, clustermaps, violin plots, etc; 	* Differences between groups can be modeled to find significant differences. Because of this duality, the integration of our tools into scanpy is not straightforward. If we store the activities in `obsm` they can be used as a dimensonality reduction embedding but then we lose acces to all the fantastic plotting functions based on `X`. Then if we add add our activities to `X`, they have a very different distribution than gene expression plus there would be an overlap of names between genes and TFs. A solution to this would be to have a separate `.layer` to store this matrices but layers must contain the same dimensions as `X`. Another workaround would be to store it in `.raw` but then we force the user to use remove its previous contents, plus it is used in some methods as default which could cause problems. . What would be a smart solution to integrate our tools in your universe?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:758,integrat,integration,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,3,['integrat'],"['integrate', 'integration']"
Integrability,"Hi everyone,. a while back, @giovp asked me to create a pull request that integrates analytical Pearson residuals in scanpy. We already discussed a bit with @LuckyMD and @ivirshup over at berenslab/umi-normalization#1 how to structure it, and now I made a version that should be ready for review. As discussed earlier, this pull request implements two core methods:; - `sc.pp.normalize_pearson_residuals()`, which applies the method to `adata.X`. Overall, the function is very similar in structure to `sc.pp.normalize_total()` (support for layers, inplace operation etc).; - `sc.pp.highly_variable_genes(flavor='pearson_residuals')`, which selects genes based on Pearson residual variance. The ""inner"" function `_highly_variable_pearson_residuals()` is structured similarly to `_highly_variable_seurat_v3()` (support for multiple batches, median ranks for tie breaking). It includes the `chunksize` argument to allow for memory-efficient computation of the residual variance. We discussed quite a lot how to implement a third function that would bundle gene selection, normalization by analytical residuals and PCA. This PR includes the two options that emerged at the end of that discussion, so now we have to choose ;). - `sc.pp.recipe_pearson_residuals()` which does HVG selection and normalization both via Pearson residuals prior to PCA; - `sc.pp.normalize_pearson_residuals_pca()` which applies any HVG selection if the user previously added one to the `adata` object, and then normalizes via Pearson residuals and does PCA. Both functions retain the raw input counts as `adata.X` and add fields for PCA/Normalization/HVG selection results (or return them) as applicable, most importantly the `X_pca` in `adata.obsm['pearson_residuals_X_pca']`. I hope this addresses some of the issues we discussed over at the other repo in a scanpy-y way. Let me know what you think and where you think improvements are needed!. Cheers, Jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715:74,integrat,integrates,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715,1,['integrat'],['integrates']
Integrability,"Hi everyone,; I am using pip3 to install scanpy, this is the error message:. <details>. ```pytb; File ""/global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/lib/python3.6/site-packages/setuptools-18.3.2-py3.6.egg/setuptools/command/build_py.py"", line 94, in find_data_files; TypeError: must be str, not list. ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Failed to build scanpy; Installing collected packages: scanpy, decorator; Running setup.py install for scanpy ... error; Complete output from command /global/software/MPI/GCC/4.9.2/OpenMPI/1.8.5/Python/3.6.0/bin/python -u -c ""import setuptools, tokenize;__file__='/tmp/xs-ttgump/pip-install-xpuhp0co/scanpy/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/xs-ttgump/pip-record-v4z7bmhr/install-record.txt --single-version-externally-managed --compile --user --prefix=:; running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/logging.py -> build/lib/scanpy; copying scanpy/exporting.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/utils.py -> build/lib/scanpy; copying scanpy/__init__.py -> build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; creating build/lib/scanpy/tools; copying scanpy/tools/dpt.py -> build/lib/scanpy/tools; copying scanpy/tools/paga.py -> build/lib/scanpy/tools; copying scanpy/tools/louvain.py -> build/lib/scanpy/tools; copying scanpy/tools/_utils.py -> build/lib/scanpy/tools; copying scanpy/tools/pca.py -> build/lib/scanpy/tools; copying scanpy/tools/umap.py -> build/lib/scanpy/tools; copying scanpy/tools/sim.py -> build/lib/scanpy/tools; copying scanpy/tools/tsne.py -> build/lib/scanpy/tools; copying scanpy/tools/__init__.py -> build/lib/scanpy/tools; copyi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/148:67,message,message,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/148,1,['message'],['message']
Integrability,"Hi scanpy folks,. This PR adds our cell identity classification model [`scnym`](https://github.com/calico/scnym) as a tool in `sc.external.tl.scnym`. The `scnym` API was inspired by `scanpy` and intended to be compatible, so the implementation in `external/_scnym.py` is simply a wrapper.; I also added a test in `tests/external/test_scnym.py` that passes.; Everything was linted with `black,flake8,autopep8` through `pre-commit`. Please let me know if there are any issues or changes you'd like to see.; Thanks for building a great ecosystem!. Best,; Jacob",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1775:280,wrap,wrapper,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775,1,['wrap'],['wrapper']
Integrability,"Hi there! Thanks for adding the ingest method to scanpy!; I was wondering what would be the suggested approach when the reference data contains batch effects that should be removed before actually performing the asymmetric integration with a query dataset. I tried to use the neighbors structure returned by BBKNN (which correctly adjusts for batch effects), but the 'metric' object is missing. Here the error:. ```; KeyError Traceback (most recent call last); <ipython-input-22-a805d117788e> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='time', embedding_method='umap'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 115 labeling_method = labeling_method * len(obs); 116 ; --> 117 ing = Ingest(adata_ref); 118 ing.fit(adata); 119 . /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in __init__(self, adata); 268 ; 269 if 'neighbors' in adata.uns:; --> 270 self._init_neighbors(adata); 271 ; 272 if 'X_umap' in adata.obsm:. /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_neighbors(self, adata); 229 else:; 230 dist_args = (); --> 231 dist_func = named_distances[adata.uns['neighbors']['params']['metric']]; 232 self._random_init, self._tree_init = make_initialisations(dist_func, dist_args); 233 self._search = make_initialized_nnd_search(dist_func, dist_args). KeyError: 'metric'. ```; I'm running scanpy version 1.4.5.post2. Any help would be highly appreciated!! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1108:223,integrat,integration,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1108,1,['integrat'],['integration']
Integrability,"Hi there. Everytime I run the code _sc.pp.neighbors_ the kernel dies. Unfortunately, there is no error message or error code. It just dies while computing neighbors. Other scanpy codes like _sc.pp.filter_cells_ and _sc.pp.filter_genes_ work without a problem. I'm using:. - windows 10 64-bit 24 gb ram; - python 3.8.5 in jupyter notebook; - numpy 1.19.4; - scanpy 1.6.0. Is there someone who would be able to solve this issue?; Thank you very much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567:103,message,message,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567,1,['message'],['message']
Integrability,"Hi! I am wondering why `loompy` and `pybiomart` are not in the list of dependencies. As of now ; the package (at least in bioconda) is not fully functional and requieres some extra installation; steps if one wants to use certain functionalities provided. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### Minimal code sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:71,depend,dependencies,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['depend'],['dependencies']
Integrability,"Hi! I've added two possible APIs for PHATE: I imagine you'll only want to include one of them. . The first, `sc.tl.PHATE`, is an object-oriented interface that matches `phate.PHATE`, which looks like an `sklearn` estimator class. The second is a functional interface which is equivalent to running `phate.PHATE().fit_transform(data)`. The second is more simple, but less powerful as any changes in parameters will require the entire operator to be recomputed, where the object-oriented interface allows partial recomputation. Please let me know which you prefer (or if you would like to include both!) If we only include the functional version I would probably rename it to `phate` rather than `run_phate`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136:145,interface,interface,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136,3,['interface'],['interface']
Integrability,Hi!. I am wondering if you could add https://github.com/BayraktarLab/cell2location to your list of scRNA->spatial integration methods (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html). Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1574:114,integrat,integration,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574,2,['integrat'],"['integration', 'integration-scanorama']"
Integrability,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:217,depend,dependencies,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,1,['depend'],['dependencies']
Integrability,"Hi!; Thank you for tutorials, they're very helpful. ; Do you have a spatial/sc-rna seq integrative analysis tutorial? . Thanks in advance!. ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1386:87,integrat,integrative,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1386,1,['integrat'],['integrative']
Integrability,"Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using ‘cellranger’ software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In ‘Seurat’, I can read the data from my three treatments separated, do quality control, and then integrate them using ‘FindIntegrationAnchors’ and ‘IntegrateData’ functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in ‘Scanpy’ to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that?. In ‘Scanpy’ I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type?. Thank you very much!. Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:576,integrat,integrate,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,7,"['Integrat', 'integrat']","['IntegrateData', 'integrate', 'integrated', 'integrating']"
Integrability,"Hi, . I was wondering what is a good/accepted way to calculate differential gene expression after batch alignment of multiple datasets?. After reading into it, it seems to me that the DEG are calculated on the raw (=non batch corrected values) and after all some batch correction algorithms don't even transform the data matrix (I don't understand why). See: [Mnn correct docs](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.external.pp.mnn_correct.html), [Seurat issue](https://github.com/satijalab/seurat/issues/1224#issuecomment-473416336), [Harmony preprint](https://www.biorxiv.org/content/biorxiv/early/2018/11/05/461954.full.pdf). But that means I would need to include _batch_ as an interaction in the DEG calculation, therefore I could use _logistic regression_ in scanpy with:; `scanpy.tl.rank_genes_groups(adata, use_raw=True, method='logreg')`; I am struggling though to find out how to add interactions to sklearns logistic regression via scanpy. When using sklearn directly it should work through [patsy or PolynomialFeatures()](https://stackoverflow.com/questions/45828964/how-to-add-interaction-term-in-python-sklearn). [Others](https://github.com/theislab/scanpy/issues/95) seem to use sklearn without the wrapper.; Or maybe I don't need to add interactions if the biological difference between the samples is bigger than the batch effect?. Do you think this is the right way to do this and could you point me in the right direction to solve this?; I think this might actually be not an _issue_ of scanpy but more a matter of understanding how to properly do this and how to use the tool so no worries if you decide to close this. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669:1239,wrap,wrapper,1239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669,1,['wrap'],['wrapper']
Integrability,"Hi, . This is a minor point. Thanks for linking to the scVI repo in your ecosystem page. However, would it be possible to put scVI in another category than ""data integration"" ? Since scVI can also do differential expression for example. . Thanks, ; Romain; <!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1072:162,integrat,integration,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1072,1,['integrat'],['integration']
Integrability,"Hi, . we developed [scirpy](https://github.com/icbi-lab/scirpy), a scanpy extension to analyse single-cell TCR data. You can learn more about the project in our [preprint](https://www.biorxiv.org/content/10.1101/2020.04.10.035865v1), the [apidocs](https://icbi-lab.github.io/scirpy/api.html) and the [tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html). . Since I heard that some people around here (@davidsebfischer, @b-schubert) are really interested in this topic, I created this issue to coordinate efforts. I would love to work together with you guys to take this to the next level. . Currently, some ideas of mine are; * extension to BCR data; * integration with @davidsebfischer's [tcellmatch](https://github.com/theislab/tcellmatch) as a distance metric for clonotype networks; * integration with epitope databases. Let me know what you think! . Best, ; Gregor . CC @ffinotello, @szabogtamas, @mlist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163:676,integrat,integration,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163,2,['integrat'],['integration']
Integrability,"Hi, ; I was wondering, if you can synchronize the functionality of the louvain and leiden clustering algorithm implementations. ; `sc.tl.louvain` has the `restrict_to` parameter, which allows subclustering of a specific cluster (set), while `sc.tl.leiden` does not (Note: I have `scanpy==1.4+18.gaabe446`). ; I'd be happy to have that. . Best,; M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/582:34,synchroniz,synchronize,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/582,1,['synchroniz'],['synchronize']
Integrability,"Hi, ; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!; Lei . <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2353:39,integrat,integration,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2353,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi, I have a Seruat processed dataset, of which I wanted to use scVI for integration. I stored the raw count and cell information then assembled them in scanpy as anndata via method mentioned: `https://smorabit.github.io/tutorials/8_velocyto/`. . So I could get batch HVG function to work without specifying flavor, however, I couldn't get it to work with specifying `flavor=""seurat_v3""`, I wonder how important it is to set this parameter and why does it not work for me?. Thanks a lot!. ```; adata; AnnData object with n_obs × n_vars = 73998 × 13639; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'barcode', 'Celltype2', 'Clusters', 'Sample'; uns: 'log1p'; layers: 'counts'. for i in adatas:; i.layers['counts'] = i.X; adata = ad.concat(adatas); adata.obs_names_make_unique; sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; flavor=""seurat_v3"",; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [197], line 1; ----> 1 sc.pp.highly_variable_genes(; 2 adata_new,; 3 flavor=""seurat_v3"",; 4 layer=""counts"",; 5 batch_key=""Sample"",; 6 subset=True; 7 ); 8 adata_new. File ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:422, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 416 raise ValueError(; 417 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 418 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 419 ); 421 if flavor == 'seurat_v3':; --> 422 return _highly_variable_genes_seurat_v3(; 423 adata,; 424 layer=layer,; 425 n_top_genes=n_top_genes,; 426 batch_key=batch_key,; 427 check_values=check_values,; 428 span=span,; 429 subset=subset,; 430 inplace=inplace,; 431 ); 433 if batch_key is None:; 434 df = _highly_variable_genes_single_batch(; 435 adata,; 436 layer=layer,; (...); 443",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427:73,integrat,integration,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427,1,['integrat'],['integration']
Integrability,"Hi, I was doing a dataset integration on quite some datasets. . ```py; adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7] #each is an adata object from scRNA-seq. for i in adatas:; i.layers['counts'] = i.X. adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7]. adata = ad.concat(adatas); adata.obs_names_make_unique. sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; layer=""logcounts"",; batch_key=""Sample"",; subset=True; ). scvi.model.SCVI.setup_anndata(adata, layer=""counts"", batch_key=""Sample""). vae = scvi.model.SCVI(adata, n_hidden=256); vae.train(); adata.obsm[""X_scVI""] = vae.get_latent_representation(); sc.pp.neighbors(adata, use_rep=""X_scVI""); from scvi.model.utils import mde; import pymde; adata.obsm[""X_mde""] = mde(adata.obsm[""X_scVI""]); adata.obsm[""X_normalized_scVI""] = vae.get_normalized_expression(); adata.write_h5ad('Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:26,integrat,integration,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,3,"['Integrat', 'integrat']","['Integrated', 'integration']"
Integrability,"Hi, I wrapped a R function from scran (mnnCorrect) in the preprocessing module. It's a new algorithm for batch effect removal ([Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors](https://www.nature.com/articles/nbt.4091)) and has not been implemented in Python, so I thought it would be useful. (Extremely useful for myself 😁 )",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125:6,wrap,wrapped,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125,1,['wrap'],['wrapped']
Integrability,"Hi,. I am writing the Galaxy integration for `pl.paga_path`. From the documentation, I understand that if we can provide an annotation (other than `dpt_pseudotime`) that are keys of `adata.obs`.; I tested with a dataset in which `dpt_pseudotime` is not a key of `adata.obs` and I got the error:. ```; Traceback (most recent call last):; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328:29,integrat,integration,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328,1,['integrat'],['integration']
Integrability,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:254,rout,routinely,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['rout'],['routinely']
Integrability,"Hi,. I corrected these small mistakes while checking the documentation to write Galaxy wrapper. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/282:87,wrap,wrapper,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/282,1,['wrap'],['wrapper']
Integrability,"Hi,. I found that in some tutorial documents, they does not use sc.pp.scale before sc.tl.pca. https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. But for some documents, they used. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. This thing also happened in several tools' analysis codes. https://docs.scvi-tools.org/en/stable/tutorials/notebooks/api_overview.html. Therefore, I wonder if the scale function is a key step for PCA analysis or not. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164:144,integrat,integrating-data-using-ingest,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164,1,['integrat'],['integrating-data-using-ingest']
Integrability,"Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```; computing UMAP; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap; verbose=settings.verbosity > 3,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding; verbose=verbose,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:328,message,message,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['message'],['message']
Integrability,"Hi,. when using the command ; ```python; sc.preprocessing.highly_variable_genes.highly_variable_genes(all_data,n_top_genes=5000); ```. I get this informative message:; ```; --> added; 'highly_variable', boolean vector (adata.var); 'means', boolean vector (adata.var); 'dispersions', boolean vector (adata.var); 'dispersions_norm', boolean vector (adata.var); ```. Even though not all of them are booleans! I guess it is just a copy-paste residue from implementation :); Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/411:158,message,message,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/411,1,['message'],['message']
Integrability,"Hi,; I find in scanpy 1.9.1 sc.pl.umap(adata,color=['leiden','batch']) can't show the color correctly. The color of ""leiden"" and ""batch"" are different, It seems that sc.pl.umap set the colors randomly, and the batch_color depends on leiden_color. There are some conflicts between leiden_color and batch_color. If I plot leiden and batch at the same time, then leiden will be the same as louvain color in https://github.com/scverse/scanpy/issues/156, and batch color is the same as the leiden color in https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. I remeber that this maybe an old issue, but I cannot find it. This bug was fixed in some versions, but now 1.9.1 introduces it again.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2299:222,depend,depends,222,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2299,1,['depend'],['depends']
Integrability,"Hi,; I have some problems running Louvain clustering.; The first time I tried to run, it complains about missing library `igraph`.; I installed `igraph` but now this same library throws a `DeprecationWarning` when calling Louvain clustering. ```; ---------------------------------------------------------------------------; DeprecationWarning Traceback (most recent call last); <ipython-input-17-329d7c2ac26c> in <module>(); ----> 1 sc.tl.louvain(adata). ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/tools/louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, n_jobs, copy); 79 directed = False; 80 if not directed: logg.m(' using the undirected graph', v=4); ---> 81 g = utils.get_igraph_from_adjacency(adjacency, directed=directed); 82 if flavor == 'vtraag':; 83 import louvain. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed); 92 def get_igraph_from_adjacency(adjacency, directed=None):; 93 """"""Get igraph graph from adjacency matrix.""""""; ---> 94 import igraph as ig; 95 sources, targets = adjacency.nonzero(); 96 weights = adjacency[sources, targets]. ~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/igraph/__init__.py in <module>(); 6 __license__ = ""MIT""; 7 ; ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, ""; 9 ""this visualization library has been renamed to ""; 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient.; ```. I can work on a PR and change line 94 of `scanpy/utils.py` to:. ```; import jgraph as ig; ```. This should solve the issue afaik (and `jgraph` should be listed in the dependencies I think). Let me know if you accept PRs. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138:1825,depend,dependencies,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138,1,['depend'],['dependencies']
Integrability,"Hi,; I was trying to run the quick example described in the magic api cmd using datasets.paul15 but it keeps on giving me the same error. See below the code I used and the error it gives. . import numpy as np; import pandas as pd; import scanpy.api as sc; import matplotlib.pyplot as pl; import phate; import magic. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; sc.logging.print_version_and_date(); # we will soon provide an update with more recent dependencies; sc.logging.print_versions_dependencies_numerics(). Running Scanpy 1.2.2+72.gbc6661c on 2018-07-18 19:40.; Dependencies: anndata==0.6.5 numpy==1.14.3 scipy==1.1.0 pandas==0.23.0 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . adata = sc.datasets.paul15(). WARNING: In Scanpy 0.*, this returned logarithmized data. Now it returns non-logarithmized data.; ... storing 'paul15_clusters' as categorical. sc.pp.normalize_per_cell(adata); sc.pp.sqrt(adata); adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); adata_magic.shape. computing PHATE. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-79-129f35d34dbd> in <module>(); 2 sc.pp.normalize_per_cell(adata); 3 sc.pp.sqrt(adata); ----> 4 adata_magic = sc.pp.magic(adata.X, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5); 5 adata_magic.shape. ~/software/scanpy/scanpy/preprocessing/magic.py in magic(adata, name_list, k, a, t, n_pca, knn_dist, random_state, n_jobs, verbose, copy, **kwargs); 131 n_jobs=n_jobs,; 132 verbose=verbose,; --> 133 **kwargs).fit_transform(adata,; 134 genes=name_list); 135 logg.info(' finished', time=True,. TypeError: 'module' object is not callable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/208:582,depend,dependencies,582,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/208,2,"['Depend', 'depend']","['Dependencies', 'dependencies']"
Integrability,"Hi,; I'd like to plot a bunch of figures using the sc.pl.xx functions.; Is there some solution to suppress the Warning message during saving the figure?; the warning looks like that:; ```; WARNING: saving figure to file /home/test/figure/umap.marker1.png; WARNING: saving figure to file /home/test/figure/umap.marker2.png; WARNING: saving figure to file /home/test/figure/umap.marker3.png; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2238:119,message,message,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2238,1,['message'],['message']
Integrability,"Hi,; Is it necessary to use only high variable genes for the downstream analysis ?; If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:269,integrat,integrating,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,1,['integrat'],['integrating']
Integrability,"Hi,; Would it be possible to create a panel of plots using both rows and columns when plotting tsne?; I did something similar to this:; ![tsne markers](https://user-images.githubusercontent.com/697622/39486721-19f44a02-4d4b-11e8-986e-d78079a06e0c.png). Basically the code calls the matplotlib subplots method based on the number of plots:. ```py; def _build_subplots(n):; '''; Build subplots grid; n: number of subplots; '''; nrow = int(np.sqrt(n)); ncol = int(np.ceil(n / nrow)); fig, axs = plt.subplots(nrow, ncol, dpi=100, figsize=(ncol*5, nrow*5)). return fig, axs, nrow, ncol; ```. Then the plots are drawn:. ```py; genes = [...list of gene symbols...]; fig, axs, nrow, ncol = _build_subplots(len(genes)). if type(axs) != np.ndarray:; axs = [axs]; else:; axs = axs.ravel(). for i in range(nrow*ncol):; if i < len(genes):; gene = genes[i]; # df is the numpy array containing tSNE; axs[i].scatter(df[:, 0], df[:, 1], ...); ```. Is it something that is already done, planned or that you don't want to integrate?. Thanks,; Francesco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137:1003,integrat,integrate,1003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137,1,['integrat'],['integrate']
Integrability,"Hi,; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355:38,integrat,integration,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi,; we want to add SCALEX, an online integration method that integrate different single-cell experiments including scRNA-seq and scATAC-seq, which also enable accurate projecting new incoming datasets onto the existing cell space. This method is published on Nature Communications. SCALEX is developed based-on scanpy. We appreciate and will be honored to contribute to the scanpy community. Thank you!; Lei",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2354:38,integrat,integration,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2354,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/985:262,integrat,integrated,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985,1,['integrat'],['integrated']
Integrability,"Hi. I have a problem with reproducing my analysis using scanpy 1.4.3. I have used two different machines using the same software version (all dependencies as well), and I keep having different results using the same commands and seed numbers. In one machine I get this:. <img width=""340"" alt=""Screenshot 2019-06-05 at 15 44 31"" src=""https://user-images.githubusercontent.com/3297906/58965800-13a03500-87a9-11e9-8a7c-bec104f4718e.png"">. and in the other I get this:. <img width=""334"" alt=""Screenshot 2019-06-05 at 15 44 14"" src=""https://user-images.githubusercontent.com/3297906/58965827-1e5aca00-87a9-11e9-95e5-90c09303a1c2.png"">. Can you recommend which aspects to check? or how to deal with this issue?. Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/681:142,depend,dependencies,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681,1,['depend'],['dependencies']
Integrability,"Hi. I have a question. I am using the scanorama for integrating multiple datasets. In my use-case, I will have a new dataset again. The question is, is there any way to transfer the results of scanorama to the new dataset? Or should I retrain everything again. This is also important if one has a train and test set. Ideally, you do not want to use the test set to use in the data integration training and only use the already trained transformation on the test data. . I am wondering if there is a solution for these use-cases? Would you please help us with that. Cheers; Ali",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162:52,integrat,integrating,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162,2,['integrat'],"['integrating', 'integration']"
Integrability,"Hi; I am trying to concatenate two anndata objects(adata = adata1.concatenate(adata3, join='inner')), but get an error message informing me that concatenate is not found. Any idea what might cause this behavior? Here is what I have installed:scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. and here is the error:; AttributeError Traceback (most recent call last); <ipython-input-187-32c3eda3cdc8> in <module>; ----> 1 adata = adata1.concatenate(adata3, join='inner'). ~/anaconda3/envs/scenv/lib/python3.6/site-packages/scipy/sparse/base.py in __getattr__(self, attr); 687 return self.getnnz(); 688 else:; --> 689 raise AttributeError(attr + "" not found""); 690 ; 691 def transpose(self, axes=None, copy=False):. AttributeError: concatenate not found; Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/760:119,message,message,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/760,1,['message'],['message']
Integrability,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3273:7,integrat,integrate,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273,1,['integrat'],['integrate']
Integrability,"I am following the SCENIC protocol (https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/PBMC10k_SCENIC-protocol-CLI.ipynb) with an admittedly different data set, but still using 10x data of similar type. I have to admit that I am relatively new to the python world and don't know yet where to turn to... ; haven't found any way to debug any further I conclude that this is a scanpy issue, at a minimal level at the error reporting stage as the information doesn't help me track down what is wrong. thanks for your time! . adata. ```; AnnData object with n_obs × n_vars = 4578 × 3389; obs: 'nGene', 'nUMI', 'n_genes', 'percent_mito', 'n_counts', 'louvain', 'leiden'; var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'log1p', 'hvg', 'pca', 'neighbors', 'umap', 'louvain', 'leiden', 'louvain_colors', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap', 'X_tsne'; varm: 'PCs'; obsp: 'distances', 'connectivities'; ```. my adata.raw.X looks like this:. ```; <4578x18247 sparse matrix of type '<class 'numpy.float32'>'; 	with 9236127 stored elements in Compressed Sparse Row format>; ```. adatat.X. ```; array([[ 0.23202083, 0.07064813, -0.05003222, ..., 1.4681866 ,; -0.21488723, 2.620106 ],; [ 0.09879599, 0.03607919, -0.08120057, ..., -0.3384455 ,; -0.19780253, 2.0771198 ],; [-0.5213845 , -0.1292537 , -0.1755099 , ..., -0.23126683,; -0.10592338, 0.02626752],; ...,; [ 2.4987383 , -0.14190508, -0.20776471, ..., -0.20877847,; -0.10354204, 0.14313072],; [ 0.1960011 , 0.06290449, -0.07691702, ..., -0.34954828,; 2.5718384 , 2.468825 ],; [-0.53571457, -0.13106212, -0.20085879, ..., -0.21621887,; -0.10943384, 0.05686853]], dtype=float32); ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # find marker genes; sc.tl.rank_genes_groups(adata, 'louvain', method='t-test', reference = 'rest'); ```. ```pytb; ranking genes; ---------------------------------------------------------------------------; AttributeEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:26,protocol,protocol,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,2,['protocol'],"['protocol', 'protocol-CLI']"
Integrability,"I am not sure if it has been already addressed.; This should fix the following import error of scanpy from master, due to missing `__init__.py` in external. Probably a more clean solution would be to wrap the import for external in a try/except block. ```python; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); in ; ----> 1 import scanpy as sc; 2 sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); 3 sc.settings.set_figure_params(dpi=200) # low dpi (dots per inch) yields small inline figures; 4 sc.settings.figdir = out('fig_supp'). ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/__init__.py in ; 31 from . import preprocessing as pp; 32 from . import plotting as pl; ---> 33 from . import datasets, logging, queries, settings, external; 34 ; 35 from anndata import AnnData. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/__init__.py in ; ----> 1 from . import tl; 2 from . import pl; 3 from . import pp; 4 ; 5 from .. import _exporting as exporting. ~/.pyenv/versions/3.6.5/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/external/tl.py in ; 2 from ..tools._phate import phate; 3 from ..tools._phenograph import phenograph; ----> 4 from ._tools._palantir import palantir. ModuleNotFoundError: No module named 'scanpy.external._tools'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/585:200,wrap,wrap,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/585,1,['wrap'],['wrap']
Integrability,I believe everything is in place. We should have now a generic `add_score` which scores cells according to expression of gene lists. That is wrapped twice in `cell_cycle_score`.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/76:141,wrap,wrapped,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/76,1,['wrap'],['wrapped']
Integrability,"I can't install MulticoreTSNE, and it may not even work on python >3.6. Since we want to drop 3.6 support (#1697), it would be good to stop recommending it, and pass the n_jobs parameter to sklearn's tsne. This PR attempts to do that, along with a bunch of deprecation warnings. I've also bumped the sklearn dependency to make sure TSNE is multithreaded. Metric was added to test if n_jobs was working. Either way, it seems to be using all the cpu on my laptop. Not sure what's up with that. ## TODO:. - ~~[ ] Figure out how to get n_jobs to actually limit cpu usage~~ Leaving this up to sklearn; - [x] Test metric; - [x] Test deprecations",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1854:308,depend,dependency,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1854,1,['depend'],['dependency']
Integrability,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63:93,message,message,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63,1,['message'],['message']
Integrability,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:357,message,message,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['message'],['message']
Integrability,"I have a similar issue to [this comment](https://github.com/theislab/scanpy/issues/1916#issuecomment-927497782). `Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids')`. Switching to `gene_symbols` didn't work. Error messages:; ```; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise V",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:224,message,messages,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['message'],['messages']
Integrability,"I have an issue similar to this https://github.com/lmcinnes/pynndescent/issues/133. Code and Error message::; ```; sc.tl.ingest(bdata,; lungreference,obs='new_celltype'; ); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save(self, key, data); 486 # If key already exists, we will overwrite the file; --> 487 data_name = overloads[key]; 488 except KeyError:. KeyError: ((array(int32, 1d, C), array(int32, 1d, C), array(float32, 1d, C), array(float32, 2d, C), type(CPUDispatcher(<function squared_euclidean at 0x7fbafd5a19d0>)), array(int64, 1d, C), float64), ('x86_64-unknown-linux-gnu', 'skylake-avx512', '+64bit,+adx,+aes,+avx,+avx2,-avx512bf16,-avx512bitalg,+avx512bw,+avx512cd,+avx512dq,-avx512er,+avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,+avx512vl,-avx512vnni,-avx512vpopcntdq,+bmi,+bmi2,-cldemote,+clflushopt,+clwb,-clzero,+cmov,+cx16,+cx8,-enqcmd,+f16c,+fma,-fma4,+fsgsbase,+fxsr,-gfni,+invpcid,-lwp,+lzcnt,+mmx,+movbe,-movdir64b,-movdiri,-mwaitx,+pclmul,-pconfig,+pku,+popcnt,-prefetchwt1,+prfchw,-ptwrite,-rdpid,+rdrnd,+rdseed,+rtm,+sahf,-sgx,-sha,-shstk,+sse,+sse2,+sse3,+sse4.1,+sse4.2,-sse4a,+ssse3,-tbm,-vaes,-vpclmulqdq,-waitpkg,-wbnoinvd,-xop,+xsave,+xsavec,+xsaveopt,+xsaves'), ('447c56dc5e270e4f82ab71861b297ed6de3def7f442a5fd25f557203e9177f64', 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855')). During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); /tmp/ipykernel_875/1088574315.py in <module>; 2 print(transgene); 3 bdata=adata[adata.obs.treatment==transgene]; ----> 4 sc.tl.ingest(bdata,; 5 lungreference,obs='new_celltype'; 6 ). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 128 ; 129 for method in embedd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:99,message,message,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,1,['message'],['message']
Integrability,"I have two suggestions/questions about dot/matrix plots:. 1- If `standard_scale='var'` is given, we can write `Mean expression\nin group\n(min-max scaled)` on the color legend to be more accurate about what is being displayed. 2- People/journals usually expect gene names to be written with italicized characters (don't ask why, see https://en.wikipedia.org/wiki/Gene_nomenclature). So I was wondering if we can simply do that in the plots. One important thing to consider is whether this is organism-dependent. I guess it's not but would be cool to discuss.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913:501,depend,dependent,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913,1,['depend'],['dependent']
Integrability,"I just found a small mistake in the documentation of `scanorama_integrate`:; **kwargs are passed to assemble, not integrate. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2647:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2647,1,['integrat'],['integrate']
Integrability,I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4.; In your requirements you state that this breaks the scatter plot.; In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876:231,depend,dependencies,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I just tried; ```python; import scanpy.api as sc; sc.queries.mitochondrial_genes('www.ensembl.org', 'strange_organism'); ```; I would expect scanpy complains that it does not know `'strange_organism'`, but I get the error ; ```python; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-13-6a41b361ab41> in <module>(); 1 import scanpy.api as sc; ----> 2 sc.queries.mitochondrial_genes('www.ensembl.org', 'drerio'). ~/software/scanpy/scanpy/queries/__init__.py in mitochondrial_genes(host, org); 34 s.add_attribute_to_xml('mgi_symbol'); 35 else:; ---> 36 logg.msg('organism ', str(org), ' is unavailable', v=4, no_indent=True); 37 return None; 38 s.add_attribute_to_xml('chromosome_name'). NameError: name 'logg' is not defined; ```; It seems to me like `queries/__init__.py` misses an `from .. import logging as logg` statement. Would maybe also make sense to show the the message that an organism is not available at verbosity level 1 instead of 4?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/258:958,message,message,958,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/258,1,['message'],['message']
Integrability,"I kicked out `save_knn` from BBKNN as it doesn't really accomplish anything, and that ended up breaking the scanpy wrapper for it. Took it out, and took the opportunity to update the docstring.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/636:115,wrap,wrapper,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/636,1,['wrap'],['wrapper']
Integrability,"I know this (quite ancient) pull request has been open (#403), but I wasn't sure on its status. I think the consensus was to wait for sklearn to integrate the necessary changes? If that's still the case, then please feel free to remove this PR. Here I make use of scipy's extremely nifty [LinearOperator](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html) class to customize the dot product functions for an input sparse matrix. In this case, the 'custom' dot product performs implicit mean centering. In my benchmarks, performing implicit mean centering in this way does not affect the runtime whatsoever. However, this approach has to use [svds](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html), for which randomized SVD is not implemented. So we have to use 'arpack', which can be significantly slower (but not intractably so.... in my hands, I could still do PCA on datasets of 200k+ cells in minutes, and it sure beats densifying the data, if you want more thorough benchmarks I am happy to generate them!). The way I incorporated this functionality into scanpy/preprocessing/_simple.py might be questionable, and would love any suggestions or advice on how to better integrate this if there is interest in pushing this PR through. Let me know!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066:145,integrat,integrate,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066,2,['integrat'],['integrate']
Integrability,"I like the `calculateQCMetrics` function from [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html), and have had a half finished python version drifting between my notebooks for a while. Is there is interest in adding this to scanpy?. I'm aiming to mostly copy the interface of `calculateQCMetrics` while being memory efficient, since this is likely run before filtering. # Todo. - [x] Figure out how I want to deal with more types of sparse matrix; - [x] Add `feature_control` argument, possibly `variable_control`; - [x] Clean up and expand tests; - [x] Expand documentation. # Questions. * What's up with sparse matrix choice in scanpy? Is `adata.X` expected to be any of the matrix types, or is it more limited? Just trying to figure out how much effort I should put into that.; * Are there any additional metrics that could be useful?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316:288,interface,interface,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316,1,['interface'],['interface']
Integrability,"I tried to collect in one file the code used for plotting functions that use matplotlib scatter like `sc.pl.tsne`, `sc.pl.pca` and `sc.pl.umap` and others. Also, I tried to annotate the code and improve the readability. . Currently, the code is on a separate file called `scatter.py` and not integrated into the API as this facilitates comparison with previous code. . Besides readability the proposed code can:; * Plot a large number of plots in multiple columms (instead of a long row of plots); * Pass arguments directly to `matplotlib.pyplot.scatter` like vmax and vmin to adjust the color scale. When plotting multiple plots, this is useful to have a consistent range of values). See cells 15 and 15 in this example: https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe ; If the admins would like to merge these changes I can replaced the previous functions. An example on how to use the code:. ```python; import scanpy.plotting.tools.scatter as spl; spl.tsne(adata, color='louvain'); ```. ![image](https://user-images.githubusercontent.com/4964309/44652273-c908b580-a9eb-11e8-86fa-aa1b55fa9b0a.png). Further examples [here](https://gist.github.com/fidelram/8b43f786e7519bcfb7ffc0d5ccdbb0fe)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244:292,integrat,integrated,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244,1,['integrat'],['integrated']
Integrability,I tried to install louvain through conda; `conda install -c vtraag louvain`; but got error message:; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - louvain; - python-igraph[version='>=0.7.1.0']. However I could install it by; `conda install -c conda-forge louvain`. Can you please update it on the webpage. Thanks!; [https://scanpy.readthedocs.io/en/latest/installation.html](url),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/143:91,message,message,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/143,1,['message'],['message']
Integrability,"I want to split AnnData after tl.diffmap according to each cell's library. But it appears that row-slicing AnnData after diffmap, dpt, or louvain gives the error message `AttributeError: 'AnnData' object has no attribute '_n_obs'`. But AnnData.X and AnnData.obs can be sliced. Could you please give me advice?. ```py; >>> adata = sc.read_10x_h5('filtered_gene_bc_matrices_h5.h5', 'mm10'); >>> scanpy.api.tl.diffmap(adata); >>> adata_diffmap[:, 0]; View of AnnData object with n_obs × n_vars = 5000 × 1; >>> adata_diffmap[0, :] ; AttributeError: 'AnnData' object has no attribute '_n_obs'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/62:162,message,message,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/62,1,['message'],['message']
Integrability,"I want to start off by mentioning how much I love scanpy. I recommend this package to everyone I know who is doing single cell sequencing. I love how PAGA and UMAP can be integrated together. PAGA makes appreciation of data topology so simple. In addition, it's so easy to do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:171,integrat,integrated,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,2,['integrat'],"['integrated', 'integration']"
Integrability,"I was just wondering if you guys have any plans for scATAC data analysis pipelines and especially integration with scRNA-seq? . cheers,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/725:98,integrat,integration,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725,1,['integrat'],['integration']
Integrability,"I was running this code `sc.pl.umap(adata, color = ['KIR3DL1'], frameon = False, layer = 'scvi_normalized')` to create umap but it gave me an error message with empty heatmap color bar legend. ![스크린샷 2022-09-02 오전 9 34 00](https://user-images.githubusercontent.com/64761042/188034720-20eacca2-efa0-4d6c-9e7f-0543f85d1cd7.png). ![스크린샷 2022-09-01 오후 6 58 54](https://user-images.githubusercontent.com/64761042/188034773-c32610cc-60a3-4c17-a000-8296f013b3e7.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2318:148,message,message,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2318,1,['message'],['message']
Integrability,"I was trying to plot a heatmap using this command:; `ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,; var_group_rotation=0, dendrogram=True, save='ClusterMap.png')`. And it didn't finish running after an overnight, with the following warning message:; WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`; /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633:319,message,message,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633,2,['message'],['message']
Integrability,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:591,message,message,591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['message'],['message']
Integrability,"I was wondering if we could deprecate the scvi external wrapper as we now have `scvi-tools`. I could also update the wrapper to have minimal functionality, but I think it would be better for people to use our API now that it's tightly integrated with scanpy anyway.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443:56,wrap,wrapper,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443,3,"['integrat', 'wrap']","['integrated', 'wrapper']"
Integrability,"I would like to color the umap representation using gene expression values. For ease of use I'd like to display the Gene name instead of gene_id which are the adata.var_names in my case. Setting `gene_symbols = 'Symbol'` doesn't seem to work for me or I am using it the wrong way. When running `sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2'])`. I get the follwoing error message:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-116-e09d49f2528c> in <module>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455:385,message,message,385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455,1,['message'],['message']
Integrability,"I would like to get an opinion from @gokceneraslan, @adamgayoso (who implemented this originally), or someone else more familiar in hvg selection here. I feel like we should probably have multiple methods for this, which we expose via a kwarg. I'm not confident median rank makes sense as a value here. I think there are assumptions about what kinds of values a rank can have that get broken by taking the median. Is the method for aggregating hvgs from multiple batches here reasonable? For reference, a similar code block can be found in `_highly_variable_genes_seurat_v3`, which says this is the method used in `SelectIntegrationFeatures` from Seurat. _Originally posted by @ivirshup in https://github.com/theislab/scanpy/pull/1715#discussion_r736690390_. @jlause commented:; I agree that exposing multiple options could make sense here. Any opinion @gokceneraslan and @adamgayoso which ones we should implement / which one should be the default?. I just copied this part from the seurat_v3 batch integration, so I don't have an expert opinion - but if I remember correctly, the other flavors use dispersions_norm to break ties instead of using the median rank. If I read the code correctly, this is the within-batch dispersion averaged over batches. We could also do something similar here: Compute residual variance within each batch, take the average across batches, and use that to break ties instead of using median rank.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2151:1000,integrat,integration,1000,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2151,1,['integrat'],['integration']
Integrability,"I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```; output_dir_fig = ""chosen/path/to/directory""; sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 103 ax.set_xscale(""log""); 104 show = settings.autoshow if show is None else show; --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save); 106 if show:; 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save); 337 show = settings.autoshow if show is None else show; 338 if save:; --> 339 savefig(writekey, dpi=dpi, ext=ext); 340 if show:; ...; -> 2456 fp = builtins.open(filename, ""w+b""); 2458 try:; 2459 save_handl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276:1355,wrap,wrapper,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276,1,['wrap'],['wrapper']
Integrability,"I'd like to integrate DCA with scanpy and I need to ask some questions regarding that. . There are two use cases for DCA so far. One is denoising as a preprocessing step and using denoised matrix in downstream analyses, especially in the ones that require original feature space such as differential expression. This seems to be a better fit for the `preprocessing` package i.e. `sc.pp.dca(adata)` which overwrites `adata.X` with the denoised matrix. Users can use `copy=True` to keep adata.X of original matrix unchanged. The other use case is the low dimensional representations of cells produced by the encoder. This can also be used for the downstream analyses that do not require original feature space e.g. clustering, and visualization. This is more like `sc.tl.dca(adata)` which stores the new representation into `adata.obsm['X_dca']`. Optionally it can store dropout probabilities (pi matrix) and the dispersion, as well. However, having both sc.tl.dca and sc.pp.dca might be confusing for the users. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/142:12,integrat,integrate,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/142,1,['integrat'],['integrate']
Integrability,"I'd like to start using [pre-commit](https://pre-commit.com) with scanpy and anndata. Pre-commit is essentially a tool that manages scripts we'd like to run before each commit, e.g. linting and formatting, so it becomes essentially impossible to forget these. I think this can allow PRs to progress faster since it gives us a way to codify formatting requirements – so we don't have to remember them – and have these checks happen locally – so we don't have to wait on CI. Of course, having these checks run depends on developers installing pre-commit, so we can also run these checks on CI ([example ci script](https://github.com/pandas-dev/pandas/blob/master/.github/workflows/pre-commit.yml), [example run](https://github.com/pandas-dev/pandas/pull/38745/checks?check_run_id=1624558250)). There is a question of what things we'd like to add here. For sure: `black`. I think import checks (e.g. no unused imports) and `flake8` would be good too. We can also add custom checks for things like slow imports. I think this would be a good time to run `black` over the whole codebase so we don't have exempted files any more. My questions for the dev team:. * Does this sound good?; * Do you have more ideas for checks/ tools?. @michalk8, I saw you added this to [`squidpy`](https://github.com/theislab/squidpy/pull/203). How's the experience been there – that is, any major foot guns we should look out for? Also, are there any tools you're using (beyond the basic `black`, `isort`, `flake8`) you'd especially recommend?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563:508,depend,depends,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563,1,['depend'],['depends']
Integrability,"I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis (like UMAP or Louvain) on this kNN graph. Is that right? I suppose this is currently not implemented for t-SNE? With openTSNE it'd be easy to use the pre-built kNN graph and run t-SNE directly on that. Also, the default parameters of t-SNE in scanpy could IMHO be improved, see https://www.nature.com/articles/s41467-019-13056-x. Some of these recommendations (learning rate, initialization) are now default in openTSNE. There are some open issues at scanpy related to t-SNE such as https://github.com/theislab/scanpy/issues/1150 and https://github.com/theislab/scanpy/issues/996 but I think this suggestion would supersede them. We had a brief discussion of this at openTSNE here https://github.com/pavlin-policar/openTSNE/issues/102. I can see four somewhat separate suggestions:. 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph; 2. add tSNE support for `ingest` using openTSNE functionality.; 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults.; 4. add some tSNE ""recipes"" based on https://www.nature.com/articles/s41467-019-13056-x. What of this, if any, makes sense from the scanpy point of view?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233:213,depend,depend,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233,1,['depend'],['depend']
Integrability,"I'm trying to import some data I downloaded from GEO using the read_10x_mtx() function. Since this data was generated with an older version of Cellranger, there is no features.tsv.gz file. I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. I am pasting the error message below: . ```pytb; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=Non",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:302,message,message,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['message'],['message']
Integrability,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:506,message,message,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['message'],['message']
Integrability,"I'm using scanpy 1.8.2, anndata 0.8.0 and h5py 3.1.0. I got this error while reading an h5ad file:. ```; adata=sc.read_h5ad('XXXX.h5ad'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 if zarr and isinstance(elem, (zarr.Group, zarr.Array)):; --> 156 parent = elem.store # Not sure how to always get a name out of this; 157 elif isinstance(elem, SparseDataset):. /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group). /opt/conda/lib/python3.8/enum.py in __getitem__(cls, name); 386 def __getitem__(cls, name):; --> 387 return cls._member_map_[name]; 388 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-15-a2632df74a34> in <module>; ----> 1 adata=sc.read_h5ad('XXXX.h5ad'). /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). /opt/conda/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 parent = elem.file.name; 161 return parent; --> 162 ; 163 ; 164 def report_read_key_on_error(func):. AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:1154,wrap,wrapper,1154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['wrap'],['wrapper']
Integrability,I've had better luck integrating data from multiple experiments using [Harmony](https://portals.broadinstitute.org/harmony/) than the current integration methods in scanpy.external.pp. This PR adds a wrapper for Harmony to the external API.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1306:21,integrat,integrating,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1306,3,"['integrat', 'wrap']","['integrating', 'integration', 'wrapper']"
Integrability,"IPython/core/formatters.py in __call__(self, obj); ... FileNotFoundError: [Errno 2] No such file or directory: '/Users/name/.matplotlib/fontlist-v310.json.matplotlib-lock'. ```. #### Versions. <details>. ```; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; 0294638c8bf50491b025b096f3dba0a1 NA; absl NA; anyio NA; appnope 0.1.0; astunparse 1.6.3; attr 19.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; certifi 2020.06.20; cffi 1.14.5; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; gast NA; get_version 2.2; google NA; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.3; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.36.0; markupsafe 1.1.1; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.0.7; numba 0.53.1; numexpr 2.7.3; numpy 1.19.0; opt_einsum v3.3.0; packaging 20.4; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2019.3; requests 2.24.0; scipy 1.4.1; seaborn 0.10.0; send2trash NA; six 1.14.0; sklearn 0.23.1; sniffio 1.2.0; statsmodels 0.12.2; storemagic NA; swig_runtime_data4 NA; tables 3.6.1; tensorboard 2.2.2; tensorflow 2.2.0; termcolor 1.1.0; texttable 1.6.3; tornado 6.1; traitlets 4.3.3; typing_extensions NA; umap 0.5.1; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.3.1; zipp NA; zmq 19.0.1; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 3.0.5; notebook 6.0.3; -----; Python 3.7.7 (v3.7.7:d7c567b08f, Mar 10 2020, 02:56:16) [Clang 6.0 (clang-600.0.57)]; Darwin-20.2.0-x86_64-i386-64bit; 4 logical CPU cores, i386; -----; Session information updated at 2021-05-26 22:36. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1857:2387,wrap,wrapt,2387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1857,1,['wrap'],['wrapt']
Integrability,Identify optional dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59:18,depend,dependencies,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59,1,['depend'],['dependencies']
Integrability,Ingest won't integrate datasets of different lengths,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085:13,integrat,integrate,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085,1,['integrat'],['integrate']
Integrability,Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:0,Integrat,Integrate,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,1,['Integrat'],['Integrate']
Integrability,"Integrated [Shannon Component Analysis](https://www.biorxiv.org/content/10.1101/2021.01.19.427303v2.full) into the external API, with full documentation and references. SCA operates like PCA, storing a lower-dimensional representation of `adata.X` (or the chosen `layer`) in `adata.obsm[key_added]`. . Like PCA and ICA, SCA is linear; however, we have found SCA representations better than PCA or ICA at separating true cell types, yielding better clusters downstream. The source repository can be found [here](https://github.com/bendemeo/shannonca) and installed using `pip`. If you decide you'd like to integrate this into the main API (i.e. as `sc.tl.sca`), I would be happy to assist.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780:0,Integrat,Integrated,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780,2,"['Integrat', 'integrat']","['Integrated', 'integrate']"
Integrability,Integration across SmartSeq2 and 10X Datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662,1,['Integrat'],['Integration']
Integrability,Integration of Marsilea to create Heatmap from AnnData,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512,1,['Integrat'],['Integration']
Integrability,Integration of dorothea and progeny,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:0,Integrat,Integration,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,1,['Integrat'],['Integration']
Integrability,"Is it possible to have one figure pf spatial gene expression stack over (superimpose) another? The following function will give me two subplots instead of an integrated one ; `sc.pl.spatial(ada, img_key=""hires"", color=[""Gene1"", ""Gene2""]); `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2284:158,integrat,integrated,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2284,1,['integrat'],['integrated']
Integrability,Is there a way to trace back the version of Scanpy used from an h5ad file ? Our collaborator has shared some h5ad files generated over a year ago and I wanted to figure out the exact versions Scanpy and other dependencies used. Thank you.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1192:209,depend,dependencies,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1192,1,['depend'],['dependencies']
Integrability,"It works fine when I set the color to 'seurat_clusters'. But I want to show the pie chart for each node so I changed the color parameter according to the format in the documentation. This caused a ""unhashable type: 'dict'"" error. I looked into the paga.py code but couldn't figure out where the problem is. . The 'seurat_clusters' contains 8 clusters:. ```; adata.obs['seurat_clusters'].cat.categories; ```; `Index(['0', '1', '2', '3', '4', '5', '6', '7'], dtype='object')`. My command-line to run pl.paga with pie nodes:; ```; sc.pl.paga(adata,color={'0':{'red':0.2,'blue':0.8},'1':{'red':0.2,'blue':0.8},'2':{'red':0.2,'blue':0.8},'3':{'red':0.2,'blue':0.8},'4':{'red':0.2,'blue':0.8},'5':{'red':0.2,'blue':0.8},'6':{'red':0.2,'blue':0.8},'7':{'red':0.2,'blue':0.8}}, root=3, layout='eq_tree', node_size_scale=2, save='PAGA_tree.pdf'); ```. Error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-327-1f686f2dc40b> in <module>(); ----> 1 sc.pl.paga(adata,color=colors). /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, fontoutline, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 474 or (c in var_names); 475 ); --> 476 for c in colors; 477 ]; 478 else:. /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/_tools/paga.py in <listcomp>(.0); 474 or (c in var_names); 475 ); --> 476 for c in colors; 477 ]; 478 else:. /nfs/med-bfx-activeprojects/weishwu/common/Anaconda3/envs/scan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1497:849,message,message,849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497,1,['message'],['message']
Integrability,"It would be nice if we had a better way of handling upstream releases. E.g. when pandas makes a release it would be good that we had tested against their release candidates, or if we had a good process for dealing with bugs if they do occur. One think we could do, is defensively pin dependencies to below their current release series. I don't like doing this since I think it's pretty restrictive when most of the time we don't have issues. Maybe we could do this for breaking releases, but that wouldn't have prevented issues like #1917. It would be nice to automate the process of testing against upstream release candidates. Basically, when something comes out, we build against it so we can report issues early and don't have to deal with it in live releases. I'm not sure how to do this with `pip search` not working anymore.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1919:284,depend,dependencies,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1919,1,['depend'],['dependencies']
Integrability,"It'd be nice if some level of tab completion of argument names could be maintained for functions like `sc.pl.umap`. This came up recently in #455, and I find myself frequently misspelling (mostly mis-pluralizing) argument names like `color/ colors` and `gene_name/ gene_names`. I've given this a shot using `functools.wraps`, but no luck yet. Any ideas on if we could do this @flying-sheep?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535:318,wrap,wraps,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535,1,['wrap'],['wraps']
Integrability,"Let's say I start a new session and generate a plot, then save it. All is fine. When I plot anything after the first save, a ""Do not localize"" message pops up and so does the previous ""save as"" window. The do not localize message window cannot be exited out of, and so I drag them to the upper right corner of the screen so they're out of the way. Then I exit out of the previous save as window, which by the way if you try to use it to save the current figure, it won't work. So I exit out of that window. But you can click the save button on the figure itself, then you can save. As I continue to plot and save figures, the do not localize windows pile up, and the chain of previous save as windows continues to grow. If you can follow my explanation, you can probably get the sense of how bothersome this can be. I have to go through the process of dragging the accumulating do not localize windows to the corner, and exiting out of the accumulating save as windows as I continue to generate and save figures. Has there been any similar experiences and if so how do I get rid of this situation?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/202:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202,2,['message'],['message']
Integrability,"Louvain is being difficult to build since a new setuptools release dropped any python2 compatibility https://github.com/vtraag/louvain-igraph/issues/57. We've largely worked around this in #2063, by making louvain dependent tests optional. However, the paul15 PAGA test is difficult to extract louvian from. It checks hardcoded values based on the results of a louvain clustering. To adapt this test to use leiden, we would have to redo the tutorial and create new results. Or louvain building could be fixed, but the package is deprecated anyways.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2065:214,depend,dependent,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2065,1,['depend'],['dependent']
Integrability,"M / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1368,wrap,wrapper,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,Make sure dependencies are up to date in travis builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320:10,depend,dependencies,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320,1,['depend'],['dependencies']
Integrability,Message from highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/411:0,Message,Message,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/411,1,['Message'],['Message']
Integrability,Minimum dependency test job,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816:8,depend,dependency,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816,1,['depend'],['dependency']
Integrability,"My version of scanpy:; scanpy 1.8.1 pyhd8ed1ab_0 conda-forge; I'm working on a linux system based server, and uses miniconda3 for environment management.; After some changes in my environment, I tried to run the routine process of my analysis.; But when running sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40), I encountered the following error: . > Traceback (most recent call last):; File ""/data1/exhaustT/process.py"", line 118, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""/data1/exhaustT/umap.py"", line 48, in <module>; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""/data1/exhaustT/miniconda3/lib/python3.9/site-packages/scanpy/neighbors/ __init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; ModuleNotFoundError: No module named 'umap.umap_'; 'umap' is not a package. I've tried to re-install umap-learn from conda-forge, and/or simply pip install umap, neither worked for me. #update, the problem found to be that I named my own script umap.py, even though it's in a different direction, it still caused trouble.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1987:212,rout,routine,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1987,1,['rout'],['routine']
Integrability,"Need a file `normalization.py` in `scanpy.preprocessing`. In that, there should be cleaned-up versions of `normalize_per_cell` and `normalize_per_cell_weinreib17` without any recursive calls and no calls to `filter_cells`. The new names of these function should be `normalize_total` and `normalize_quantile`. There should be a backup version of `normalize_per_cell` that behaves exactly as it currently does for backwards compatibility, which wraps `filter_cells` and `normalize_per_cell`. In the ideal case, `normalize_total` would wrap `normalize_quantile(quantile=1)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/301:443,wrap,wraps,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301,2,['wrap'],"['wrap', 'wraps']"
Integrability,"NeighborsTransformer; - [x] unify selection: algorithm+backend, metric, connectivity (maybe separate out connectivity); - [x] figure out how to do connectivities: can mode be changed after fit? *no, we just use umap connectivities as before*; - [x] check out where we have coverage; - is there paga specific stuff? *not in the parts I changed*; - gauss: dense matrix when knn=True (“build a symmetric mask”, …) *not covered, but also the logic shouldn’t have changed*; - pre-computed in umap transformer; handling of disconnected components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:2459,wrap,wrapper,2459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,2,['wrap'],['wrapper']
Integrability,"No recent version of legacy-api-wrap has been uploaded to conda. So, we can't make a conda release of scanpy 1.10. * https://github.com/conda-forge/scanpy-feedstock/pull/15. Since it's a single file with a single function, I'm very up for vendor-ing it:. * https://github.com/scverse/anndata/issues/1301. cc: @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966:32,wrap,wrap,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966,1,['wrap'],['wrap']
Integrability,"Numba can’t correctly detect when a threading backend is available. This was fine in the past, since `pynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, y",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:861,depend,dependency,861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependency']
Integrability,"OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2404:31,rout,routine,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2404,1,['rout'],['routine']
Integrability,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/201:326,message,message,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201,1,['message'],['message']
Integrability,"Partially reverts #2220 to restore CI to a functional state. I will skip the review since this is a revert of a breakage. - broken links are banned again; - the tutorial links are just links again, but using intersphinx and therefore still not broken. A follow up PR can make the tutorials a submodule again if we go that route, but we won’t disable link checking again. Not for a PR like this, and not for anything else. PS: one of the `{tutorial}` links wasn’t converted to a ``{doc}`/tutorials/…` `` link in #2220. Fixed that too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2614:322,rout,route,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2614,1,['rout'],['route']
Integrability,"Potentially fixes #1355. * Would still need tests/ further consideration.; * Need to fix missing values not being plotted below present ones. Using this branch:. ```python; import scanpy as sc; import numpy as np; import matplotlib as mpl. pbmc = sc.datasets.pbmc3k_processed(); pbmc.obs[""louvain""].iloc[::2] = np.nan; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain""); ```. ![image](https://user-images.githubusercontent.com/8238804/89258138-e54b0d80-d66a-11ea-8d13-e7bf975c3203.png). ```python; with mpl.rc_context({""figure.dpi"": 150}):; sc.pl.umap(pbmc, color=""louvain"", groups=list(pbmc.obs[""louvain""].cat.categories[:3])); ```. ![image](https://user-images.githubusercontent.com/8238804/89258165-f6941a00-d66a-11ea-9eaf-3a51a5a49a38.png). ## Update:. This PR expanded in scope quite a bit, so I'd like to wrap it up. Most things are implemented and seem to work. Regression tests need to be added, for these cases. - [x] Tests for all fixed cases (probably write these down as well); - [x] Decide on adding arguments, and default values; - [x] Decide on whether continuous legend update happens in this PR",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356:837,wrap,wrap,837,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356,1,['wrap'],['wrap']
Integrability,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:182,wrap,wrap,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,1,['wrap'],['wrap']
Integrability,Question: plans for scATAC integration with scRNA?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/725:27,integrat,integration,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/725,1,['integrat'],['integration']
Integrability,"Ref https://github.com/theislab/scanpy/issues/1698#issuecomment-824634639, cut down in https://github.com/theislab/scanpy/issues/1698#issuecomment-826712541. Basically, `gearys_c`, `morans_i` can run into silent numba parallelization errors when all values for a variable are the same. We should specifically address these cases. As suggested by @Hrovatin, `nan` and a warning is probably sufficient for this case. That said, it may take some work to consistently throw a warning. * Depending on the threading backend, this may error (solving the problem of incorrect values); * We may be able to control this, but how the parallel backend interacts with the `error_model` seems unclear; * I don't think we can throw a warning from numba code, let alone parallel numba code; * It would be nice if we could guarantee a `tbb` or `omp` threading backend, since they seem more consistent, but I personally have not been able to `pip install` these for a bit over a year now. This may be macOS specific.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806:483,Depend,Depending,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806,1,['Depend'],['Depending']
Integrability,"Remove batch effect (""Integrate"" in Seurat"")",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873:22,Integrat,Integrate,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873,1,['Integrat'],['Integrate']
Integrability,Remove dependency on scvelo for doc builds,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1608:7,depend,dependency,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608,1,['depend'],['dependency']
Integrability,Remove legacy-api-wrap dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1926:18,wrap,wrap,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1926,2,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,Remove pytables dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064,1,['depend'],['dependency']
Integrability,"Removes the need for a pytables dependency. * pytables has been a source of installation heisenbugs, particularly on windows (#1468, #1284, #454); * why use two hdf5 libraries; * Makes it easier to move reading 10x files into anndata or elsewhere #1387. I've edited the code as lightly as possible, since these readers were originally contributed by someone at 10X, so I assume they had better knowledge of possible edge cases. - [ ] ~~Test with h5py 2~~; - [ ] Add release note",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064:32,depend,dependency,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064,1,['depend'],['dependency']
Integrability,"Right now we get different PCs when using sparse data (applying TruncatedSVD) compared to using dense data (applying PCA). . `TruncatedSVD(X-X.mean(0))` would be equivalent to `PCA(X)`. `X-X.mean(0)` would obviously not be sparse anymore, which is why it is currently implemented as `TruncatedSVD(X)`. The first PC will be mainly representing the vector of means, thus be very different from zero-centered PCA. The following components would approximately resemble PCA. However, since all subsequent PCs are orthogonal to the first PC, we will never get to the exact solution. Hence, the PCs are questionable, in particular when the very first ones are quite misleading. That's not desirable. I think we should obtain the same PCA representation regardless of the data type. Don't we have to densify `X` at some point anyways, as we would have to compute `X.dot(X.T)`. Thus it might be worth thinking of some EM approach?. Whatsoever, I think as long the data is manageable and fits into the RAM, we should just use the densified `X`. . Line 486 in preprocessing/simple I don't quite understand:; ```; if zero_center is not None:; zero_center = not issparse(adata_comp.X); ```; It doesn't depend on the actual value of the attribute `zero_center` anymore. Is that a bug, or what is the rationale behind this?. For now, we can change that into something like; ```; zero_center = zero_center if zero_center is not None else False if issparse(adata.X) and adata.X.shape[0] > 1e4 else True; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393:1189,depend,depend,1189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393,1,['depend'],['depend']
Integrability,ScanPy's BSD license and GPLed dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3272:31,depend,dependencies,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272,1,['depend'],['dependencies']
Integrability,"Scanpy vs. 1.3.6; installed using pip3; OSX 10.10.5; Jupyter lab. code:; `list_of_list_of_marker_genes = [mg1, mg2, mg3]; for mg in list_of_list_of_marker_genes:; sc.pl.stacked_violin(adata, mg, groupby = 'louvain’, rotation=90); print(mg)`. This works for some mg's but not for all; sc.pl.matrixplot works for all of them; the same for sc.pl.violin; it works also if I would combine all marker genes into one list and then run; sc.pl.stacked_violin. When I say it does not work: it typically generates one stacked plot and then; 'hangs up' with an error message for subsequent plots:. Error message:. IndexError: list index out of range. When I run sc.pl.stacked_violin for the individual mg’s, some work others don’t. It is a reproducible results. I explicitly tested whether the genes in the respective marker gene lists are present in ’n_cells’. If not, I update the lists. What am I missing here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/405:555,message,message,555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405,2,['message'],['message']
Integrability,Skip louvain-dependent tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1524:13,depend,dependent,13,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1524,1,['depend'],['dependent']
Integrability,"Some of the arguments for bbknn have changed, so the wrapper is broken at the moment. https://github.com/Teichlab/bbknn/issues/10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/635:53,wrap,wrapper,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635,1,['wrap'],['wrapper']
Integrability,Tansfering data integration fom scanorama to a new dataset,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162:16,integrat,integration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162,1,['integrat'],['integration']
Integrability,Test against pre-release dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:25,depend,dependencies,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['depend'],['dependencies']
Integrability,"Test collection was taking a while, this should cut that time by about 90%. For some reason, pytest test collection seems dependent on the number of files the `python_files` parameter matches. Having a more specific path under `testpaths` doesn't – which would be a simpler change. Should close #326.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/327:122,depend,dependent,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327,1,['depend'],['dependent']
Integrability,"Thank you for creating `scanpy`! It's such a useful tool!. ## Overview of request; Is there any chance that you can start tagging versions of your documentation on readthedocs? Currently, the only available versions are **stable** and **latest**. That makes it easy to view the current documentation but difficult to view docs from old releases of `scanpy`. ## Desired behavior; I would like to be able to...; 1. Click on `v: stable` in the flyout menu at the bottom-left corner of the page; ![image](https://user-images.githubusercontent.com/23412689/220255767-3fbf84e9-ccf5-420b-b067-7a4054aed047.png); 2. Click on the version of the documentation tagged for the older release of `scanpy` that I'm using. ## Resources; - [How to version RTD documentation](https://docs.readthedocs.io/en/stable/versions.html); - [How to automatically maintain versions upon creating new git tags](https://docs.readthedocs.io/en/stable/automation-rules.html#activate-all-new-tags); - [Automated versioning](https://docs.readthedocs.io/en/stable/integrations.html#automated-versioning) as part of continuous documentation deployment. ## An example; As an example of versioned documentation, take a look at [Snakemake's documentation](https://snakemake.readthedocs.io/). ![image](https://user-images.githubusercontent.com/23412689/220255676-39e9eee9-e658-4866-b8b9-63a6174577ba.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2425:1029,integrat,integrations,1029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2425,1,['integrat'],['integrations']
Integrability,"The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053:206,depend,depend,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053,1,['depend'],['depend']
Integrability,"The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray); > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3332:306,Depend,Depending,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332,1,['Depend'],['Depending']
Integrability,"The packages dorothea-py and progeny-py are now deprecated, instead one should use decoupler (https://github.com/saezlab/decoupler-py). It contains both prior knowledge resources plus many more since it integrates the meta-resource OmniPath, and it also contains many footprint enrichment methods instead of a fixed one.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2186:203,integrat,integrates,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2186,1,['integrat'],['integrates']
Integrability,"The tests started failing with 4499d9460b8fe6e08fffb3c4cae0948849d40586, which obviously changed nothing that could make tests fail. So I’m sure some dependency updating is responsible (Travis started installing the newwer versions and the tests fail). We need to investigate why that is and how to fix it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/162:150,depend,dependency,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162,1,['depend'],['dependency']
Integrability,These changes enable Scanpy's pre-processing functions to run on distributed engines including Dask and Spark. The Spark integration itself relies on [Zap](https://github.com/lasersonlab/zap) for a distributed version of NumPy. . The main change is the `materialize_as_ndarray` function that is used at certain points of the computation to materialize intermediate results (not the full matrix). This is a no-op in the non-distributed case.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/283:121,integrat,integration,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/283,1,['integrat'],['integration']
Integrability,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661:1221,depend,dependencies,1221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661,1,['depend'],['dependencies']
Integrability,"This PR addresses https://github.com/theislab/scanpy/issues/1645, which was caused by my injection of normalised matrices from Scanpy workflows, thereby bypassing a sparseness check Scrublet does with the raw matrix. The fix contained here is apply the Scrublet sparseness check, before calling Scrublet functions with a dependency on sparseness.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1707:89,inject,injection,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1707,2,"['depend', 'inject']","['dependency', 'injection']"
Integrability,"This PR addresses https://scanpy.discourse.group/t/workflow-for-selecting-number-of-marker-genes-in-sc-queries-enrich/286. I wanted to have a simple interface to get the top n marker genes. Right now, `rank_genes_groups_df` only allows to threshold on logfc and pval, but especially for marker genes pval computation might not be statistically meaningful. It adds the following kind of functionality:; ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, 'louvain'). print(sc.get.rank_genes_groups_df(adata, ""1"", n_top_genes=2)); ```; output is just the top 2 genes of the list.; ```; names scores logfoldchanges pvals pvals_adj; 0 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; ```. it also works for multiple groups:. ```python; print(sc.get.rank_genes_groups_df(adata, None, n_top_genes=2)); ```; ```; group names scores logfoldchanges pvals pvals_adj; 0 0 CD3D 26.250046 3.859759 4.379061e-75 2.233321e-73; 1 0 LDHB 21.207499 2.134979 1.488480e-67 5.993089e-66; 2 1 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 3 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; 4 2 LYZ 38.981312 5.096991 1.697105e-172 1.298285e-169; 5 2 CST3 34.241749 4.388617 1.448193e-149 5.539337e-147; 6 3 NKG7 34.214161 6.089183 2.356710e-55 2.575547e-53; 7 3 CTSW 24.584066 5.091688 2.026294e-39 9.118324e-38; 8 4 CD79A 52.583344 6.626956 4.032974e-84 7.713062e-82; 9 4 CD79B 32.102913 4.990217 1.958507e-51 1.872822e-49; 10 5 FTL 26.084383 1.844273 1.236398e-74 2.364611e-72; 11 5 LST1 25.554073 3.170759 5.653851e-81 4.325196e-78; 12 6 LYZ 31.497107 4.328516 9.041131e-106 6.916466e-103; 13 6 CST3 23.850258 3.281016 2.491629e-83 9.530482e-81; 14 7 CST3 33.024582 4.195395 5.768439e-136 4.412856e-133; 15 7 LYZ 31.264187 4.267053 9.712334e-101 1.485987e-98; 16 8 PPIB 39.260998 3.990153 7.159966e-47 3.651583e-45; 17 8 MZB1 33.305500 8.979518 7.611322e-26 1.878278e-24; 18 9 STMN1 27.133045 5.9",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2145:149,interface,interface,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2145,1,['interface'],['interface']
Integrability,"This PR addresses issues from #979 and #1103 related to `sc.pl.dotplot` and replaces PR #1127. Furthermore, the current PR attempts to unify common code between `dotplot`, `matrixplot` and `stacked_violin` plots while at the same time adding more flexibility to the plots. . This PR also makes possible to plot fold changes, log fold changes and p-values from `sc.tl.rank_genes_groups` as suggested in #562. To achieve this the `sc.pl.dotplot`, `sc.pl.matrixplot` and `sc.pl.stacked_violin` method had been transformed into wrappers for the new `DotPlot`, `MatrixPlot` and `StackedVioling` classes. Accessing the new classes directly allows further fine tuning of the plots. The use of the `sc.pl.dotplot`, `sc.pl.matrixplot` and `sc.pl.stacked_violin` didn't change, only the new parameter `return_fig` was added. For clarity, the relevant code has been moved to `scanpy.plotting._groupby_plots.py`. . The new plot classes are all descendants of a `BasePlot` class that captures most of the common code shared between the plots. The design of the classes follows the method chaining (as found in Pandas or Altair). This allows the addition of independent features (via well documented methods) to the plot without increasing the number parameters of a single function. This was first suggested here #956. . For example, for dotplot is now is possible to `add category totals`, define the titles for the colorbar and size legend and modify the values for edge color and edge line with for dot plot and the size exponent used (among may other options). For example:. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], ; 'myeloid': ['CST3', 'LYZ']}; dp = sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True); dp.add_totals(size=1.2)\; .legend(color_title='log(UMI count+1)', width=1.6, show_size_legend=True)\; .style(cmap='Blues', dot_edge_color='black', dot_edge_lw=1, size_exponent=1.5)\; .show(); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210:524,wrap,wrappers,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210,1,['wrap'],['wrappers']
Integrability,"This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki pag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:763,interface,interfaces,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['interface'],['interfaces']
Integrability,This PR adds the `--doctest-modules` flag to pytest and makes changes necessary to allow the doctests to run. Those changes include:. - Fixing verbosity; - using [pytest-doctestplus](https://github.com/scientific-python/pytest-doctestplus) to skip doctests entirely or when their dependencies are absent; - fixing broken doctests (the bulk of the changes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605:280,depend,dependencies,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605,1,['depend'],['dependencies']
Integrability,"This PR aims to add more GPU functionalities and to integrate more an exisiting one:; * `tl.draw_graph` and `tl.leiden` can now both be GPU accelerated using rapids framework.; * on `pp.neighbors`, now 'rapids' method allows for more metrics. Calculated distance does not need to be root squared anymore (_See https://github.com/rapidsai/cuml/issues/1078#issuecomment-551284134_). I also have slightly rearranged the code to integrate more 'rapids' into the general processing of neighbors, to ensure that distances and connectivities results between 'rapids' and 'umap' are the same.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533,2,['integrat'],['integrate']
Integrability,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528:195,integrat,integrated,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528,1,['integrat'],['integrated']
Integrability,"This PR introduces a family of plots to validate gene markers obtained using `scanpy.api.tl.rank_genes_groups`. The plots work similarly as `scanpy.api.pl.rank_genes_groups`:; ; ![image](https://user-images.githubusercontent.com/4964309/43768141-efa3633a-9a36-11e8-856f-3970352c33a8.png). ![image](https://user-images.githubusercontent.com/4964309/43768035-b8ba9082-9a36-11e8-91e6-8c828f456981.png). ![image](https://user-images.githubusercontent.com/4964309/43768064-c78dc64c-9a36-11e8-95ed-351ef6fff99e.png). ![image](https://user-images.githubusercontent.com/4964309/43768099-d776a510-9a36-11e8-9708-959d5d0afe92.png). Those functions are wrappers around `scanpy.api.pl.heatmap`, `scanpy.api.pl.stacked_violin`, `scanpy.api.pl.dotplot` and `scanpy.api.pl.matrixplot`. `heatmap` and `dotplot` were modified to allow the 'brackets' on top of the images. This functionality can be used directly from those plots. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771788-83bbb604-9a40-11e8-90d6-51f084343a98.png). The `pl.stacked_violin` plot was before part of `pl.violin` but I thought that the code is cleaner by separating the two plots. Also, this PR adds the new plot `scanpy.api.pl.matrixplot` that plots the average gene expression per category. E.g.:. ![image](https://user-images.githubusercontent.com/4964309/43771966-07c9bd92-9a41-11e8-818e-263dfae69b7f.png). This PR also updates several test for the plotting options and adds new ones. . I tried to update the documentation to reflect the changes. Also a new dataset called `pbmc68k_reduced` was added. This dataset is used for tests and for the example notebook [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). This dataset contains around 700 cells and 200 genes from the original 68k 10x genomics dataset and is saved as a small anndata object. It contains cell type annotations, UMAP coordinates and rank_gene_groups. The dataset can be accessed as; ```python; adata = sc.datasets.pbmc68k_r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228:642,wrap,wrappers,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228,1,['wrap'],['wrappers']
Integrability,"This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692; 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3264:160,wrap,wrapper,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python; import scanpy as sc. sc.settings.verbosity = 3; adata = sc.datasets.pbmc3k(); sc.pp.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata); sc.tl.louvain(adata, resolution=2.0, key_added='newkey'); adata.uns; ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819:576,message,messages,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819,1,['message'],['messages']
Integrability,"This also fixes a few problems namely that tests are no modules, so you should use fixtures instead of importing. If we want test tools that dependent packages can use we should create a submodule like `scanpy.test_utils` or so. I forgot to add the new locations to `tool.black.exclude`, so the files got reformatted. I hope that’s OK?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528:141,depend,dependent,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528,1,['depend'],['dependent']
Integrability,"This fixes the small progress when downloading the data as `sc.datasets.paul_15()`, etc..; Also fixes removing the file when `KeyboardInterrupt` occurs (previously, it was just on exceptions and at least for me I sometimes stop a download by interupting the jupyter kernel - now it also correctly removes the partially downloaded file). I've also opted for `requests` library, since it's much cleaner than urllib3 and as a dependency.; However, this is completely unrelated to the 2 things I've mentioned above and can be quickly reverted. Relevant PRs I could find:; - tqdm version (I opted for bumping the version [search through open issues, lowest one had 4.32]): https://github.com/theislab/scanpy/issues/1244. Not relevant:; My [reaction](https://www.youtube.com/watch?v=nixR6wVa4HY&t=72) when using the progress bar (slightly NSFW [foul language]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507:423,depend,dependency,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507,1,['depend'],['dependency']
Integrability,"This is a PR attempting to address https://github.com/theislab/scanpy/issues/173, and building on the work of @swolock in his fork (see https://github.com/theislab/scanpy/compare/master...swolock:master). . I've taken a different approach to that fork:. - Various component functions wrapped individually (core Scrublet functionality, doublet simulation), to allow for custom workflows with different Scanpy preprocessing where required.; - ... but a scrub_doublets() function that does pre-processing analogously with Scrublet (but using Scanpy calls) is what imagine most people will use. ; - I've moved what was sensible to use Scanpy functions. ; - ... but some parts are left alone. In particular the PCA seems like it's used a bit differently in Scrublet, fitting the model to the observed transcriptomes (.fit()) and then applying dimensionality reductions to both observed and simulated matrices (.transform()). That's complex to implement with Scanpy functions (which just uses fit_transform()), so thought I'd leave that in Scrublet (which calls the same libraries anyway). Similarly the neighbour graph generation done in the classifier is quite wired in, so I haven't tried to pull it into Scanpy. I hope that this is a low-maintenance solution, allowing use of Scrublet with Scanpy functions, without a large amount of re-implementation which might cause drift wrt Scrublet itself. . It does seem to work, but happy to make any changes you or @swolock want- would just be really useful for us to have this in Scanpy :-).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476:284,wrap,wrapped,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476,1,['wrap'],['wrapped']
Integrability,"This is a bit of a catch all to improve cite-seq support. Currently a dependency of https://github.com/theislab/scanpy-tutorials/pull/14. I'll write a bit more about this after some sleep. The main goals here are:. * Implement analysis functions for cite-seq data (like joint clustering, geometric mean normalization, etc.); * Improve generality of existing APIs. Basically, if the antibody counts are in obsm, we should be able to apply most functions to that. This is not currently the case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117:70,depend,dependency,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117,1,['depend'],['dependency']
Integrability,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:112,integrat,integration,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,1,['integrat'],['integration']
Integrability,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:26,integrat,integrate,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['integrat'],['integrate']
Integrability,"This is an implementation for the support of anndatas with multiple tissues (slides). It depends on the anndata PR theislab/anndata#329; I made a short notebook to explain how it should work https://github.com/giovp/spatial-scripts/blob/master/multiple_slices_functionality.ipynb. There is a lot of room for improvement in terms of code, but in terms of functionality it should cover most of the use cases",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1073:89,depend,depends,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1073,1,['depend'],['depends']
Integrability,"This is basically the minimum amount of changes to separate things out and fix some problems with the test setup, plus unification of how we handle optional dependencies in tests. Fixes #2225",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235:157,depend,dependencies,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235,1,['depend'],['dependencies']
Integrability,"This is in an Ubuntu 16..04 Docker container:; ```; docker run --rm -it ubuntu:16.04; ```; Then I ran:; ```; apt-get update && apt-get install -y \; python3-pip \; python3-setuptools; python3-wheel. pip3 install scanpy; ```. I get the following output (after all of the dependencies are downloaded):. ```; warning: manifest_maker: standard file '-c' not found; ; reading manifest file 'scanpy.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; writing manifest file 'scanpy.egg-info/SOURCES.txt'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/tmp/pip-build-33o4crd7/scanpy/setup.py"", line 51, in <module>; 'Topic :: Scientific/Engineering :: Visualization',; File ""/usr/lib/python3.5/distutils/core.py"", line 148, in setup; dist.run_commands(); File ""/usr/lib/python3.5/distutils/dist.py"", line 955, in run_commands; self.run_command(cmd); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3/dist-packages/wheel/bdist_wheel.py"", line 179, in run; self.run_command('build'); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:270,depend,dependencies,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['depend'],['dependencies']
Integrability,"This is inspired by the Spring exporter. It also fills a directory with output files, but in addition to the matrix, adds cluster markers, meta data and embeddings. I keep the conversion code in the cellbrowser package, so I can change it easily in the future. We could also copy the whole function over, but I don't know if this makes a lot of sense, as people will need the cellbrowser package anyways and currently it doesn't have any dependencies, so should be easy to install. This function can optionally convert to html files, start a webserver and serve the result.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382:438,depend,dependencies,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382,1,['depend'],['dependencies']
Integrability,"This is the only way the README renders on PyPI. depends on github/markup#1222, which in turn depends on jch/html-pipeline#302",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/234:49,depend,depends,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/234,2,['depend'],['depends']
Integrability,This pr moves the (in)famous `_prepare_dataframe` function from scanpy.plotting._anndata to sc.get as `_indexed_expression_df` (bcs why would it be in plotting anyway) and implements a simple public interface called `sc.get.summarized_expresion_df` which simply provides nonzero mean/var and fraction using `_indexed_expression_df` function. . As discussed here (https://github.com/theislab/scanpy/pull/1388#issuecomment-678739734) we can use this in rank_genes_groups_df.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1390:199,interface,interface,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1390,1,['interface'],['interface']
Integrability,"This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s ; after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3294:253,depend,depends,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294,2,['depend'],"['depending', 'depends']"
Integrability,"This still works in `1.4.4.post1`. It's very likely caused by changes to `setup.py`. I experienced similar problems before and fixed them via `package_data`. But this got removed. It's probably only a problem for the source-based installs. https://github.com/theislab/scanpy/commit/881f0bef31cdfe0df7333641dc847a60894b5c41#diff-2eeaed663bd0d25b7e608891384b7298. ```; >>> import scanpy; >>> scanpy.__version__; <Version('1.4.5.post2')>; >>> scanpy.datasets.pbmc68k_reduced(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/datasets/__init__.py"", line 239, in pbmc68k_reduced; return read(filename); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 114, in read; **kwargs,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/scanpy/readwrite.py"", line 524, in _read; return read_h5ad(filename, backed=backed); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 447, in read_h5ad; constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/readwrite/read.py"", line 481, in _read_args_from_h5ad; f = h5py.File(filename, 'r'); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/anndata/h5py/h5sparse.py"", line 162, in __init__; **kwds,; File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 312, in __init__; fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr); File ""/Users/alexwolf/miniconda3/lib/python3.6/site-packages/h5py/_hl/files.py"", line 142, in make_fid; fid = h5f.open(name, flags, fapl=fapl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/995:1792,wrap,wrapper,1792,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995,2,['wrap'],['wrapper']
Integrability,"This uses the `__array__` method on ndarray-like classes to convert from; a distributed array to a regular NumPy ndarray when `materialize_as_ndarray` is called. This was prompted by an [improvement in Zappy](https://github.com/lasersonlab/zappy/pull/7) (released in 0.2.0) that makes Zappy arrays implement the `__array__` method. As another benefit, this change should make it easier to use other implementations of the ndarray interface in Scanpy in the future. Note that there is still a Dask-specific call for the case of a sequence of arrays, since Dask can materialize these in one call to `compute`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439:430,interface,interface,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439,1,['interface'],['interface']
Integrability,This. - moves all the external plotting routines to `scanpy/external/pl.py`; - adds one for harmony; - Fixes a plotting bug this triggered; - Makes harmony the first tool using obsp/varp. @awnimo can you please test this? Does the plot look like you want it to?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1004:40,rout,routines,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1004,1,['rout'],['routines']
Integrability,Unlock h5py dependency,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1006:12,depend,dependency,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1006,1,['depend'],['dependency']
Integrability,Update doc dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2775:11,depend,dependencies,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2775,1,['depend'],['dependencies']
Integrability,Update setup.py to update dependent packages,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/518:26,depend,dependent,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/518,1,['depend'],['dependent']
Integrability,"Update to `downsample_counts` to allow downsampling total counts, similar to normalization by `cellranger aggr` (I'm pretty sure on this, there's a lot going on in their code). Additionally, enabled caching for the `numba`'d function, which cuts down on test time. As adding this feature meant renaming `target_counts` to `counts_per_cell`, this becomes a breaking change. Since it's breaking, I've also gone ahead and set `replace=False` by default as mentioned before (#340). Definitely willing to make changes. I've implemented this since I'm doing some integration work and figured it'd be nice to be able to try the basic `cellranger` strategy. Edit: The failing PAGA test occurs locally on master as well, but I don't think I broke that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474:557,integrat,integration,557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474,1,['integrat'],['integration']
Integrability,Use pynndescent dependency to support threaded nearest neighbors,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,1,['depend'],['dependency']
Integrability,"Using the progress bar from tqdm.auto causes a `ImportError` when `ipywidgets` is not installed. ; The progressbar from the top level `tqdm` module does not have this dependency. . Repex: . ```python; import scanpy as sc; sc.datasets.moignard15(); ```. Output: ; ```; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols); 97 else: # No total? Show info style bar with no progress tqdm status; ---> 98 pbar = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filena",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:167,depend,dependency,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['depend'],['dependency']
Integrability,"We have some failing tests due to a couple bugs introduced in pandas 1.3.0:. * https://github.com/pandas-dev/pandas/issues/42380. I think this one is small in scope. Has problems when `df.agg` is called, when all columns are categorical and index is non-unique. Definitely a bug in pandas, and I don't think we do this much. Switching to `df.apply` works around the problem. * https://github.com/pandas-dev/pandas/issues/42376. Assignment of single columns `np.matrix` to dataframe columns no longer works as if the matrix were a 1d array. I think this is a bug since it's an undocumented behaviour change. Fixes are pretty easy, since we can just wrap occurrences with `np.ravel`, however I wouldn't be surprised if there were many places in the codebase that this happened. I would be good if these didn't happen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1917:648,wrap,wrap,648,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1917,1,['wrap'],['wrap']
Integrability,We never wanted APIs that can be used with more that ~2 positional parameters. We should go to keyword-only-parameters. This can be done via [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/464:153,wrap,wrap,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/464,2,['wrap'],['wrap']
Integrability,We should replace the whole logging module with. ```py; import logging. root_logger = logging.getLogger('scanpy'); root_logger.setLevel('INFO'); root_logger.propagate = False # Don’t pass log messages on to the root logger and its handler. handler = logging.StreamHandler(sys.stderr) # Why did we use stdout?; handler.setFormatter(logging.Formatter('%(message)s')); handler.setLevel('INFO'); root_logger.addHandler(handler). def get_logger(name):; return root_logger.getChild(name); ```. and in all submodules just do. ```py; from .logging import get_logger. logger = get_logger(__name__); ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/256:192,message,messages,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/256,2,['message'],"['message', 'messages']"
Integrability,"We want notebooks that automatically run through on readthedocs: https://nbsphinx.readthedocs.io/en/0.3.5. So that the basic tutorial (https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb) can be integrated into the scanpy repository in `docs/tutorials` as a notebook with all output cleared (no images etc.). It is run on the readthedocs server and will produce exactly the same output, as is guaranteed by this test: https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/pbmc3k.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/302:240,integrat,integrated,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302,1,['integrat'],['integrated']
Integrability,"We're having trouble installing louvain on CI due to a recent setuptools release (would have been nice if setuptools had more vocal warnings about this ahead of time, but alas). See: vtraag/louvain-igraph/issues/57. This PR makes louvain optional. This was done by:. ### Skip louvain dependent tests. While these largely were tests checking that louvain works, some of these are testing other things. The biggest example here is `test_paga_paul15_subsampled.py`, which is really a test of PAGA. This should be corrected. ### Remove louvain dependency from tests. Some tests, like those for `rank_genes_groups_logreg` used louvain, but really didn't have to. `test_pbmc3k` could just have `louvain` calls replaced with `leiden` with only one plot triggering an error. ### `louvain` is no longer installed on CI. This should get around the build issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2063:284,depend,dependent,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2063,2,['depend'],"['dependency', 'dependent']"
Integrability,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:599,integrat,integration,599,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,2,"['depend', 'integrat']","['depending', 'integration']"
Integrability,"When I tried to import scanpy into python 3.5.2, I got the following error message,. ```; >>> import scanpy as sc; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>; from .utils import check_versions, annotate_doc_types; File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>; from ._settings import settings; File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351; f'{k} = {v!r}'; ^; SyntaxError: invalid syntax; ```; My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/855:75,message,message,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855,1,['message'],['message']
Integrability,Windows compatibility issues with dependencies,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:34,depend,dependencies,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,1,['depend'],['dependencies']
Integrability,Wishbone integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1063:9,integrat,integration,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063,1,['integrat'],['integration']
Integrability,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223:543,depend,depend,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223,1,['depend'],['depend']
Integrability,Wrap legacy APIs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2702:0,Wrap,Wrap,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2702,1,['Wrap'],['Wrap']
Integrability,Wrong dependencies on bioconda for 1.4.4,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876:6,depend,dependencies,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876,1,['depend'],['dependencies']
Integrability,[Proposal] Integrate Marsilea to visualize AnnData,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:11,Integrat,Integrate,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['Integrat'],['Integrate']
Integrability,"[`sinfo` has been replaced](https://pypi.org/project/sinfo/) with [`session_info`](https://gitlab.com/joelostblom/session_info), which is definitely a better name. We should switch over to using this. I think we'll be calling it like: `import session_info; session_info.show(dependencies=True, html=False, **extra_kwargs)`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1852:275,depend,dependencies,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1852,1,['depend'],['dependencies']
Integrability,"_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:1921,message,message,1921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['message'],['message']
Integrability,"_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py"", line 45, in <module>; @numba.njit(cache=True); File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function 'sparse_mean_var_minor_axis': no locator available for file '/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py'. ```. I would highly appreciate if you could please point out how to fix this issue. . Thank you in advance!. Best wishes,; Abdelrahman . ```. #### Versions. <details>. numba==0.53.1; scanpy==1.8.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:3450,wrap,wrapper,3450,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['wrap'],['wrapper']
Integrability,"_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 413 d[k] = read_attribute(f[k]); 414 ; 415 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/envs/rpy2_3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 else:; 161 parent = _get_parent(elem); --> 162 raise AnnDataReadError(; 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/X' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions:; ```; scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:2826,wrap,wrapper,2826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean""); ```. ```; AnnData object with n_obs × n_vars = 11 × 765; obs: 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; layers: 'mean'; ```. ```python; sc.get.aggregate(adata.obsm[""X_umap""], by=adata.obs[""louvain""].array, func=""mean""); ```. ```; {'mean': array([[ -6.18019123, -6.12846152],; [ -3.10995685, 8.4991954 ],; [ 6.30307056, -2.15245383],; [ -4.72268065, -3.24033642],; [-11.94002487, -5.39480163],; [ -1.39242794, 6.6239316 ],; [ 4.3991326 , -0.16749119],; [ 4.847834 , -9.30549509],; [-10.41891144, -1.15700949],; [ -7.91249486, -4.06782072],; [ 1.12418592, -6.94506866]])}; ```. So it returns an `AnnData` when an `AnnData` is passed, but a dict when a less structured object is passed. This is probably because it's `singledispatched` under the hood, but IDK that this behaviour is great. I think it could make more sense for this to either:. * Always return an `AnnData`; * Throw an error if something other than an AnnData is passed in. A third option is that we document this behaviour, but I generally don't love it. There are other places that we do something like this, i.e. return a different type depending on the input. However, I feel like there's more of a loss of information here and less of an obvious return type. Maybe in future this could get a `return_type: type[AnnData] | type[Dict] | type[xr.Dataset] = AnnData` argument that controls what is returned?. WDYT @ilan-gold @Intron7?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2930:1275,depend,depending,1275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2930,1,['depend'],['depending']
Integrability,a 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; imagecodecs 2024.1.1; imageio 2.34.0; importlib_metadata 7.0.2; importlib_resources 6.1.3; inflect 7.0.0; ipykernel 6.29.3; ipylab 1.0.0; ipython 8.22.2; ipywidgets 8.1.2; isoduration 20.11.0; jedi 0.19.1; Jinja2 3.1.3; joblib 1.3.2; json5 0.9.22; jsonpointer 2.4; jsonschema 4.21.1; jsonschema-specifications 2023.12.1; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyter-events 0.9.0; jupyter-lsp 2.2.4; jupyter_server 2.13.0; jupyter_server_proxy 4.1.0; jupyter_server_terminals 0.5.2; jupyterlab 4.1.4; jupyterlab_pygments 0.3.0; jupyterlab_server 2.25.3; jupyterlab_widgets 3.0.10; kiwisolver 1.4.5; lamin_utils 0.13.0; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; linkify-it-py 2.0.3; llvmlite 0.42.0; locket 1.0.0; louvain 0.8.1; lz4 4.3.3; mapclassify 2.6.1; Markdown 3.5.2; markdown-it-py 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; matplotlib-inline 0.1.6; mdit-py-plugins 0.4.0; mdurl 0.1.2; mistune 3.0.2; msgpack 1.0.7; mudata 0.2.3; multidict 6.0.5; multipledispatch 0.6.0; munkres 1.1.4; muon 0.1.5; natsort 8.4.0; nbclient 0.8.0; nbconvert 7.16.2; nbformat 5.9.2; nbproject 0.10.1; nest_asyncio 1.6.0; networkx 3.2.1; notebook 7.1.1; notebook_shim 0.2.4; numba 0.59.0; numcodecs 0.12.1; numpy 1.24.4; nvtx 0.2.10; omnipath 1.0.8; openpyxl 3.1.2; orjson 3.9.15; overrides 7.7.0; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:3851,wrap,wrap,3851,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['wrap'],['wrap']
Integrability,"a in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram integration in the plotting api for QC metrics would be helpful. While scatter plots and violin plots are effective, I find myself wanting to make cuts (bounds on percent mitochondrial or nGenes) in my data based off histograms. Again, thank you so much for the amazing software!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1723,integrat,integrate,1723,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,2,['integrat'],"['integrate', 'integration']"
Integrability,"a.caching'; 0.53.1; 0.7.8; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/librosa/__init__.py"", line 211, in <module>; from . import core; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/__init__.py"", line 5, in <module>; from .convert import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/convert.py"", line 7, in <module>; from . import notation; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/notation.py"", line 8, in <module>; from ..util.exceptions import ParameterError; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/__init__.py"", line 83, in <module>; from .utils import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py"", line 1848, in <module>; def __shear_dense(X, factor=+1, axis=-1):; File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:1836,wrap,wrapper,1836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['wrap'],['wrapper']
Integrability,"a.uns['iroot']=0; sc.tl.dpt(adata). import matplotlib.pyplot as plt; fig,ax=plt.subplots(1,1,figsize=(7,1)); path_data = sc.pl.paga_path(; adata,; [4, 5],; [""Elane""],; ax=ax,; show_node_names=False,; ytick_fontsize=12,; return_data=True,; #n_avg=1,; color_map=""Greys"",; groups_key=""leiden"",; color_maps_annotations={""dpt_pseudotime"": ""viridis""}; ); ```. ### Error output. ```pytb; TypeError Traceback (most recent call last); Cell In[1], line 15; 13 import matplotlib.pyplot as plt; 14 fig,ax=plt.subplots(1,1,figsize=(7,1)); ---> 15 path_data = sc.pl.paga_path(; 16 adata,; 17 [4, 5],; 18 [""Elane""],; 19 ax=ax,; 20 show_node_names=False,; 21 ytick_fontsize=12,; 22 return_data=True,; 23 #n_avg=1,; 24 color_map=""Greys"",; 25 groups_key=""leiden"",; 26 color_maps_annotations={""dpt_pseudotime"": ""viridis""}; 27 ). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:2084,wrap,wrapper,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wrapper']
Integrability,"ache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 716 convert_float=convert_float,; 717 mangle_dupe_cols=mangle_dupe_cols,; --> 718 **kwds); 719 ; 720 @property. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2952,wrap,wrapper,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,2,['wrap'],['wrapper']
Integrability,added integration tutorial spatial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1229:6,integrat,integration,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1229,1,['integrat'],['integration']
Integrability,"anaconda3/envs/ml/lib/python3.9/site-packages/IPython/core/pylabtools.py:151, in print_figure(fig, fmt, bbox_inches, base64, **kwargs); 148 from matplotlib.backend_bases import FigureCanvasBase; 149 FigureCanvasBase(fig); --> 151 fig.canvas.print_figure(bytes_io, **kw); 152 data = bytes_io.getvalue(); 153 if fmt == 'svg':. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/backend_bases.py:2230, in FigureCanvasBase.print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs); 2226 ctx = (renderer._draw_disabled(); 2227 if hasattr(renderer, '_draw_disabled'); 2228 else suppress()); 2229 with ctx:; -> 2230 self.figure.draw(renderer); 2232 if bbox_inches:; 2233 if bbox_inches == ""tight"":. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/artist.py:74, in _finalize_rasterization.<locals>.draw_wrapper(artist, renderer, *args, **kwargs); 72 @wraps(draw); 73 def draw_wrapper(artist, renderer, *args, **kwargs):; ---> 74 result = draw(artist, renderer, *args, **kwargs); 75 if renderer._rasterizing:; 76 renderer.stop_rasterizing(). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/artist.py:51, in allow_rasterization.<locals>.draw_wrapper(artist, renderer, *args, **kwargs); 48 if artist.get_agg_filter() is not None:; 49 renderer.start_filter(); ---> 51 return draw(artist, renderer, *args, **kwargs); 52 finally:; 53 if artist.get_agg_filter() is not None:. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/figure.py:2790, in Figure.draw(self, renderer); 2787 # ValueError can occur when resizing a window.; 2789 self.patch.draw(renderer); -> 2790 mimage._draw_list_compositing_images(; 2791 renderer, self, artists, self.suppressComposite); 2793 for sfig in self.subfigs:; 2794 sfig.draw(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/image.py:132, in _draw_list_compositing_images(renderer, parent, artists, suppress_composi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:6387,wrap,wraps,6387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['wrap'],['wraps']
Integrability,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/809:3233,wrap,wrapper,3233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809,2,['wrap'],['wrapper']
Integrability,"ast NA; genericpath NA; google NA; gprofiler 1.0.0; h5py 3.3.0; idna 3.1; igraph 0.9.6; imagecodecs 2021.6.8; imageio 2.9.0; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.36.0; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.5; ntpath NA; numba 0.53.1; numexpr 2.7.3; numpy 1.21.1; opcode NA; openpyxl 3.0.7; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pooch v1.4.0; posixpath NA; prompt_toolkit 3.0.19; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pydoc_data NA; pyexpat NA; pygments 2.9.0; pynndescent 0.5.4; pyparsing 2.4.7; pytz 2021.1; requests 2.26.0; scanpy 1.8.1; scipy 1.7.0; seaborn 0.11.1; sinfo 0.3.1; sip NA; six 1.16.0; skimage 0.18.2; sklearn 0.24.2; socks 1.7.1; soupsieve 2.0.1; sphinxcontrib NA; spyder 5.0.5; spyder_kernels 2.0.5; spydercustomize NA; sre_compile NA; sre_constants NA; sre_parse NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tensorboard 2.5.0; tensorflow 2.5.0; termcolor 1.1.0; texttable 1.6.4; tifffile 2021.7.2; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.61.2; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; urllib3 1.26.6; wcwidth 0.2.5; wrapt 1.12.1; wurlitzer 2.1.0; xlsxwriter 1.4.4; yaml 5.4.1; zmq 22.1.0; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; -----; Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) [GCC 9.3.0]; Linux-5.4.0-72-generic-x86_64-with-glibc2.27; 40 logical CPU cores, x86_64; -----; Session information updated at 2021-07-29 21:02; </details>. PS - As a side note, another issue that pandas 1.3 introduced is that adata.write() no longer works unless I manually convert every column in adata.obs back to its original data type (e.g. adata.obs['leiden']=adata.obs['leiden'].astype('int')), because I get a TypeError from h5py until I do that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:5685,wrap,wrapt,5685,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['wrap'],['wrapt']
Integrability,"ate the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs); ...; --> 700 artists = ax.bxp(**boxplot_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1815,wrap,wraps,1815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['wrap'],['wraps']
Integrability,"atest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 1019 dp = DotPlot(; 1020 adata,; 1021 var_names,; (...); 1042 **kwds,; 1043 ); 1045 if dendrogram:; -> 1046 dp.add_dendrogram(dendrogram_key=dendrogram); 1047 if swap_axes:; 1048 dp.swap_axes(). File ~/.local/share/hatch/env/virtual/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:1471,wrap,wraps,1471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wraps']
Integrability,"ath(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be conv",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4468,wrap,wrapper,4468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ath(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:1953,wrap,wrapper,1953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"atterplots.umap(all_data_flt_clst, color=markers, cmap=""Blues"", ncols=5); 27 ; 28 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only work",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1772,wrap,wrapper,1772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['wrap'],['wrapper']
Integrability,"b/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; start, end = indptr[i], indptr[i + 1]; sums[i] = np.sum(data[start:end]); ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ```; numba is 0.47.0 but 0.43.1 gave the same error.; It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:5115,message,message,5115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['message'],['message']
Integrability,"b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot_ng 2.0.0; pygments 2.6.1; pynndescent 0.5.6; pyparsing 3.0.7; pytz 2018.9; requests 2.23.0; rsa 4.8; scipy 1.4.1; seaborn 0.11.2; session_info 1.0.0; simplegeneric NA; sitecustomize NA; six 1.15.0; sklearn 1.0.2; socks 1.7.1; sphinxcontrib NA; statsmodels 0.10.2; storemagic NA; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; tensorflow_probability 0.16.0; termcolor 1.1.0; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.11.2; tornado 5.1.1; tqdm 4.63.0; traitlets 5.1.1; tree 0.1.6; typing_extensions NA; umap 0.5.2; uritemplate 3.0.1; urllib3 1.24.3; wcwidth 0.2.5; wrapt 1.14.0; yaml 3.13; zipp NA; zmq 22.3.0. IPython 5.5.0; jupyter_client 5.3.5; jupyter_core 4.9.2; notebook 5.3.1. Python 3.7.13 (default, Mar 16 2022, 17:37:17) [GCC 7.5.0]; Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic. Session information updated at 2022-04-04 17:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:5369,wrap,wrapt,5369,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['wrap'],['wrapt']
Integrability,"batch_queries, verbose); 802 self._angular_trees,; 803 ); --> 804 leaf_array = rptree_leaf_array(self._rp_forest); 805 else:; 806 self._rp_forest = None. ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array(rp_forest); 1095 def rptree_leaf_array(rp_forest):; 1096 if len(rp_forest) > 0:; -> 1097 return np.vstack(rptree_leaf_array_parallel(rp_forest)); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /om",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:3905,message,message,3905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['message'],['message']
Integrability,bbknn integrates multiple variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:6,integrat,integrates,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['integrat'],['integrates']
Integrability,bbknn wrapper needs an update,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/635:6,wrap,wrapper,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635,1,['wrap'],['wrapper']
Integrability,"been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python; import scanpy; scanpy.logging.print_versions(); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,; 183 excludes=[; 184 'builtins',; 185 'stdlib_list',; 186 'importlib_metadata',; 187 # Special module present if test coverage being calculated; 188 # https://gitlab.com/joelostblom/session_info/-/issues/10; 189 ""$coverage"",; 190 ],; 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones.; 212 try:. KeyError: 'numcodecs'; ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```; scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2580:1005,depend,dependencies,1005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580,1,['depend'],['dependencies']
Integrability,"bxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; dill 0.3.7; gmpy2 2.1.2; google NA; h5py 3.9.0; igraph 0.11.3; joblib 1.3.2; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.40.1; louvain 0.8.2; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; opt_einsum v3.3.0; packaging 23.1; pandas 2.0.3; pkg_resources NA; plotly 5.16.1; psutil 5.9.5; pyparsing 3.0.9; pytz 2023.3; scipy 1.11.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.0; sympy 1.12; texttable 1.7.0; threadpoolctl 3.2.0; torch 2.0.1; tqdm 4.66.2; typing_extensions NA; wcwidth 0.2.6; yaml 6.0.1; -----; Python 3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:15465,wrap,wrapt,15465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrapt']
Integrability,"c(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,. ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 147 ; --> 148 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); 149 dset = dataset.Dataset(dsid). ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter); 139 if (data is not None) and (not isinstance(data, Empty)):; --> 140 dset_id.write(h5s.ALL, h5s.ALL, data); 141 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.write(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_conv.pyx in h5py._conv.str2vlen(). h5py/_conv.pyx in h5py._conv.generic_converter(). h5py/_conv.pyx in h5py._conv.conv_str2vlen(). TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_dataframe(f, key, df, dataset_kwargs); 262 for col_name, (_, series) in zip(col_names, df.items()):; --> 263 write_series(group, col_name, series, dataset_kwargs=dataset_kwargs); 264 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_par",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:2725,wrap,wrapper,2725,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,c3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.0.1; tomlkit 0.12.1; toolz 0.12.0; toposort 1.10; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; transformers 4.34.0; truststore 0.8.0; Twisted 22.10.0; types-python-dateutil 2.8.19.14; typing_extensions 4.8.0; typing-utils 0.1.0; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.8.0; umap-learn 0.5.4; uri-template 1.3.0; urllib3 1.26.15; virtualenv 20.24.5; w3lib 2.1.2; watchdog 3.0.0; wcwidth 0.2.8; webcolors 1.13; webencodings 0.5.1; websocket-client 1.6.4; Werkzeug 3.0.0; whatthepatch 1.0.5; wheel 0.38.4; widgetsnbextension 4.0.9; wrapt 1.15.0; wurlitzer 3.0.3; xarray 2023.9.0; xxhash 3.4.1; xyzservices 2023.10.0; yapf 0.24.0; yarl 1.9.2; yte 1.5.1; zict 3.0.0; zipp 3.17.0; zope.interface 6.1; zstandard 0.21.0. ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:10579,wrap,wrapt,10579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,2,"['interface', 'wrap']","['interface', 'wrapt']"
Integrability,"canpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appdirs 1.4.4; attr 20.1.0; autoreload NA; backcall 0.2.0; bioservices 1.7.8; bs4 4.9.1; cairo 1.19.1; certifi 2020.12.05; cffi 1.14.4; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.3; colorlog NA; cupy 7.8.0; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.1; dateutil 2.8.1; decorator 4.4.2; deprecated 1.2.10; easydev 0.9.38; fa2 NA; fastrlock 0.5; fsspec 0.8.7; future 0.18.2; future_fstrings NA; get_version 2.1; graphtools 1.5.2; gseapy 0.10.1; h5py 2.10.0; idna 2.10; igraph 0.8.2; iniconfig NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; lxml 4.5.2; magic 2.0.3; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; pluggy 0.13.1; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; py 1.9.0; pyarrow 0.16.0; pycparser 2.20; pygments 2.6.1; pygsp 0.5.1; pylab NA; pyparsing 2.4.7; pytest 6.1.2; pytz 2020.1; requests 2.24.0; requests_cache 0.5.2; sca NA; scanpy 1.7.0; scipy 1.6.1; scprep 1.0.5.post2; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; skmisc 0.1.3; soupsieve 2.0.1; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tasklogger 1.0.0; tblib 1.7.0; texttable 1.6.2; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; tqdm 4.48.2; traitlets 4.3.3; typing_extensions NA; umap 0.4.6; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-05-03 16:30; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1827:2835,wrap,wrapt,2835,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1827,1,['wrap'],['wrapt']
Integrability,"confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeserie",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:1209,wrap,wraps,1209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wraps']
Integrability,"cs.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5. https://www.10xgenomics.com/resources/datasets/5-k-mouse-e-18-combined-cortex-hippocampus-and-subventricular-zone-nuclei-3-1-standard-6-0-0 . Using this function, . ```; scanpy.read_10x_h5(filename, genome=None, gex_only=True, backup_url=None); ```. I get the following error, ; ```; ValueError: 'SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5' contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['barcode_idx', 'barcode_info', 'barcodes', 'count', 'feature_idx', 'features', 'gem_group', 'library_idx', 'library_info', 'metrics_json', 'umi', 'umi_type']; ```; and I don't know how to fix this. The dataset is only from mouse. This is not a legacy h5 file. The data are output from CR v6.0.0. Can you please assist? . The ""available genomes"" suggested in the error message are not genome, but columns in the h5. Ideally, I want to import all of these. Are we really only allowed to select 1? Even selecting one, does not seem fix the problem. . ```; h5_info = scanpy.read_10x_h5(molecule_info_file,genome='umi_type'); ```; ```; TypeError: node ``/umi_type`` is not a group; ```. - [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; h5_info = scanpy.read_10x_h5(molecule_info_file, backup_url=""https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex/SC3_v3_NextGem_DI_Nuclei_5K_Multiplex_count_raw_molecule_info.h5""); ```. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2149:1138,message,message,1138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2149,1,['message'],['message']
Integrability,"d mode: https://scverse-tutorials.readthedocs.io/en/latest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 1019 dp = DotPlot(; 1020 adata,; 1021 var_names,; (...); 1042 **kwds,; 1043 ); 1045 if dendrogram:; -> 1046 dp.add_dendrogram(dendrogram_key=dendrogram); 1047 if swap_axes:; 1048 d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:1418,wrap,wrapper,1418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wrapper']
Integrability,diffxpy integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955:8,integrat,integration,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955,1,['integrat'],['integration']
Integrability,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:3208,wrap,wrapper,3208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,2,['wrap'],"['wrap', 'wrapper']"
Integrability,"ds. /usr/local/lib/python3.8/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 237 else:; 238 nan_dtype = dtype; --> 239 val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype); 240 arrays.loc[missing] = [val] * missing.sum(); 241 . /usr/local/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in construct_1d_arraylike_from_scalar(value, length, dtype); 1438 else:; 1439 if not isinstance(dtype, (np.dtype, type(np.dtype))):; -> 1440 dtype = dtype.dtype; 1441 ; 1442 if length and is_integer_dtype(dtype) and isna(value):. AttributeError: type object 'object' has no attribute 'dtype'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.4; -----; MulticoreTSNE NA; PIL 8.0.1; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; cffi 1.14.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.01.0; dateutil 2.8.1; decorator 4.4.2; dunamai 1.7.0; fsspec 2022.01.0; get_version 3.5.3; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; jsonschema 3.2.0; jupyter_server 1.13.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.38.0; loompy 3.0.6; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.1; nbformat 5.0.8; numba 0.55.0; numexpr 2.7.3; numpy 1.21.5; numpy_groupies 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:4383,message,message,4383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,1,['message'],['message']
Integrability,"dtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; llvmlite 0.42.0; locket 1.0.0; magic-impute 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; msgpack 1.0.7; multipledispatch 1.0.0; natsort 8.4.0; networkx 3.2.1; nodeenv 1.8.0; numba 0.59.0; numcodecs 0.12.1; numpy 1.26.4; packaging 23.2; pandas 2.0.3; partd 1.4.1; patsy 0.5.6; pbr 6.0.0; pillow 10.2.0; pip 24.0; platformdirs 4.2.0; pluggy 1.4.0; pre-commit 3.6.2; profimp 0.1.0; psutil 5.9.8; PyGSP 0.5.1; pynndescent 0.5.11; pyparsing 3.1.1; pytest 8.0.1; pytest-mock 3.12.0; pytest-nunit 1.0.6; python-dateutil 2.8.2; pytz 2024.1; PyYAML 6.0.1; scanorama 1.7.4; scanpy 1.10.0.dev220+g534145f6; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; scprep 1.2.3; seaborn 0.13.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; sortedcontainers 2.4.0; sparse 0.15.1; statsmodels 0.14.1; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tomli 2.0.1; toolz 0.12.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:7658,wrap,wrap,7658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['wrap'],['wrap']
Integrability,"e ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2501,wrap,wrapper,2501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"e error when I try to do; adata.write('trial.hdf5') ; #or; sc.pl.violin(adata, 'volume'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[8], line 1; ----> 1 sc.pl.violin(adata, 'volume'). File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_anndata.py:749, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 645 """"""\; 646 Violin plot.; 647 ; (...); 745 pl.stacked_violin; 746 """"""; 747 import seaborn as sns # Slow import, only import if called; --> 749 sanitize_anndata(adata); 750 use_raw = _check_use_raw(adata, use_raw); 751 if isinstance(keys, str):. File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/_utils/__init__.py:406, in sanitize_anndata(adata); 404 def sanitize_anndata(adata):; 405 """"""Transform string annotations to categoricals.""""""; --> 406 adata._sanitize(). File ~/.local/lib/python3.9/site-packages/anndata/_core/anndata.py:1228, in AnnData.strings_to_categoricals(self, df); 1226 if len(c.categories) >= len(c):; 1227 continue; ...; 1232 ""AnnData, not on this view. You might encounter this""; 1233 ""error message while copying or writing to disk.""; 1234 ). TypeError: reorder_categories() got an unexpected keyword argument 'inplace'; ```. ### Versions. <details>. ```; anndata 0.7.8; scanpy 1.9.3; -----; PIL 10.0.0; asttokens NA; backcall 0.2.0; clustergrammer2 0.18.0; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7.post1; decorator 5.1.1; executing 1.2.0; google NA; h5py 3.9.0; igraph 0.10.6; importlib_resources NA; ipykernel 6.25.1; ipywidgets 8.1.0; jedi 0.19.0; joblib 1.3.2; kiwisolver 1.4.4; leidenalg 0.9.0; ...; Python 3.9.12 (main, Jun 1 2022, 11:38:51) [GCC 7.5.0]; Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-glibc2.31; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2645:2931,message,message,2931,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645,1,['message'],['message']
Integrability,"e latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:1156,wrap,wrapper,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapper']
Integrability,"e output for PAGA looks weird. . Please see below. Is this something to do with figure margins/axes?. Thanks in advance. ![Screenshot 2023-09-13 at 20 48 43](https://github.com/scverse/scanpy/assets/40166451/c7313231-a2dc-49cb-a3df-21e97d86d278). ### Minimal code sample. ```python; adata2 = sc.datasets.pbmc3k_processed(); sc.tl.paga(adata2, groups='louvain'); sc.pl.paga(adata2); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.5; -----; PIL 8.0.1; backcall 0.2.0; bottleneck 1.3.7; cellrank 1.5.1; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; decorator 5.1.1; docrep 0.3.2; google NA; h5py 3.8.0; igraph 0.10.4; importlib_resources NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 3.0.3; joblib 1.2.0; kiwisolver 1.3.0; leidenalg 0.9.1; llvmlite 0.34.0; lz4 3.1.10; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; numba 0.51.2; numexpr 2.8.5; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 4.2.0; prompt_toolkit 3.0.8; psutil 5.7.2; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pygpcca 1.0.4; pyparsing 2.4.7; python_utils NA; pytz 2020.1; ruamel NA; scipy 1.10.1; scvelo 0.2.5; seaborn 0.11.0; session_info 1.0.0; six 1.15.0; sklearn 1.2.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.1.0; tlz 0.12.1; toolz 0.11.1; tornado 6.1; tqdm 4.50.2; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; wrapt 1.15.0; yaml 5.3.1; zipp NA; zmq 19.0.2; zope NA; -----; IPython 7.25.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.4.0-137-generic-x86_64-with-glibc2.10; -----; Session information updated at 2023-09-15 09:42; Running scvelo 0.2.5 (python 3.8.5) on 2023-09-15 09:42. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2665:2035,wrap,wrapt,2035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2665,1,['wrap'],['wrapt']
Integrability,"e, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6256,message,message,6256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['message'],['message']
Integrability,"e:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20400,wrap,wrapper,20400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wrapper']
Integrability,"eapdict NA; idna 2.10; igraph 0.9.1; ipykernel 5.4.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.4.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; netifaces 0.10.9; networkx 2.5.1; numba 0.53.1; numexpr 2.7.3; numpy 1.19.5; nvtx NA; opt_einsum v3.3.0; packaging 20.8; pandas 1.2.4; parso 0.8.1; petsc4py 3.14.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 3.53.1; prometheus_client NA; prompt_toolkit 3.0.10; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 1.0.1; pycparser 2.20; pygam 0.8.0; pygments 2.7.4; pygpcca 1.0.2; pynndescent 0.5.2; pynvml 8.0.4; pyparsing 2.4.7; pyrsistent NA; python_utils NA; pytz 2021.1; requests 2.25.1; rmm 0.20.0a+28.g7768d4d; scanpy 1.7.2; scanpy_gpu_funcs NA; scipy 1.6.3; scvelo 0.2.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.2; slepc4py 3.14.0; sniffio 1.2.0; socks 1.7.1; sortedcontainers 2.3.0; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; tensorboard 2.6.0a20210510; tensorflow 2.6.0-dev20210510; termcolor 1.1.0; texttable 1.6.3; threadpoolctl 2.1.0; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; treelite 1.1.0; treelite_runtime 1.1.0; typing_extensions NA; ucp 0.20.0a+30.g2aa87da; umap 0.5.1; urllib3 1.26.4; virtualenvwrapper NA; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zict 2.0.0; zipp NA; zmq 21.0.1; -----; IPython 7.19.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.5; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-5.8.0-50-generic-x86_64-with-glibc2.10; 32 logical CPU cores, x86_64; -----; Session information updated at 2021-05-12 13:23. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:4363,wrap,wrapt,4363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['wrap'],['wrapt']
Integrability,"eature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I am integrating multiple datasets. In the integrated object I wanted to see for each cluster the percentage of each dataset it consisted of (also see [this question](https://scanpy.discourse.group/t/cluster-statistics/134) in the scanpy discourse group). I wrote code for it on my own, but am not sure which part of `sc.pl` you want it to go to, so here is the code for your consideration:. ```python; import matplotlib.pyplot as plt; import scanpy as sc; import numpy as np. # given integrated object adata, clustered via the leiden algorithm and; # with the batch ID in the 'batch' slot, and a collection of batch_names:. # count the number of occurrences of each batch ID for each cluster ID; count_series = adata.obs.groupby(['leiden', 'batch']).size(); new_df = count_series.to_frame(name = 'size').reset_index(); # convert from multi index to pivot; constitution = new_df.pivot(index='leiden', columns='batch')['size']; # convert to %batch (but could be modified to show different things instead; perc_clust = np.array((constitution.T / np.sum(constitution.T, axis=0))); # keep track of the batch, cluster IDs so we can use them for plotting; clusters = adata.obs.leiden.cat.categories; batches = adata.obs.batch.cat.categories. # actual plotting; basically stacked barplots; # replace styling with scanpy defaults probably?; fig, ax = plt.subplots(); ax.grid(False); ax.bar(clusters, perc_clust[0], 0.6, yerr=0, label=platy_names[0]); bottom = np.zeros(clusters.shape); for i, b in enumerate(batches):; ax.bar(clusters, perc_clust[i], 0.6, bo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573:954,integrat,integrated,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573,1,['integrat'],['integrated']
Integrability,"ed globally and shared by all users. I am looking to change that. # The issue. When calculating the `sc.tl.marker_gene_overlap` I get the expected and reasonable results on the agando environment, but completely rubbish results when running the same code with a fresh Conda environment and the latest dependencies installed. ![image](https://user-images.githubusercontent.com/21954664/106739402-659dfb80-6619-11eb-84f1-e75abfa6167d.png). Top = new, trash results; Bottom = old=agando expected results. The old environment has:. ```; scanpy==1.6.1.dev110+gb4234d81 anndata==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - icu=58.2=he6710b0_3; - importlib_metadata=2.0.0=1; - ipykernel=5.3.4=py37h5ca1d4c_0; - ipython=7.20.0=py37hb070fc8_1; - ipython_genutils=0.2.0=pyhd3eb1b0_1; - ipywidgets=7.6.3=pyhd3eb1b0_1; - jedi=0.17.0=py37_0; - jinja2=2.11.3=pyhd3eb1b0_0; - jpeg=9b=h024ee3a_2; - jsonschema=3.2.0=py_2; - jupyter=1.0.0=py37_7; - jupyter_client=6.1.7=py_0; - jupyter_console=6.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1451,depend,dependencies,1451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['depend'],['dependencies']
Integrability,"ed:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:3220,wrap,wrapper,3220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,"elf.sca(current_ax). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in colorbar_factory(cax, mappable, **kwargs); 1732 cb = ColorbarPatch(cax, mappable, **kwargs); 1733 else:; -> 1734 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:2806,wrap,wrapper,2806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['wrap'],['wrapper']
Integrability,"ellRanger that is changing the output format. This pull request makes Scanpy forward compatible with the new version. In particular, the following changes are made:. Updated `read_10x_h5`:; - Renamed the original `read_10x_h5` as `_read_legacy_10x_h5`;; - Added `_read_v3_10x_h5` to read the new Cell Ranger output format;; - The new `read_10x_h5` determines the version of HDF5 input by the presence of the matrix key, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is True (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` and `genome` were added into the outcome object as new attributes. Updated `read_10x_mtx`:; - Renamed the original `read_10x_mtx` as `_read_legacy_10x_mtx`;; - Added `_read_v3_10x_mtx` to read the new Cell Ranger output format;; - The new `read_10x_mtx` determines the version of matrix input by the presence of the `genes.tsv` file under the input directory, and wraps the above two functions. In addition, it takes a `gex_only` argument which filters out feature barcoding counts from the outcome object when it is `True` (default). Otherwise, the full matrix will be retained.; - For CR-v3, `feature_types` was added into the outcome object as a new attribute. Added small test datasets and code for the revised functions to verify the expected behavior. Note for the `genome` argument:; - There is a genome argument in Scanpy's `read_10x_h5` function but not in `read_10x_mtx` as the genome was already specified by the path of input directory. The outcome object of the two functions should be the same which always take one genome at a time.; - In this PR, when there are multiple genomes (e.g. Barnyard), `read_10x_mtx` always read them all, whereas `read_10x_h5` always need to specify one of them (mm10 by default). However, when `gex_only == False`, the `genome` argument will be ignored and the whole matrix will be read.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/334:1083,wrap,wraps,1083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/334,1,['wrap'],['wraps']
Integrability,"en/latest/pbmc3k.html). The first steps worked quit well, but when I want to run PCA the following error occurs (see below). <!-- To reproduce -->; Follow the tutorial ([pbmc3k.ipynb](https://github.com/scverse/scanpy-tutorials/blob/75c5ebb5b63769aee65f38842a34b7f7e1bbd476/pbmc3k.ipynb)) until the following line:. ```python; sc.tl.pca(adata, svd_solver='arpack'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb; Traceback (most recent call last):; File ""/Users/pbinder/Nextcloud/projects/memory-tcell-scRNA-seq/tutorials/scanpy/scanpy-tutorials-master/test.py"", line 51, in <module>; sc.tl.pca(adata, svd_solver='arpack'); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py"", line 188, in pca; X_pca = pca_.fit_transform(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/utils/_set_output.py"", line 140, in wrapped; data_to_wrap = f(self, X, *args, **kwargs); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 462, in fit_transform; U, S, Vt = self._fit(X); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 514, in _fit; return self._fit_truncated(X, n_components, self._fit_svd_solver); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/sklearn/decomposition/_pca.py"", line 609, in _fit_truncated; U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/_svds.py"", line 532, in svds; _, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,; File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py"", line 1697, in eigsh; params.iterate(); File ""/Users/pbinder/opt/anaconda3/envs/scRNA/lib/python3.9/site-packages/scipy/sparse/linalg/_eigen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2473:1088,wrap,wrapped,1088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2473,1,['wrap'],['wrapped']
Integrability,"ently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue after pynndescent is imported, but make pynndescent import fast. A kinda simpler solution would be to req",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:1241,depend,dependency,1241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependency']
Integrability,"everyone agrees it’s objectively bad 😉. therefore the characters “jet” should only appear in the commit message “jettisoned bad colormap defaults”. * https://jakevdp.github.io/blog/2014/10/16/how-bad-is-your-colormap/; * http://cresspahl.blogspot.de/2012/03/expanded-control-of-octaves-colormap.html; * http://stats.stackexchange.com/questions/223315/why-use-colormap-viridis-over-jet; * https://eagereyes.org/basics/rainbow-color-map; * https://courses.washington.edu/engageuw/why-you-should-dump-the-rainbow/; * http://researchweb.watson.ibm.com/people/l/lloydt/color/color.HTM. i’ll do it if you want, and i’ll make sure contrast and distinction is preserved in all figures",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3:104,message,message,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3,1,['message'],['message']
Integrability,"exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the KeyboardInterrupt and reraise it as. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self); 260 # change the default number of processes to -1; 261 with parallel_backend(self._backend, n_jobs=self._n_jobs):; --> 262 return [func(*args, **kwargs); 263 for func, args, kwargs in self.items]; 264 . /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in <listcomp>(.0); 260 # change the default number of processes to -1; 261 with parallel_backend(self._backend, n_jobs=self._n_jobs):; --> 262 return [func(*args, **kwargs); 263 for func, args, kwargs in self.items]; 264 . ValueError: cannot assign slice from input of different size```. #### Versions; ```. <details>. here is the error from ` sc.logging.print_versions()` ; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-81-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). ~/.local/lib/python3.8/site-packages/scanpy/logging.py in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,. ~/.local/lib/python3.8/site-packages/session_info/main.py in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones. KeyError: 'dask'. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:6451,depend,dependencies,6451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,2,['depend'],['dependencies']
Integrability,"exists on the main branch of scanpy. ### What happened?. When I try to plot a scatter plot color coded by categorical data, it still output the image, but the legend does not include colors. Additionally, if i use continuous data as the key for plotting, the code executes as expected. Thanks!. ![image](https://github.com/scverse/scanpy/assets/43973217/8c1b0a03-3c0b-4452-b759-9cf588b45c53). ### Minimal code sample. ```python; sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in ada",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1228,wrap,wraps,1228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['wrap'],['wraps']
Integrability,"from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```. The messages when updating anndata:; ```; The following packages will be REMOVED:. pytables-3.6.1-py38h9f153d1_1. The following packages will be UPDATED:. anndata 0.7.6-py38h578d9bd_0 --> 0.8.0-py38h578d9bd_0; ca-certificates pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0; h5py 2.10.0-nompi_py38h513d04c_102 --> 3.6.0-nompi_py38hfbb2109_100; hdf5 1.10.5-nompi_h5b725eb_1114 --> 1.12.1-nompi_h2750804_100. The following packages will be SUPERSEDED by a higher-priority channel:. certifi pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5.18.1-py38h578d9bd_0; openssl pkgs/main::openssl-1.1.1o-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0. Proceed ([y]/n)? y. Downloading and Extracting Packages; keyutils-1.6.1 | 115 KB | ############################################################################# | 100% ; h5py-3.6.0 | 1.4 MB | ############################################################################# | 100% ; cached_property-1.5. | 11 KB | ###############",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:1400,message,messages,1400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['message'],['messages']
Integrability,"g codes:. ```; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); sc.pp.log1p(adata); adata.raw = adata. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=4, min_disp=0.5); sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]; sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); sc.pp.scale(adata, max_value=10). sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='cell_types'). sc.pp.neighbors(adata, n_neighbors=20, n_pcs=20); sc.tl.louvain(adata). sc.tl.umap(adata); sc.tl.diffmap(adata, n_comps=15). adata.write(""Scanpy.h5ad""); ```. Then reload into another session and tring to plot:; ```; adata2 = anndata.read_h5ad(""Scanpy.h5ad"", backed = 'r'); sc.pl.umap(adata, color=['louvain', ""MMP3""]); ```. However the bug come out as:; ```; sc.pl.umap(adata, color=['louvain', ""MMP3""]); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 280, in plot_scatter; color_vector = color_vector[order]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/dataset.py"", line 476, in __getitem__; selection = sel.select(self.shape, args, dsid=self.id); File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 72, in select; sel[arg]; File ""/Users/temp/anaconda/envs/python36/lib/python3.6/site-packages/h5py/_hl/selections.py"", line 212, in __getitem__; raise TypeError(""PointSelection __getitem__ only works with bool arrays""); TypeError: PointSelection __getitem__ only works with bool arrays; ```. The version of packages:; ```; 1.4 for scanpy; 0.6.22 for anndata; ```. Thanks for the help.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/777:1317,wrap,wrapper,1317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/777,2,['wrap'],['wrapper']
Integrability,"g.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:141, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 138 labeling_method = labeling_method * len(obs); 140 ing = Ingest(adata_ref, neighbors_key); --> 141 ing.fit(adata); 143 for method in embedding_method:; 144 ing.map_embedding(method). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:404, in Ingest.fit(self, adata_new); 401 self._obsm = _DimDict(adata_new.n_obs, axis=0); 403 self._adata_new = adata_new; --> 404 self._obsm[""rep""] = self._same_rep(). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:371, in Ingest._same_rep(self); 369 adata = self._adata_new; 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:1764,wrap,wraps,1764,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['wrap'],['wraps']
Integrability,h code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; llvmlite 0.42.0; locket 1.0.0; magic-impute 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; msgpack 1.0.7; multipledispatch 1.0.0; natsort 8.4.0; networkx 3.2.1; nodeenv 1.8.0; numba 0.59.0; numcodecs 0.12.1; numpy 1.26.4; packaging 23.2; pandas 2.0.3; partd 1.4.1; patsy 0.5.6; pbr 6.0.0; pillow 10.2.0; pip 24.0; platformdirs 4.2.0; pluggy 1.4.0; pre-commit 3.6.2; profimp 0.1.0; psutil 5.9.8; PyGSP 0.5.1; pynndescent 0.5.11; pyparsing 3.1.1; pytest 8.0.1; pytest-mock 3.12.0; pytest-nunit 1.0.6; python-dateutil 2.8.2; pytz 2024.1; PyYAML 6.0.1; scanorama 1.7.4; scanpy 1.10.0.dev220+g534145f6; scikit-image 0.22.0; scikit-learn 1.4.1.post1; scikit-misc 0.3.1; scipy 1.12.0; scprep 1.2.3; seaborn 0.13.2; session-info 1.0.0; setuptools 58.1.0; setuptools-scm 8.0.4; six 1.16.0; sortedcontainers 2.4.0; sparse 0.15.1; statsmodels 0.14.1; stdlib-list 0.10.0; tasklogger 1.2.0; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.3.0; tifffile 2024.2.12; tomli 2.0.1; toolz 0.12.1; tornado 6.4; tqdm 4.66.2; typing_extensions 4.9.0; tzdata 2024.1; umap-learn 0.5.5; urllib3 2.2.1; virtualenv 20.25.0; wheel 0.42.0; wrapt 1.16.0; zarr 2.17.0; zict 3.0.0; zipp 3.17.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:8791,wrap,wrapt,8791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['wrap'],['wrapt']
Integrability,"heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float; ```; Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; ```; scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4749,wrap,wrapper,4749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `adata.X` to sparse matrix format, I have the following error:; ```python; adata.X = sci.sparse.csr_matrix(adata.X); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```. ```pytb; ---------------------------------------------------------------------------; Typ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:2234,wrap,wrapper,2234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,hon 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0 ; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0 ; jupyter_core 5.5.0 py311haa95532_0 ; jupyter_events 0.8.0 py311haa95532_0 ; jupyter_server 2.10.0 py311haa95532_0 ; jupyter_server_terminals 0.4.4 py311haa95532_1 ; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.25.1 py311haa95532_0 ; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0 ; libsodium 1.0.18 h62dcd97_0 ; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2 ; m2w64-bzip2 1.0.6 6 ; m2w64-expat 2.1.1 2 ; m2w64-fftw 3.3.4 6 ; m2w64-flac 1.3.1 3 ; m2w64-gcc-libgfortran 5.3.0 6 ; m2w64-gcc-libs 5.3.0 7 ; m2w64-gcc-libs-core 5.3.0 7 ; m2w64-gettext 0.19.7 2 ; m2w64-gmp 6.1.0 2 ; m2w64-gsl 2.1 2 ; m2w64-libiconv 1.14 6 ; m2w64-libjpeg-turbo 1.4.2 3 ; m2w64-libogg 1.3.2 3 ; m2w64-libpng 1.6.21 2 ; m2w64-libsndfile 1.0.26 2 ; m2w64-libsodium 1.0.10 2 ; m2w64-libtiff 4.0.6 2 ; m2w64-libvorbis 1.3.5 2 ; m2w64-libwinpthread-git 5.0.0.4634.697f757 2 ; m2w64-libxml2 2.9.3 4 ; m2w64-mpfr 3.1.4 4 ; m2w64-openblas 0.2.19 1 ; m2w64-pcre 8.38 2 ; m2w64-speex 1.2rc2 3 ; m2w64-speexdsp 1.2rc3 3 ; m2w64-tcl 8.6.5 3 ; m2w64-tk 8.6.5 3 ; m2w64-tktable 2.10 5 ; m2w64-wineditline 2.101 5 ; m2w64-xz 5.2.2 2 ; m2w64-zeromq 4.1.4 2 ; m2w64-zlib 1.2.8 10 ; markupsafe 2.1.5 pypi_0 pypi; matplotlib 3.8.3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:6955,wrap,wrap,6955,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['wrap'],['wrap']
Integrability,"how?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:655, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 653 sig = signature(_highly_variable_genes_seurat_v3); 654 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 655 return _highly_variable_genes_seurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:2699,wrap,wrapper,2699,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['wrap'],['wrapper']
Integrability,"https://github.com/theislab/scanpy/blob/63b42e4bff46a9e1386abfede4adeef3a8100c7b/pyproject.toml#L65. > The sinfo package has changed name and is now called session_info to become more discoverable and self-explanatory. The sinfo PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install session_info instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1996:366,message,message,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1996,1,['message'],['message']
Integrability,"ib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_col = np.arange(distances.shape[0])[:, None]; --> 286 init_indices = np.hstack((first_col, np.stack(distances.tolil().rows))); 287 ; 288 self._nnd_idx = NNDescent(. <__array_function__ internals> in stack(*args, **kwargs). ~/.local/lib/python3.7/site-packages/numpy/core/shape_base.py in stack(arrays, axis, out); 425 shapes = {arr.shape for arr in arrays}; 426 if len(shapes) != 1:; --> 427 raise ValueError('all input arrays must have the same shape'); 428 ; 429 result_ndim = arrays[0].ndim + 1. ValueError: all input arrays must have the same shape; ```. ### Versions. ```; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.8.0; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 7.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.1.1; google NA; h5py 3.7.0; ipykernel 5.3.0; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.0; joblib 1.1.0; kiwisolver 1.2.0; llvmlite 0.38.1; matplotlib 3.3.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; netifaces 0.11.0; numba 0.55.2; numexpr 2.8.3; numpy 1.20.0; packaging 21.3; pandas 1.1.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.9.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 3.0.8; pytz 2022.1; ruamel NA; scipy 1.7.0; seaborn 0.11.2; setuptools 62.1.0; simplejson 3.17.6; six 1.16.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2635:3590,message,message,3590,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2635,1,['message'],['message']
Integrability,"ign"", experiment_dir / ""experimental_design.tsv"",; 44 ); 45 _download(. ~/anaconda3/envs/sc-py/lib/python3.6/site-packages/scanpy/readwrite.py in _download(url, path); 877 ; 878 try:; --> 879 urlretrieve(url, str(path), reporthook=update_to); 880 except Exception:; 881 # Make sure file doesn’t exist half-downloaded. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlretrieve(url, filename, reporthook, data); 246 url_type, path = splittype(url); 247 ; --> 248 with contextlib.closing(urlopen(url, data)) as fp:; 249 headers = fp.info(); 250 . ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context); 221 else:; 222 opener = _opener; --> 223 return opener.open(url, data, timeout); 224 ; 225 def install_opener(opener):. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in open(self, fullurl, data, timeout); 530 for processor in self.process_response.get(protocol, []):; 531 meth = getattr(processor, meth_name); --> 532 response = meth(req, response); 533 ; 534 return response. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in http_response(self, request, response); 640 if not (200 <= code < 300):; 641 response = self.parent.error(; --> 642 'http', request, response, code, msg, hdrs); 643 ; 644 return response. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in error(self, proto, *args); 568 if http_err:; 569 args = (dict, 'default', 'http_error_default') + orig_args; --> 570 return self._call_chain(*args); 571 ; 572 # XXX probably also want an abstract factory that knows when it makes. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in _call_chain(self, chain, kind, meth_name, *args); 502 for handler in handlers:; 503 func = getattr(handler, meth_name); --> 504 result = func(*args); 505 if result is not None:; 506 return result. ~/anaconda3/envs/sc-py/lib/python3.6/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs); 648 class HTTPDefaultErrorHandler(Ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1221:2159,protocol,protocol,2159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1221,1,['protocol'],['protocol']
Integrability,"ile ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, in write_dataframe; write_elem(group, colname, series._values, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 220, in func_wrapper; raise type(e)(; TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'orig.ident' of <class 'h5py._hl.group.Group'> to /; /home/joyzheng/.conda/envs/cellrank/lib/python3.8/tempfile.py:818: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpzjzxl3q5'>; _warnings.warn(warn_message, ResourceWarning)```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:3797,wrap,wrapper,3797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['wrap'],['wrapper']
Integrability,"ing, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:204, in scrublet.<locals>._run_scrublet(ad_obs, ad_sim); 201 # HVG process needs log'd data.; 203 logged = pp.log1p(ad_obs, copy=True); --> 204 pp.highly_variable_genes(logged); 205 ad_obs = ad_obs[:, logged.var[""highly_variable""]].copy(); 207 # Simulate the doublets based on the raw expressions from the normalised; 208 # and filtered object. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:648, in highly_variable_genes(***failed resolving arguments***); 645 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 647 if batch_key is None:; --> 648 df = _highly_variable_genes_single_batch(; 649 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 650 ); 651 else:; 652 df = _highly_variable_genes_batched(; 653 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 654 ). File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:281, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:8660,wrap,wrapper,8660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wrapper']
Integrability,"input-21-ded14f7730cd> in <module>; 8 zf_48.var.index = zf_48.var[""gene_name""]; 9 ; ---> 10 zf_48.write_h5ad(""/Users/julius/Desktop/zf_48.h5ad""). ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1903 filename = self.filename; 1904 ; -> 1905 _write_h5ad(; 1906 Path(filename),; 1907 self,. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""raw"", adata.raw, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""var"", adata.var, dataset_kwargs=dataset_kwargs); 113 write_attribute(f, ""obsm"", adata.obsm, dataset_kwargs=dataset_kwargs); 114 write_attribute(f, ""varm"", adata.varm, dataset_kwargs=dataset_kwargs). ~/opt/anaconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'var' of <class 'h5py._hl.files.File'> from /. ```. Am ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:5223,wrap,wrapper,5223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,integrate with CCA and pyscenic,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265:0,integrat,integrate,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265,1,['integrat'],['integrate']
Integrability,ion=0.17.2; - tornado=6.4.1; - tqdm=4.66.4; - traitlets=5.14.3; - types-python-dateutil=2.9.0.20240316; - typing-extensions=4.12.2; - typing_extensions=4.12.2; - typing_utils=0.1.0; - tzdata=2024a; - umap-learn=0.5.5; - uri-template=1.3.0; - urllib3=2.2.1; - wcwidth=0.2.13; - webcolors=24.6.0; - webencodings=0.5.1; - websocket-client=1.8.0; - wheel=0.43.0; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.5; - zipp=3.19.2; - zlib-ng=2.0.7; - zstd=1.5.6; - pip:; - absl-py==2.1.0; - astunparse==1.6.3; - bcbio-gff==0.7.1; - flatbuffers==24.3.25; - gast==0.5.4; - google-pasta==0.2.0; - grpcio==1.64.1; - keras==3.3.3; - libclang==18.1.1; - markdown==3.6; - markdown-it-py==3.0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:8536,wrap,wrapt,8536,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['wrap'],['wrapt']
Integrability,"is None:; --> 564 self.linkage = self.calculated_linkage; 565 else:; 566 self.linkage = linkage. /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in calculated_linkage(self); 624 def calculated_linkage(self):; 625 try:; --> 626 return self._calculate_linkage_fastcluster(); 627 except ImportError:; 628 return self._calculate_linkage_scipy(). /usr/local/lib/python3.6/site-packages/seaborn/matrix.py in _calculate_linkage_fastcluster(self); 618 else:; 619 linkage = fastcluster.linkage(self.array, method=self.method,; --> 620 metric=self.metric); 621 return linkage; 622 . /usr/local/lib/python3.6/site-packages/fastcluster.py in linkage(X, method, metric, preserve_input); 241 assert X.ndim==2; 242 N = len(X); --> 243 X = pdist(X, metric); 244 X = array(X, dtype=double, copy=False, order='C', subok=True); 245 Z = empty((N-1,4)). /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in pdist(X, metric, *args, **kwargs); 1932 if metric_name is not None:; 1933 X, typ, kwargs = _validate_pdist_input(X, m, n,; -> 1934 metric_name, **kwargs); 1935 ; 1936 # get pdist wrapper. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _validate_pdist_input(X, m, n, metric_name, **kwargs); 287 typ = types[types.index(X.dtype)] if X.dtype in types else types[0]; 288 # validate data; --> 289 X = _convert_to_type(X, out_type=typ); 290 ; 291 # validate kwargs. /usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py in _convert_to_type(X, out_type); 182 ; 183 def _convert_to_type(X, out_type):; --> 184 return np.ascontiguousarray(X, dtype=out_type); 185 ; 186 . /usr/local/lib/python3.6/site-packages/numpy/core/numeric.py in ascontiguousarray(a, dtype); 588 ; 589 """"""; --> 590 return array(a, dtype, copy=False, order='C', ndmin=1); 591 ; 592 . ValueError: setting an array element with a sequence.; ```. </details>. -------------------. I think this is happening because `pandas` doesn't like being passed a sparse array ([which can happen in `sc.pl.cluster",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/356:3601,wrap,wrapper,3601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/356,1,['wrap'],['wrapper']
Integrability,"joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; lxml 4.6.3; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 21.0; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.9.0; pylab NA; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; requests_cache 0.6.4; scipy 1.6.2; seaborn 0.11.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; soupsieve 2.2.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; url_normalize 1.4.3; urllib3 1.26.6; wcwidth 0.2.5; webencodings 0.5.1; wrapt 1.12.1; yaml 5.4.1; zmq 20.0.0; zope NA; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.4.0; -----; Python 3.8.11 (default, Aug 3 2021, 05:10:14) [Clang 10.0.0 ]; macOS-10.16-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-08-16 12:24",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:5417,wrap,wrapt,5417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['wrap'],['wrapt']
Integrability,"just pin pytest for now. ### Minimal code sample. ```python; Examples; --------; >>> import scanpy as sc; >>> adata = sc.datasets.krumsiek11(); UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; utils.warn_names_duplicates(""obs""); ```. ### Error output. ```pytb; ======================================================================================================================= FAILURES =======================================================================================================================; _________________________________________________________________________________________________ [doctest] scanpy.preprocessing._simple.filter_cells __________________________________________________________________________________________________; 081 Boolean index mask that does filtering. `True` means that the; 082 cell is kept. `False` means the cell is removed.; 083 number_per_cell; 084 Depending on what was thresholded (`counts` or `genes`),; 085 the array stores `n_counts` or `n_cells` per gene.; 086 ; 087 Examples; 088 --------; 089 >>> import scanpy as sc; 090 >>> adata = sc.datasets.krumsiek11(); UNEXPECTED EXCEPTION: UserWarning('Observation names are not unique. To make them unique, call `.obs_names_make_unique`.'); Traceback (most recent call last):; File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/doctest.py"", line 1353, in __run; exec(compile(example.source, filename, ""single"",; File ""<doctest scanpy.preprocessing._simple.filter_cells[1]>"", line 1, in <module>; File ""/mnt/workspace/repos/scanpy/scanpy/datasets/_datasets.py"", line 109, in krumsiek11; adata = read(filename, first_column_names=True); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py"", line 80, in fn_compatible; return fn(*args_all, **kw); ^^^^^^^^^^^^^^^^^^^; File ""/mnt/workspace/repos/scanpy/scanpy/readwrite.py"", line 124, in r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2836:1639,Depend,Depending,1639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2836,1,['Depend'],['Depending']
Integrability,"kages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:2578,message,message,2578,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['message'],['message']
Integrability,"key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,. ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 147 ; --> 148 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); 149 dset = dataset.Dataset(dsid). ~/anaconda3/lib/python3.8/site-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter); 139 if (data is not None) and (not isinstance(data, Empty)):; --> 140 dset_id.write(h5s.ALL, h5s.ALL, data); 141 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.write(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_conv.pyx in h5py._conv.str2vlen(). h5py/_conv.pyx in h5py._conv.generic_converter(). h5py/_conv.pyx in h5py._conv.conv_str2vlen(). TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:. TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_dataframe(f, key, df, dataset_kwargs); 262 for col_name, (_, series) in zip(col_names, df.items()):; --> 263 write_series(group, col_name, series, dataset_kwargs=dataset_kwargs); 264 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:2669,wrap,wrapper,2669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,"late the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code; See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs; I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:; * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`; * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`); * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is); * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17; I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:2355,wrap,wrapper,2355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,1,['wrap'],['wrapper']
Integrability,"left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} cannot be converted to ""; --> 685 ""float"".format(self._A.dtype)); 686 ; 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float; ```; Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:; <!-- Output of scanpy.logging.print_versions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4552,wrap,wrapper,4552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for image data""; --> 690 .format(self._A.shape)); 691 ; 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```; If I convert the `adata.X` to sparse matrix format, I have the following error:; ```python; adata.X = sci.sparse.csr_matrix(adata.X); sc.pl.paga_pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:2037,wrap,wrapper,2037,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ler/issues/9). Do you know why the enrich function uses capitalization, and how I can get my gprofiler compatible with it? Versions listed below. Thanks. ```pytb; ImportError Traceback (most recent call last); /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 264 try:; --> 265 from gprofiler import GProfiler; 266 except ImportError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 26",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:1288,wrap,wrapper,1288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,1,['wrap'],['wrapper']
Integrability,lite 0.34.0 ; markdown-it-py 0.5.6 ; MarkupSafe 1.1.1 ; matplotlib 3.3.3 ; mccabe 0.6.1 ; mistune 0.8.4 ; mypy-extensions 0.4.3 ; natsort 7.0.1 ; nbclient 0.5.1 ; nbconvert 6.0.7 ; nbdime 2.1.0 ; nbformat 5.0.8 ; nbresuse 0.3.6 ; nest-asyncio 1.4.3 ; networkx 2.5 ; notebook 6.1.5 ; numba 0.51.2 ; numexpr 2.7.1 ; numpy 1.19.4 ; packaging 20.4 ; pandas 1.1.4 ; pandocfilters 1.4.3 ; parso 0.7.1 ; path 15.0.0 ; pathspec 0.8.1 ; pathtools 0.1.2 ; patsy 0.5.1 ; peepdis 0.1.13 ; pexpect 4.8.0 ; pickleshare 0.7.5 ; Pillow 8.0.1 ; pip 20.0.2 ; plotly 4.12.0 ; pluggy 0.13.1 ; prometheus-client 0.8.0 ; prompt-toolkit 3.0.8 ; psutil 5.7.3 ; ptvsd 4.3.2 ; ptyprocess 0.6.0 ; py 1.9.0 ; pycodestyle 2.6.0 ; pycparser 2.20 ; pydocstyle 5.1.1 ; pyflakes 2.2.0 ; Pygments 2.7.2 ; PyGObject 3.36.0 ; pylint 2.6.0 ; pymongo 3.11.0 ; pyparsing 2.4.7 ; pyrsistent 0.17.3 ; pytest 6.1.2 ; python-apt 2.0.0+ubuntu0.20.4.1 ; python-dateutil 2.8.1 ; python-debian 0.1.36ubuntu1 ; python-jsonrpc-server 0.4.0 ; python-language-server 0.36.1 ; pytoml 0.1.21 ; pytz 2020.4 ; PyYAML 5.3.1 ; pyzmq 20.0.0 ; regex 2020.11.13 ; requests 2.22.0 ; requests-unixsocket 0.2.0 ; retrying 1.3.3 ; rich 9.2.0 ; rope 0.18.0 ; scikit-learn 0.23.2 ; scikit-misc 0.1.3 ; scipy 1.5.4 ; scriptedforms 0.10.1 ; scvi-tools 0.7.1 ; seaborn 0.11.0 ; Send2Trash 1.5.0 ; setuptools 50.3.2 ; setuptools-scm 4.1.2 ; sinfo 0.3.1 ; six 1.14.0 ; smmap 3.0.4 ; snowballstemmer 2.0.0 ; SQLAlchemy 1.3.20 ; statsmodels 0.12.1 ; stdlib-list 0.7.0 ; tables 3.6.1 ; termcolor 1.1.0 ; terminado 0.9.1 ; testpath 0.4.4 ; threadpoolctl 2.1.0 ; toml 0.10.2 ; torch 1.7.0 ; tornado 6.1 ; tqdm 4.51.0 ; traitlets 5.0.5 ; typed-ast 1.4.1 ; typeguard 2.10.0 ; typing-extensions 3.7.4.3 ; ujson 4.0.1 ; umap-learn 0.4.6 ; unattended-upgrades 0.1 ; urllib3 1.25.8 ; watchdog 0.10.3 ; wcwidth 0.2.5 ; webencodings 0.5.1 ; Werkzeug 1.0.1 ; wheel 0.35.1 ; widgetsnbextension 3.5.1 ; wrapt 1.12.1 ; xeus-python 0.8.3 ; xlrd 1.2.0 ; yapf 0.30.0 ; zipp 3.4.0. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:6913,wrap,wrapt,6913,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['wrap'],['wrapt']
Integrability,"llowing error which is fixed when rolling back to scanpy=1.8.2; ```pytb; ValueError Traceback (most recent call last); <ipython-input-3-8ddd0a13aab2> in <module>; 8 print(results_file); ----> 9 adata = sc.read_10x_h5(results_file); 10 adata.var_names_make_unique(); 11 adata.obs.index = meta.iloc[idx,2] + '-' + adata.obs.index. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only, backup_url); 181 v3 = '/matrix' in f; 182 if v3:; --> 183 adata = _read_v3_10x_h5(filename, start=start); 184 if genome:; 185 if genome not in adata.var['genome'].values:. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_h5(filename, start); 266 try:; 267 dsets = {}; --> 268 _collect_datasets(dsets, f[""matrix""]); 269 ; 270 from scipy.sparse import csr_matrix. /opt/conda/lib/python3.8/site-packages/scanpy/readwrite.py in _collect_datasets(dsets, group); 254 for k, v in group.items():; 255 if isinstance(v, h5py.Dataset):; --> 256 dsets[k] = v[:]; 257 else:; 258 _collect_datasets(dsets, v). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args, new_dtype); 767 if self.shape == ():; 768 fspace = self.id.get_space(); --> 769 selection = sel2.select_read(fspace, args); 770 if selection.mshape is None:; 771 arr = numpy.ndarray((), dtype=new_dtype). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in select_read(fspace, args); 99 """"""; 100 if fspace.shape == ():; --> 101 return ScalarReadSelection(fspace, args); 102 ; 103 raise NotImplementedError(). /opt/conda/lib/python3.8/site-packages/h5py/_hl/selections2.py in __init__(self, fspace, args); 84 self.mshape = (); 85 else:; ---> 86 raise ValueError(""Illegal slicing argument for scalar dataspace""); 87 ; 88 self.mspace = h5s.create(h5s.SCALAR). ValueError: Illegal slicing argument for scalar dataspace; ```. Thanks!! . Nadav",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203:1276,wrap,wrapper,1276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203,2,['wrap'],['wrapper']
Integrability,"m of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; anndata2ri 1.1; annoy NA; backcall 0.2.0; backports NA; bbknn NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; entrypoints 0.4; fsspec 2022.11.0; future_fstrings NA; google NA; h5py 3.7.0; igraph 0.9.1; ipykernel 6.14.0; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.7.2; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.3; numpy 1.21.6; packaging 21.3; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.31; psutil 5.9.3; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pynndescent 0.5.7; pyparsing 3.0.9; pytz 2022.5; pytz_deprecation_shim NA; rpy2 3.5.1; scib 1.0.4; scipy 1.7.3; seaborn 0.12.1; session_info 1.0.0; six 1.16.0; sklearn 1.0.2; statsmodels 0.13.2; storemagic NA; texttable 1.6.4; threadpoolctl 3.1.0; tlz 0.12.0; toolz 0.12.0; tornado 6.2; tqdm 4.64.1; traitlets 5.5.0; typing_extensions NA; tzlocal NA; umap 0.5.3; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zipp NA; zmq 24.0.1; zope NA; -----; IPython 7.33.0; jupyter_client 7.4.4; jupyter_core 4.11.1; notebook 6.5.1; -----; Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]; Linux-5.4.0-131-generic-x86_64-with-debian-buster-sid; -----; Session information updated at 2022-12-28 13:52; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:5483,wrap,wrapt,5483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['wrap'],['wrapt']
Integrability,mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:26503,wrap,wrap,26503,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,2,['wrap'],['wrap']
Integrability,"me NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; debugpy 1.5.1; decorator 5.0.6; dot_parser NA; dunamai 1.6.0; executing 0.8.2; fbpca NA; flatbuffers NA; fsspec 0.7.4; gast 0.5.3; get_version 3.5; google NA; gprofiler 1.0.0; h5py 3.7.0; idna 2.10; igraph 0.10.2; importlib_resources NA; intervaltree NA; ipykernel 6.8.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.2; jmespath 1.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.7.0; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.7.1; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numpy 1.22.0; opt_einsum v3.3.0; packaging 20.9; pandas 1.2.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pure_eval 0.2.2; pvectorc NA; pycparser 2.20; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.8.1; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.3; requests 2.25.1; rpy2 3.4.2; ruamel NA; scanorama 1.7.1; scipy 1.6.2; scrublet NA; scvelo 0.2.5; seaborn 0.11.1; send2trash NA; session_info 1.0.0; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sortedcontainers 2.3.0; sparse 0.13.0; sphinxcontrib NA; stack_data 0.1.4; statsmodels 0.12.2; tblib 1.7.0; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.1.1; typing_extensions NA; tzlocal NA; umap 0.5.1; urllib3 1.26.4; wcwidth 0.2.5; wrapt 1.12.1; yaml 5.4.1; zipp NA; zmq 20.0.0; zope NA; -----; IPython 8.0.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.14; notebook 6.3.0; -----; Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2443:3213,wrap,wrapt,3213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2443,1,['wrap'],['wrapt']
Integrability,"n func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d[k] = read_dataframe(f[k]); 412 else: # Base case; --> 4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1681,wrap,wrapper,1681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"n reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed at nopython (nopython frontend); Unknown attribute 'partition' of type Module(<module 'numpy' from '/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/__init__.py'>). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. [1] During: typing of get attribute at /home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../../../home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def _top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; elif (end - start) > maxidx:; partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]; ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ... [Version](url) of the packages in path : ; scanpy 1.4.4.post1; anndata 0.6.22.post1; anndata2ri 1.0.1; umap-learn 0.3.10; numpy 1.16.5; scipy 1.3.1; pandas 1.0.1; scikit-learn 0.21.3; statsmodels 0.10.1; python-igraph 0.7.1.post6; louvain 0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:4328,message,message,4328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['message'],['message']
Integrability,"n39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper; return fn(*args, **kwargs); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl; state.stop = stop; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__; self[self._datamodel.get_field_position(field)] = value; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__; raise TypeError(""Invalid store of {value.type} to ""; TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>; sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:2015,wrap,wrapper,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['wrap'],['wrapper']
Integrability,"n_info.; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.3.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anyio NA; astunparse 1.6.3; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; celltypist 0.2.0; certifi 2021.05.30; cffi 1.14.6; charset_normalizer 2.0.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; entrypoints 0.3; et_xmlfile 1.1.0; flatbuffers 2.0; gast 0.5.3; google NA; h5py 3.3.0; idna 3.2; igraph 0.9.9; ipykernel 6.2.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.10.2; jupyterlab_server 2.7.1; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.9; llvmlite 0.38.0; louvain 0.7.1; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; openpyxl 3.0.9; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pyexpat NA; pygments 2.10.0; pynndescent 0.5.6; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.26.0; scipy 1.7.1; send2trash NA; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; statsmodels 0.13.2; storemagic NA; tables 3.7.0; tensorboard 2.8.0; tensorflow 2.8.0; termcolor 1.1.0; terminado 0.11.1; texttable 1.6.4; tornado 6.1; tqdm 4.63.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.2; urllib3 1.26.6; wcwidth 0.2.5; websocket 1.2.1; wrapt 1.14.0; xlrd 1.2.0; zmq 22.2.1; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.1.7; notebook 6.4.3; -----; Python 3.9.6 (default, Aug 18 2021, 11:08:34) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]; Linux-3.10.0-1160.41.1.el7.x86_64-x86_64-with-glibc2.17; 36 logical CPU cores, x86_64; -----; Session information updated at 2022-03-26 18:52. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2193:2938,wrap,wrapt,2938,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2193,1,['wrap'],['wrapt']
Integrability,"na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; --> 399 **kwargs,; 400 ); 401 . ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_utils.py in circles(x, y, s, ax, marker, c, vmin, vmax, scale_factor, **kwargs); 1106 # while you can only set `facecolors` with a value for all.; 1107 if scale_factor != 1.0:; -> 1108 x = x * scale_factor; 1109 y = y * scale_factor; 1110 zipped = np.broadcast(x, y, s). TypeError: can't multiply sequence by non-int of type 'float'; ```; Bascially, the same type of error keeps showing up no matter what the `color = ` parameter is. I put a gene name there, and the same message showed up. Additional information about the error (the image that showed up together with the error message from the jupyter notebook cell): ![image](https://user-images.githubusercontent.com/78611089/209900312-f6acd040-93e5-43b8-858d-3d1ef0c91cd0.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; annoy NA; asciitree NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; cycler 0.10.0; cython_runtime NA; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.18; fbpca NA; fsspec 2022.11.0; google NA; h5py 3.7.0; igraph 0.10.2; intervaltree NA; ipykernel 6.16.2; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.4; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.4; numcodecs 0.10.2; numpy 1.21.6; packaging 22.0; pandas 1.3.5; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391:3127,message,message,3127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391,1,['message'],['message']
Integrability,"niconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 176 try:; --> 177 return func(elem, *args, **kwargs); 178 except Exception as e:. ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. ~/miniconda3/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-17-97568eff5295> in <module>; ----> 1 adata=anndata.read_h5ad('./visium_merge_inter_upload.h5ad'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2376:1299,wrap,wrapper,1299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376,1,['wrap'],['wrapper']
Integrability,"nt; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 1 import pkg_resources; 2 import numba; ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer; 4 ; 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>; 19 import heapq; 20 ; ---> 21 import pynndescent.sparse as sparse; 22 import pynndescent.sparse_nndescent as sparse_nnd; 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>; 341 },; 342 ); --> 343 def sparse_alternative_jaccard(ind1, data1, ind2, data2):; 344 num_non_zero = arr_union(ind1, ind2).shape[0]; 345 num_equal = arr_intersect(ind1, ind2).shape[0]. ~/.local/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 216 with typeinfer.register_dispatcher(disp):; 217 for sig in sigs:; --> 218 disp.compile(sig); 219 disp.disable_compile(); 220 return disp. ~/.local/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 817 self._cache_misses[sig] += 1; 818 try:; --> 819 cres = self._compiler.compile(args, return_type); 820 except errors.ForceLiteralArg as e:; 821 def folded(args, kws):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 80 return retval; 81 else:; ---> 82 raise retval; 83 ; 84 def _compile_cached(self, args, return_type):. ~/.local/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 90 ; 91 try:; ---> 92 retval = self._compile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:3262,wrap,wrapper,3262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['wrap'],['wrapper']
Integrability,"ntime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; docrep 0.3.2; entrypoints 0.4; etils 0.6.0; executing 0.8.3; flatbuffers 2.0; flax 0.5.2; fsspec 2022.5.0; google NA; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; igraph 0.9.11; iniconfig NA; ipykernel 6.15.1; ipython_genutils 0.2.0; ipywidgets 7.7.1; jax 0.3.14; jaxlib 0.3.14; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.3; leidenalg 0.8.10; llvmlite 0.38.1; louvain 0.7.1; matplotlib 3.5.2; matplotlib_inline NA; mpl_toolkits NA; msgpack 1.0.4; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; newick 1.0.0; numba 0.55.2; numpy 1.22.4; numpyro 0.10.0; opt_einsum v3.3.0; optax 0.1.3; packaging 21.3; pandas 1.4.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pluggy 1.0.0; prompt_toolkit 3.0.30; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; py 1.11.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.12.0; pyparsing 3.0.9; pyro 1.8.1; pytest 7.1.2; pytorch_lightning 1.6.5; pytz 2022.1; requests 2.28.1; rich NA; scHPL NA; scarches 0.5.3; scipy 1.8.1; scvi 0.17.1; seaborn 0.11.2; session_info 1.0.0; setuptools 63.1.0; six 1.16.0; sklearn 1.1.1; socks 1.7.1; stack_data 0.3.0; statsmodels 0.13.2; tensorboard 2.9.1; texttable 1.6.4; threadpoolctl 3.1.0; toolz 0.12.0; torch 1.12.0+cu102; torchmetrics 0.9.2; tornado 6.2; tqdm 4.64.0; traitlets 5.3.0; tree 0.1.7; typing_extensions NA; urllib3 1.26.10; wcwidth 0.2.5; wrapt 1.14.1; yaml 6.0; zmq 23.2.0; -----; IPython 8.4.0; jupyter_client 7.3.4; jupyter_core 4.10.0; notebook 6.4.12; -----; Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]; Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17; -----; Session information updated at 2022-09-09 14:21; combined_data.write(f""{workspace}Data/Models/Healthy/combined_hvgs_recalculated.h5ad""); combined_data.write(f""{workspace}Data/Models/H",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2321:3165,wrap,wrapt,3165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2321,1,['wrap'],['wrapt']
Integrability,nx <2.3 depends on a since-removed matplotlib API,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1323:8,depend,depends,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1323,1,['depend'],['depends']
Integrability,"o do velocity analysis with the scvelo integration. Installing scanpy as well as hdf5/loom compatibility is remarkably easier on python than in R, which gives scanpy users an obvious advantage. . I've learned so much using this package and it has allowed me to display my data in a simple and intuitive way. Truly, from the bottom of my heart, thank you, this package is awesome and I deeply appreciate all the work the scanpy team and Theis lab has put into it. . With that said, I find myself wanting to do things with scanpy that aren't currently supported. . (1) Clustering cells based on a restricted set of genes as opposed to principle components: I understand that this is a biased approach, but I find myself wanting to do this to explore datasets and how genes vary together. I believe it would aid in understanding data topology to reduce the dimensionality of the dataset by looking at a subset of key genes that are picked by an expert who has domain knowledge. I think this is best integrated with a separate sc.pp.neighbors-like function that uses a gene list, as opposed to PCs. Integration into downstream plotting can be done with separate functions that have a similar name (sc.tl.umap_genelist or sc.tl.umap_sparse; sc.tl.paga_genelist or sc.tl.paga_sparse). . I envision that this would also be a nice stepping stone towards multi-modal analysis with CITE-seq. . (2) I want to use PAGA and UMAP to display my CytOF data as well as integrate scRNA-seq/CytOF data together. I saw a pre-print on bioRxiv that indicated that you are working on this aspect of PAGA/scanpy. This was very exciting to me and I am glad that this is being developed. Is it possible to access the current version of this software? . I think that major partitions identified with CytOF or scRNA-seq can be linked together providing a coarse-grained mechanism to demonstrate how heterogeneity identified with each technique relates to each other based on a given experimental time point. . (3) Histogram inte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510:1267,integrat,integrated,1267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510,1,['integrat'],['integrated']
Integrability,"obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e; ; ~/miniforge3/envs/scVelo/lib",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:3451,wrap,wrapper,3451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['wrap'],['wrapper']
Integrability,"of post doublets removal and QC plot; Running Scrublet; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:282, in scrublet(adata, adata_sim, batch_key, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:7130,wrap,wrapper,7130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wrapper']
Integrability,"okexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/python.py"", line 183, in pytest_pyfunc_call; result = testfunction(**testargs); File ""/home/vsts/work/1/s/scanpy/tests/notebooks/test_paga_paul15_subsampled.py"", line 39, in test_paga_paul15_subsampled; sc.tl.draw_graph(adata); File ""/home/vsts/work/1/s/scanpy/tools/_draw_graph.py"", line 181, in draw_graph; logg.info(; File ""/home/vsts/work/1/s/scanpy/logging.py"", line 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:2331,Message,Message,2331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['Message'],['Message']
Integrability,"ompile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: cannot pickle 'weakref' object; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:5535,protocol,protocol,5535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,2,['protocol'],['protocol']
Integrability,"onfirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am trying to concatenate multiple datasets. I have Anndata's of 4 runs that I'm trying to analyze together. I tried the plotting functions (e.g. sc.pl. violin) and saving to h5ad files on individual Anndata's and it works. When I merge them via ad.concat I keep getting a Type Error due to Anndata calling sanitize and strings_to_categoricals when I try to save or use plotting functions. I thought it might be because I'm adding new obs before merging but then I tried to just merge without any manipulation of individual Anndata's and it still gave the same error. Then I tried to manually merge the Anndata's by saving X as dataframe and obs as separate dataframes, merging them as dataframes and then creating a new Anndata object. I still keep getting these errors. The error message says that I'm trying to manipulate a view of the Anndata object although I'm not subsetting it and when I do adata.is_view it says False. . I'm not sure how to provide a code sample that can be replicated without data in this case. . ### Minimal code sample. ```python; samples= [ <list of 4 hdf5 files>]; all_adata = []; i = 0; for s in samples:; curr_adata = sc.read_h5ad(f""/mnt/d/Labmembers/Deniz/aging_data/{s}""); curr_adata.var_names_make_unique(); all_adata.append(curr_adata); adata= ad.concat(all_adata); #I get the same type error when I try to do; adata.write('trial.hdf5') ; #or; sc.pl.violin(adata, 'volume'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[8], line 1; ----> 1 sc.pl.violin(adata, 'volume'). File /home/denizparmaksiz/anaconda3/envs/scanpy/lib/python3.9/site-packages/scanpy/plotting/_anndata.py:749, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2645:1074,message,message,1074,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2645,1,['message'],['message']
Integrability,"ors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(); sc.settings.set_figure_params(dpi=80, frameon=False, figsize=(3, 3), facecolor=""white""). adata_ref = sc.datasets.pbmc3k_processed(); adata = sc.datasets.pbmc68k_reduced(). var_names = adata_ref.var_names.intersection(adata.var_names); adata_ref = adata_ref[:, var_names]; adata = adata[:, var_names]. # start from scratch; del adata.obs[""louvain""]; adata.uns = {}; adata_ref.uns = {}. # example code for ingest function:; sc.pp.neighbors(adata_ref); sc.tl.umap(adata_ref); sc.tl.ingest(adata, adata_ref, obs=""louvain""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[11], line 23; 21 sc.pp.neighbors(adata_ref); 22 sc.tl.umap(adata_ref); ---> 23 sc.tl.ingest(adata, adata_ref, obs=""louvain""). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:141, in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 138 labeling_method = labeling_method * len(obs); 140 ing = Ingest(adata_ref, neighbors_key); --> 141 ing.fit(adata); 143 for method in embedding_method:; 144 ing.map_embedding(method). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:404, in Ingest.fit(self, adata_new); 401 self._obsm = _DimDict(adata_new.n_obs, axis=0); 403 self._adata_new = adata_new; --> 404 self._obsm[""rep""] = self._same_rep(). File ~/miniconda3/envs/sc/lib/python3.12/site-packages/scanpy/tools/_ingest.py:371, in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3074:1711,wrap,wrapper,1711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3074,1,['wrap'],['wrapper']
Integrability,"ots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 383 rasterized=settings._vector_friendly,; 384 norm=normalize,; --> 385 **kwargs,; 386 ); 387 . TypeError: functools.partial object got multiple values for keyword argument 'marker'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cached_property 1.5.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.09.1; dateutil 2.8.2; debugpy 1.5.0; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fsspec 2021.10.0; google NA; h5py 3.4.0; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.2; joblib 1.1.0; kiwisolver 1.3.2; llvmlite 0.37.0; markupsafe 2.0.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; mudata 0.1.0; muon 0.1.1; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.3; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2122:2787,message,message,2787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2122,1,['message'],['message']
Integrability,"ows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 397 rasterized=settings._vector_friendly,; 398 norm=normalize,; --> 399 **kwargs,; 400 ); 401 . ~/.conda/envs/python_spatial/lib/python3.7/site-packages/scanpy/plotting/_utils.py in circles(x, y, s, ax, marker, c, vmin, vmax, scale_factor, **kwargs); 1106 # while you can only set `facecolors` with a value for all.; 1107 if scale_factor != 1.0:; -> 1108 x = x * scale_factor; 1109 y = y * scale_factor; 1110 zipped = np.broadcast(x, y, s). TypeError: can't multiply sequence by non-int of type 'float'; ```; Bascially, the same type of error keeps showing up no matter what the `color = ` parameter is. I put a gene name there, and the same message showed up. Additional information about the error (the image that showed up together with the error message from the jupyter notebook cell): ![image](https://user-images.githubusercontent.com/78611089/209900312-f6acd040-93e5-43b8-858d-3d1ef0c91cd0.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; annoy NA; asciitree NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; cloudpickle 2.2.0; cycler 0.10.0; cython_runtime NA; dask 2022.02.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.18; fbpca NA; fsspec 2022.11.0; google NA; h5py 3.7.0; igraph 0.10.2; intervaltree NA; ipykernel 6.16.2; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; markupsafe 2.1.1; matplotlib 3.5.3; matplotlib_inline 0.1.6; mpl_toolkits NA; msgpack 1.0.4; natsort 8.2.0; nbinom_ufunc NA; numba 0.56.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391:3019,message,message,3019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391,1,['message'],['message']
Integrability,"packages\scanpy\neighbors\__init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>; from .umap_ import UMAP; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>; from umap.layouts import (; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>; def rdist(x, y):; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160:4025,wrap,wrapper,4025,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160,1,['wrap'],['wrapper']
Integrability,"paga.py:911: in _paga_graph; sct = ax.scatter(; ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/__init__.py:1442: in inner; return func(ax, *map(sanitize_sequence, args), **kwargs); ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/axes/_axes.py:4602: in scatter; self._parse_scatter_color_args(; ../../../../anaconda3/envs/scanpy-dev/lib/python3.8/site-packages/matplotlib/axes/_axes.py:4400: in _parse_scatter_color_args; and isinstance(cbook._safe_first_finite(c), str))); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. obj = [nan, nan, nan, nan, nan, nan, ...]. def _safe_first_finite(obj, *, skip_nonfinite=True):; """"""; Return the first non-None (and optionally finite) element in *obj*.; ; This is a method for internal use.; ; This is a type-independent way of obtaining the first non-None element,; supporting both index access and the iterator protocol.; The first non-None element will be obtained when skip_none is True.; """"""; def safe_isfinite(val):; if val is None:; return False; try:; return np.isfinite(val) if np.isscalar(val) else True; except TypeError:; # This is something that numpy can not make heads or tails; # of, assume ""finite""; return True; if skip_nonfinite is False:; if isinstance(obj, collections.abc.Iterator):; # needed to accept `array.flat` as input.; # np.flatiter reports as an instance of collections.Iterator; # but can still be indexed via [].; # This has the side effect of re-setting the iterator, but; # that is acceptable.; try:; return obj[0]; except TypeError:; pass; raise RuntimeError(""matplotlib does not support generators ""; ""as input""); return next(iter(obj)); elif isinstance(obj, np.flatiter):; # TODO do the finite filtering on this; return obj[0]; elif isinstance(obj, collections.abc.Iterator):; raise RuntimeError(""matplotlib does not """,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2459:4163,protocol,protocol,4163,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2459,1,['protocol'],['protocol']
Integrability,"pe({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 193 f""Above error raised while writing key {key!r} of {type(elem)}""; 194 f"" from {parent}.""; --> 195 ) from e; 196 ; 197 return func_wrapper. NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. Above error raised while writing key 'uns/umap/params/random_state' of <class 'h5py._hl.files.File'> from /.; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:4844,wrap,wrapper,4844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,pi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0; jupyter_core 5.5.0 py311haa95532_0; jupyter_events 0.8.0 py311haa95532_0; jupyter_server 2.10.0 py311haa95532_0; jupyter_server_terminals 0.4.4 py311haa95532_1; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0; jupyterlab_server 2.25.1 py311haa95532_0; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0; libsodium 1.0.18 h62dcd97_0; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2; m2w64-bzip2 1.0.6 6; m2w64-expat 2.1.1 2; m2w64-fftw 3.3.4 6; m2w64-flac 1.3.1 # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1 ; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-bindings 21.2.0 py311h2bbff1b_0 ; array-api-compat 1.5.1 pypi_0 pypi; arrow 1.3.0 pypi_0 pypi; asttokens 2.4.1 pypi_0 pypi; async-lru 2.0.4 py311haa95532_0 ; attrs 23.2.0 pypi_0 pypi; babel 2.14.0 pypi_0 pypi; beautifulsoup4 4.12.3 pypi_0 pypi; bleach 6.1.0 pypi_0 pypi; brotli-python 1.0.9 py311hd77b12b_7 ; bzip2 1.0.8 h2bbff1b_5 ; ca-certificates 2023.12.12 haa95532_0 ; certifi 2024.2.2 py311haa95532_0 ; cffi 1.16.0 py311h2bbff1b_0 ; chardet 5.2.0 pypi_0 pypi; charset-normalizer 3.3.2 pypi_0 pypi; colorama 0.4.6 py311haa95532_0 ; comm 0.2.2 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; cycler ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:4512,wrap,wrap,4512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['wrap'],['wrap']
Integrability,"portError:. ImportError: cannot import name 'GProfiler'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-383-c1b09359d1a1> in <module>; 14 ; 15 #get gene set enrichment; ---> 16 print(sc.queries.enrich(this_adata, org='hsapiens', group='malignant', key='malignantvshealthy', pval_cutoff=0.01, log2fc_min=np.log2(1.5))); 17 ; 18 #plot volcano (makes a sep df along the way, should consolidate with above). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in _enrich_anndata(adata, group, org, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols, gprofiler_kwargs); 305 else:; 306 gene_list = list(de[""names""].dropna()); --> 307 return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/functools.py in wrapper(*args, **kw); 805 '1 positional argument'); 806 ; --> 807 return dispatch(args[0].__class__)(*args, **kw); 808 ; 809 funcname = getattr(func, '__name__', 'singledispatch function'). /anaconda3/envs/mm_singlecell_v2/lib/python3.6/site-packages/scanpy/queries/_queries.py in enrich(container, org, gprofiler_kwargs); 266 except ImportError:; 267 raise ImportError(; --> 268 ""This method requires the `gprofiler-official` module to be installed.""; 269 ); 270 gprofiler = GProfiler(user_agent=""scanpy"", return_dataframe=True). ImportError: This method requires the `gprofiler-official` module to be installed.; ```. #### Versions. gprofiler-official bioconda/noarch::gprofiler-official-1.0.0-py_0. scanpy==1.7.1 anndata==0.7.5 umap==0.5.1 numpy==1.19.5 scipy==1.5.3 pandas==1.1.5 scikit-learn==0.19.1 statsmodels==0.12.2 python-igraph==0.8.3 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1896:1875,wrap,wrapper,1875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896,1,['wrap'],['wrapper']
Integrability,"port_read_key_on_error.<locals>.func_wrapper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad.<locals>.callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File D:\Python3.10.9\lib\site-packages\anndata\_io\specs\methods.py:323, in read_array(elem, _reader); 319 @_REGISTRY.register_read(H5Array, IOSpec(""array"", ""0.2.0"")); 320 @_REGISTRY.register_read(ZarrArray, IOSpec(""array"", ""0.2.0"")); 321 @_REGISTRY.register_read(ZarrArray, IOSpec(""string-array"", ""0.2.0"")); 322 def read_array(elem, _reader):; --> 323 return elem[()]. File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File D:\Python3.10.9\lib\site-packages\h5py\_hl\dataset.py:768, in Dataset.__getitem__(self, args, new_dtype); 767 try:; --> 768 return self._fast_reader.read(args); 769 except TypeError:. File h5py\_selector.pyx:368, in h5py._selector.Reader.read(). File h5py\_selector.pyx:342, in h5py._selector.Reader.make_array(). MemoryError: Unable to allocate 9.90 GiB for an array with shape (310385, 8563) and data type float32. The above exception was the direct cause of the following exception:. AnnDataReadError Traceback (most recent call last); Cell In[2], line 4; 2 import pandas as pd; 3 import scanpy as sc; ----> 4 annData = sc.read_h5ad(""ReplogleWeissman2022_K562_essential.h5ad""). File D:\Python3.10.9\lib\site-packages\anndata\_io\h5ad.py:243, in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 240 return read_dataframe(elem); 241 return func(elem); --> 243 adata = read_dispatched(f, callback=callback)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2551:2169,wrap,wrapper,2169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2551,1,['wrap'],['wrapper']
Integrability,"pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 844 X, adata.var[""mean""], adata.var[""std""] = scale(; 845 X,; 846 zero_center=zero_center,; 847 max_value=max_value,; 848 copy=False, # because a copy has already been made, if it were to be made; 849 return_mean_std=True,; 850 ); 851 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 852 if copy:. File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). TypeError: scale() got an unexpected keyword argument 'return_mean_std'; ```. #### Versions. <details>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.5.0; appnope 0.1.3; asciitree NA; asttokens NA; attr 23.1.0; backcall 0.2.0; brotli NA; certifi 2023.05.07; cffi 1.15.1; charset_normalizer 3.1.0; click 8.1.3; cloudpickle 2.2.1; colorama 0.4.6; colorful 0.5.5; colorful_orig 0.5.5; comm 0.1.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.5.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; distributed 2023.5.0; entrypoints 0.4; executing 1.2.0; fasteners 0.17.3; filelock 3.12.0; fsspec 2023.5.0; google NA; grpc 1.43.0; h5py 3.8.0; idna 3.4; igraph 0.10.4; ipykernel 6.23.1; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; jsonschema 4.17.3; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; locket NA; lz4 4.3.2; markupsafe 2.1.2; matplotlib 3.6.3; matplotlib_i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:4242,wrap,wrapper,4242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapper']
Integrability,"py.; - [ ✔] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; I installed Scanpy, scVelo, CellRank, bbknn 2 months ago and never upgrade the packages. They were running very smoothly until I reimage my PC and reinstall Scanpy in anaconda today (Anaconda3-2021.05-Windows-x86_64, python3.8.12).; I tired `pip install scanpy[leiden]`. Tried `conda install seaborn scikit-learn statsmodels numba pytables`, `conda install -c conda-forge python-igraph leidenalg`. Tried installing `Java` and `visual C++ 2012-2022 redistributable`. Also tried rebuilding a new environment and reinstalled everything. Whatever I try, this bug still exists when I import Scanpy.; I guess it may be the incompatibility issue of packages. Some dependency packages which were upgraded by the developer in these months caused this incompatibility issue. Could you please help me with this bug?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import matplotlib.pyplot as pl; from matplotlib import rcParams; ```. ```pytb; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_7844/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:1088,depend,dependency,1088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['depend'],['dependency']
Integrability,"py2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error message = 'Input/output error', buf = 0x55ec782e9031, total read size = 7011, bytes this sub-read = 7011, bytes actually read = 18446744073709551615, offset = 0). During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-14-faac769583f8> in <module>; 17 #while True:; 18 #try:; ---> 19 adatas.append(sc.read_h5ad(file)); 20 file_diffs.append('_'.join([file.split('/')[i] for i in diff_path_idx])); 21 #break. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 411 d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:1625,wrap,wrapper,1625,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['wrap'],['wrapper']
Integrability,"python; _, axs = pl.subplots(ncols=3, figsize=(6, 2.5), gridspec_kw={'wspace': 0.05, 'left': 0.12}); pl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2); for ipath, (descr, path) in enumerate(paths):; _, data = sc.pl.paga_path(; adata, path, gene_names,; show_node_names=False,; ax=axs[ipath],; ytick_fontsize=8,; left_margin=0.15,; n_avg=50,; annotations=['distance'],; show_yticks=True if ipath==0 else False,; show_colorbar=False,; color_map='Greys',; groups_key='clusters',; color_maps_annotations={'distance': 'viridis'},; title='{} path'.format(descr),; return_data=True,; show=False); data.to_csv('./write/paga_path_{}.csv'.format(descr)); pl.savefig('./figures/paga_path_panglao.pdf'); pl.show(); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-8-86ecf06e6589> in <module>(); 18 title='{} path'.format(descr),; 19 return_data=True,; ---> 20 show=False); 21 data.to_csv('./write/paga_path_{}.csv'.format(descr)); 22 pl.savefig('./figures/paga_path_panglao.pdf'). 5 frames; <__array_function__ internals> in cumsum(*args, **kwargs). /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.18.5 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 leidenalg==0.8.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295:2023,wrap,wrap,2023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295,2,['wrap'],['wrap']
Integrability,"python_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; lifelines 0.28.0; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.59.1; numexpr 2.8.4; numpy 1.26.4; packaging 23.1; pandas 2.2.2; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prometheus_client NA; prompt_toolkit 3.0.36; psutil 5.9.0; pure_eval 0.2.2; pvectorc NA; pyarrow 11.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pyrsistent NA; pythoncom NA; pytz 2023.3.post1; pywintypes NA; requests 2.31.0; rfc3339_validator 0.1.4; rfc3986_validator 0.1.1; ruamel NA; scipy 1.11.1; seaborn 0.13.2; send2trash NA; session_info 1.0.0; setuptools 69.5.1; six 1.16.0; skewnorm_ufunc NA; sklearn 1.3.0; sniffio 1.2.0; socks 1.7.1; sparse 0.15.1; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 1.7.0; terminado 0.17.1; texttable 1.7.0; threadpoolctl 2.2.0; tlz 0.12.0; toolz 0.12.0; torch 2.2.1+cpu; torchgen NA; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; typing_extensions NA; umap 0.5.5; urllib3 1.26.16; wcwidth 0.2.5; websocket 0.58.0; win32api NA; win32com NA; win32con NA; win32trace NA; winerror NA; winpty 2.0.10; wrapt 1.14.1; xxhash 2.0.2; yaml 6.0; zipp NA; zmq 23.2.0; zope NA; -----; IPython 8.15.0; jupyter_client 7.4.9; jupyter_core 5.3.0; jupyterlab 3.6.3; notebook 6.5.4; -----; Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]; Windows-10-10.0.22631-SP0; -----; Session information updated at 2024-06-02 17:20. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:8617,wrap,wrapt,8617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapt']
Integrability,"r=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1737,integrat,integration,1737,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['integrat'],['integration']
Integrability,"records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorica",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:3351,interface,interface,3351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['interface'],['interface']
Integrability,"rgs); 5657 self.set_aspect(aspect); 5658 im = mimage.AxesImage(self, cmap=cmap, norm=norm,; 5659 interpolation=interpolation, origin=origin,; 5660 extent=extent, filternorm=filternorm,; 5661 filterrad=filterrad, resample=resample,; 5662 interpolation_stage=interpolation_stage,; 5663 **kwargs); -> 5665 im.set_data(X); 5666 im.set_alpha(alpha); 5667 if im.get_clip_path() is None:; 5668 # image does not already have clipping set, clip to axes patch. File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\image.py:710, in _ImageBase.set_data(self, A); 706 self._A = self._A[:, :, 0]; 708 if not (self._A.ndim == 2; 709 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; --> 710 raise TypeError(""Invalid shape {} for image data""; 711 .format(self._A.shape)); 713 if self._A.ndim == 3:; 714 # If the input data has values outside the valid range (after; 715 # normalisation), we issue a warning and then clip X to the bounds; 716 # - otherwise casting wraps extreme values, hiding outliers and; 717 # making reliable interpretation impossible.; 718 high = 255 if np.issubdtype(self._A.dtype, np.integer) else 1. TypeError: Invalid shape (633,) for image data; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.10.1; -----; PIL 9.5.0; anyio NA; arrow 1.3.0; asttokens NA; astunparse 1.6.3; attr 23.1.0; attrs 23.1.0; babel 2.13.0; backcall 0.2.0; certifi 2023.07.22; cffi 1.16.0; charset_normalizer 3.3.0; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dask 2023.10.0; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.0; fastjsonschema NA; fqdn NA; h5py 3.9.0; idna 3.4; igraph 0.10.8; ipykernel 6.25.2; ipywidgets 8.1.1; isoduration NA; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.19.1; jsonschema_specifications NA; jupyter_events 0.7.0; jupyter_server 2.7.3; jupyterlab_server 2.25.0; kiwi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:4955,wrap,wraps,4955,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Integrability,"rical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python; import numpy as np; pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""); # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms; # Wall time: 74.9 ms; to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]; sc.pl.umap(pbmc, color=to_plot, ncols=2); ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reduct",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:1900,wrap,wrapping,1900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['wrap'],['wrapping']
Integrability,"rids.; 482 zorder_offset = max(axis.get_zorder(); 483 for axis in self._get_axis_list()) + 1; 484 for i, col in enumerate(; --> 485 sorted(self.collections,; 486 key=do_3d_projection,; 487 reverse=True)):; 488 col.zorder = zorder_offset + i; 489 for i, patch in enumerate(; 490 sorted(self.patches,; 491 key=do_3d_projection,; 492 reverse=True)):. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:471, in Axes3D.draw.<locals>.do_3d_projection(artist); 458 """"""; 459 Call `do_3d_projection` on an *artist*, and warn if passing; 460 *renderer*.; (...); 464 *renderer* and raise a warning.; 465 """"""; 467 if artist.__module__ == 'mpl_toolkits.mplot3d.art3d':; 468 # Our 3D Artists have deprecated the renderer parameter, so; 469 # avoid passing it to them; call this directly once the; 470 # deprecation has expired.; --> 471 return artist.do_3d_projection(); 473 _api.warn_deprecated(; 474 ""3.4"",; 475 message=""The 'renderer' parameter of ""; 476 ""do_3d_projection() was deprecated in Matplotlib ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:9118,message,message,9118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['message'],['message']
Integrability,"rm, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:204, in scrublet.<locals>._run_scrublet(ad_obs, ad_sim); 201 # HVG process needs log'd data.; 203 logged = pp.log1p(ad_obs, copy=True); --> 204 pp.highly_variable_genes(logged); 205 ad_obs = ad_obs[:, logged.var[""highly_variable""]].copy(); 207 # Simulate the doublets based on the raw expressions from the normalised; 208 # and filtered object. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:648, in highly_variable_genes(***failed resolving arguments***); 645 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 647 if batch_key is None:; --> 648 df = _highly_variable_genes_single_batch(; 649 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 650 ); 651 else:; 652 df = _highly_variable_genes_batched(; 653 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 654 ). File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py:281, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 279 # all of the following quantities are ""per-gene""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:8713,wrap,wraps,8713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wraps']
Integrability,"s the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5o.pyx in h5py.h5o.link(). RuntimeError: Unable to create link (name already exists). The above exception was the direct cause of the following exception:. RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 283 write_array(group, category_key, categories, dataset_kwargs=dataset_kwargs); --> 284 write_array(group, key, codes, dataset_kwargs=dataset_kwargs); 285 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <cla",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:2087,wrap,wrapper,2087,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,"s,; 1784 plot_kws=kwargs,; 1785 ); 1787 p._add_axis_labels(ax); 1788 p._adjust_cat_axis(ax, axis=p.orient). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/seaborn/categorical.py:1047, in _CategoricalPlotter.plot_violins(self, width, dodge, gap, split, color, fill, linecolor, linewidth, inner, density_norm, common_norm, kde_kws, inner_kws, plot_kws); 1045 # Plot the main violin body; 1046 plot_func = {""x"": ax.fill_betweenx, ""y"": ax.fill_between}[self.orient]; -> 1047 plot_func(; 1048 inv_val(data[value_var]),; 1049 inv_pos(data[self.orient] - offsets[0]),; 1050 inv_pos(data[self.orient] + offsets[1]),; 1051 **violin[""kwargs""]; 1052 ); 1054 # Adjust the observation data; 1055 obs = violin[""observations""]. File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/__init__.py:1473, in _preprocess_data.<locals>.inner(ax, data, *args, **kwargs); 1470 @functools.wraps(func); 1471 def inner(ax, *args, data=None, **kwargs):; 1472 if data is None:; -> 1473 return func(; 1474 ax,; 1475 *map(sanitize_sequence, args),; 1476 **{k: sanitize_sequence(v) for k, v in kwargs.items()}); 1478 bound = new_sig.bind(ax, *args, **kwargs); 1479 auto_label = (bound.arguments.get(label_namer); 1480 or bound.kwargs.get(label_namer)). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5662, in Axes.fill_betweenx(self, y, x1, x2, where, step, interpolate, **kwargs); 5660 def fill_betweenx(self, y, x1, x2=0, where=None,; 5661 step=None, interpolate=False, **kwargs):; -> 5662 return self._fill_between_x_or_y(; 5663 ""y"", y, x1, x2,; 5664 where=where, interpolate=interpolate, step=step, **kwargs). File ~/miniconda3/envs/scarf_env/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5629, in Axes._fill_between_x_or_y(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs); 5625 pts = pts[:, ::-1]; 5627 polys.append(pts); -> 5629 collection = mcoll.PolyCollection(polys, **kwargs); 5631 # now update the datalim and autoscale; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3140:5441,wrap,wraps,5441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3140,1,['wrap'],['wraps']
Integrability,"s. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . ~/.local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```; #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>. scanpy==1.7.0rc2.dev7+g57ec8a7e anndata==0.7.3 umap==0.4.6 numpy==1.19.5 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:2677,wrap,wrap,2677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['wrap'],['wrap']
Integrability,"savefig(writekey, dpi=dpi, ext=ext); 313 if show:; 314 pl.show(). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in savefig(writekey, dpi, ext); 280 else:; 281 dpi = rcParams['savefig.dpi']; --> 282 settings.figdir.mkdir(parents=True, exist_ok=True); 283 if ext is None:; 284 ext = settings.file_format_figs. AttributeError: 'str' object has no attribute 'mkdir'; ```. #### Versions. scanpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3558,message,message,3558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['message'],['message']
Integrability,"sc.pl creates ""Do not localize"" message",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/202:32,message,message,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/202,1,['message'],['message']
Integrability,"sc.pl.scatter() is a wrapper for _scatter_obs(). It checks to make sure; the variable names the caller is requesting to plot exist in var and/or; obs, but does not take into account whether it should look in raw based; on the use_raw flag, as _scatter_obs() does. This leads to errors when a; user asks to plot variables that are in the raw but not the filtered; matrix of adata. This commit fixes that bug. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:21,wrap,wrapper,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,1,['wrap'],['wrapper']
Integrability,sc.pl.umap error message if sc.tl.umap has not been computed.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1460:17,message,message,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1460,1,['message'],['message']
Integrability,"scvelo docs have been changed so the url for the sphinx inventory is different. We also probably don't want to depend on scvelo's documentation for our doc builds, especially since it's pre 1.0, and we weren't really doing much with it. Should fix current doc build problems.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1608:111,depend,depend,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1608,1,['depend'],['depend']
Integrability,scvi integration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520:5,integrat,integration,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520,1,['integrat'],['integration']
Integrability,"sing=2.4.7=pyhd3eb1b0_0; - pyqt=5.9.2=py37h05f1152_2; - pyrsistent=0.17.3=py37h7b6447c_0; - python=3.7.9=h7579374_0; - python-dateutil=2.8.1=pyhd3eb1b0_0; - pyzmq=20.0.0=py37h2531618_1; - qt=5.9.7=h5867ecd_1; - qtconsole=4.7.7=py_0; - qtpy=1.9.0=py_0; - readline=8.1=h27cfd23_0; - send2trash=1.5.0=pyhd3eb1b0_1; - setuptools=52.0.0=py37h06a4308_0; - sip=4.19.8=py37hf484d3e_0; - six=1.15.0=py37h06a4308_0; - sqlite=3.33.0=h62c20be_0; - terminado=0.9.2=py37h06a4308_0; - testpath=0.4.4=pyhd3eb1b0_0; - tk=8.6.10=hbc83047_0; - tornado=6.1=py37h27cfd23_0; - traitlets=5.0.5=pyhd3eb1b0_0; - wcwidth=0.2.5=py_0; - webencodings=0.5.1=py37_1; - wheel=0.36.2=pyhd3eb1b0_0; - widgetsnbextension=3.5.1=py37_0; - xz=5.2.5=h7b6447c_0; - zeromq=4.3.3=he6710b0_3; - zipp=3.4.0=pyhd3eb1b0_0; - zlib=1.2.11=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the same results. ; If you need access to the data and the container please contact me and I will make it available to you.; The data is already at the ICB cluster. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:4621,wrap,wrap,4621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['wrap'],['wrap']
Integrability,"sion of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I was running an older jupyter notebook based on a scanpy tutorial. I had included a call to `scanpy.logging.print_versions()` for debugging purposes. I just ran the code using the the current main branch of scanpy, and it errored out. See below for output. ### Minimal code sample. ```python; import scanpy; scanpy.logging.print_versions(); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.logging.print_versions(). File ~/Documents/Projects/githubPackages/scanpy/scanpy/logging.py:180, in print_versions(file); 178 print_versions(); 179 else:; --> 180 session_info.show(; 181 dependencies=True,; 182 html=False,; 183 excludes=[; 184 'builtins',; 185 'stdlib_list',; 186 'importlib_metadata',; 187 # Special module present if test coverage being calculated; 188 # https://gitlab.com/joelostblom/session_info/-/issues/10; 189 ""$coverage"",; 190 ],; 191 ). File ~/Desktop/data/env/lib/python3.11/site-packages/session_info/main.py:209, in show(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 207 for mod_name in clean_modules:; 208 mod_names.append(mod_name); --> 209 mod = sys.modules[mod_name]; 210 # Since modules use different attribute names to store version info,; 211 # try the most common ones.; 212 try:. KeyError: 'numcodecs'; ```. ### Versions. <details>. The function we are asked to run here is the one that produces the error. As an alternative, I'm pasting the output of `scanpy.settings.set_figure_params(dpi=80, facecolor='white')`, which includes several versions in the output. ```; scanpy==1.10.0.dev88+gedd61302 anndata==0.9.2 umap==0.5.3 numpy==1.24.4 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 igraph==0.10.6 pynndescent==0.5.10; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2580:1391,depend,dependencies,1391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2580,1,['depend'],['dependencies']
Integrability,"t call last); Input In [3], in <cell line: 1>(); ----> 1 adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'). File ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py:180, in read_10x_h5(filename, genome, gex_only, backup_url); 178 if not is_present:; 179 logg.debug(f'... did not find original file {filename}'); --> 180 with h5py.File(str(filename), 'r') as f:; 181 v3 = '/matrix' in f; 182 if v3:. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:507, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds); 502 fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,; 503 locking, page_buf_size, min_meta_keep, min_raw_keep, **kwds); 504 fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,; 505 fs_persist=fs_persist, fs_threshold=fs_threshold,; 506 fs_page_size=fs_page_size); --> 507 fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); 509 if isinstance(libver, tuple):; 510 self._libver = libver. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:220, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 218 if swmr and swmr_support:; 219 flags |= h5f.ACC_SWMR_READ; --> 220 fid = h5f.open(name, flags, fapl=fapl); 221 elif mode == 'r+':; 222 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). File h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper(). File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper(). File h5py/h5f.pyx:106, in h5py.h5f.open(). IsADirectoryError: [Errno 21] Unable to open file (file read failed: time = Fri Sep 16 14:17:08 2022; , filename = 'GSE164690_RAW/GSM5017021_HN01_PBL/', file descriptor = 75, errno = 21, error message = 'Is a directory', buf = 0x3072d2728, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0); `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2328:2004,wrap,wrapper,2004,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2328,3,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"t.py"", line 974, in run_command; cmd_obj.run(); File ""/usr/lib/python3.5/distutils/command/build.py"", line 135, in run; self.run_command(cmd_name); File ""/usr/lib/python3.5/distutils/cmd.py"", line 313, in run_command; self.distribution.run_command(command); File ""/usr/lib/python3.5/distutils/dist.py"", line 974, in run_command; cmd_obj.run(); File ""/tmp/pip-build-33o4crd7/scanpy/versioneer.py"", line 1559, in run; _build_py.run(self); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 52, in run; self.build_package_data(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-33o4crd7/scanpy/; You are using pip version 8.1.1, however version 18.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command.; ```. Same error message after upgrading pip. Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:11988,message,message,11988,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['message'],['message']
Integrability,t: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 h10d778d_5 conda-forge; ca-certificates 2024.2.2 h8857fd0_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.3.0 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.49.0 pypi_0 pypi; fsspec 2024.2.0 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.3.2 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h73e2aa4_0 conda-forge; libffi 3.4.2 h0d85af4_5 conda-forge; libsqlite 3.45.2 h92b6c6a_0 conda-forge; libzlib 1.2.13 h8a1eda9_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.3 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 h93d8f39_2 conda-forge; networkx 3.2.1 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.0 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd75f5a5_0 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.2.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pynndescent 0.5.11 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 4.1.0 pypi_0 pypi; pytest-mock 3.12.0 pypi_0 pypi; pytest-,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:33147,wrap,wrap,33147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['wrap'],['wrap']
Integrability,"t; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrublet\__init__.py:282, in scrublet(adata, adata_sim, batch_key, sim_doublet_ratio, expected_doublet_rate, stdev_doublet_rate, synthetic_doublet_umi_subsampling, knn_dist_metric, normalize_variance, log_transform, mean_center, n_prin_comps, use_approx_neighbors, get_doublet_neighbor_parents, n_neighbors, threshold, verbose, copy, random_state); 279 adata.uns[""scrublet""][""batched_by""] = batch_key; 281 else:; --> 282 scrubbed = _run_scrublet(adata_obs, adata_sim); 284 # Copy outcomes to input object from our processed version; 286 adata.obs[""doublet_score""] = scrubbed[""obs""][""doublet_score""]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_scrub",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:7183,wrap,wraps,7183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['wrap'],['wraps']
Integrability,"t__.py:80: in fn_compatible; return fn(*args_all, **kw); scanpy/preprocessing/_highly_variable_genes.py:651: in highly_variable_genes; df = _highly_variable_genes_single_batch(; scanpy/preprocessing/_highly_variable_genes.py:288: in _highly_variable_genes_single_batch; df[""highly_variable""] = _subset_genes(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . adata = AnnData object with n_obs × n_vars = 700 × 765; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score...'pca', 'rank_genes_groups', 'log1p'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'. def _subset_genes(; adata: AnnData,; *,; mean: NDArray[np.float64] | DaskArray,; dispersion_norm: NDArray[np.float64] | DaskArray,; cutoff: _Cutoffs | int,; ) -> NDArray[np.bool_] | DaskArray:; """"""Get boolean mask of genes with normalized dispersion in bounds.""""""; if isinstance(cutoff, _Cutoffs):; > dispersion_norm[np.isnan(dispersion_norm)] = 0 # similar to Seurat; E ValueError: assignment destination is read-only. scanpy/preprocessing/_highly_variable_genes.py:365: ValueError; ```. </details>. Dependencies are different, looks like a dask update and a pyarrow added dep. I suspect this has to do with the new dask-expr. ----. I can replicate locally by install the new dask, dask-expr, and pyarrow. ----. Importing dask.dataframe changes the settings for pandas somehow:. ```python; In [1]: import pandas as pd. In [2]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[2]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : True; ALIGNED : True; WRITEBACKIFCOPY : False. In [3]: import dask.dataframe as ddf. In [4]: pd.DataFrame({""a"": [1,2,3, None]})[""a""].to_numpy().flags; Out[4]: ; C_CONTIGUOUS : True; F_CONTIGUOUS : True; OWNDATA : False; WRITEABLE : False; ALIGNED : True; WRITEBACKIFCOPY : False; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2902:6192,Depend,Dependencies,6192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2902,1,['Depend'],['Dependencies']
Integrability,"ta_dist = AnnData object with n_obs × n_vars = 10000 × 1000; var: 'gene_ids'; uns: 'dist-mode'. def test_normalize_total(adata: AnnData, adata_dist: AnnData):; > normalize_total(adata_dist). scanpy/tests/test_preprocessing_distributed.py:93: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; /home/phil/.local/share/hatch/env/virtual/scanpy/q4In3tK-/hatch-test.stable/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80: in fn_compatible; return fn(*args_all, **kw); scanpy/preprocessing/_normalization.py:240: in normalize_total; adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer; scanpy/preprocessing/_normalization.py:49: in _normalize_data; return axis_mul_or_truediv(; /usr/lib/python3.12/functools.py:909: in wrapper; return dispatch(args[0].__class__)(*args, **kw); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . X = <zappy.direct.array.DirectZappyArray object at 0x7f06eb607a40>, scaling_array = array([0.9270073, 0.4379562, 1.5985402, ..., 0.8540146, 1.5109489,; 0.8978102], dtype=float32), axis = 0, op = <built-in function truediv>. @singledispatch; def axis_mul_or_truediv(; X: sparse.spmatrix,; scaling_array,; axis: Literal[0, 1],; op: Callable[[Any, Any], Any],; *,; allow_divide_by_zero: bool = True,; out: sparse.spmatrix | None = None,; ) -> sparse.spmatrix:; check_op(op); if out is not None:; if X.data is not out.data:; raise ValueError(; ""`out` argument provided but not equal to X. This behavior is not supported for sparse matrix scaling.""; ); if not al",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3087:1993,wrap,wrapper,1993,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3087,1,['wrap'],['wrapper']
Integrability,"taset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:4032,wrap,wrapper,4032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['wrap'],['wrapper']
Integrability,"te_h5ad(""/Users/julius/Desktop/zf_48.h5ad""); ```; Here is the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5o.pyx in h5py.h5o.link(). RuntimeError: Unable to create link (name already exists). The above exception was the direct cause of the following exception:. RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 283 write_array(group, category_key, categories, dataset_kwargs=dataset_kwargs); --> 284 write_array(group, key, codes, dataset_kwargs=dataset_kwargs); 285 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n"". RuntimeError: Unable to create link (name already exists). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:2031,wrap,wrapper,2031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['wrap'],['wrapper']
Integrability,"te_vlen_string_array; f.create_dataset(k, data=elem.astype(str_dtype), dtype=str_dtype, **dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/group.py"", line 183, in create_dataset; dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds); File ""/home/joyzheng/.local/lib/python3.8/site-packages/h5py/_hl/dataset.py"", line 168, in make_new_dset; dset_id.write(h5s.ALL, h5s.ALL, data); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5d.pyx"", line 280, in h5py.h5d.DatasetID.write; File ""h5py/_proxy.pyx"", line 145, in h5py._proxy.dset_rw; File ""h5py/_conv.pyx"", line 444, in h5py._conv.str2vlen; File ""h5py/_conv.pyx"", line 95, in h5py._conv.generic_converter; File ""h5py/_conv.pyx"", line 249, in h5py._conv.conv_str2vlen; TypeError: Can't implicitly convert non-string objects to strings. The above exception was the direct cause of the following exception:; Traceback (most recent call last):; File ""integration.py"", line 66, in <module>; adata.write_h5ad('Integrated.h5ad'); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_core/anndata.py"", line 1918, in write_h5ad; _write_h5ad(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py"", line 98, in write_h5ad; write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 175, in write_elem; _REGISTRY.get_writer(dest_type, t, modifiers)(f, k, elem, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 514, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:2991,integrat,integration,2991,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['integrat'],['integration']
Integrability,"this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689; - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3267:595,Depend,Depending,595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267,2,"['Depend', 'depend']","['Depending', 'depend']"
Integrability,"thon3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new; ```. What I basically do from raw UMI counts:. 1. total counts normalization / logarithmization; 2. PCA, bbknn, louvain; 3. combat, HVG, PCA, UMAP (works well); 4. Paga (with louvain from 2., works well); 5. UMAP (with positions from 4., does not work). Any idea? Any further info needed?; Best,; Jens",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666:3286,message,message,3286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666,1,['message'],['message']
Integrability,"thub.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1671,wrap,wraps,1671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['wrap'],['wraps']
Integrability,"try.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path\site-packages\anndata\_io\specs\methods.py"", line 92, in read_basic; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 92, in <dictcomp>; return {k: read_elem(v) for k, v in elem.items()}; File ""[python-path]\site-packages\anndata\_io\specs\registry.py"", line 183, in read_elem; return _REGISTRY.get_reader(type(elem), get_spec(elem), frozenset(modifiers))(elem); File ""[python-path]\site-packages\anndata\_io\specs\methods.py"", line 475, in read_sparse; return SparseDataset(elem).to_memory(); File ""[python-path]\site-packages\anndata\_core\sparse_dataset.py"", line 380, in to_memory; mtx.indices = self.group[""indices""][...]; File ""h5py\_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py\_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""[python-path]\site-packages\h5py\_hl\dataset.py"", line 741, in __getitem__; return self._fast_reader.read(args); File ""h5py\_selector.pyx"", line 362, in h5py._selector.Reader.read; File ""h5py\_selector.pyx"", line 336, in h5py._selector.Reader.make_array; numpy.core._exceptions._ArrayMemoryError: Unable to allocate 56.8 GiB for an array with shape (7621750342,) and data type int64. Process finished with exit code 1. ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; beta_ufunc NA; binom_ufunc NA; colorama 0.4.5; console_thrift NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; h5py 3.7.0; hypergeom_ufunc NA; joblib 1.2.0; kiwisolver 1.4.4; llvmlite 0.39.1; matplotlib 3.6.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; nt NA; numba 0.56.2; numpy 1.22.3; packaging 21.3; pandas 1.4.1; pkg_resources NA; pydev_console NA; pydev_ipython NA; pydevconsole NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pyparsing 3.0.7; pytz 2022.1; scipy 1.8.0; session_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:3574,wrap,wrapper,3574,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['wrap'],['wrapper']
Integrability,"types.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. /usr/local/lib/python3.7/dist-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with self._guard_against_spurious_io_errors():; --> 671 self._save_overload(sig, data); 672 ; 673 def _save_overload(self, sig, data):. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_overload(self, sig, data); 679 key = self._index_key(sig, _get_codegen(data)); 680 data = self._impl.reduce(data); --> 681 self._cache_file.save(key, data); 682 ; 683 @contextlib.contextmanager. /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save(self, key, data); 494 break; 495 overloads[key] = data_name; --> 496 self._save_index(overloads); 497 self._save_data(data_name, data); 498 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _save_index(self, overloads); 540 def _save_index(self, overloads):; 541 data = self._source_stamp, overloads; --> 542 data = self._dump(data); 543 with self._open_for_write(self._index_path) as f:; 544 pickle.dump(self._version, f, protocol=-1). /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in _dump(self, obj); 568 ; 569 def _dump(self, obj):; --> 570 return pickle.dumps(obj, protocol=-1); 571 ; 572 @contextlib.contextmanager. TypeError: can't pickle weakref objects. How to solve this problem? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951:4772,protocol,protocol,4772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951,2,['protocol'],['protocol']
Integrability,"uild_update; len(to_build)); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/builders/__init__.py"", line 311, in build; updated_docnames = set(self.read()); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/contextlib.py"", line 88, in __exit__; next(self.gen); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 213, in pending_warnings; memhandler.flushTo(logger); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 178, in flushTo; logger.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1454, in handle; self.callHandlers(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 1516, in callHandlers; hdlr.handle(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 861, in handle; rv = self.filter(record); File ""/home/docs/.pyenv/versions/3.6.8/lib/python3.6/logging/__init__.py"", line 720, in filter; result = f.filter(record); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1204/lib/python3.6/site-packages/sphinx/util/logging.py"", line 415, in filter; raise SphinxWarning(location + "":"" + str(message)); sphinx.errors.SphinxWarning: /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/1204/scanpy/plotting/_dotplot.py:docstring of scanpy.pl.dotplot:122:Inline strong start-string without end-string. Warning, treated as error:; /home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/1204/scanpy/plotting/_dotplot.py:docstring of scanpy.pl.dotplot:122:Inline strong start-string without end-string.; ```. </details>. I get the same error locally from just running `make html`, and was able to get rid of it by adding a page for the `DotPlot` class (though more errors came up after that).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1307:2947,message,message,2947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1307,1,['message'],['message']
Integrability,"ule>; ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):; 684 raise TypeError(""Image data of dtype {} ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:4271,wrap,wrapper,4271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"ule>; ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1057 if as_heatmap:; 1058 img = ax.imshow(; -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map; 1060 ); 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1599 def inner(ax, *args, data=None, **kwargs):; 1600 if data is None:; -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs); 1602 ; 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs); 367 f""%(removal)s. If any parameter follows {name!r}, they ""; 368 f""should be pass as keyword, not positionally.""); --> 369 return func(*args, **kwargs); 370 ; 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs); 5669 resample=resample, **kwargs); 5670 ; -> 5671 im.set_data(X); 5672 im.set_alpha(alpha); 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A); 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 689 raise TypeError(""Invalid shape {} for im",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953:1756,wrap,wrapper,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953,1,['wrap'],['wrapper']
Integrability,"unning highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:655, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 653 sig = signature(_highly_variable_genes_seurat_v3); 654 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 655 return _highly_variable_genes_seurat_v3(; 656 adata,; 657 flavor=flavor,; 658 layer=layer,; 659 n_top_genes=n_top_genes,; 660 batch_key=batch_key,; 661 check_values=check_values,; 662 span=span,; 663 subset=subset,; 664 inplace=inplace,; 665 ); 667 cutoff = _Cutoffs.validate(; 668 n_top_genes=n_top_genes,; 669 min_disp=min_disp,; (...); 672 max_mean=max",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:2752,wrap,wraps,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['wrap'],['wraps']
Integrability,"uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 253 timeseries(; 254 adata.X[adata.obs[""dpt_order_indices""].values],; 255 var_names=adata.var_names,; (...); 258 marker=marker,; 259 ). File D:\anaconda\Lib\site-packages\scanpy\plotting\_utils.py:227, in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 223 x_new[:, _hold:] = X[:, hold:]; 225 _, ax = plt.subplots(figsize=(1.5 * 4, 2 * 4)); 226 img = ax.imshow(; --> 227 np.array(X, dtype=np.float_),; 228 aspect=""auto"",; 229 interpolation=""nearest"",; 230 cmap=color_map,; 231 ); 232 plt.colorbar(img, shrink=0.5); 233 plt.yticks(range(X.shape[0]), var_names). ValueError: setting an array element with a sequence.; ```. Error in dpt_groups_pseudotime:. ```pytb; ValueError Traceback (most recent call last); Cell In[91], line 1; ----> 1 sc.pl.dpt_groups_pseudotime(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:276, in dpt_groups_pseudotime(adata, color_map, palette, show, save, marker); 274 """"""Plot groups and pseudotime.""""""; 275 _, (ax_grp, ax_ord) = plt.subplots(2, 1); --> 276 timeseries_subplot(; 277 adata.obs[""dpt_groups""].cat.codes,; 278 time=adata.obs[""dpt_order""].values,; 279 color=np.asarray(adata.obs[""dpt_groups""]),; 280 highlights_x=adata.uns[""dpt_changepoints""],; 281 ylabel=""dpt groups"",; 282 yticks=(; 283 np.arange(len(adata.obs[""dpt_groups""].cat.categories), dtype=int); 284 if len(adata.obs[""dpt_groups""].cat.categories) < 5; 285 else None; 286 ),; 287 palette=palette,; 288 ax=ax_grp,; 289 marker=marker,; 290 ); 291 timeseri",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:2875,wrap,wrapper,2875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['wrap'],['wrapper']
Integrability,"up(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-2-2626ee07d023> in <module>; ----> 1 sc.read_h5ad('/xchip/beroukhimlab/michelle/jjeang_plgg/sc_labeled.h5ad'). ~/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:2269,wrap,wrapper,2269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['wrap'],['wrapper']
Integrability,"upby != dendro_info[""groupby""]:. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_anndata.py:2384, in _get_dendrogram_key(adata, dendrogram_key, groupby); 2377 from ..tools._dendrogram import dendrogram; 2379 logg.warning(; 2380 f""dendrogram data not found (using key={dendrogram_key}). ""; 2381 ""Running `sc.tl.dendrogram` with default parameters. For fine ""; 2382 ""tuning it is recommended to run `sc.tl.dendrogram` independently.""; 2383 ); -> 2384 dendrogram(adata, groupby, key_added=dendrogram_key); 2386 if ""dendrogram_info"" not in adata.uns[dendrogram_key]:; 2387 raise ValueError(; 2388 f""The given dendrogram key ({dendrogram_key!r}) does not contain ""; 2389 ""valid dendrogram information.""; 2390 ). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/tools/_dendrogram.py:121, in dendrogram(adata, groupby, n_pcs, use_rep, var_names, use_raw, cor_method, linkage_method, optimal_ordering, key_added, inplace); 25 @old_positionals(; 26 ""n_pcs"",; 27 ""use_rep"",; (...); 49 inplace: bool = True,; 50 ) -> dict[str, Any] | None:; 51 """"""\; 52 Computes a hierarchical clustering for the given `groupby` categories.; 53 ; (...); 118 >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True); 119 """"""; --> 121 raise_not_implemented_error_if_backed_type(adata.X, ""dendrogram""); 122 if isinstance(groupby, str):; 123 # if not a list, turn into a list; 124 groupby = [groupby]. File ~/.local/sha",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:4375,wrap,wraps,4375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['wrap'],['wraps']
Integrability,update ValueError message in pca,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/858:18,message,message,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858,1,['message'],['message']
Integrability,"url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in read_excel(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds); 373 convert_float=convert_float,; 374 mangle_dupe_cols=mangle_dupe_cols,; --> 375 **kwds); 376 ; 377 . ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/io/excel.py in parse(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2698,wrap,wrapper,2698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,2,['wrap'],['wrapper']
Integrability,"use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 118 adata._init_as_actual(adata.copy()); 119 neighbors = Neighbors(adata); --> 120 neighbors.compute_neighbors(; 121 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 122 method=method, metric=metric, metric_kwds=metric_kwds,. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 732 X = pairwise_distances(X, metric=metric, **metric_kwds); 733 metric = 'precomputed'; --> 734 knn_indices, knn_distances, forest = compute_neighbors_umap(; 735 X, n_neighbors, random_state, metric=metric, metric_kwds=metric_kwds); 736 # very cautious here. ~/.local/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors_umap(X, n_neighbors, random_state, metric, metric_kwds, angular, verbose); 273 # umap 0.5.0; 274 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 275 from umap.umap_ import nearest_neighbors; 276 ; 277 random_state = check_random_state(random_state). ~/.local/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. ~/.local/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 1 import pkg_resources; 2 import numba; ----> 3 from .pynndescent_ import NNDescent, PyNNDescentTransformer; 4 ; 5 # Workaround: https://github.com/numba/numba/issues/3341. ~/.local/lib/python3.9/site-packages/pynndescent/pynndescent_.py in <module>; 19 import heapq; 20 ; ---> 21 import pynndescent.sparse as sparse; 22 import pynndescent.sparse_nndescent as sparse_nnd; 23 import pynndescent.distances as pynnd_dist. ~/.local/lib/python3.9/site-packages/pynndescent/sparse.py in <module>; 341 },; 342 ); --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:2015,message,message,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['message'],['message']
Integrability,"ut your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/miniconda3/envs/path/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2282, in <module>; switch_backend(rcParams[""backend""]); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/miniconda3/envs/path/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/miniconda3/envs/path/lib/python3.7/site-packages/matplotlib/backends/backend_wxagg.py"", line 1, in <module>; import wx; ModuleNotFoundError: No module named 'wx'; ```. The solution is simple, install `wxPython` https://pypi.org/project/wxPython/. However, it would be nice if scanpy could handle this OS-specific dependancy. #### Versions:; The latest scanpy version (1.5.1) installed via conda- of course I cannot print the versions since the scanpy import fails, other details;. ```; >>> import sys; print(sys.version); 3.7.6 | packaged by conda-forge | (default, Jun 1 2020, 18:33:30) ; [Clang 9.0.1 ]; >>> import platform; print(platform.python_implementation()); print(platform.platform()); CPython; Darwin-17.7.0-x86_64-i386-64bit; ```; ...; ```; $ sw_vers; ProductName:	Mac OS X; ProductVersion:	10.13.6; BuildVersion:	17G11023; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1302:1783,depend,dependancy,1783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1302,1,['depend'],['dependancy']
Integrability,"v for k, v in kw.items() if k not in NON_COLORBAR_KEYS}; -> 2343 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2344 ; 2345 self.sca(current_ax). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in colorbar_factory(cax, mappable, **kwargs); 1732 cb = ColorbarPatch(cax, mappable, **kwargs); 1733 else:; -> 1734 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\progr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:2609,wrap,wrapper,2609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['wrap'],['wrapper']
Integrability,"vals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right?; ares calculated from the p-values? What's the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [NOTE: This doesn't just depend on a specific type of clustering technique. Irrespective of whether I try the in-built Leiden clustering technique, or the Schit library, or even cluster the cells randomly, this issue keeps occuring--which makes me think it's ether a bug, or I don't quite understand how the score generation works, maybe both. I've provided a little code snipper below.]. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12243486/ClusterOneVsRest.csv). ### Minimal code sample. ```python; import pickle; import numpy as np; import pandas as pd; from PIL import Image; import glob; import matplotlib.pyplot as plt; from skimage.morphology import convex_hull_image; from skimage import data, img_as_float; from skimage.util import invert; from scipy.spatial import ConvexHull, convex_hull_plot_2d; from multiprocessing import Pool; import time; import math; from collections import Counter; import scanpy as sc; import networkx as nx; import pandas as pd; import numpy as np; import itertools; import random;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2586:1485,depend,depend,1485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586,1,['depend'],['depend']
Integrability,"ward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.from_zarr(elem); else:; return func(elem). adata = ad.experimental.read_dispatched(f, callback=callback). return adata; adata_dask = read_dask(f'./{rel_zarr_path}'). # perform preprocessing; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pp.scale(adata_dask, max_value=10); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[1], line 39; 37 sc.pp.log1p(adata); 38 # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); ---> 39 sc.pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer, obsm=obsm); --> 844 X, adata.var[""mean""], adata.var[""std""] = scale(; 845 X,; 846 zero_center=zero_center,; 847 max_value=max_value,; 848 copy=False, # because a copy has already been made, if it were to be made; 849 return_mean_std=True,; 850 ); 851 _set_obs_rep(adata, X, layer=layer, obsm=obsm); 852 if copy:. File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). TypeError: scale() got an unexpected keyword argument 'return_mean_std'; ```. #### Versions.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:3437,wrap,wrapper,3437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['wrap'],['wrapper']
Integrability,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/826:130,rout,routines,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826,1,['rout'],['routines']
Integrability,why umap showing bbknn perfectively integrated but tsne,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370:36,integrat,integrated,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370,1,['integrat'],['integrated']
Integrability,"write anndata failed, pearson_residuals_df header message is too large",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2383:50,message,message,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2383,1,['message'],['message']
Integrability,"x that contains 8 samples to analyze. My goal is to read count matrix as AnnData object. I am transposing the matrix because I wanted the rows to represent cells (5258) and the columns to represent genes (17143). . ```python; # Read the count matrix ; adata = sc.read_text('/file_path').T #transposed the matrix because I wanted the rows to represent cells (5258) and the columns to represent genes (17143). adata #5258 cells, 17143 genes. # extract the sample name from the column names; adata.obs['sample_name'] = adata.obs.index.str.split('_').str[0]; adata.obs['barcode'] = adata.obs.index.str.split('_').str[1]. # create a list of the sample names; sample_list = ['Pb.F1', 'Pb.M1', 'Ctl.M1', 'Pb.F2', 'Ctl.F1', 'Pb.M2', 'Ctl.M2', 'Ctl.F2']. # iterate through the sample names and create a new AnnData object for each sample; for sample in sample_list:; sample_adata = adata[:, adata.obs['sample_name'] == sample]. ```; However when I run the code for ""iterate through the sample names and create a new AnnData object for each sample"" this is where an error message is generated. ; ```pytb; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); Input In [5], in <cell line: 2>(); 1 # iterate through the sample names and create a new AnnData object for each sample; 2 for sample in sample_list:; ----> 3 sample_adata = adata[:, adata.obs['sample_name'] == sample]. File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1113, in AnnData.__getitem__(self, index); 1111 def __getitem__(self, index: Index) -> ""AnnData"":; 1112 """"""Returns a sliced view of the object.""""""; -> 1113 oidx, vidx = self._normalize_indices(index); 1114 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ~/opt/anaconda3/lib/python3.9/site-packages/anndata/_core/anndata.py:1094, in AnnData._normalize_indices(self, index); 1093 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1094 return _nor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2402:1144,message,message,1144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2402,1,['message'],['message']
Integrability,"x=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1762,wrap,wrapper,1762,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['wrap'],['wrapper']
Integrability,"xception:. TypeError Traceback (most recent call last); <ipython-input-17-f0b30fa7797a> in <module>; ----> 1 gex_matrix.write('/Volumes/Bf110/ct5/raw_data/single_cell/external/GSE156793/GSE156793_GEX_ctl210604.raw.h5ad'). ~/anaconda3/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1903 filename = self.filename; 1904 ; -> 1905 _write_h5ad(; 1906 Path(filename),; 1907 self,. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 109 else:; 110 write_attribute(f, ""raw"", adata.raw, dataset_kwargs=dataset_kwargs); --> 111 write_attribute(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs); 112 write_attribute(f, ""var"", adata.var, dataset_kwargs=dataset_kwargs); 113 write_attribute(f, ""obsm"", adata.obsm, dataset_kwargs=dataset_kwargs). ~/anaconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:4952,wrap,wrapper,4952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['wrap'],['wrapper']
Integrability,"y. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```; output_dir_fig = ""chosen/path/to/directory""; sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 103 ax.set_xscale(""log""); 104 show = settings.autoshow if show is None else show; --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save); 106 if show:; 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save); 337 show = settings.autoshow if show is None else show; 338 if save:; --> 339 savefig(writekey, dpi=dpi, ext=ext); 340 if show:; ...; -> 2456 fp = builtins.open(filename, ""w+b""); 2458 try:; 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3276:1408,wrap,wraps,1408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276,1,['wrap'],['wraps']
Integrability,"ynndescent`/ `umap` were forcing a `workqueue` backend which is always available. `pynndescent` 0.5.3 recently came out, and started favoring the `tbb` backend. This is a problem since numba thinks this backend is available on our CI (and on my HPC), but it’s actually not. I also can’t get tbb to even install in a way numba sees in on these systems. If tbb isn’t actually available, but numba detects it we get errors with horrible tracebacks anytime parallelized numba code is used https://github.com/lmcinnes/pynndescent/issues/129. These tracebacks are so horrible they break our CI further https://github.com/pytest-dev/pytest-nunit/issues/47. So what do we do?. ## Possible solutions:. * Pin pynndescent below 0.5.3. This makes pynndescent a required dependency. We’ve previously avoided this since it would change results for people using `umap<0.4` (e.g. anyone with `scvelo` installed) who did not explicitly install pynndescent. However, given the lack of complaints around umap results changing as dependencies have increased, this may not be so bad. It would be great if we could constrain the version without having it be a dependency. This would be similar to what's possible with `pip` and [constraints files](https://pip.pypa.io/en/stable/user_guide/#constraints-files), I don't see how one would be able to specify this for a package. I don't think it's possible, but maybe I'm missing something about the version string syntax. * Make sure the numba threading layer is `“workqueue”` after pynndescent is imported. This is tricky. pynndescent<0.5.3 takes a long time to import, so we don’t want to do this at the top level. So we would need to add a check after everytiem pynndescent could possibly be imported to check that it didn’t set the threading backend to anything else. My understanding of the numba threading system is that once you’ve called for parallel compilation, you’re locked into the threading backend for that session. * Make sure the threading layer is workqueue ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931:1113,depend,dependencies,1113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931,1,['depend'],['dependencies']
Integrability,"ysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2267,depend,dependency,2267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['depend'],['dependency']
Integrability,ython-snappy 0.6.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz-comms 2.3.0; PyWavelets 1.4.1; pyxdg 0.27; PyYAML 6.0; pyzmq 23.2.0; QDarkStyle 3.0.2; qstylizer 0.2.2; QtAwesome 1.2.2; qtconsole 5.4.2; QtPy 2.2.0; queuelib 1.5.0; regex 2022.7.9; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; responses 0.13.3; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rope 1.7.0; Rtree 1.0.1; ruamel.yaml 0.17.21; ruamel-yaml-conda 0.17.21; s3fs 2023.4.0; safetensors 0.3.2; scikit-image 0.20.0; scikit-learn 1.3.0; scikit-learn-intelex 20230426.111612; scipy 1.11.1; Scrapy 2.8.0; seaborn 0.12.2; SecretStorage 3.3.1; Send2Trash 1.8.0; service-identity 18.1.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 5.2.1; sniffio 1.2.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.4; Sphinx 5.0.2; sphinxcontrib-applehelp 1.0.2; sphinxcontrib-devhelp 1.0.2; sphinxcontrib-htmlhelp 2.0.0; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.3; sphinxcontrib-serializinghtml 1.1.5; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 1.4.39; stack-data 0.2.0; statsmodels 0.14.0; sympy 1.11.1; tables 3.8.0; tabulate 0.8.10; TBB 0.2; tblib 1.7.0; TELR 1.0; tenacity 8.2.2; terminado 0.17.1; text-unidecode 1.3; textdistance 4.2.1; texttable 1.7.0; threadpoolctl 2.2.0; three-merge 0.1.1; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.2.0; tokenizers 0.13.2; toml 0.10.2; tomlkit 0.11.1; toolz 0.12.0; tornado 6.3.2; tqdm 4.65.0; traitlets 5.7.1; transformers 4.32.1; Twisted 22.10.0; typing_extensions 4.7.1; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.4.0; Unidecode 1.2.0; urllib3 1.26.16; w3lib 1.21.0; watchdog 2.1.6; wcwidth 0.2.5; webencodings 0.5.1; websocket-client 0.58.0; Werkzeug 2.2.3; whatthepatch 1.0.2; wheel 0.38.4; widgetsnbextension 4.0.5; wrapt 1.14.1; wurlitzer 3.0.2; xarray 2023.6.0; xxhash 2.0.2; xyzservices 2022.9.0; y-py 0.5.9; yapf 0.31.0; yarl 1.8.1; ypy-websocket 0.8.2; zict 2.2.0; zipp 3.11.0; zope.interface 5.4.0; zstandard 0.19.0; ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:7763,wrap,wrapt,7763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,2,"['interface', 'wrap']","['interface', 'wrapt']"
Integrability,"~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\scanpy\plotting\_tools\paga.py:1255, in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1253 print(X.shape); 1254 if as_heatmap:; -> 1255 img = ax.imshow(X, aspect=""auto"", interpolation=""nearest"", cmap=color_map); 1256 if show_yticks:; 1257 ax.set_yticks(range(len(X))). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\__init__.py:1459, in _preprocess_data.<locals>.inner(ax, data, *args, **kwargs); 1456 @functools.wraps(func); 1457 def inner(ax, *args, data=None, **kwargs):; 1458 if data is None:; -> 1459 return func(ax, *map(sanitize_sequence, args), **kwargs); 1461 bound = new_sig.bind(ax, *args, **kwargs); 1462 auto_label = (bound.arguments.get(label_namer); 1463 or bound.kwargs.get(label_namer)). File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\matplotlib\axes\_axes.py:5665, in Axes.imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs); 5657 self.set_aspect(aspect); 5658 im = mimage.AxesImage(self, cmap=cmap, norm=norm,; 5659 interpolation=interpolation, origin=origin,; 5660 extent=extent, filternorm=filternorm,; 5661 filterrad=filterrad, resample=resample,; 5662 interpolation_stage=interpolation_stage,; 5663 **kwargs); -> 5665 im.set_data(X); 5666 im.set_alpha(alpha); 5667 if im.get_clip_path() is None:; 5668 # image does not already have clipping set, clip to axes patch. File ~\AppData\Local\Pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3025:3300,wrap,wraps,3300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3025,1,['wrap'],['wraps']
Modifiability,"	with 9992 stored elements in Compressed Sparse Row format>, index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + where <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'> = sc.metrics; E + and 0.13099293222276967 = <function morans_i at 0x7f354779d9d0>(AnnData object with n_obs × n_vars = 700 × 765\n obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'\n var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'\n uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'\n obsm: 'X_pca', 'X_umap'\n varm: 'PCs'\n layers: 'raw'\n obsp: 'distances', 'connectivities', vals=index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + where <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'> = sc.metrics. scanpy/tests/test_metrics.py:78: AssertionError; ```. ### Versions. <details>. ```; Package Versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2688:2642,layers,layers,2642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2688,1,['layers'],['layers']
Modifiability," 0:; -> 1097 return np.vstack(rptree_leaf_array_parallel(rp_forest)); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:4216,extend,extend,4216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['extend'],['extend']
Modifiability," 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2212,layers,layers,2212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability," 295 X=X,; --> 296 log1p=log1p,; 297 ); 298 var_metrics = describe_var(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 112 if percent_top:; 113 percent_top = sorted(percent_top); --> 114 proportions = top_segment_proportions(X, percent_top); 115 for i, n in enumerate(percent_top):; 116 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in top_segment_proportions(mtx, ns); 377 mtx = csr_matrix(mtx); 378 return top_segment_proportions_sparse_csr(; --> 379 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 380 ); 381 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 806 self._cache_misses[sig] += 1; 807 try:; --> 808 cres = self._compiler.compile(args, return_type); 809 except errors.ForceLiteralArg as e:; 810 def folded(args, kws):. ~\AppData\Local\Continuum\anaconda3\lib\site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:6376,config,config,6376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['config'],['config']
Modifiability," I'm getting weird errors when running scanpy on scvelo objects. I first had this issue when running `sc.pp.pca(adata_panc, n_comps=50)`, but managed to solve it by previously setting `adata_panc.X = np.array(adata_panc.X.todense())`. However, I'm now getting the exact same error when running `sc.pp.neighbors(adata_panc)` and I'm not sure which matrix to test. Any advice would be very much appreciated!. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata_panc = scv.datasets.pancreas(); scv.pp.filter_and_normalize(adata_panc, n_top_genes=3000, min_shared_counts=20); del adata_panc.obsm['X_pca']; del adata_panc.obsm['X_umap']; del adata_panc.obsp['distances']; del adata_panc.obsp['connectivities']; adata_panc.X = np.array(adata_panc.X.todense()); sc.pp.pca(adata_panc, n_comps=50); sc.pp.neighbors(adata_panc); ```. ```pytb; Filtered out 20801 genes that are detected 20 counts (shared).; Normalized count data: X, spliced, unspliced.; Extracted 3000 highly variable genes.; Logarithmized X.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-10-04/ipykernel_2322052/531027197.py in <module>; 7 adata_panc.X = np.array(adata_panc.X.todense()); 8 sc.pp.pca(adata_panc, n_comps=50); ----> 9 sc.pp.neighbors(adata_panc). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:1331,variab,variable,1331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['variab'],['variable']
Modifiability," Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:1487,variab,variables,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['variab'],['variables']
Modifiability," available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; adata_dist = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; uns: 'dist-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1431,plugin,plugins,1431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['plugin'],['plugins']
Modifiability," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). ; This is the error I get:. > Traceback (most recent call last):; > File ""<input>"", line 48, in <module>; > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__; > return self._getitem_view(index); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view; > return AnnData(self, oidx=oidx, vidx=vidx, asview=True); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__; > self._init_as_view(X, oidx, vidx); > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view; > self._raw = adata_ref.raw[oidx]; > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__; > new._varm = self._varm._view(self, vidx); > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884:1537,variab,variables,1537,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884,1,['variab'],['variables']
Modifiability," data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared); 230 paired_distances : distances betweens pairs of elements of X and Y.; 231 """"""; --> 232 X, Y = check_pairwise_arrays(X, Y); 233 ; 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype); 107 if Y is X or Y is None:; 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,; --> 109 estimator=estimator); 110 else:; 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator); 519 ""Reshape your data either using array.reshape(-1, 1) if ""; 520 ""your data has a single feature or array.reshape(1, -1) ""; --> 521 ""if it contains a single sample."".format(array)); 522 ; 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:; array=[0. 0. 1. ... 0. 3. 0.].; Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.; ```. To reproduce:; ```python; import scanpy as sc. adata = sc.datasets.paul15(); sc.pp.neighbors(adata[:, 0]); ```; Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes.; My versions are:; ```python; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/837:3013,flexible,flexible,3013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837,1,['flexible'],['flexible']
Modifiability," emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; sc.tl.cluster.{leiden,louvain}; sc.tl.score.{genes,cell_cycle}; sc.pl.rank_genes.{dotplot,matrixplot,...}; sc.pl.groups.{dot,matrix,violin,...}; sc.pl.embed.{umap,tsne,pca,...}; ```. There are a few issues I can think of. 1. I can imagine some resistance from some developers due to losing a few milliseconds by typing more characters 😄 but if you imagine the long term effects of option 2, I think this might save you some time 😛 . 1. What happens to the fun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:1526,layers,layers,1526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,1,['layers'],['layers']
Modifiability," for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:1385,variab,variables,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['variab'],['variables']
Modifiability," for {key}, ""; 146 f""since a writer for type {type(value)} has not been implemented yet."". NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2111,layers,layers,2111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['layers'],['layers']
Modifiability," import filter_genes_dispersion, filter_genes_cv_deprecated; 8 from ._normalization import normalize_total. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py in; 8 from typing import Union, Optional, Tuple, Collection, Sequence, Iterable; 9; ---> 10 import numba; 11 import numpy as np; 12 import scipy as sp. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/init.py in; 32; 33 # Re-export decorators; ---> 34 from numba.core.decorators import (cfunc, generated_jit, jit, njit, stencil,; 35 jit_module); 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler.py in; 11 from numba.core.environment import lookup_environment; 12; ---> 13 from numba.core.compiler_machinery import PassManager; 14; 15 from numba.core.untyped_passes i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2343,config,config,2343,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,1,['config'],['config']
Modifiability," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3490,variab,variables,3490,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,2,['variab'],['variables']
Modifiability," metric, metric_kwds, verbose); 984 initial_alpha,; 985 negative_sample_rate,; --> 986 verbose=verbose,; 987 ); 988 . /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. /opt/conda/lib/python3.7/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. /opt/conda/lib/python3.7/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 . TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fb3c827f840>)); [2] During: typing of call at /opt/conda/lib/python3.7/site-packages/umap/umap_.py (776). File ""../../../opt/conda/lib/python3.7/site-packages/umap/umap_.py"", line 776:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/dev/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/dev/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please repor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/666:2339,parameteriz,parameterized,2339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666,1,['parameteriz'],['parameterized']
Modifiability," mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 399 e.patch_message(msg); 400 ; --> 401 error_rewrite(e, 'typing'); 402 except errors.UnsupportedError as e:; 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 342 raise e; 343 else:; --> 344 reraise(type(e), e, None); 345 ; 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb); 666 value = tp(); 667 if value.__traceback__ is not tb:; --> 668 raise value.with_traceback(tb); 669 raise value; 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64); * parameterized; In definition 0:; ValueError: Argument types for wrap_index must match; raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72; In definition 1:; ValueError: Argument types for wrap_index must match; raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<intrinsic wrap_index>); [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; start, end = indptr[i], indptr[i + 1]; sums[i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/978:2609,parameteriz,parameterized,2609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978,1,['parameteriz'],['parameterized']
Modifiability," node_size_scale, node_size_power, edge_width_scale, normalize_to_color, title, pos, cmap, frameon, min_edge_width, max_edge_width, export_to_gexf, cax, colorbar, use_raw, cb_kwds, single_component, arrowsize); 612 adata_gene = adata.raw[:, colors]; 613 else:; --> 614 adata_gene = adata[:, colors]; 615 x_color.append(np.mean(adata_gene.X[subset])); 616 colors = x_color. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1307 def __getitem__(self, index):; 1308 """"""Returns a sliced view of the object.""""""; -> 1309 return self._getitem_view(index); 1310 ; 1311 def _getitem_view(self, index):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1311 def _getitem_view(self, index):; 1312 oidx, vidx = self._normalize_indices(index); -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1314 ; 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 723 self._X = None; 724 else:; --> 725 self._init_X_as_view(); 726 ; 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). C:\ProgramData\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 750 shape = (; 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 753 ); 754 if np.isscalar(X):. C:\ProgramData\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 148 return 1; 149 else:; --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/445:2350,layers,layers,2350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/445,1,['layers'],['layers']
Modifiability," object with n_obs × n_vars = 29322 × 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363:1024,layers,layers,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363,1,['layers'],['layers']
Modifiability," of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_cou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2122,layers,layers,2122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability," pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2327,layers,layers,2327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call; https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167; we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:2653,variab,variable,2653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability," reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1385,layers,layers,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability," we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1111,variab,variables,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,1,['variab'],['variables']
Modifiability," | 297|; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:1773,variab,variable,1773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,1,['variab'],['variable']
Modifiability," |; | Updated | 1.59 |; | Speedup | 7.433962264 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100:1679,variab,variable,1679,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100,1,['variab'],['variable']
Modifiability,"""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch"")>>> pbmc = sc.datasets.pbmc3k(). >>> log_anndata = sc.pp.log1p(pbmc, copy=True); >>> pbmc.layers['log_transformed'] = log_anndata.X.copy(); >>> ; >>> # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; >>> # ValueError: cannot specify integer `bins` when input data contains infinity; >>> sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False); .venv/lib/python3.10/site-packages/scipy/sparse/_data.py:133: RuntimeWarning: overflow encountered in expm1; result = op(self._deduped_data()); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""venv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 434, in highly_variable_genes; df = _highly_variable_genes_single_batch(; File "".venv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 215, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File "".venv/lib/python3.10/site-packages/pandas/core/reshape/tile.py"", line 263, in cut; raise ValueError(; ValueError: cannot",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:2364,layers,layers,2364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"# Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1859,layers,layers,1859,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Dear `scanpy` developers, . I was exploring the new features in the latest version of Scanpy, but encountered a prolonged pause when running the `sc.pp.scrublet(adata)`. Initially I thought the problem was due to the large size (~100k cells) of the dataset I was exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:884,variab,variable,884,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipyw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:803,layers,layers,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I'm working with the `sc.get.aggregate` function from the latest release candidate. In certain combinations of `groupby` variables, some valuese get lost. . I wasn't able to make a minimal reproducible example, but I obfuscated the `obs` table of my real data and can share it here: ; https://www.dropbox.com/scl/fi/jsbrb2ulki7mmih2242kc/adata_aggregate_bug.h5ad?rlkey=qczuaf2v5vlwb00zyuxmzjkix&dl=1. ### Minimal code sample. ```python; >>> test_adata = sc.read_h5ad(""adata_aggregate_bug.h5ad""). >>> test_adata.obs[""patient_id""].nunique(); 69. >>> test_adata.obs.isnull().sum(); patient_id 0; timepoint 0; external_batch_id 0; dtype: int64. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 15. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:410,variab,variables,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['variab'],['variables']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. On version 1.10.1 & manuals for v.1.10.x:. [If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""]. If provided, values of adata.uns[""{var}_colors""] will be set.](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pl.embedding.html). ![image](https://github.com/user-attachments/assets/9d6f328b-3afb-41c5-92ca-0a75bec6ce2c). ### Minimal code sample. Here I have an anndata with a categorical obs variable and having {var}_colors in uns; still, mpl.rcParams[""axes.prop_cycle""] is used:. ```python; ## the type of data.obs['study']: category. Name: study, Length: 48256, dtype: category; Categories (2, object): ['NatGenet', 'HongProj']. data.uns['study_colors']; -> array(['#ff7f0e', '#17becf'], dtype=object); sc.pl.embeddings(data, 'anyembedding', 'study'); ->; ```. ### Error output. _No response_. ### Versions. <details>. ```; scanpy==1.10.1 anndata==0.8.0 umap==0.5.5 numpy==1.26.3 scipy==1.11.4 pandas==1.5.3 scikit-learn==1.4.0 statsmodels==0.14.0 igraph==0.10.3 pynndescent==0.5.8; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3193:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3193,2,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.dpt` was successfully done. But when I want to plot the result of dpt,the error comes out. ### Minimal code sample. ```python; sc.tl.dpt(a1,n_branchings=2); sc.pl.dpt_groups_pseudotime(a1); sc.pl.dpt_timeseries(a1); ```. ### Error output. Error in dpt_timeseries:. ```pytb; WARNING: Plotting more than 100 genes might take some while, consider selecting only highly variable genes, for example.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a real number, not 'csr_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); Cell In[85], line 1; ----> 1 sc.pl.dpt_timeseries(a1). File D:\anaconda\Lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File D:\anaconda\Lib\site-packages\scanpy\plotting\_tools\__init__.py:245, in dpt_timeseries(adata, color_map, show, save, as_heatmap, marker); 242 # only if number of genes is not too high; 243 if as_heatmap:; 244 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 245 timeseries_as_heatmap(; 246 adata.X[adata.obs[""dpt_order_indices""].values],; 247 var_names=adata.var_names,; 248 highlights_x=adata.uns[""dpt_changepoints""],; 249 color_map=color_map,; 250 ); 251 else:; 252 # plot time series as gene expression vs time; 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:662,variab,variable,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. **Note**: This bug seems to have been mentioned on 2/13/2022 in this discourse.scverse.org thread: https://discourse.scverse.org/t/error-in-highly-variable-gene-selection/276. However, I can't seem to access scverse.org so I can't see what the cause/resolution was. **Issue**: I am running highly_variable_genes with flavor='seurat_v3'. When I do not include a batch_key, the function runs fine. When I add a batch_key, I get a numerical error. ### Minimal code sample. ```python; sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=1000, batch_key=""Covariate""); ```. ### Error output. ```pytb; ----> 1 sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=1000, batch_key=""Covariate""). File ~/miniconda3/envs/singlecell/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:428, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 422 raise ValueError(; 423 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 424 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 425 ); 427 if flavor == 'seurat_v3':; --> 428 return _highly_variable_genes_seurat_v3(; 429 adata,; 430 layer=layer,; 431 n_top_genes=n_top_genes,; 432 batch_key=batch_key,; 433 check_values=check_values,; 434 span=span,; 435 subset=subset,; 436 inplace=inplace,; 437 ); 439 if batch_key is None:; 440 df = _highly_variable_genes_single_batch(; 441 adata,; 442 layer=layer,; (...); 449 flavor=flavor,; 450 ). File ~/miniconda3/envs/singlecell/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:85, in _highly_variable_genes_seurat_v3(adata, la",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2669:438,variab,variable-gene-selection,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2669,1,['variab'],['variable-gene-selection']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. After upgrading annData to 0.10.4 I tried to read in some Visium data with read_10x_mtx(); The resulting table only had one variable (/gene/column), and had it over and over again. ### Minimal code sample. ```python; S = scanpy.read_10x_mtx(mydata); ```. ### Error output. ```pytb; /home/lhw/pkgs/mambaforge/envs/env2/lib/python3.11/site-packages/anndata/_core/anndata.py:1908: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.; utils.warn_names_duplicates(""var""). In [6]: S.var_names; Out[6]: ; Index(['NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L',; 'NOC2L', 'NOC2L',; ...; 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L', 'NOC2L',; 'NOC2L', 'NOC2L'],; dtype='object', length=18085). To test, I ran in another environment with scanpy 1.9.6 but annData 0.10.3.; Below are the results:. In [4]: S = scanpy.read_10x_mtx(mydata). In [5]: S.var_names; Out[5]: ; Index(['SAMD11', 'NOC2L', 'KLHL17', 'PLEKHN1', 'PERM1', 'HES4', 'ISG15',; 'AGRN', 'RNF223', 'C1orf159',; ...; 'MT-ND2', 'MT-CO2', 'MT-ATP6', 'MT-CO3', 'MT-ND3', 'MT-ND4L', 'MT-ND4',; 'MT-ND5', 'MT-ND6', 'MT-CYB'],; dtype='object', length=18085); ```. ### Versions; Scanpy 1.9.6; annData 0.10.3 works, annData 0.10.4 does not; <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2825:415,variab,variable,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2825,2,"['Variab', 'variab']","['Variable', 'variable']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. During preprocessing of concatenated adata file for scvi-based label transfer, processing fails when applying ""sc.pp.highly_variable_genes"" function with ""ValueError: b'Extrapolation not allowed with blending'"". ### Minimal code sample. ```python; aadata = aadata.concatenate(ref_data_WT). aadata.X; <15445x13343 sparse matrix of type '<class 'numpy.float64'>'; 	with 107849393 stored elements in Compressed Sparse Row format>. # pre-processing:; aadata.layers[""counts""] = aadata.X.copy(); sc.pp.normalize_total(aadata, target_sum=1e4); sc.pp.log1p(aadata); aadata.raw = aadata. sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5; ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); Cell In[37], line 7; 4 sc.pp.log1p(aadata); 5 aadata.raw = aadata; ----> 7 sc.pp.highly_variable_genes(aadata, flavor = 'seurat_v3', n_top_genes=2000,; 8 layer = ""counts"", batch_key=""batch"", subset = True)#, span =0.5. File ~/mambaforge/envs/soupxEnv/lib/python3.10/site-packages/scanpy/preprocessing/_highly_variable_genes.py:441, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 439 sig = signature(_highly_variable_genes_seurat_v3); 440 n_top_genes = cast(int, sig.parameters[""n_top_genes""].default); --> 441 return _highly_variable_genes_seurat_v3(; 442 adata,; 443 layer=layer,; 444 n_top_genes=n_top_genes,; 445 batch_key=batch_key,; 446 check_values=check_values,; 447 span=span,; 448 subset=subset,; 449 inplace=inplace,; 450 ); 452 if batch_key is None:; 453 df = _highly_variable_genes_single_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2853:745,layers,layers,745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2853,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. In scanpy version 1.9.3. ### Minimal code sample. ```python; adata = sc.read_visium(; '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/',; count_file='filtered_feature_bc_matrix.h5',; source_image_path='/data_disk/ST01/User/zhanghh/project/GSE206391/GSM6252954_11-V19T12-012-V4.jpg',; ); ```. ### Error output. ```pytb; /data_disk/ST01/Software/conda_env/zhhenv/lib/python3.9/site-packages/anndata/_core/anndata.py:1832: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.; utils.warn_names_duplicates(""var""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/zhanghaihao/.local/lib/python3.9/site-packages/scanpy/readwrite.py"", line 390, in read_visium; raise OSError(f""Could not find '{f}'""); OSError: Could not find '/data_disk/ST01/User/zhanghh/project/analysis/GSM6252954/outs/spatial/tissue_positions_list.csv'; ```. ### Versions. <details>; <summary>Details</summary>. ```; >>> import scanpy; scanpy.logging.print_versions(); -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dot_parser NA; gmpy2 2.1.2; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; importlib_resources NA; invgauss_ufunc NA; joblib 1.1.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.0; matplotlib 3.7.1; mpl_toolkits NA; mpmath 1.2.1; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.57.0; numpy 1.24.3; nvfuser NA; opt_einsum v3.3.0; packaging 23.0; pandas 2.0.1; pkg_resources NA; pydot 1.4.2; pyparsing 3.0.9; pytz 2022.7; scipy 1.10.1; session_info 1.0.0; setuptools 66.0.0; six 1.16.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2565:739,Variab,Variable,739,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2565,1,['Variab'],['Variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Installation using pip & installation from github repository. ### Minimal code sample. ```python; pip install scanpy; ```. ### Error output. ```pytb; Installing collected packages: tbb, distlib, asciitree, stdlib-list, setuptools-scm, pbr, numcodecs, nodeenv, natsort, igraph, identify, filelock, fasteners, docutils, cfgv, array-api-compat, accessible-pygments, zarr, virtualenv, sphinx, session-info, pytest-nunit, pytest-mock, profimp, mdit-py-plugins, leidenalg, sphinxext-opengraph, sphinx-design, sphinx-copybutton, sphinx-autodoc-typehints, scanpydoc, pynndescent, pydata-sphinx-theme, pre-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryorn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:738,plugin,plugins,738,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['plugin'],['plugins']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydev",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:926,variab,variable,926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Function Rank_genes_groups() does not return a figure -> returns None type; Cannot get Figure via plt.gcf(), plt.gca(). Potential Fix:; https://github.com/scverse/scanpy/blob/main/src/scanpy/plotting/_tools/__init__.py; Line 485: cann be extended to the following as in other functions below:; ```; savefig_or_show(f""{key}_"", show=show, save=save); show = settings.autoshow if show is None else show; if show:; return None; return ax; ```. ### Minimal code sample. ```python; fig = sc.pl.rank_genes_groups(adata, show=False); type(fig); #NoneType; plt.gca() -> empty axes; plt.gcf() -> empty figure; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.8; scanpy 1.10.2; -----; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205:527,extend,extended,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205,1,['extend'],['extended']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. HVG can produce more than the number of genes asked for as highly variable. This occurs on these two datasets:. ```; wget https://datasets.cellxgene.cziscience.com/e00ab1f4-28cd-497d-b889-94d45840f423.h5ad; ```. ### Minimal code sample. ```python; import scanpy as sc. adata1 = sc.read('e00ab1f4-28cd-497d-b889-94d45840f423.h5ad'). sc.pp.normalize_total(adata1, target_sum=1e4). sc.pp.log1p(adata1). n_top_gene = 10000; sc.pp.highly_variable_genes(adata1, n_top_genes = n_top_gene). hvg_system1 = set(adata1.var[adata1.var['highly_variable']].index); assert len(hvg_system1) == n_top_gene, f""found {len(hvg_system1)} instead of {n_top_gene}"". ```. ### Error output. ```pytb; AssertionError Traceback (most recent call last); Cell In[12], line 1; ----> 1 assert len(hvg_system1) == n_top_gene, f""found {len(hvg_system1)} instead of {n_top_gene}"". AssertionError: found 13355 instead of 10000; ```. ### Versions. <details>. ```; import scanpy; scanpy.logging.print_versions(); -----; anndata 0.10.8; scanpy 1.10.0rc2.dev85+gb918a23e; -----; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157:355,variab,variable,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime?. ### Minimal code sample. ```python; import scanpy as sc; # Core scverse libraries; import anndata as ad. # Data retrieval; import pooch; # Core scverse libraries; import anndata as ad. # Data retrieval; import pooch; EXAMPLE_DATA = pooch.create(; path=pooch.os_cache(""scverse_tutorials""),; base_url=""doi:10.6084/m9.figshare.22716739.v1/"",; ); EXAMPLE_DATA.load_registry_from_doi(). samples = {; ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",; ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",; }; adatas = {}. for sample_id, filename in samples.items():; path = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""); adata.obs_names_make_unique(); print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse; adata.var[""mt""] = adata.var_names.str.startswith(""MT-""); # ribosomal genes; adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")); # hemoglobin genes; adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True; ). sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter=0.4,; multi_panel=True,; ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=3). sc.pp.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:507,extend,extended,507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['extend'],['extended']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I have always had a question: do I need to scale my adata before running sc.tl.score_genes?. ### Minimal code sample. ```python; sc.tl.score_genes_cell_cycle(adata_hvg, layers='scaled', s_genes=s_genes, g2m_genes=g2m_genes); ```. ### Error output. _No response_. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3080:458,layers,layers,458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3080,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Leiden and Louvain clustering params are not saved to matching `key_added` key in `uns` dictionary but are ovewritten to hardcoded key instead. One use case is that a user may want to run Leiden/Louvain clustering multiple times with different resolutions / parameters. One may specify different keys to store results under. However, if you do so, the metadata for the parameterization of the clustering algorithms are overwritten because the lines below do not respect the user provided `key_added` parameter. I think the desired behavior is to store data under `adata.uns[key_added][""params""]`. I think I've found the pertinent lines below. Happy to submit a PR if maintainers agree :D.; - https://github.com/scverse/scanpy/blob/91ea0fbb03392795d1506d297d4b4847c646db04/scanpy/tools/_leiden.py#L206; - https://github.com/scverse/scanpy/blob/91ea0fbb03392795d1506d297d4b4847c646db04/scanpy/tools/_louvain.py#L259. ### Minimal code sample. ```python; sc.tl.leiden(adata, resolution=0.8, key_added=""leiden_0.8""); assert ""leiden_0.8"" not in adata.uns; params = adata.uns[""leiden""] . sc.tl.leiden(adata, resolution=1.2, key_added=""leiden_1.2""); assert ""leiden_1.2"" not in adata.uns; overwritten_params = adata.uns[""leiden""] ; assert params == overwritten_params # should fail; ```. ### Error output. _No response_. ### Versions. <details>; Confirmed that params are overwritten in source in main branch. (see permalinks); </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2887:658,parameteriz,parameterization,658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2887,1,['parameteriz'],['parameterization']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. cc: @Intron7 . The array types returned for the various aggregations in `sc.get.aggregate` are different (see example). This can lead to somewhat confusing behavior downstream, especially while we are using the sparse matrix classes. I would suggest we default to a dense result and consider adding an argument `array_type` that determines the type of the arrays added to `layers`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k_processed().raw.to_adata(). aggregated = sc.get.aggregate(adata, ""louvain"", [""sum"", ""count_nonzero""]); type(aggregated.layers[""sum""]); # numpy.ndarray. type(aggregated.layers[""count_nonzero""]); # scipy.sparse._csr.csr_matrix; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.10.0.dev315+gf6d5ac94; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.1; dateutil 2.8.2; decorator 5.1.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.11.3; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.0; parso 0.8.3; pexpect 4.9.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pygments 2.17.2; pyparsing 3.1.1; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl 3.2.0; tlz 0.12.1; toolz 0.12.1; traitlets 5.14.1; wcwidth 0.2.13; yaml 6.0.1; zarr 2.16.1; zipp NA; -----; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2892:662,layers,layers,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2892,3,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi! I am not sure if this is a bug... ; Every time I rescale the `pbmc3k_processed` matrix using it as input in the scanpy `sc.pp.scale` function, I get a very slightly different matrix in output, enough to generate a different UMAP with each run. But if I rewrite it using numpy in a simple function called `my_scale_function` it outputs the exact same matrix as the input, generating the same UMAP down the line... Could someone explain to me what is happening?; (Note: The matrix is not sparse). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; def my_scale_function(X, clip=False):; mean, var = mean_var(X, axis=0); X -= mean; std = np.sqrt(var); std[std == 0] = 1; X /= std; if clip:; X = np.clip(X, -10, 10); return np.matrix(X). ### Scanpy scale vs my_scale_function; mtx = adata.X; from scipy.sparse import issparse; print(""mtx is parse="" + str(issparse(np.matrix(mtx))) + ""\n""); print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(mtx); print((mtx == mtx_rescaled).all()); print(""Rescaled with scanpy:""); mtx_rescaled = sc.pp.scale(mtx, zero_center=True, max_value=None, copy=True); print(str((np.matrix(mtx) == mtx_rescaled).all()) + ""\n""); print(""\nOriginal matrix:""); print(mtx); print(""\nMatrix rescaled with scanpy:""); print(mtx_rescaled); ```. ### Error ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629:548,rewrite,rewrite,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629,1,['rewrite'],['rewrite']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Running the highly_variable_genes function produced an error. ### Minimal code sample. ```python; sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Cell In[101], line 2; 1 # Identify highly-variable genes and plot; ----> 2 sc.pp.highly_variable_genes(test_adata, min_mean=0.0125, max_mean=3, min_disp=0.25). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:440, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 428 return _highly_variable_genes_seurat_v3(; 429 adata,; 430 layer=layer,; (...); 436 inplace=inplace,; 437 ); 439 if batch_key is None:; --> 440 df = _highly_variable_genes_single_batch(; 441 adata,; 442 layer=layer,; 443 min_disp=min_disp,; 444 max_disp=max_disp,; 445 min_mean=min_mean,; 446 max_mean=max_mean,; 447 n_top_genes=n_top_genes,; 448 n_bins=n_bins,; 449 flavor=flavor,; 450 ); 451 else:; 452 sanitize_anndata(adata). File /opt/conda/envs/cell2loc_env/lib/python3.11/site-packages/scanpy/preprocessing/_highly_variable_genes.py:223, in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 219 # retrieve those genes that have nan std, these are the ones where; 220 # only a single gene fell in the bin and implicitly set them to have; 221 # a normalized disperion of 1; 222 one_gene_per_bin = disp_std_bin.isnull(); --> 223 gen_indices = np.where(one_gene_per_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2547:669,variab,variable,669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2547,1,['variab'],['variable']
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The test suite keeps failing with a segfault on the `python=3.9` build. I haven't been able to reproduce locally. Interestingly, I haven't seen it error when I rerun the check. It looks like this always happens during the call to `nn_approx`. ### Minimal code sample. ```python; NA; ```. ### Error output. ```pytb; platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/hostedtoolcache/Python/3.9.18/x64/bin/python; cachedir: .pytest_cache; rootdir: /home/vsts/work/1/s; configfile: pyproject.toml; testpaths: scanpy; plugins: nunit-1.0.6, mock-3.12.0; [1mcollecting ... [0mcollected 1474 items. scanpy/_utils/compute/is_constant.py::scanpy._utils.compute.is_constant.is_constant [32mPASSED[0m[32m [ 0%][0m; scanpy/datasets/_ebi_expression_atlas.py::scanpy.datasets._ebi_expression_atlas.ebi_expression_atlas [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pl.py::scanpy.external.pl.phate [33mSKIPPED[0m (needs modul...)[32m [ 0%][0m; scanpy/external/pp/_bbknn.py::scanpy.external.pp._bbknn.bbknn [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_harmony_integrate.py::scanpy.external.pp._harmony_integrate.harmony_integrate [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_hashsolo.py::scanpy.external.pp._hashsolo.hashsolo [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_magic.py::scanpy.external.pp._magic.magic [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_scanorama_integrate.py::scanpy.external.pp._scanorama_integrate.scanorama_integrate Fatal Python error: Illegal instruction. Thread 0x00007f00347c4640 (most recent call first):; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/threading.py"", line 316 in wait; File ""/opt/hostedtoolcache/Python/3.9.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:775,config,configfile,775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,2,"['config', 'plugin']","['configfile', 'plugins']"
Modifiability,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When doing HVG selection with batch_key = ""something"" and subset = True, I noticed unexpected genes to be selected in the subset anndata. Upon further investigation, it seems that somehow the inplace subsetting goes wrong. (Though I checked the source code and could not find any issue that may explain it there.). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; np.random.seed(0). # Get AnnData; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X.copy().tocsr(); adata.obs[""Age""] = np.random.randint(0, 6, (2700,)); adata.obs[""Age""] = adata.obs[""Age""].astype('category'). # Filter genes, preprocess; sc.pp.filter_genes(adata, min_counts = 10); sc.pp.normalize_total(adata); sc.pp.log1p(adata). # Subset = False; ad_nosub = adata.copy(); sc.pp.highly_variable_genes(ad_nosub, n_top_genes = 1000, batch_key = ""Age"", subset = False). # Subset = False, manual subset afterwards; ad_nosub_subbed = ad_nosub.copy(); ad_nosub_subbed._inplace_subset_var(ad_nosub_subbed.var[""highly_variable""].to_numpy()). # Subset = True; ad_sub = adata.copy(); sc.pp.highly_variable_genes(ad_sub, n_top_genes = 1000, batch_key = ""Age"", subset = True); ```. ### Error output. ```pytb; >>> # As expected; >>> print(np.sum(ad_nosub.var[""highly_variable""])); 1000; >>> ; >>> # As expected; >>> print(np.sum(ad_nosub_subbed.var[""highly_variable""])); 1000; >>> ; >>> # Not as expected; >>> print(np.sum(ad_sub.var[""highly_variable""])); 101; ```. ### Versions. <details>. ``` bash; → conda list | grep scanpy; scanpy 1.10.1 pyhd8ed1ab_0 conda-forge. → conda list | grep anndata; anndata 0.10.7 pyhd8ed1ab_0 conda-forge; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3027:751,layers,layers,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3027,1,['layers'],['layers']
Modifiability,"### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py; import numpy as np; import pandas as pd; import anndata as ad. # Create a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:643,variab,variable,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. ### Issue Description; In examining the `MatrixPlot` class within the provided code, it appears that the documentation for some parameters could be enhanced for clarity and completeness. This improvement is crucial for users to understand how to effectively utilize the class and its functionalities. ### Specific Areas for Improvement; While the overall structure of the documentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illust",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:310,enhance,enhanced,310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhanced']
Modifiability,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:751,Enhance,Enhanced,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,1,['Enhance'],['Enhanced']
Modifiability,"### What kind of feature would you like to request?. New plotting function: A kind of plot you would like to seein `sc.pl`?. ### Please describe your wishes. Hello Scanpy,; Thank you for developing this amazing package.; I have 2 categorical variables in `adata.obs`; ```python; adata_sub2.obs['patient']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 C106_N; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 C106_N; ... ; TTTGCGCAGACACGAC-1-SMC25T SMC25T; TTTGCGCCATGGAATA-1-SMC25T SMC25T; TTTGCGCCATGTTCCC-1-SMC25T SMC25T; TTTGGTTGTAGGGTAC-1-SMC25T SMC25T; TTTGTCAAGAGGGATA-1-SMC25T SMC25T; Name: patient, Length: 44245, dtype: category; Categories (39, object): ['C106_N', 'C126_N', 'C130_N', 'C133_N', ..., 'C138_T', 'C168_T', 'SMC17T', 'SMC20T']; ```; ```python; adata_sub2.obs['type']; AAACCTGAGGTTACCT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTCAGAAGC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGGTGCTCTTC-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCCCATTAT-1-C106_N_1_1_0_c1_v2 Normal; AAACCTGTCGGAGCAA-1-C106_N_1_1_0_c1_v2 Normal; ... ; TTTGCGCAGACACGAC-1-SMC25T NMCAD; TTTGCGCCATGGAATA-1-SMC25T NMCAD; TTTGCGCCATGTTCCC-1-SMC25T NMCAD; TTTGGTTGTAGGGTAC-1-SMC25T NMCAD; TTTGTCAAGAGGGATA-1-SMC25T NMCAD; Name: type, Length: 44245, dtype: category; Categories (3, object): ['Normal', 'NMCAD', 'MCAD']; ```. I know Scanpy has `sc.pl.matrixplot()`, and we can make matrix plot for gene set score, like; ```python; sc.pl.matrixplot(adata_sub2, var_names=['Normal_signature'], groupby='patient', use_raw=True, dendrogram=False, cmap='inferno', standard_scale='var', swap_axes=True, save='27.pdf'); ```; <img width=""1052"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/75048821/ccde5e13-ae61-4a04-b098-e693e9543103"">. Could you please let me know whether Scanpy can make a matrixplot-like plot showing the 'type' grouped by 'patient'? like:; ```python; sc.pl.matrixplot(adata_sub2, var_na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2797:242,variab,variables,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2797,1,['variab'],['variables']
Modifiability,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. There are two levels of shared parameters that people could benefit from:. 1. Multiple Notebooks that work with the same data and could benefit from shared configuration, e.g. labels and color maps defined for a certain axis/annotation; 2. Plotting using the same labels, color map or so. This could be achieved using object oriented plotting (todo issue number). Having shared configuration files could be achieved either by direct support in the plotting functions (`x='cell type'`) or by adding a convenience function that loads a config object.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:250,config,configuration,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,3,['config'],"['config', 'configuration']"
Modifiability,"'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('su",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1619,layers,layers,1619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622:1344,adapt,adapted,1344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622,1,['adapt'],['adapted']
Modifiability,"); 1098 else:; 1099 return np.array([[-1]]). ~/.local/lib/python3.8/site-packages/pynndescent/rp_trees.py in rptree_leaf_array_parallel(rp_forest); 1087 ; 1088 def rptree_leaf_array_parallel(rp_forest):; -> 1089 result = joblib.Parallel(n_jobs=-1, require=""sharedmem"")(; 1090 joblib.delayed(get_leaves_from_tree)(rp_tree) for rp_tree in rp_forest; 1091 ). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable); 1054 ; 1055 with self._backend.retrieval_context():; -> 1056 self.retrieve(); 1057 # Make sure that we get a last message telling us we are done; 1058 elapsed_time = time.time() - self._start_time. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/parallel.py in retrieve(self); 933 try:; 934 if getattr(self._backend, 'supports_timeout', False):; --> 935 self._output.extend(job.get(timeout=self.timeout)); 936 else:; 937 self._output.extend(job.get()). /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in get(self, timeout); 769 return self._value; 770 else:; --> 771 raise self._value; 772 ; 773 def _set(self, i, obj):. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/multiprocessing/pool.py in worker(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception); 123 job, i, func, args, kwds = task; 124 try:; --> 125 result = (True, func(*args, **kwds)); 126 except Exception as e:; 127 if wrap_exception and func is not _helper_reraises_exception:. /omics/groups/OE0540/internal/B260/users/olga/.conda/envs/10x_analysis/lib/python3.8/site-packages/joblib/_parallel_backends.py in __call__(self, *args, **kwargs); 593 def __call__(self, *args, **kwargs):; 594 try:; --> 595 return self.func(*args, **kwargs); 596 except KeyboardInterrupt as e:; 597 # We capture the KeyboardInterrupt and reraise it as. /omics/groups/OE0540/internal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472:4283,extend,extend,4283,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472,1,['extend'],['extend']
Modifiability,"); File ""/home/vsts/work/1/s/scanpy/tools/_draw_graph.py"", line 181, in draw_graph; logg.info(; File ""/home/vsts/work/1/s/scanpy/logging.py"", line 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:2871,config,config,2871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[37], line 1; ----> 1 sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts', color='reference', show=True). File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:166, in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in adata.var.keys() or y in adata.obs.index); 170 and (color is None or color in adata.var.keys() or color in adata.obs.index); 171 ):; 172 adata_T = adata.T. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:521, in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:1630,layers,layers,1630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['layers'],['layers']
Modifiability,"**The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**; ```; sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean; ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**; Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905:350,variab,variable,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905,1,['variab'],['variable']
Modifiability,"+unknown anndata==0.6.9 numpy==1.14.5 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; ... storing 'blobs' as categorical. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-4cd21e9edf25> in <module>(); 3 adata = sc.datasets.blobs(); 4 sc.tl.pca(adata); ----> 5 sc.pl.pca(adata, components=['1,2', '2,3']). ~/software/scanpy/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. ~/software/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). ~/software/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 291 if components is None: components = '1,2' if '2d' in projection else '1,2,3'; 292 if isinstance(components, str): components = components.split(','); --> 293 components = np.array(components).astype(int) - 1; 294 keys = ['grey'] if color is None else [color] if isinstance(color, str) else color; 295 if title is not None and isinstance(title, str):. ValueError: invalid literal for int() with base 10: '1,2'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/254:1292,layers,layers,1292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/254,2,['layers'],['layers']
Modifiability,", :]; zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1); ```; Here is the error:; ```pytb; ValueError Traceback (most recent call last); <ipython-input-170-37cd37b7326e> in <module>; 16 zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; 17 zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2231,layers,layers,2231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['layers'],['layers']
Modifiability,", alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 160 raise ValueError(""Either provide a `basis` or `x` and `y`.""); 161 if (; 162 (x in adata.obs.keys() or x in var_index); 163 and (y in adata.obs.keys() or y in var_index); 164 and (color is None or color in adata.obs.keys() or color in var_index); 165 ):; --> 166 return _scatter_obs(**args); 167 if (; 168 (x in adata.var.keys() or x in adata.obs.index); 169 and (y in adata.var.keys() or y in adata.obs.index); 170 and (color is None or color in adata.var.keys() or color in adata.obs.index); 171 ):; 172 adata_T = adata.T. File /project/hipaa_ycheng11lab/atlas/CAMR2024/py311env/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:521, in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, marker, title, show, save, ax); 517 legend = axs[ikey].legend(; 518 frameon=False, loc=legend_loc, fontsize=legend_fontsize; 519 ); 520 if legend is not None:; --> 521 for handle in legend.legendHandles:; 522 handle.set_sizes([300.0]); 524 # draw a frame around the scatter. AttributeError: 'Legend' object has no attribute 'legendHandles'; ```. ### Versions. <details>. ```; -----; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; cffi 1.16.0; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0.post0; defusedxml 0.7.1; dill 0.3.8; h5py 3.11.0; joblib 1.4.2; kiwisolver 1.4.5; legacy_api_wrap NA; llvmlite 0.42.0; matplotlib 3.9.0; mpl_toolkits NA; natsort 8.4.0; numba 0.59.1; numexpr 2.10.0; numpy 1.26.4; packaging 24.0; pandas 2.2.2; psutil 5.9.8; pyparsing 3.1.2; pytz 2024.1; scipy 1.13.1; session_info 1.0.0; six 1.16.0; sklearn 1.5.0; threadpoolctl 3.5.0; torch 2.3.1+cu121; torchgen NA; tqdm 4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102:2549,layers,layers,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102,1,['layers'],['layers']
Modifiability,", engine, **kwds); 1617 self.options[""has_index_names""] = kwds[""has_index_names""]; 1619 self.handles: IOHandles | None = None; -> 1620 self._engine = self._make_engine(f, self.engine). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1880, in TextFileReader._make_engine(self, f, engine); 1878 if ""b"" not in mode:; 1879 mode += ""b""; -> 1880 self.handles = get_handle(; 1881 f,; 1882 mode,; 1883 encoding=self.options.get(""encoding"", None),; 1884 compression=self.options.get(""compression"", None),; 1885 memory_map=self.options.get(""memory_map"", False),; 1886 is_text=is_text,; 1887 errors=self.options.get(""encoding_errors"", ""strict""),; 1888 storage_options=self.options.get(""storage_options"", None),; 1889 ); 1890 assert self.handles is not None; 1891 f = self.handles.handle. File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); 761 if compression == ""gzip"":; 762 if isinstance(handle, str):; 763 # error: Incompatible types in assignment (expression has type; 764 # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> 765 handle = gzip.GzipFile( # type: ignore[assignment]; 766 filename=handle,; 767 mode=ioargs.mode,; 768 **compression_args,; 769 ); 770 else:; 771 handle = gzip.GzipFile(; 772 # No overload variant of ""GzipFile"" matches argument types; 773 # ""Union[str, BaseBuffer]"", ""str"", ""Dict[str, Any]""; (...); 776 **compression_args,; 777 ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); 190 mode += 'b'; 191 if fileobj is None:; --> 192 fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); 193 if filename is None:; 194 filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. ### Versions. <details>. ```; '1.10.2'; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:24317,variab,variable,24317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['variab'],['variable']
Modifiability,", min_genes=200) #get rid of cells with fewer than 200 genes; sc.pp.filter_genes(adata, min_cells=3) #get rid of genes that are found in fewer than 3 cells; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); upper_lim = np.quantile(adata.obs.n_genes_by_counts.values, .98); lower_lim = np.quantile(adata.obs.n_genes_by_counts.values, .02); adata = adata[(adata.obs.n_genes_by_counts < upper_lim) & (adata.obs.n_genes_by_counts > lower_lim)]; adata = adata[adata.obs.pct_counts_mt < 25]; sc.pp.normalize_total(adata, target_sum=1e4) #normalize every cell to 10,000 UMI; sc.pp.log1p(adata) #change to log counts; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20); sc.tl.umap(adata); return adata. adata = sc.read_csv(""./myfile.csv"", first_column_names=True); adata = pp(adata); ```. My computer is Mac book Intel i5. Thanks!; #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; OpenSSL 22.0.0; PIL 9.2.0; PyObjCTools NA; absl NA; appnope 0.1.2; astunparse 1.6.3; attr 21.4.0; backcall 0.2.0; bcrypt 3.2.0; beta_ufunc NA; binom_ufunc NA; boto3 1.24.28; botocore 1.27.28; bottleneck 1.3.5; brotli NA; certifi 2022.09.24; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; chex 0.1.5; cloudpickle 2.0.0; colorama 0.4.5; contextlib2 NA; cryptography 37.0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:1736,variab,variable,1736,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['variab'],['variable']
Modifiability,", percent_top, layer, use_raw, inplace, parallel); 281 percent_top=percent_top,; 282 inplace=inplace,; --> 283 X=X,; 284 ); 285 var_metrics = describe_var(. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel); 107 if percent_top:; 108 percent_top = sorted(percent_top); --> 109 proportions = top_segment_proportions(X, percent_top); 110 for i, n in enumerate(percent_top):; 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in top_segment_proportions(mtx, ns); 364 mtx = csr_matrix(mtx); 365 return top_segment_proportions_sparse_csr(; --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_for_args(self, *args, **kws); 418 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 419 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 420 raise e; 421 ; 422 def inspect_llvm(self, signature=None):. ~\anaconda3\lib\site-packages\numba\dispatcher.py in _compile_for_args(self, *args, **kws); 351 argtypes.append(self.typeof_pyval(a)); 352 try:; --> 353 return self.compile(tuple(argtypes)); 354 except errors.ForceLiteralArg as e:; 355 # Received request for compiler re-entry with the list of arguments. ~\anaconda3\lib\site-packages\numba\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~\anaconda3\lib\site-packages\numba\dispatcher.py in compile(self, sig); 766 self._cache_misses[sig] += 1; 767 try:; --> 768 cres = self._compiler.compile(args, return_type); 769 except errors.ForceLiteralArg as e:; 770 def folded(args, kws):. ~\anaconda3\lib\site-packages\numba\dispatcher.py in compile(self, args, return_type); 75 ; 76 def compile(self, args, return_type):; --->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:6502,config,config,6502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,",; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2328,Layers,Layers,2328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,"['Layers', 'layers']","['Layers', 'layers']"
Modifiability,"- I have checked that this issue has not already been reported.; - I have confirmed this bug exists on the latest version of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy.api as sc; ```. ```pytb; File ""./scanpy_normalization.py"", line 4, in <module>; import scanpy.api as sc; File ""/usr/local/lib/python3.8/site-packages/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/usr/local/lib/python3.8/site-packages/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/usr/local/lib/python3.8/site-packages/scanpy/plotting/_anndata.py); ```. This is with the latest version of scanpy. I looked at the code and scanpy/apt/pl.py still has **from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot**, even as the plotting library has been refactored and the dotplot, matrixplot and stacked_violin are now in separate files. I tested this a few days ago and it was working fine then, the update to anndata probably happened in the last couple of days",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1397:1086,refactor,refactored,1086,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397,1,['refactor'],['refactored']
Modifiability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000); print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-46-616fc10e63ff> in <module>; ----> 1 sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000); 2 print('\n','Number of highly variable genes: {:d}'.format(np.sum(adata.var['highly_variable']))). ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 424 ; 425 if batch_key is None:; --> 426 df = _highly_variable_genes_single_batch(; 427 adata,; 428 layer=layer,. ~\anaconda3\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, layer, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor); 242 from statsmodels import robust; 243 ; --> 244 df['mean_bin'] = pd.cut(; 245 df['means'],; 246 np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered); 273 raise ValueError(""bins must increase monotonically.""); 274 ; --> 275 fac, bins = _bins_to_cuts(; 276 x,; 277 bins,. ~\anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560:604,variab,variable,604,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560,2,['variab'],['variable']
Modifiability,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); ```; the simple simulation dataset(filter_gene_dis.h5,which is the adata in the simulation code) is; AnnData object with n_obs × n_vars = 3 × 50; [[ 8. 10. 7. 11. 14. 12. 11. 9. 14. 11. 8. 2. 8. 10. 7. 12. 11. 12.; 12. 10. 3. 11. 13. 7. 8. 11. 14. 9. 11. 9. 5. 13. 8. 13. 9. 15.; 11. 8. 7. 7. 5. 12. 9. 12. 11. 8. 11. 6. 10. 11.]; [19. 22. 19. 23. 20. 16. 13. 26. 20. 29. 22. 16. 19. 22. 24. 20. 19. 15.; 17. 25. 23. 19. 18. 18. 24. 18. 25. 22. 25. 16. 25. 23. 27. 22. 14. 21.; 24. 23. 16. 15. 14. 27. 23. 24. 21. 27. 17. 20. 20. 12.]; [27. 31. 32. 30. 29. 31. 24. 29. 29. 33. 29. 29. 26. 38. 27. 32. 21. 24.; 28. 27. 25. 19. 28. 24. 23. 23. 30. 39. 29. 42. 34. 28. 25. 26. 27. 32.; 28. 35. 34. 26. 27. 22. 24. 42. 30. 32. 29. 28. 29. 34.]]```; ```. ```python; import scanpy as sc; import logging; sc.settings.verbosity = 3 ; adata=sc.read(""filter_gene_dis.h5"")#a simulation dataset; #when n_top_genes=10,it returns 3*10; #when n_top_genes=20,it returns 3*19; sc.pp.filter_genes_dispersion(adata,n_top_genes=20); print(adata); ```. ```pytb; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; finished (0:00:00); AnnData object with n_obs × n_vars = 3 × 19; var: 'means', 'dispersions', 'dispersions_norm'; ```. #### Versions. <details>; scanpy==1.6.0 anndata==0.7.4 umap==0.4.6 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.1; </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1457:1600,variab,variable,1600,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1457,1,['variab'],['variable']
Modifiability,"- [ x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; I have tried to plot `umap` `embedding` with the `groups` parameters set to 4 out of ~200 groups (since palettes with ~200 colors cant be distinguished and scanpy anyways assigns gray to everything). However, even when we have only 5 elements in the legend (NA and 4 groups) they still remain all gray. I thus tried to specify the `palette` as dictionary of group_names:colors (for the four group), but then I get the error that groups that are in fact not gonna be plotted are missing in the palette dict input. . I think it would be nice that when using `groups` the `palette` argument would be flexible enough to take only the colors for these groups.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2380:827,flexible,flexible,827,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2380,1,['flexible'],['flexible']
Modifiability,"- [ x] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:868,variab,variables,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,2,['variab'],['variables']
Modifiability,"- [ ✔] I have checked that this issue has not already been reported.; - [✔ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python 3.8; # Your code here; ```sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_pcs=50, knn=True); sc.tl.leiden(adata, resolution=10); sc.tl.umap(adata); sc.pl.umap(adata, color=['leiden'], legend_loc='on data', frameon=False, title='', use_raw=False). ```pytb; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:07); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:10); running Leiden clustering; finished: found 137 clusters and added; 'leiden', the cluster labels (adata.obs, categorical) (0:00:02); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:07); the obs value 'leiden' has more than 103 categories. Uniform 'grey' color will be used for all categories.; ```. #### Versions. <details>. scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.3 scipy==1.7.2 pandas==1.3.4 scikit-learn==1.0.1 statsmodels==0.13.1 python-igraph==0.9.8 pynndescent==0.5.5. </details>. Hello Scanpy,; When I make >100 clusters, the 'leiden' becomes gray and cannot be changed back in the same notebook.; Could you please help me to solve this issue?; Thanks!; Best; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2058:797,variab,variable,797,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2058,1,['variab'],['variable']
Modifiability,"- [X ] I have checked that this issue has not already been reported.; - [X ] I have confirmed this bug exists on the latest version of scanpy.; - [X ] (optional) I have confirmed this bug exists on the master branch of scanpy. I'm working on comit `63b42e4b` (latest master).; I'm not sure if intended or not but it seems like it would be usefull if one were able to ingest data that don't share 100% of all features. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.datasets.paul15(); sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref); sc.tl.leiden(adata_ref); adata = adata_ref[:, :1000].copy() # assume adata_ref has more than 1000 genes.; sc.tl.ingest(adata, adata_ref, obs='leiden'); ```. Error message; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-37-b3cd11e67810> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='leiden'). ~/projects/scanpy/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 125 ; 126 ing = Ingest(adata_ref, neighbors_key); --> 127 ing.fit(adata); 128 ; 129 for method in embedding_method:. ~/projects/scanpy/scanpy/tools/_ingest.py in fit(self, adata_new); 437 ; 438 if not ref_var_names.equals(new_var_names):; --> 439 raise ValueError(; 440 'Variables in the new adata are different '; 441 'from variables in the reference adata'. ValueError: Variables in the new adata are different from variables in the reference adata; ```. --- . #### Versions. <details>. sc.logging.print_header(); scanpy==1.8.0.dev78+gc488909a anndata==0.7.6 umap==0.5.0 numpy==1.19.4 scipy==1.5.4 pandas==1.1.4 scikit-learn==0.23.2 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.3 pynndescent==0.5.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001:1402,Variab,Variables,1402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001,4,"['Variab', 'variab']","['Variables', 'variables']"
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy. ---. Hi, . Trying to run `scVI` to analyse my data using the latest `scanpy+scvi-tools` workflow, as described [here](https://www.scvi-tools.org/en/stable/user_guide/notebooks/harmonization.html). However, I'm running into a weird issue with the new `seurat_v3` flavour to call HVGs. When I run this:. ```python; sc.pp.highly_variable_genes(; adata,; flavor = ""seurat_v3"",; n_top_genes = 7000,; layer = ""counts"",; batch_key = ""combined"",; subset = True; ); ```. I get the following error:. ```pytb; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-19-3748de5bacdc> in <module>; 5 layer = ""counts"",; 6 batch_key = ""combined"",; ----> 7 subset = True; 8 ); 9 adata. /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 420 span=span,; 421 subset=subset,; --> 422 inplace=inplace,; 423 ); 424 . /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'There are other near singularities as well. 0.090619\n'; ```. While looking for a solution, I came across [this](https://github.com/YosefLab/scvi-tools/issues/727#issuecomment-718717033) issue that reports a similar problem. Any ideas of what this may be? . Thanks . #### Versions. <deta",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504:711,variab,variable,711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504,1,['variab'],['variable']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hello, according to the documentation, the simulation of doublets with scrublet works best with un-normalized counts. The current wrapper implementation however uses a layer to hold the raw data, which then gets normalized anyway. . Please have in mind, that this is the first issue I have ever opened. If I made any grievous mistakes please inform me of such. . ---. ### Minimal code sample:. ```python; import scanpy; adata = scanpy.datasets.pbmc3k(); scanpy.external.pp.scrublet(adata); ```; and around line 177 in scanpy.external.pp.scrublet we need to 'observe' the values:; ```python; adata_obs.layers['raw'] = adata_obs.X; print(adata_obs.layers['raw']); pp.normalize_total(adata_obs) <--- currently normalizes all layers and X; print(adata_obs.layers['raw']); ```; ---; ### Impact; The 'raw' layer is later used in line 194 of scanpy.external.pp.scrublet to simulate doublets, however, it doesn't contain raw data anymore. ```python; adata_sim = scrublet_simulate_doublets(; adata_obs,; layer='raw',; sim_doublet_ratio=sim_doublet_ratio,; synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957:830,layers,layers,830,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957,4,['layers'],['layers']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. This is probably a bug in my thinking, but naively I thought that `sc.pp.normalize_total()` normalizes counts per cell, thus allowing comparison of different cells by correcting for variable sequencing depth. However, the log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:406,variab,variable,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['variab'],['variable']
Modifiability,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy.; Latest on pip at least scanpy-1.6.0; ---. I'm using the sc.pl.dendrogram multiple times different lists of genes on my dataset (incrementing number of highly variable genes basically). The outputted dendrogram is alway the same (I guess it's taking into account all the genes because it's using something like 32go of ram....). ### Minimal code sample (that we can copy&paste without having any data). ```python; hvegene_sets = [sc.pp.highly_variable_genes(adata, inplace=False, subset=False, n_top_genes=nhvg)[""highly_variable""] for nhvg in [500,1000,2000, 3000,4000, 5000]]; ```; then; ```python; [sum(hvgene) for hvgene in hvegene_sets]; ```; outputs:; [499, 1000, 1999, 2999, 4000, 4999] (so i have my different genesets). then ; ```python; dendro1 = sc.tl.dendrogram(adata, ; var_names=adata.var_names[hvegene_sets[1]].values, ; optimal_ordering=True,; cor_method=""spearman"", linkage_method=""complete"", inplace=False,; groupby=""Annotation""); dendro2 = sc.tl.dendrogram(adata, ; var_names=adata.var_names[hvegene_sets[5]].values, ; optimal_ordering=True,; cor_method=""spearman"", linkage_method=""complete"", inplace=False,; groupby=""Annotation""); [dendro1[key] ==dendro2[key] for key in dendro1.keys()] ; ```; outputs: ; ```code; [array([[ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True],; [ True, True, True, True]]),; Tr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1549:383,variab,variable,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1549,1,['variab'],['variable']
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Selection of highly variable genes works fine in default settings, but I get an error when I try to use seurat_v3 flavor. ```python; adata2.layers[""counts""] = adata2.X.copy(); adata2.raw = adata2 # keep full dimension safe; sc.pp.normalize_total(adata2, target_sum=1e4); sc.pp.log1p(adata2); sc.pp.highly_variable_genes(; adata2,; flavor=""seurat_v3"",; n_top_genes=3000,; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-18-64d280f5029c> in <module>; 3 sc.pp.normalize_total(adata2, target_sum=1e4); 4 sc.pp.log1p(adata2); ----> 5 sc.pp.highly_variable_genes(; 6 adata2,; 7 flavor=""seurat_v3"",. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 83 x = np.log10(mean[not_const]); 84 model = loess(x, y, span=span, degree=2); ---> 85 model.fit(); 86 estimat_var[not_const] = model.outputs.fitted_values; 87 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'svddc failed in l2fit.'; ```. #### Versions; 0.10.00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2034:251,variab,variable,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2034,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; I am trying to ingest a CITEseq dataset into another clustered dataset. These datasets have different numbers of cells but I ran neighbors(n_neighbors=30) for both prior to running umap. I have confirmed that both datasets have the same variable names and the same number of variable names (38). Both objects look identical when a call adata.var. . I receive the error: ""all input arrays must have the same shape"". . ```; sc.pp.neighbors(CODEX_sub, n_neighbors=30) ; sc.tl.umap(CODEX_sub); sc.pp.neighbors(adata_sub, n_neighbors = 30); sc.tl.umap(adata_sub); sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-214-01a03312d3df> in <module>; ----> 1 sc.tl.ingest(CODEX_sub, adata_sub, obs='leiden', embedding_method='umap'). ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, neighbors_key, inplace, **kwargs); 124 labeling_method = labeling_method * len(obs); 125 ; --> 126 ing = Ingest(adata_ref, neighbors_key); 127 ing.fit(adata); 128 . ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in __init__(self, adata, neighbors_key); 383 ; 384 if neighbors_key in adata.uns:; --> 385 self._init_neighbors(adata, neighbors_key); 386 else:; 387 raise ValueError(. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_neighbors(self, adata, neighbors_key); 349 else:; 350 self._neigh_random_state = neighbors['params'].get('random_state', 0); --> 351 self._init_pynndescent(neighbors['distances']); 352 ; 353 def _init_pca(self, adata):. ~\anaconda3\envs\scenv\lib\site-packages\scanpy\tools\_ingest.py in _init_pynndescent(self, distances); 284 ; 285 first_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085:468,variab,variable,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085,2,['variab'],['variable']
Modifiability,"- [x ] I have checked that this issue has not already been reported.; - [x ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # create new env; conda install -c pytorch pytorch; conda install -c pytorch cudatoolkit=11.3. conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/linux-64::__glibc==2.31=0; - feature:|@/linux-64::__glibc==2.31=0. Your installed version is: 2.31; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2282:758,flexible,flexible,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2282,2,['flexible'],['flexible']
Modifiability,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:378,variab,variable,378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported. ; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; ; I am reproducing [3k PBMCs tutorial](https://github.com/scverse/scanpy-tutorials/blob/master/pbmc3k.ipynb), which I run with no problem previously in Google Colab. But suddenly this error pop up during plotting which I believe due to latest update 1.9.0. I verified by installing previous version, this is not an issue:. `!pip install scanpy==1.8.2 # work fine`. ### Error code . ```python; # scatter plot in the PCA coordinates, but we will not use that later on.; sc.pl.pca(adata, color='CST3') # <-- this produces the error; ```. ```pytb; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:00); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); [<ipython-input-10-615d33c5cea9>](https://localhost:8080/#) in <module>(); 3 ; 4 # scatter plot in the PCA coordinates, but we will not use that later on.; ----> 5 sc.pl.pca(adata, color='CST3'); 6 ; 7 '''. 5 frames; [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 870 if not annotate_var_explained:; 871 return embedding(; --> 872 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 873 ); 874 else:. [/usr/local/lib/python3.7/dist-packages/scanpy/plotting/_tools/scatterplots.py](https://localhost:8080/#) in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:799,variab,variable,799,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ```python; sc.tl.pca(adata, svd_solver='arpack', use_highly_variable=True, chunked=True, chunk_size=1000); ```. ```pytb; TypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'int'; ```. I believe the error is due overwriting of `start` variable.; It is declared in [line 116](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L116) but is overwritten by loop variable of same name in [line 172](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_pca.py#L172). #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.1.0; anndata 0.7.5; asciitree NA; cffi 1.14.3; cloudpickle 1.6.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2020.12.0; dateutil 2.8.1; fasteners NA; get_version 2.1; h5py 3.1.0; highs_wrapper NA; igraph 0.8.3; joblib 1.0.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.3; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.0; numba 0.52.0; numcodecs 0.7.2; numexpr 2.7.2; numpy 1.19.4; packaging 20.8; pandas 1.2.0; pkg_resources NA; psutil 5.8.0; pyparsing 2.4.7; pytz 2020.5; scanpy 1.6.0; scipy 1.6.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.0; sparse 0.11.2; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.1; toolz 0.11.1; typing_extensions NA; yaml 5.3.1; zarr 2.6.1; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-glibc2.10; 2 logical CPU cores, x86_64; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1590:482,variab,variable,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1590,2,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; pbmc = sc.datasets.pbmc3k(); log_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:598,layers,layers,598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:618,layers,layers,618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. I have the following anndata; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'; uns: 'genome'; ```. When I run leiden clustering it shows that the parameters were added. ```computing PCA; on highly variable genes; with n_comps=30; finished (0:01:25); computing neighbors; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:04); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:18); running Leiden clustering; finished: found 23 clusters and added; 'NoBatchCorr', the cluster labels (adata.obs, categorical) (0:00:04); ```; But when I check my anndata, none present. As such if I try to generate a umap image I get the following error; ```; AnnData object with n_obs × n_vars = 28752 × 22603; obs: 'batch', 'cluster1_psc', 'all_fib_types', 'psc_batch_temp', 'batch_combined', 'overall', 'cluster1_kpc', 'kpc_batch_temp', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'Norm_Factor'; var: 'gene_ids', 'feature_types', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2330:959,variab,variable,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2330,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am running the standard scrnaseq pipeline on some data and am experiencing an issue. When I run. ```py; sc.pp.highly_variable_genes(adata,n_top_genes=4000, batch_key='batch'); ```. I get the error. ```; If you pass `n_top_genes`, all cutoffs are ignored.; extracting highly variable genes; ZeroDivisionError: division by zero; ```. I've unfortunately never seen this before, and i'm not sure how to address it. I would love if someone could help with this. Some additional information on my data. ```pycon; >>> adata.X.shape; Out[21]: (3433, 16836). >>> adata.X; array([[0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; ...,; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.],; [0., 0., 0., ..., 0., 0., 0.]], dtype=float32); ```. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; AlexFunctions NA; JonFunctions NA; PIL 9.1.0; PyQt5 NA; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bs4 4.11.1; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; h5py 3.6.0; hypergeom_ufunc NA; igraph 0.9.1; import_all NA; ipykernel 6.13.0; jedi 0.18.1; joblib 1.1.0; kiwisolver 1.4.2; leidenalg 0.8.4; llvmlite 0.38.0; matplotlib 3.5.1; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.1; numpy 1.21.6; packaging 21.3; pandas 1.4.2; params NA; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.7.0; prompt_toolkit 3.0.29; psutil 5.9.0; ptyprocess 0.7.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynnd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2236:505,variab,variable,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2236,1,['variab'],['variable']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. In short `sc.pp.scale()` throws an error when `adata.X` is a dask array solely because `sc.pp._simple.scale` doesn't have the `sc.pp._simple.scale_array` function registered for a `dask.array.Array` type. Adding `@scale.register(da.Array)` [here](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#LL758C7-L758C7) would fix that, but considering `dask.array` is [only an optional dependency,](https://github.com/scverse/scanpy/blob/f03c5b407412de447480733a1a1a0e33e0c871d2/scanpy/preprocessing/_simple.py#L32) there would have to be a conditional to wrap the function decoration. This brings me to the larger issue, is scanpy supposed to or working toward supporting dask arrays completely? I'm new to scanpy and I'm not sure if this is a bug report or an enhancement request. I could submit a pull request for this one issue but I'm curious if I'll run into many such issues as I dive in further and trying to figure out if there's existing momentum in this direction or whether I should be following some other parallelization strategy. I got here by following [AnnData's guide on using dask and zarr](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/%7Bread%2Cwrite%7D_dispatched.html) to try to parallelize processing of a large scRNA-seq file. I come from a microscopy data analysis and ML background where my image data is stored in S3 hosted zarr arrays (in an OME-NGFF schema), handled by using dask arrays wrapped in xarray DataArrays, and parallelized across compute using ray. It would be nice to use a similar stack (dropping in anndata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:1046,enhance,enhancement,1046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['enhance'],['enhancement']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. To do some trajectory analysis, I wanted to first try some tutorials to adapt to the workflow of scanpy, after calling sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) i got the valueerror below. I also tried different Drosophila Datasets and it always happens, and its due to a gene called ""nan"" in the features.tsv or genes.tsv, if this gene is renamed, scanpy works as intended. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-6-455e630e3278> in <module>; 1 adata.var['mt'] = adata.var_names.str.startswith('mt:') # annotate the group of mitochondrial genes as 'mt'; ----> 2 sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 286 X.eliminate_zeros(); 287 ; --> 288 obs_metrics = describe_obs(; 289 adata,; 290 expr_type=expr_type,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel); 119 for qc_var in qc_vars:; 120 obs_metrics[f""total_{expr_type}_{qc_var}""] = (; --> 121 X[:, adata.var[qc_var].values].sum(axis=1); 122 ); 123 if log1p:. C:\ProgramData\Anaconda3\lib\site-packages\scipy\sparse\_index.py in __getitem__(self, key); 49 return self._get_sliceXslice(row, col); 50 elif col.ndim == 1:; ---> 51 return self._get_sliceXarray(row, col); 52 raise IndexError('index results in >2 dimensions'); 53 elif row.ndim == 1:. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1708:301,adapt,adapt,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708,1,['adapt'],['adapt']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---; When trying to plot the PAGA graph some of the nodes don't show up in the graph. The nodes/clusters don't show up specifically for color=dpt_pseudotime. The nodes are still visible with categorical variables, and with other continuous variables.; Even when copying dpt_pseudotime column, the color=dpt_pseudotime_copy does not show up correctly. ### Minimal code sample ; ```python; # preprocessing; sc.pp.recipe_zheng17(adata); adata_wt= adata[adata.obs[""genotype""].isin([""WT""])]; adata_pca = sc.tl.pca(adata_wt, svd_solver='arpack', copy=True); adata_n = sc.pp.neighbors(adata_pca, n_neighbors=4, n_pcs=20, copy=True); adata_graph = sc.tl.draw_graph(adata_n, copy=True); # paga; adata_full = sc.tl.paga(adata_graph, groups='final_bulk_labels', copy=True); # dpt; adata_full.uns['iroot'] = np.flatnonzero(adata_full.obs['final_bulk_labels'] == 'HSC')[1000]; adata_paga_dpt_nonan = sc.tl.diffmap(adata_full, copy=True, n_comps=10); adata_paga_dpt_nonan = sc.tl.dpt(adata_paga_dpt_nonan, copy=True). adata_paga_dpt_nonan.obs[""dpt_pseudotime_copy""]=adata_paga_dpt_nonan.obs[""dpt_pseudotime""]. sc.pl.paga(adata_paga_dpt_nonan, ; threshold=0.05, ; color=['dpt_pseudotime', 'final_bulk_labels', 'dpt_pseudotime_copy', 'total_counts'],; ; # layout: Optional[_IGraphLayout] = None,; # layout_kwds: Mapping[str, Any] = MappingProxyType({}),; # init_pos: Optional[np.ndarray] = None,; # root: Union[int, str, Sequence[int], None] = 0,; # labels: Union[str, Sequence[str], Mapping[str, str], None] = None,; single_component = True,; solid_edges= 'connectivities',; # dashed_edges: Optional[str] = None,; # transitions: Optional[str] = None,; fontsize = 5,. fontweight='light', ; # fontoutline=2, ; # text_kwds: Mapping[str, Any] = MappingProxyType({}),; node_size_scale = 3, ; node_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2292:427,variab,variables,427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2292,2,['variab'],['variables']
Modifiability,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hey, I've noticed another potential problem within the `seurat_v3` flavor of `sc.pp.highly_variable_genes()`. The documentation of the `batch_key` argument says on how the genes are ranked. >For all flavors, genes are first sorted by how many batches they are a HVG. For dispersion-based flavors ties are broken by normalized dispersion. If `flavor = 'seurat_v3'`, ties are broken by the median (across batches) rank based on within-batch normalized variance. However, when genes are sorted after computing everything, the `seurat_v3` method sorts first by the median ranks and then by how many batches a gene is highly variable (contrary to what the docstring says):. https://github.com/theislab/scanpy/blob/da66e15eee137b51868a6fb85603cfca9de557a7/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. For comparison, the other flavors sort the other way around (as the docstring says):. https://github.com/theislab/scanpy/blob/da66e15eee137b51868a6fb85603cfca9de557a7/scanpy/preprocessing/_highly_variable_genes.py#L505-L510. Not sure which sorting would be correct here to make `seurat_v3` match the original behavior in Seurat, but I found these comments by @adamgayoso in #1204 that mention that when using `batch_key` in the current implementation, results don't match Seurat.. maybe the sorting order is the problem here?. https://github.com/theislab/scanpy/pull/1204#issuecomment-645700601; https://github.com/theislab/scanpy/pull/1204#issuecomment-663879113. Here is a piece of code that shows the top5 genes after `seurat_v3` gene selection and sorting by `highly_variable_nbatches` (which should then all be HVG, but they are not). <details>. ```python; import numpy as np; import scanpy as sc; import anndata . import sys; sys.path.append(""scanpy/preproce",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733:849,variab,variable,849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733,1,['variab'],['variable']
Modifiability,"- [x] It would be better to refer to the anndata docs for the AnnData class (http://anndata.readthedocs.io/en/latest/anndata.AnnData.html) whenever `:class:~scanpy.api.AnnData` appears. `:class:AnnData <http://anndata.readthedocs.io/en/latest/anndata.AnnData.html>` has the correct css style, but does not hyperlink. Probably a solution via http://www.sphinx-doc.org/en/1.5.1/ext/extlinks.html together with the definition of an `:extclass:` role that inherits the `:class:` properties would be the correct way to do it.; - [x] A few references, like ""[Traag1723]"" are not rendered correctly... Who knows what's going on there. I couldn't figure it out with a few tests... Let's see.; - [x] the Neighbors class docstring doesn't render properly; - [ ] changing to the slim docstring style from the numpy docstring style messes up readability when calling the docstring lookup in jupyter or other IDEs, hence I'd advocate for maintaining this information. ## AnnData; - [x] `__init__` method appears in `AnnData`; - [x] `attributes` appear after `methods`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/58:452,inherit,inherits,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58,1,['inherit'],['inherits']
Modifiability,"- [x] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [x] External tools: Do you know an existing package that should go into `sc.external.*`?. Hi, thanks for developing such a fantastic framework for analyzing single-cell data, it really helps a lot in my research. I'm the developer of [Heatgraphy](https://github.com/Heatgraphy/heatgraphy), which is a python package to visualize multi-dimensional data using the x-layout system. You may think of it as python's version of complexHeatmap, but Heatgraphy can do much more. Personally, I think Heatgraphy can help visualize the AnnData intuitively. Here is [an example](https://heatgraphy.readthedocs.io/en/latest/auto_examples/plot_pbmc3k.html) of visualization of the PBMC3K dataset using Heatgraphy. The structure of AnnData fits the structure of this visualization quite well. We can plot the `.X` as the heatmap, and other attributes in `.obs`/`.var` or `.obsm`/`.varm` as side plots. ![](https://heatgraphy.readthedocs.io/en/latest/_images/sphx_glr_plot_pbmc3k_001_2_0x.png). Therefore, I propose adding a new API using Heatgraphy to help visualize AnnData. If adding to `sc.pl`, the API may look like this:. ```python; sc.pl.heatgraphy(adata, left=[(""cell_type"", ""color""), (""cell_type"", ""label"")], right=[(""gene_name"", ""label"")]); ```. If added as an extra class, the API can be more flexible and offer much more customization. It may look like this:. ```python; viz = AnnDataViz(adata); viz.add_left(key=""cell_type"", plot=""color"", cmap=""Set2"") # use a key from .obs and plot as color strip; viz.add_left(key=""cell_type"", plot=""label""); viz.render(); ```. It can also be applied to specific visualization for analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444:1368,flexible,flexible,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444,1,['flexible'],['flexible']
Modifiability,"-------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:1481,layers,layers,1481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,1,['layers'],['layers']
Modifiability,"--------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-138-e642551f77de> in <module>(); ----> 1 sc.pl.dotplot(adata, marker_genes1, groupby='louvain'). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyEr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/593:1137,layers,layers,1137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593,1,['layers'],['layers']
Modifiability,"---------------; TypeError Traceback (most recent call last); <ipython-input-698-58a5366a0f70> in <module>; 1 adata = sc.datasets.paul15(); ----> 2 sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; --> 128 return _scatter_obs(**args); 129 if (; 130 (x in adata.var.keys() or x in adata.obs.index). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 273 palettes = [palette for _ in range(len(keys))]; 274 for i, palette in enumerate(palettes):; --> 275 palettes[i] = _utils.default_palette(palette); 276 ; 277 if basis is not None:. TypeError: 'str' object does not support item assignment; ```. I get no error if I use any of `sc.pl.palettes`. I also get no error setting `palette=""Set2""` in `sc.pl.umap`, `sc.pl.draw_graph` etc... #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 7.2.0; anndata 0.7.4; appdirs 1.4.4; atac_utils NA; atomicwrites 1.3.0; attr 20.2.0; backcall 0.2.0; brotli NA; cellrank 1.0.0-rc.10; certifi 2020.06.20; cffi 1.14.3; chardet 3.0.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; datacache 1.1.5; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; gtfparse 1.2.0; h5py 2.10.0; idna 2.10; igraph 0.8.2; importlib_metadata 0.23; i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:1299,layers,layers,1299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['layers'],['layers']
Modifiability,"--> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2621,layers,layers,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,". # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:2731,variab,variable,2731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Modifiability,.0; intervaltree 3.1.0; ipykernel 6.25.2; ipython 8.16.1; ipython-genutils 0.2.0; ipywidgets 8.1.1; isoduration 20.11.0; isort 5.12.0; itemadapter 0.8.0; itemloaders 1.1.0; itsdangerous 2.1.2; jaraco.classes 3.3.0; jedi 0.18.1; jeepney 0.8.0; jellyfish 1.0.1; Jinja2 3.1.2; jmespath 1.0.1; joblib 1.3.2; json5 0.9.14; jsonpatch 1.33; jsonpointer 2.4; jsonschema 4.19.1; jsonschema-specifications 2023.7.1; jupyter 1.0.0; jupyter_client 8.3.1; jupyter-console 6.6.3; jupyter_core 5.3.2; jupyter-events 0.7.0; jupyter-lsp 2.2.0; jupyter_server 2.7.3; jupyter_server_terminals 0.4.4; jupyterlab 4.0.6; jupyterlab-pygments 0.2.2; jupyterlab_server 2.25.0; jupyterlab-widgets 3.0.9; keyring 24.2.0; kiwisolver 1.4.5; lazy_loader 0.3; lazy-object-proxy 1.9.0; leidenalg 0.9.1; libarchive-c 5.0; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.1; locket 1.0.0; lxml 4.9.2; lz4 4.3.2; Markdown 3.5; markdown-it-py 3.0.0; MarkupSafe 2.1.3; matplotlib 3.8.0; matplotlib-inline 0.1.6; mccabe 0.7.0; mdit-py-plugins 0.4.0; mdurl 0.1.0; mistune 3.0.1; mkl-service 2.4.0; more-itertools 10.1.0; mpmath 1.3.0; msgpack 1.0.6; multidict 6.0.4; multipledispatch 0.6.0; multiprocess 0.70.15; munkres 1.1.4; mypy-extensions 1.0.0; natsort 8.4.0; natsort 8.4.0; navigator-updater 0.4.0; nbclient 0.8.0; nbconvert 7.9.2; nbformat 5.9.2; nest-asyncio 1.5.6; networkx 3.1; nltk 3.8.1; notebook 7.0.4; notebook_shim 0.2.3; numba 0.57.1; numexpr 2.8.7; numpy 1.24.4; numpydoc 1.5.0; openpyxl 3.1.2; overrides 7.4.0; packaging 23.2; pandas 2.1.1; pandocfilters 1.5.0; panel 1.2.3; parallel-fastq-dump 0.6.7; param 1.13.0; parsel 1.8.1; parso 0.8.3; partd 1.4.1; pathspec 0.11.2; patsy 0.5.3; pep8 1.7.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 10.0.1; pip 23.2.1; pkce 1.0.3; pkginfo 1.9.6; pkgutil_resolve_name 1.3.10; plac 1.3.5; platformdirs 3.11.0; plotly 5.17.0; pluggy 1.3.0; ply 3.11; pooch 1.7.0; prometheus-client 0.17.1; prompt-toolkit 3.0.39; Protego 0.3.0; psutil 5.9.5; ptyprocess 0.7.0; PuLP 2.7.0; pure-eval,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:6796,plugin,plugins,6796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['plugin'],['plugins']
Modifiability,.8; intervaltree 3.1.0; ipykernel 6.25.0; ipython 8.15.0; ipython-genutils 0.2.0; ipywidgets 8.0.4; isort 5.9.3; itemadapter 0.3.0; itemloaders 1.0.4; itsdangerous 2.0.1; jaraco.classes 3.2.1; jedi 0.18.1; jeepney 0.7.1; jellyfish 1.0.1; Jinja2 3.1.2; jinja2-time 0.2.0; jmespath 0.10.0; joblib 1.2.0; json5 0.9.6; jsonpatch 1.32; jsonpointer 2.1; jsonschema 4.17.3; jupyter 1.0.0; jupyter_client 7.4.9; jupyter-console 6.6.3; jupyter_core 5.3.0; jupyter-events 0.6.3; jupyter-server 1.23.4; jupyter_server_fileid 0.9.0; jupyter_server_ydoc 0.8.0; jupyter-ydoc 0.2.4; jupyterlab 3.6.3; jupyterlab-pygments 0.1.2; jupyterlab_server 2.22.0; jupyterlab-widgets 3.0.5; kaleido 0.2.1; keyring 23.13.1; kiwisolver 1.4.4; lazy_loader 0.2; lazy-object-proxy 1.6.0; libarchive-c 2.9; libmambapy 1.5.1; linkify-it-py 2.0.0; llvmlite 0.40.0; lmdb 1.4.1; locket 1.0.0; lxml 4.9.3; lz4 4.3.2; Markdown 3.4.1; markdown-it-py 2.2.0; MarkupSafe 2.1.1; matplotlib 3.7.2; matplotlib-inline 0.1.6; mccabe 0.7.0; mdit-py-plugins 0.3.0; mdurl 0.1.0; mistune 0.8.4; mkl-fft 1.3.8; mkl-random 1.2.4; mkl-service 2.4.0; more-itertools 8.12.0; mpmath 1.3.0; msgpack 1.0.3; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.14; munkres 1.1.4; mypy-extensions 1.0.0; navigator-updater 0.4.0; nbclassic 0.5.5; nbclient 0.5.13; nbconvert 6.5.4; nbformat 5.9.2; nest-asyncio 1.5.6; networkx 3.1; nltk 3.8.1; notebook 6.5.4; notebook_shim 0.2.2; numba 0.57.1; numexpr 2.8.4; numpy 1.24.3; numpydoc 1.5.0; openpyxl 3.0.10; packaging 23.1; pandas 2.0.3; pandocfilters 1.5.0; panel 1.2.3; param 1.13.0; parsel 1.6.0; parso 0.8.3; partd 1.4.0; pathlib 1.0.1; pathspec 0.10.3; patsy 0.5.3; pep8 1.7.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.4.0; pip 23.2.1; pkce 1.0.3; pkginfo 1.9.6; platformdirs 3.10.0; plotly 5.9.0; pluggy 1.0.0; ply 3.11; poyo 0.5.0; prometheus-client 0.14.1; prompt-toolkit 3.0.36; Protego 0.1.16; psutil 5.9.0; ptyprocess 0.7.0; pure-eval 0.2.2; py-cpuinfo 8.0.0; pyarrow 11.0.0; pyasn1 0.4.8; pyasn1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:4437,plugin,plugins,4437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['plugin'],['plugins']
Modifiability,"1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3430,extend,extend,3430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['extend'],['extend']
Modifiability,"18.49 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:1415,variab,variable,1415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,"18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.; I cannot rule out completely the possibility that my conda in those 2 vms are messed up.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:1164,flexible,flexible,1164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,2,['flexible'],['flexible']
Modifiability,"2"",; ""TI1-P3"", 'TM1-P3', 'TI2-P3', 'TM2-P3',; ""TI-P4"",'TS1-P4','TS2-P4',; 'lung_P2', 'lung_P5', 'lung_P8', 'lung_P9',; 'lung_P13', 'lung_P16',; 'lung_P20', 'lung_P21', 'lung_P24', 'lung_P28', 'lung_P29',; 'lung_P32', 'lung_P33', 'lung_P34', 'lung_P35', 'lung_P38', 'lung_P39'],; join='outer'). print('Begin of post doublets removal and QC plot'); sc.pp.scrublet(alldata, n_neighbors=10); alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); n1 = alldata.shape[0]; print(f'Cells retained after scrublet: {n1}, {n0-n1} removed.'); print(f'End of post doublets removal and QC plots.'); ```. ### Error output. ```pytb; Begin of post doublets removal and QC plot; Running Scrublet; normalizing counts per cell. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_normalization.py:233: UserWarning: Some cells have zero counts; warn(UserWarning(""Some cells have zero counts"")). finished (0:00:00); WARNING: adata.X seems to be already log-transformed.; extracting highly variable genes. C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preprocessing\_simple.py:377: RuntimeWarning: invalid value encountered in log1p; np.log1p(X, out=X). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[59], line 2; 1 print('Begin of post doublets removal and QC plot'); ----> 2 sc.pp.scrublet(alldata, n_neighbors=10); 3 alldata = alldata[alldata.obs['predicted_doublet']==False, :].copy(); 4 n1 = alldata.shape[0]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File C:\ProgramData\Anaconda3\envs\dl\lib\site-packages\scanpy\preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:6499,variab,variable,6499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['variab'],['variable']
Modifiability,"34 cb = Colorbar(cax, mappable, **kwargs); 1735 ; 1736 cid = mappable.callbacksSM.connect('changed', cb.update_normal). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, mappable, **kwargs); 1226 if isinstance(mappable, martist.Artist):; 1227 _add_disjoint_kwargs(kwargs, alpha=mappable.get_alpha()); -> 1228 ColorbarBase.__init__(self, ax, **kwargs); 1229 ; 1230 @cbook.deprecated(""3.3"", alternative=""update_normal""). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3002,extend,extend,3002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,3,['extend'],"['extend', 'extendfrac', 'extendrect']"
Modifiability,"7251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen version of umap as an intermediate solution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1429,rewrite,rewrite,1429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['rewrite'],['rewrite']
Modifiability,"9 |; | Updated | 3.97 |; | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:1312,variab,variable,1312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability,"; 0 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; ```. it also works for multiple groups:. ```python; print(sc.get.rank_genes_groups_df(adata, None, n_top_genes=2)); ```; ```; group names scores logfoldchanges pvals pvals_adj; 0 0 CD3D 26.250046 3.859759 4.379061e-75 2.233321e-73; 1 0 LDHB 21.207499 2.134979 1.488480e-67 5.993089e-66; 2 1 FCGR3A 47.682064 5.891937 3.275554e-141 3.579712e-139; 3 1 FTL 45.653259 2.497682 9.003150e-208 6.887410e-205; 4 2 LYZ 38.981312 5.096991 1.697105e-172 1.298285e-169; 5 2 CST3 34.241749 4.388617 1.448193e-149 5.539337e-147; 6 3 NKG7 34.214161 6.089183 2.356710e-55 2.575547e-53; 7 3 CTSW 24.584066 5.091688 2.026294e-39 9.118324e-38; 8 4 CD79A 52.583344 6.626956 4.032974e-84 7.713062e-82; 9 4 CD79B 32.102913 4.990217 1.958507e-51 1.872822e-49; 10 5 FTL 26.084383 1.844273 1.236398e-74 2.364611e-72; 11 5 LST1 25.554073 3.170759 5.653851e-81 4.325196e-78; 12 6 LYZ 31.497107 4.328516 9.041131e-106 6.916466e-103; 13 6 CST3 23.850258 3.281016 2.491629e-83 9.530482e-81; 14 7 CST3 33.024582 4.195395 5.768439e-136 4.412856e-133; 15 7 LYZ 31.264187 4.267053 9.712334e-101 1.485987e-98; 16 8 PPIB 39.260998 3.990153 7.159966e-47 3.651583e-45; 17 8 MZB1 33.305500 8.979518 7.611322e-26 1.878278e-24; 18 9 STMN1 27.133045 5.936039 4.998127e-18 8.312102e-17; 19 9 HMGB2 15.229477 5.016804 3.184879e-12 4.060720e-11; 20 10 HNRNPA1 18.405415 2.040915 1.570832e-12 1.560632e-11; 21 10 NPM1 14.230449 2.183721 3.424469e-10 3.046185e-09; ```. This also extends to enrichment queries (this is what I wanted originally):. ```python; sc.queries.enrich(adata, ""1"", n_top_genes=10); ```. For enrichment queries, I added to the doc string that a pval threshold of 0.05 is used. Previously, this was not obvious to me (and for cluster marker genes, this might not always be sensible). I didn't add anything to `docs/release-notes/`, yet. I first wanted to get your opinion. Is it useful, what is still needed here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2145:2223,extend,extends,2223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2145,1,['extend'],['extends']
Modifiability,"; 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler.py in; 11 from numba.core.environment import lookup_environment; 12; ---> 13 from numba.core.compiler_machinery import PassManager; 14; 15 from numba.core.untyped_passes import (ExtractByteCode, TranslateByteCode,. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/compiler_machinery.py in; 5 from numba.core.compiler_lock import global_compiler_lock; 6 from numba.core import errors, config, transforms; ----> 7 from numba.core.utils import add_metaclass; 8 from numba.core.tracing import event; 9 from numba.core.postproc import PostProcessor. ImportError: cannot import name 'add_metaclass' from 'numba.core.utils' (/home/qsw5175/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/utils.py)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2952,config,config,2952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,2,['config'],['config']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; ...When run bbknn on adata which has been calculated the pca, umap, and leiden, the AttributeError shows 'tuple' object has no attribute 'tocsr'. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.umap(adata); ...; computing PCA; on highly variable genes; with n_comps=50; finished (0:00:27); computing neighbors; using 'X_pca' with n_pcs = 50; finished: added to `.uns['neighbors']`; `.obsp['distances']`, distances for each pair of neighbors; `.obsp['connectivities']`, weighted adjacency matrix (0:00:24); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:01:27). %%time; sc.external.pp.bbknn(adata, batch_key='batch'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; computing batch balanced neighbors; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-9b24f504f73c> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', ""sc.external.pp.bbknn(adata, batch_key='batch')""). 6 frames; <decorator-gen-60> in time(self, line, cell, local_ns). <timed eval> in <module>(). /usr/local/lib/python3.6/dist-packages/bbknn/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 63 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 64 ; ---> 65 return distances, connectivities.tocsr(); 66 ; 67 def create_tree(data,approx,metric,use_faiss,n_trees):. AttributeError: 'tuple' object has no attribute 'tocsr'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; scanpy==1.5.1 anndata==0.7.3 umap==0.4.3 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.10.2 python-igraph==0.8.2 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1249:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1249,1,['variab'],['variable']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; I am calculating custom connectivities using hsnw on rep 'X', I don't want to calculate PCA, I want to compute UMAP using these connectivities. ; sc.tl.umap falls back to pca in:; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_umap.py#L153; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L23. how to get sc.tl.umap to run on the precomputed 'X 'connectivities?; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from scvelo.pp import neighbors; adata; #AnnData object with n_obs × n_vars = 4329 × 192; #obs: 'BARCODE', 'sample', 'detectable.features'; #var: 'gene_ids', 'feature_types'; #layers: 'normalized.counts'. neighbors(adata, n_neighbors = 20, use_rep = ""X"",knn = True,random_state = 0,method = 'hnsw',metric = ""euclidean"",metric_kwds = {""M"":20,""ef"":200,""ef_construction"":200},num_threads=1). adata.uns[""neighbors""]['params']; #{'n_neighbors': 20, 'method': 'hnsw', 'metric': 'euclidean', 'n_pcs': None}. sc.tl.umap(adata). #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318:847,layers,layers,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318,1,['layers'],['layers']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; Loading data using `adata = sc.datasets.visium_sge('V1_Human_Lymph_Node')` with Anndata<0.7rc1 leads to error `'AnnData' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:950,Variab,Variable,950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['Variab'],['Variable']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; Not able to install with conda and no info about the source of error.; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```bash; (scrna) $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190:420,flexible,flexible,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190,2,['flexible'],['flexible']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; When I use `sc.pp.log1p(adata)` and then `sc.pp.log1p(adata, layer='other')` it warns me that the data has already been logged even though I am logging a layer as opposed to adata.X. Would be nice to flag logging for each layer instead of when anything is logged. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. adata = sc.datasets.pbmc3k_processed(); adata.layers['other'] = adata.X; sc.pp.log1p(adata, layer='other'); sc.pp.log1p(adata); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; WARNING: adata.X seems to be already log-transformed.; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.5.2.dev5+ge5d246aa anndata==0.7.3 umap==0.3.10 numpy==1.18.5 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.7.1 louvain==0.6.1 leidenalg==0.7.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333:510,layers,layers,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333,1,['layers'],['layers']
Modifiability,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.recipe_seurat and scanpy.pp.recipe_zheng17 indicate that they expect non log-transformed data. This leads both functions to do by default the highly variable gene (HVG) selection on non log-transformed data. This seems contrary to the scanpy and seurat clustering tutorials, which perform HVG selection after log-transform. It also seems contrary to the new function scanpy.pp.highly_variable_genes which expects log-transformed inputs. scanpy version : 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251:233,variab,variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251,1,['variab'],['variable']
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; How to generate two umaps for one gene split by a condition[one variable in obs] ?; so you can compare where gene expressed in cell types and how they differ in the two conditions. could we add option split by to umap scanpy function?; https://github.com/theislab/scanpy/issues/759,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1879:538,variab,variable,538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1879,1,['variab'],['variable']
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; is there a scanpy method to do a correlation between cell types and continuous variables stored in .obs?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:553,variab,variables,553,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:477,adapt,adapting,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,3,"['adapt', 'layers']","['adapting', 'layers']"
Modifiability,<!-- What kind of feature would you like to request? -->; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?. <!-- Please describe your wishes below: -->; Computes a hierarchical clustering for the variables(genes) in sc.pl.matrixplot or sc.pl.dotplot.Thanks!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2433:224,variab,variables,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2433,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; When determining colours of categorical variables in uns they are based on alphabetical order (if I am not mistaken) - being represented just as an ordered list. Thus it is a bit inconvenient to change colours, especially if categories change during the analysis - the whole order changes and the mapping breaks. Would it be possible to use a dictionary of colours as values and categories as keys (with a default for any categories missing colours)?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340:510,variab,variables,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [X] Additional function parameters / changed functionality / changed defaults?. <!-- Please describe your wishes below: -->; Sorry to file two issues in a row! I've been using Scanpy + scVI for my analysis recently, and am really enjoying it!. Currently, if use_rep is set in `sc.pp.neighbors`, then n_pcs is ignored. These seems like sane default behaviour to me - if we aren't using 'X_pca', then n_pcs doesn't really make sense. . This can actually be limiting, as I recently discovered. If you calculate a latent representation with scVI with `n_latent = 50`, you can't then do... ```python; sc.pp.neighbors(adata, use_rep='X_scvi', n_pcs = 25); ```. ...as the neighborhood graph is calculated on all 50 latent variables. If I want to use only 25 - say, as a point of comparison to see which best represents my data - I have to recalculate the latent representation with scVI with `n_latent = 25`. Basically, if you aren't using PCA, you have to calculate a full reduction for every number of dimensions you are interested in. . I don't think the fix would be that major. The source seems to be in the `_choose_representation` function, with the directly relevant snippet below:. https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L48-L58. I think the only change needed would be to have the `n_pcs is not None` catch in all cases, not just when `use_rep = 'X_pca'`. Might also generalise the variable name to make it more clear it refers to any reduction, not just PCA. Admittedly, I only just started exploring the code base, so I'm not sure where else `_choose_representation` is called, or what other impacts this could have. . I have some blue sky time tomorrow, so I'll fork it then and see if I can whip up a pull request. Thanks for the excellent product!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1281:775,variab,variables,775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1281,2,['variab'],"['variable', 'variables']"
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Especially when we visualize large datasets with multiple categorical variables (e.g. patient, disease, cell type) using `sc.pl.dotplot`, and we use a sequence in the `groupby` argument (`e.g. sc.pl.dotplot(ad, 'genex', groupby=['individual', 'disease_status', 'cell type'])`), sometimes we end up with too few cells in some rows, in which summary statistics like fraction of nonzero expressors or mean expression are not very robust. To avoid that, I think it'd be cool to have a minimum observation cutoff in the function, where e.g. `min_cells=5` would show `groupby` combinations with at least 5 cells. Without this option, this sort of filtering becomes an annoying pandas exercise (which some might enjoy but possibly not everyone).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829:539,variab,variables,539,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829,1,['variab'],['variables']
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:719,layers,layers,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,4,"['config', 'enhance', 'layers']","['configurable', 'enhancement', 'layers']"
Modifiability,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; I find using the `adata.layers` really useful to compare normalisation strategies. I'd like to also be able to seamlessly run `sc.tl.pca` on data stored in different layers of the same `anndata` object. . At the moment my workaround is to set `adata.X` before PCA and changing the key to the `adata.obsm` element after:; ```python; adata.X = adata.layers[""mylayer""]; sc.tl.pca(adata); adata.obsm[""mylayer_pca""] = adata.obsm[""X_pca""]; ```; Ideally I'd like to just be able to set a `layer` argument in `sc.tl.pca`, as in the plotting functions. . Any plans on linking data layers to dimensionality reductions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301:493,layers,layers,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301,4,['layers'],['layers']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ X] Closes # (New Feature); - [ X] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [X ] Release notes not necessary because:. I thought I would give a shot at contributing to this awesome tool! I added a function to Scanpy's plotting API that I use in my own research for creating stacked barplots for visualizing compositions of cell groups. An example is depicted below:. ![image](https://github.com/scverse/scanpy/assets/5004419/21885a72-e15f-4f94-b1e5-84c1de8ca954). Specifically, this function enables one to plot the fraction of each cell group (e.g., cluster) that are labelled with a specific categorical variable. For example, if the cell groups are clusters, then one might be interested in examining the fraction of each cluster that originates from each ""batch"" of cells or each patient sample. This function also enables ordering of the groups according to a specific value (e.g., a patient number or batch ID) or by agglomerative clustering. An example of ordering the groups based on agglomerative clustering is shown below:. ![image](https://github.com/scverse/scanpy/assets/5004419/bfde8173-4f0d-483f-b37e-849446b65153). The function supplies an argument to specify whether the dendrogram should or should not be plotted. Please let me know if this feature is of interest and if so, what else needs to be adjusted or fixed prior to merging. Thanks!. Matt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2873:986,variab,variable,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2873,1,['variab'],['variable']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes # _no existing issue_; - [ ] Tests included or not required because: _No new tests_; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: _I did not write release notes_. Hi :). I am proposing a change that speeds up `filter_cells` (x1000 speedup) and `filter_genes` (x2 speedup) for CSR sparse matrices. On my personal machine for 1M cells, `sc.pp.filter_cells(adata, min_genes=xx)` runs in 1ms instead of 10s currently. The speedup should be even stronger on sparser modalities like ATAC. In spirit, this simply replaces `np.sum(X > 0, axis=axis)` with `X.getnnz(axis=axis)`, which is much more efficient. But the axis argument in `getnnz` in `csr_array` may be deprecated. I think it should still be fine with `csr_matrix`, but since I don't know for sure I manually implemented it for the CSR case as in https://github.com/scipy/scipy/issues/19405 . What do you think?. Regarding `getnnz`: Of course it would be nicer to be able to write `.getnnz(axis=axis)`, which extends beyond CSR to other sparse matrices. Can we assume that we're getting sparse matrices and not sparse arrays ?. Pinging @dschult from the Scipy issue liked above, who mentioned: . > I'm pretty sure that a reasonable and commonly occuring use-case would be enough to make the developers include this feature somehow. (edited because I confused `csr_array` and `csr_matrix`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2772:1348,extend,extends,1348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2772,1,['extend'],['extends']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Adresses #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:906,variab,variable,906,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['variab'],['variable']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3027; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because: they are, added. This PR fixes the bug reported in the linked issue. A new test which spots the erroneous computations has been added. I would use this chance to refactor the `_highly_variable_genes.py`, rather than using the 2-lines fix suggested in the first commit:; Doing the multi-batch hvg flagging differently for seurat_v3 and seurat/cell_ranger is what made this bug hard to spot in the first place I think.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042:647,refactor,refactor,647,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042,1,['refactor'],['refactor']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3114; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Other things done:. - Rename some variables and reorder functions so the diff between the metrics is minimal for a future unification; - Skip seurat v3 tests with numpy 2 because skmisc isn’t ready: https://github.com/has2k1/scikit-misc/issues/34. This will skip the seurat v3 tests for all but the minimum versions test job for now, but I think that’s OK?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115:520,variab,variables,520,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115,1,['variab'],['variables']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #761, closes #2322, closes #2229, closes #2267; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Also. - documents the parameter better, see #1502; - remove duplicated tests for embedding plots with continuous variables: `legend_loc` does nothing there (yet)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3163:640,variab,variables,640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3163,1,['variab'],['variables']
Modifiability,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Tests included or not required because: it's formating; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: it's formatting. Why did we ever configure black?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701:513,config,configure,513,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701,1,['config'],['configure']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This is a very small pull request to add `str` to the possible arguments for saving a figure from [`scanpy.pl.rank_genes_groups`][rank-genes-groups]. This addition matches other `save=` arguments, such as from [`scanpy.plotting.highly_variable_genes`][highly-variable-genes], [`sc.plotting.pca_variance_ratio`][pca-variance-ratio], and [`scanpy.plotting.umap`][umap]. [rank-genes-groups]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [highly-variable-genes]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_preprocessing.py; [pca-variance-ratio]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/__init__.py; [umap]: https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_tools/scatterplots.py. I have not included tests or release notes due to the single-line change nature of this pull request",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3076:493,variab,variable-genes,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3076,2,['variab'],['variable-genes']
Modifiability,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->; Modified (for `scanpy`) version of https://github.com/scverse/anndata/pull/564. Fixes https://github.com/scverse/anndata/issues/556. Big points of change:; 1. No more tuple-indices and related functionality (i.e., scoring pairwise); 2. Allow for `obs` and `var` group-by +`varm`, `obsm`, `layers` as options for data to aggregate; 3. Output is `AnnData` object instead of `DataFrame`; 4. `scanpy`-style public API. ## TODO (by @ivirshup):. Necessary:. - [x] Docs; - [x] Aggregate along other axis; - [x] Keep grouping cols in result; - [x] Reconsider API for non-anndata version (maybe return a dict of arrays?); - [ ] Decide on naming convention for `""nonzero""` variations, should this be `""nonzero_count""` so it's a little like `""nanmean""`. Optional, can do later:. - [ ] Weighted (although.... Idk, maybe can skip. Does ""weights"" affect ""count_nonzero""?); - [ ] Option for keeping around unseen groups, probably needs `fill_value` argument for those values; - [x] Support for `obsm`, `varm`; - [ ] Directly pass Series to groupby; - [ ] More aggregation functions (mean_nonzero, min, max, std, `nan*` variations); - [ ] Mask argument; - [ ] Dask support",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590:523,layers,layers,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590,1,['layers'],['layers']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Hi ; if samples contribute a different number of cells to my object, how to control for variability among samples? ; How to make sure that any difference between conditions I found is caused by biology and not because of samples variation? . downsampling, upsampling, bootstrapping, robustness test . Appreciate any feedback and any references for this issue. ![image](https://user-images.githubusercontent.com/23288387/155648374-f0d6178f-7024-4ecd-88c0-37547c5e7e19.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2155:270,variab,variability,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2155,1,['variab'],['variability']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; Hi, . In the original Coifman/Lafon paper on diffusion maps they introduce a family of kernels index by some alpha. I'm just wondering what value of alpha is implemented in scanpy. I assume it's alpha=0, based on the documentation but its a bit unclear. Is there any plans to extend it to other values of alpha? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2115:453,extend,extend,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2115,1,['extend'],['extend']
Modifiability,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; I was wondering, what is the intended way to temporarily set plotting parameters, e.g. figsize?. Say I want to increase the figsize just for a single UMAP plot. . I could either temporarily `set_figure_params` and reset it manually afterwards; ```python; sc.set_figure_params(figsize=(8, 8)); sc.pl.umap(adata, color=""cell_type""); sc.set_figure_params(figsize=(4, 4)) # or whatever it was before...; ```. Or create the figure separately:; ```python; _, ax = plt.subplots(figsize=(8, 8)); sc.pl.umap(adata, color=""cell_type"", ax=ax); ```; <p><br></p>. Would it make sense to extend `set_figure_params` to act as a context manager?. ```python; with sc.set_figure_params(figsize=(8, 8), frameon=False):; sc.pl.umap(adata, color=""cell_type""); ```. But maybe there's a comparable way already?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1648:751,extend,extend,751,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648,1,['extend'],['extend']
Modifiability,"=raise_errors); 71 typemap, restype, calltypes = infer.unify(raise_errors=raise_errors); 72 . ~/.local/lib/python3.9/site-packages/numba/core/typeinfer.py in propagate(self, raise_errors); 1069 if isinstance(e, ForceLiteralArg)]; 1070 if not force_lit_args:; -> 1071 raise errors[0]; 1072 else:; 1073 raise reduce(operator.or_, force_lit_args). TypingError: Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython frontend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython frontend); No implementation of function Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>) found for signature:; ; >>> run_quicksort(array(int32, 1d, C)); ; There are 2 candidate implementations:; - Of which 2 did not match due to:; Overload in function 'register_jitable.<locals>.wrap.<locals>.ov_wrap': File: numba/core/extending.py: Line 150.; With argument(s): '(array(int32, 1d, C))':; Rejected as the implementation raised a specific error:; UnsupportedError: Failed in nopython mode pipeline (step: analyzing bytecode); Use of unsupported opcode (LOAD_ASSERTION_ERROR) found; ; File ""../../../../.local/lib/python3.9/site-packages/numba/misc/quicksort.py"", line 180:; def run_quicksort(A):; <source elided>; while high - low >= SMALL_QUICKSORT:; assert n < MAX_STACK; ^; ; raised from /home/gabriel/.local/lib/python3.9/site-packages/numba/core/byteflow.py:269. During: resolving callee type: Function(<function make_quicksort_impl.<locals>.run_quicksort at 0x7f455d24c280>); During: typing of call at /home/gabriel/.local/lib/python3.9/site-packages/numba/np/arrayobj.py (5007). File ""../../../../.local/lib/python3.9/site-packages/numba/np/arrayobj.py"", line 5007:; def array_sort_impl(arr):; <source elided>; # Note we clobber the return value; sort_func(arr); ^. During: lowering ""$14call_method.5 = cal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652:8805,extend,extending,8805,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652,1,['extend'],['extending']
Modifiability,"> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2297,layers,layers,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"@fidelram . As noted in #1632, dotplot labels can often extend outside of the plotted area. Whether the full labels can be seen is dependent on how the plots are being output. It would be great if this always worked. <details>; <summary> Example from the docs: </summary>. ![](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). </details>. Could possibly be solved by using `matplotlib`'s `constrained_layout` or `tight_layout`. I think these would require modifying how the grid spec and axes are created.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:56,extend,extend,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['extend'],['extend']
Modifiability,"@flying-sheep This is generally the kind of simplification I was hoping we could do with plotting. ; It's not much, and is more about the dotplot, heatmap, etc. plotting methods. Also, the test errors I was running into are still happening. Another example would using a function to choose representations of X the same way for each function. Something like:. ```python; def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109:774,layers,layers,774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109,1,['layers'],['layers']
Modifiability,Adapt name of UMAP coordinates according to used representation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1403:0,Adapt,Adapt,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1403,1,['Adapt'],['Adapt']
Modifiability,Add azure pipelines plugin,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2111:20,plugin,plugin,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2111,1,['plugin'],['plugin']
Modifiability,Added visualizations for variables ordered by observations. E.g. marker genes ordered by cluster,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175:25,variab,variables,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175,1,['variab'],['variables']
Modifiability,"Adding an expression atlas downloader to `sc.datasets` (proposed in #489). I've punted on replacing where datasets are downloaded by just making it a variable in settings, since it seems contentious where datasets should be downloaded by default #558. @flying-sheep when I build the docs locally, the link to the expression atlas doesn't format properly on the main `API` page, but does on it's own page. Any ideas on if we can get that to work?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573:150,variab,variable,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573,1,['variab'],['variable']
Modifiability,Adding uns to a view fails if there is only one variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323:48,variab,variable,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323,1,['variab'],['variable']
Modifiability,"After going through the comment, _Originally posted by @LuckyMD in https://github.com/theislab/scanpy/issues/220#issuecomment-408332060_ , I have opened this issue. I am facing problem with `sc.pl.highest_expr_genes(adata_orig, n_top=20)` as well. Attaching some details:; I started by reading in the file as:. `adata_orig = sc.read_h5ad('covid_portal_210320_with_raw.h5ad', backed = 'r')`. After printing the `adata_orig` object, i get the following output:. ```; AnnData object with n_obs × n_vars = 647366 × 24929 backed at 'covid_portal_210320_with_raw.h5ad'; obs: 'sample_id', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'full_clustering', 'initial_clustering', 'Resample', 'Collection_Day', 'Sex', 'Age_interval', 'Swab_result', 'Status', 'Smoker', 'Status_on_day_collection', 'Status_on_day_collection_summary', 'Days_from_onset', 'Site', 'time_after_LPS', 'Worst_Clinical_Status', 'Outcome', 'patient_id'; var: 'feature_types'; uns: 'hvg', 'leiden', 'neighbors', 'pca', 'umap'; obsm: 'X_pca', 'X_pca_harmony', 'X_umap'; layers: 'raw'; ```. After this, when I tried running the command:; `sc.pl.highest_expr_genes(adata_orig, n_top=20, )`. I get the following output:; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-c4ab6dadfa42> in <module>; ----> 1 sc.pl.highest_expr_genes(adata_orig ). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\plotting\_qc.py in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 65 ; 66 # compute the percentage of each gene per cell; ---> 67 norm_dict = normalize_total(adata, target_sum=100, inplace=False); 68 ; 69 # identify the genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:1068,layers,layers,1068,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,"After studying the PAGA tutorial, I tried to apply it to my data (40,000 cells, 8,000 highly variable genes). I ran the following commands:; ```python; sc.pp.neighbors(adata_hvg); sc.tl.louvain(adata_hvg); sc.tl.draw_graph(adata_hvg); ```; Till here, everything works nicely, but then I try to get the PAGA representation:. ```python; sc.tl.paga(adata_hvg, groups=""louvain""); ```. This returns the following error:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-249-7cc787ba28f9> in <module>; ----> 1 sc.tl.paga(adata_hvg, groups=""louvain""). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in paga(adata, groups, use_rna_velocity, model, copy); 93 adata.uns['paga'] = {}; 94 if not use_rna_velocity:; ---> 95 paga.compute_connectivities(); 96 adata.uns['paga']['connectivities'] = paga.connectivities; 97 adata.uns['paga']['connectivities_tree'] = paga.connectivities_tree. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in compute_connectivities(self); 127 def compute_connectivities(self):; 128 if self._model == 'v1.2':; --> 129 return self._compute_connectivities_v1_2(); 130 elif self._model == 'v1.0':; 131 return self._compute_connectivities_v1_0(). ~/miniconda3/envs/wot/lib/python3.6/site-packages/scanpy/tools/_paga.py in _compute_connectivities_v1_2(self); 161 if scaled_value > 1:; 162 scaled_value = 1; --> 163 connectivities[i, j] = scaled_value; 164 expected_n_edges[i, j] = expected_random_null; 165 # set attributes. ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/_index.py in __setitem__(self, key, x); 67 if x.size != 1:; 68 raise ValueError('Trying to assign a sequence to an item'); ---> 69 self._set_intXint(row, col, x.flat[0]); 70 return; 71 . ~/miniconda3/envs/wot/lib/python3.6/site-packages/scipy/sparse/compressed.py in _set_intXint(self, row, col, x); 795 def _set_intXint(self, row, col, x):; 796 i, j = self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/695:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695,1,['variab'],['variable']
Modifiability,"After the highly variable genes information was added to .var `pl.pca_loadings` no longer works. This is an example that reproduces the problem:; ```PYTHON; import scanpy.api as sc; import numpy as np; import pandas as pd. N = 1000; M = 2000. adata = sc.AnnData(; X=np.random.random_sample((N, M)); ). sc.pp.filter_genes_dispersion(adata, subset=False); sc.tl.pca(adata); sc.pl.pca_loadings(adata); ```. If ` subset=True` then the pca_loadings works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/315:17,variab,variable,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/315,1,['variab'],['variable']
Modifiability,Also move the `MPLBACKEND=agg` to variable definitions. Threading layer docs: https://numba.pydata.org/numba-doc/latest/user/threading-layer.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933:34,variab,variable,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933,1,['variab'],['variable']
Modifiability,"An implementation of highly deviant gene identification from the 2019 GLMPCA paper. I'm rather fond of the method, as it's a straightforward statistical measure, and comes with significance testing as a form of data-driven cutoff. I put it in a new `highly_deviant_genes()` function, as:; - it comes with a number of unique parameters, and there's only so many different algorithms `highly_variable_genes()` can house; - the paper argues that highly deviant is different from highly variable. I acknowledge that there are no tests, I'm hoping to get some assistance with that if possible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765:483,variab,variable,483,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765,1,['variab'],['variable']
Modifiability,"As discussed in issue #313, score_genes function returns different values on various machines. This is due to using float32 dtype in the `np.nanmean` calls. This PR fixes this behaviour by changing the dtype to float64 in the relevant sections of code ie. functions `gene_score()` and `_sparse_nanmean`. Following the suggestion of @ivirshup the returned value is now also float64. I also adapted the tests `test_add_score` and `test_npnanmean_vs_sparsemean` to use and expect float64.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1890:389,adapt,adapted,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890,1,['adapt'],['adapted']
Modifiability,"As explained in plotting docs; > The color map can also be set individually for each value in adata.obs and adata.var, by setting `adata.uns[""{var}_cmap""]`. The individual values overwrite `color_map`. I think it's very useful when plotting multiple .obs or .var variables using ""color"", as the cmaps can then be defined for each embedding individually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489:263,variab,variables,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489,1,['variab'],['variables']
Modifiability,"As we are refactoring scvi, I'm wondering what the utility would be to spin off the i/o part of scanpy into it's own lightweight package that's more general for reading single cell pipeline outputs into anndata. For example, we'd like to use the scanpy read from 10x/visium functions, but don't necessarily want to have scanpy be a full requirement. It's also a bit confusing why something like `read_umi_tools` is in anndata but not `read_10x_h5`. The same goes for loom to some extent. I could imagine either moving such functionality to anndata or a standalone package that could be expanded to include support for other technologies like scATAC-seq. . This overall could be a big benefit to methods developers who would like to build on anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387:10,refactor,refactoring,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387,1,['refactor'],['refactoring']
Modifiability,Backport PR #1789 on branch 1.7.x (Fix malformed flake8 config file),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1790:56,config,config,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1790,1,['config'],['config']
Modifiability,Backport PR #1789: Fix malformed flake8 config file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1790:40,config,config,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1790,1,['config'],['config']
Modifiability,"Backport PR #2027 on branch 1.8.x (Fix finding variables with sc.pl.scatter(..., use_raw=True))",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2049:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2049,1,['variab'],['variables']
Modifiability,"Backport PR #2027: Fix finding variables with sc.pl.scatter(..., use_raw=True)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2049:31,variab,variables,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2049,1,['variab'],['variables']
Modifiability,Backport PR #3031 on branch 1.10.x (Extend benchmarks from basic tutorial),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3056:36,Extend,Extend,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3056,1,['Extend'],['Extend']
Modifiability,Backport PR #3031: Extend benchmarks from basic tutorial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3056:19,Extend,Extend,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3056,1,['Extend'],['Extend']
Modifiability,Backport PR #3170 on branch 1.10.x (Refactor score_genes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3171:36,Refactor,Refactor,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3171,1,['Refactor'],['Refactor']
Modifiability,Backport PR #3182 on branch 1.10.x (Some refactoring ahead of key_added),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3185:41,refactor,refactoring,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3185,1,['refactor'],['refactoring']
Modifiability,Backport PR #3182: Some refactoring ahead of key_added,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3185:24,refactor,refactoring,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3185,1,['refactor'],['refactoring']
Modifiability,Backport PR #3316 on branch 1.10.x (Refactor regress_out),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3322:36,Refactor,Refactor,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322,1,['Refactor'],['Refactor']
Modifiability,Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3322:19,Refactor,Refactor,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322,1,['Refactor'],['Refactor']
Modifiability,CPUDispatcher error with highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1995:32,variab,variable,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1995,1,['variab'],['variable']
Modifiability,"CT_sub2 = sc.read('C:/Users/Park_Lab/Documents/ACT_sub2.h5ad') # Scanpy proceeded data; ACT_sub2; AnnData object with n_obs × n_vars = 2636 × 5000; obs: 'leiden', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variable]; 2 adata. ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in __getitem__(self, index); 1114 def __getitem__(self, index: Index) -> ""AnnData"":; 1115 """"""Returns a sliced view of the object.""""""; -> 1116 oidx, vidx = self._normalize_indices(index); 1117 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1118 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in _normalize_indices(self, index); 1095 ; 1096 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1097 return _normalize_indices(index, se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:1831,layers,layers,1831,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['layers'],['layers']
Modifiability,"Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does. And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito, cell cycle et al.,) when I execute MNN ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168:8,extend,extend,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,Coloring by boolean variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460:20,variab,variables,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460,1,['variab'],['variables']
Modifiability,Computes a hierarchical clustering for the variables(genes),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2433:43,variab,variables,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2433,1,['variab'],['variables']
Modifiability,"Conda installation fails silently with no error. Installation command:; ```; conda install -c bioconda scanpy; ```. Output:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```. Increasing the verbosity did not help. Using older python version did not helpeither.. It looks like the metadata are not correct but I am not able to validate this. I tried miniconda anaconda clean installs and I had no luck whatsoever. Pip install works fine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298:258,flexible,flexible,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298,2,['flexible'],['flexible']
Modifiability,Could you add Moran's I calculation to Scanpy? It could be used in scIB and to also find variable genes across embedding (could be an alternative to SEMITONES that takes a while to be computed).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698:89,variab,variable,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698,1,['variab'],['variable']
Modifiability,"Currently it’s almost a MVP. What do you people think (especially @mbuttner)? Possible improvements:. - [x] **Heuristic for neighborhood size**; - [x] Expose more data: E.g. a column in `.obs` with the corrected p values; - [ ] **Better docs**; - [ ] **groupby**; - [ ] Mean of multiple samples instead of whole data; - [ ] Adapt to cells that appear in no neighborhoods; - [ ] Speedups?. I guess at least the heuristic and the docs have to go in!. I also don’t know if relying on the previous `neighbors` call is a good idea. Its default is 15 Neighbors, and even for a small (200 cells) dataset, the number the heuristic determined was 75.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364:324,Adapt,Adapt,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364,1,['Adapt'],['Adapt']
Modifiability,"Currently, test collection takes about 11 seconds. This seemed a little long so I played around with the config a bit, and found if all the test files names are prepended with `test_`, and I set `python_files = test_*.py`, collection takes about 1 second. Mind if I make that change?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/326:105,config,config,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/326,1,['config'],['config']
Modifiability,De layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/801:3,layers,layers,3,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/801,1,['layers'],['layers']
Modifiability,"Dear author,. Can bbknn integrate multiple variables？such as Platform and Individual. Looking forward your reply; Siyu. >>> sc.external.pp.bbknn(mydata, batch_key=[""Platform"",""Individual_2""],n_pcs=50,set_op_mix_ratio=ratio). computing batch balanced neighbors; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/scanpy/external/pp/_bbknn.py"", line 134, in bbknn; return bbknn(; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/bbknn/__init__.py"", line 110, in bbknn; if batch_key not in adata.obs:; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py"", line 1721, in __contains__; return key in self._info_axis; File ""/home/scCell2/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 4071, in __contains__; hash(key); TypeError: unhashable type: 'list",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:43,variab,variables,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['variab'],['variables']
Modifiability,"Dear people,. In our plotting functions, vmin and vmax are great arguments, we all love them. First of all, let's all hold our hands, close our eyes and thank matplotlib developers for implementing them. But when we jointly plot some continuous variables that are not on the exact same scale (e.g. some genes), it's not possible to specify a single vmin/vmax value that fits all variables especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/627204",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:245,variab,variables,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,2,['variab'],['variables']
Modifiability,Docs Enhancements,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/58:5,Enhance,Enhancements,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/58,1,['Enhance'],['Enhancements']
Modifiability,"Dotplot / Matrixplot Bug/Suggestion [Key Error] Because ""var_group_labels"" & ""categories_order"" using the same variable (memory), mostly happened when ""swap_axes=True""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3081:111,variab,variable,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3081,1,['variab'],['variable']
Modifiability,Dotplot labels can extend off plot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1705:19,extend,extend,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705,1,['extend'],['extend']
Modifiability,"Enhance scanpy.tl.rank_gene_groups with additional filters (min_pct, etc.,)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3159:0,Enhance,Enhance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3159,1,['Enhance'],['Enhance']
Modifiability,Enhancement,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/627:0,Enhance,Enhancement,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627,1,['Enhance'],['Enhancement']
Modifiability,Enhancement of Parameter Descriptions in MatrixPlot Class Documentation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:0,Enhance,Enhancement,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['Enhance'],['Enhancement']
Modifiability,"Every time I build the docs locally, they do a complete rebuild. This is painfully slow (especially with our examples that run on each build) and really discourages editing the docs. This is happening because the sphinx sees the config being modified. There are two causes of this:. * The version being set dynamically – at each commit the version string changes.; * `scanpydoc.elegant_typehints` sets some properties of the config after it's loaded. E.g.:. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```. ### Solution. Version being set dynamically does really add that much value for us, so I just removed that part of the version string. `scanpydoc.elegant_typehints` does make the doc-strings nicer, but it is not worth a five minute build to update the docs. Ideally it can be implemented in a way that doesn't make sphinx think the config has changed, but I am disabling it until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2199:229,config,config,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2199,4,['config'],['config']
Modifiability,Extend benchmarks from basic tutorial,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3031:0,Extend,Extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3031,1,['Extend'],['Extend']
Modifiability,Extend functionality of tl.rank_genes_groups(),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2317:0,Extend,Extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2317,1,['Extend'],['Extend']
Modifiability,Extended gex_only function to visium. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3113; - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: not a big change,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3278:0,Extend,Extended,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278,1,['Extend'],['Extended']
Modifiability,Extending gex_only option to Visium function,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3278:0,Extend,Extending,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278,1,['Extend'],['Extending']
Modifiability,"File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:2926, in ParforPass.run(self); 2924 # Validate reduction in parfors.; 2925 for p in parfors:; -> 2926 get_parfor_reductions(self.func_ir, p, p.params, self.calltypes); 2928 # Validate parameters:; 2929 for p in parfors:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3549, in get_parfor_reductions(func_ir, parfor, parfor_params, calltypes, reductions, reduce_varnames, param_uses, param_nodes, var_to_param); 3547 if param_name in used_vars and param_name not in reduce_varnames:; 3548 param_nodes[param].reverse(); -> 3549 reduce_nodes = get_reduce_nodes(param, param_nodes[param], func_ir); 3550 # Certain kinds of ill-formed Python (like potentially undefined; 3551 # variables) in combination with SSA can make things look like; 3552 # reductions except that they don't have reduction operators.; 3553 # If we get to this point but don't find a reduction operator; 3554 # then assume it is this situation and just don't treat this; 3555 # variable as a reduction.; 3556 if reduce_nodes is not None:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3637, in get_reduce_nodes(reduction_node, nodes, func_ir); 3635 defs[lhs.name] = rhs; 3636 if isinstance(rhs, ir.Var) and rhs.name in defs:; -> 3637 rhs = lookup(rhs); 3638 if isinstance(rhs, ir.Expr):; 3639 in_vars = set(lookup(v, True).name for v in rhs.list_vars()). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return lookup(val); 3628 else:; 3629 return var if (varonly or val is None) else val. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:10120,variab,variable,10120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['variab'],['variable']
Modifiability,"First, very nice job on this package! It is a welcome replacement to seurat for python programmers! Yay!. I have a suggestion to improve sc.pl.dotplot(). I would like to be able to adjust the limits of the color_map used for sc.pl.dotplot(). The documentation states that kwargs get passed to matplotlib.pyplot.scatter(). But the dotplot does not change when vmin and vmax are included as kwargs. In the source code for anndata.dotplot includes the following lines:. import matplotlib.colors; normalize = matplotlib.colors.Normalize(vmin=min(mean_flat), vmax=max(mean_flat)); colors = [cmap(normalize(value)) for value in mean_flat]. I believe these lines are the issue. Because vmin and vmax are calculated from the mean_flat variable the vmin and vmax provided as parameters are ignored. Perhaps rather than determining the scale using mean_flat, the vmin and vmax should be used when provided by the user. This would allow the user to set the color_bar scale as they need. Currently, there is no way to change the color_bar and the documentation is misleading, since it suggests that it can be done.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/388:727,variab,variable,727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/388,1,['variab'],['variable']
Modifiability,Fix `layers` parameter in `score_genes` with `.raw`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3155:5,layers,layers,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3155,1,['layers'],['layers']
Modifiability,"Fix finding variables with sc.pl.scatter(..., use_raw=True)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:12,variab,variables,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,1,['variab'],['variables']
Modifiability,Fix graph metrics when some variables are constant,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,1,['variab'],['variables']
Modifiability,Fix malformed flake8 config file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1789:21,config,config,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1789,1,['config'],['config']
Modifiability,Fix scatter plots for sparse layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/713:29,layers,layers,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713,1,['layers'],['layers']
Modifiability,Fixes #1646 . Now supports coloring by boolean variables such as `True` and `False`. **Tasks to complete:** . - [x] Add test,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460,1,['variab'],['variables']
Modifiability,"Fixes #1806. New behaviour for Moran's I and Geary's C. If one of the variables passed has constant values, the score for that variable is `nan` and the function warns the user about this. Previously, the presence of this variable would silently fail, corrupting the other outputs as well. Adds a new utility `is_constant` to check if values in an array are constant. * Could have less code repetition, since now there's some logic that is applied to any case which is 2d, but the conditional isn't structured this way.; * Performance hit pretty minor, as computing the metric itself is expensive.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:70,variab,variables,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,3,['variab'],"['variable', 'variables']"
Modifiability,"Fixes #1892. Scipy now returns `np.nan` for Mann-Whitney U tests where there it used to error. Namely, variables for which all values are the same.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1893:103,variab,variables,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1893,1,['variab'],['variables']
Modifiability,"Fixes #2211. I’ll first try with all removed. I expect the min_tests run to fail. Seeing what fails,. - If it’s not much, I’ll just refactor the fixtures a bit and so on; - Else I’ll move algorithms to the `test` extra. Then we can merge this PR and over time refactor our tests so more and more extras go from `tests` to `tests-full`. @ivirshup do you like the collection extras’ names (`io`, `speedups`, `algorithms`)? Should we add an extra named `all` that installs all the `-full` extras?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2222:132,refactor,refactor,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2222,2,['refactor'],['refactor']
Modifiability,"Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version – entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset – 10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>; <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>; <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability; - [ ] Figure out if I should be be scaling the whitening matrix differently; - [ ] More in depth comparison of results with sklearn based ICA; - [ ] Documentation; - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941:756,variab,variable,756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941,1,['variab'],['variable']
Modifiability,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730:605,layers,layers,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730,1,['layers'],['layers']
Modifiability,"Fixes the doctest setup that was broken in https://github.com/scverse/scanpy/pull/2874. The goal here is to make `_modify_doctests` work again without breaking coverage. That means; 1. the plugin can’t be imported in `scanpy/tests` but has to be imported globally; 2. the plugin has to live outside of `scanpy` since importing scanpy while importing it breaks coverage; 1. The plugin can’t import scanpy at the top level (neither `import testing.scanpy` nor `import testing.scanpy._pytest` is allowed to transitively `import scanpy`). having `testing.scanpy` in `src` and `scanpy` in the root of the repo is a bit gross, but better than adding even more top level stuff",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3057:189,plugin,plugin,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3057,3,['plugin'],['plugin']
Modifiability,"Fixes; ```; Traceback (most recent call last):; File ""/tmp/tmptq4o33we/job_working_directory/000/50/configs/tmpe21tizb1"", line 29, in <module>; scale='width'); File ""/usr/local/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 564, in rank_genes_groups_violin; df[g] = X_col; File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item; self._ensure_valid_index(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index; value = Series(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__; data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True); File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array; raise Exception(""Data must be 1-dimensional""); Exception: Data must be 1-dimensional; ```. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669:100,config,configs,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669,1,['config'],['configs']
Modifiability,Following discussions with @giovp I've extended the `scanpy.datasets.visium_sge` function to optionally return a path to the high-resolution tissue image also available in the visium Spatial Transcriptomics datasets.; This makes it easy to leverage `scanpy.datasets` to fully explore visium datasets.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506:39,extend,extended,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506,1,['extend'],['extended']
Modifiability,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358:903,variab,variables,903,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358,1,['variab'],['variables']
Modifiability,Getting Error Variable names are not unique when using .read_10x_h5 function,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/534:14,Variab,Variable,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534,1,['Variab'],['Variable']
Modifiability,"Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing?. ```python; user_input = input(""Enter the path of your file: ""); def convert_h5_to_adata(filename):; filename = str(filename); if os.access(filename, os.R_OK):; sc.read_10x_h5(filename); return; convert_h5_to_adata(user_input); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/534:318,Variab,Variable,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534,1,['Variab'],['Variable']
Modifiability,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438:1393,variab,variables,1393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438,1,['variab'],['variables']
Modifiability,"Hello all,; For these 2 functions,; `sc.pp.filter_cells(adata, min_genes=200)`; `sc.pp.filter_genes(adata, min_cells=3)`; the authors make `inplace=True` as default. Because I want to tranfer the output into an variable, I change these functions to; ```python; a=sc.pp.filter_cells(adata, min_genes=200, inplace=False); sc.pp.filter_genes(a, min_cells=3, inplace=False); ```; but it creates errors and the output of a is NoType:; ```python; aceback (most recent call last):; File “C:\Users\Yuanjian\AppData\Local\Programs\Python\Python36\lib\site-packages\IPython\core\interactiveshell.py”, line 3343, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File “”, line 2, in; sc.pp.filter_genes(a, min_cells=3, inplace=False) # exclude genes only expressed in <3 cells; File “C:\Users\Yuanjian\AppData\Local\Programs\Python\Python36\lib\site-packages\scanpy\preprocessing_simple.py”, line 259, in filter_genes; X if min_cells is None and max_cells is None else X > 0, axis=0; ```. Does anybody know why inplace=False doesn’t work?; Thanks!; Best,; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2030:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030,1,['variab'],['variable']
Modifiability,"Hello scanpy team,. this is my first pull request, I hope I at least vaguely adhered to your contribution guidelines. I did not find an issue tackling this exact situation, but if I overlooked it, feel free to point that out. This PR changes few lines of code involved in getting dimensionality reductions like X_pca. The current code skips subsetting the respective array to the n_pcs requested, if the representation name is not X_pca. I do not think this is optimal. This change allows to only take the dimensions needed also for non X_pca representations. This can be useful when using harmony for example or storing different PCA embeddings in the same object. Then it is not needed to first store them as X_pca to be able to properly use the n_pcs parameter in e.g. pp.neighbors. It might make sense then to adapt the documentation respectively as well. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. ----------. Fixes #1846",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179:814,adapt,adapt,814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179,1,['adapt'],['adapt']
Modifiability,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156:46,variab,variables,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156,1,['variab'],['variables']
Modifiability,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560,1,['variab'],['variable']
Modifiability,"Hello,. In some cases, I need to visualize clustering results (categorical) on UMAP for each batch. . I know `sc.pl.umap(adata[adata.obs['batch] == 'batch1], color = 'louvain')` is a solution. However, other cells are missing. I think the other cells colored by grey as background should be a better way. . I notice that `sc.pl.umap(adata, color = 'batch', groups = ['batch1'] )` can retain other cells as grey, though sometimes cells were submerged in the bottom layer (I used reoder_categories to bypass this issue). But, `color` and `groups` must be correspondence! . Is there any way to fulfill my needs in `Scanpy` if I missed something. ; Or, could the authors add a parameters, such as `restrict_to` in `sc.tl.louvain`, to implement this function: ① liberate strong associations between `color` and `groups`, and ② add support for ordering categorical variable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/759:859,variab,variable,859,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/759,1,['variab'],['variable']
Modifiability,"Hello,; When I call 'dpt_scatter' with the groups parameter I get the following error:; NameError: name 'names' is not defined. It looks like this is from line 230 in scanpy/plotting/ann_data.py and the 'names' variable just doesn't exist.; I'm assuming it should just be 'groups'?. Thanks,; Sarah",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32,1,['variab'],['variable']
Modifiability,"Hi ,; In my case, the seurat object using the sceasy algorithm to transfer into anndata object for trajectory inference analysis.; The code lying below:; import numpy as np; import pandas as pd; import matplotlib.pyplot as pl; from matplotlib import rcParams; import scanpy as sc; sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20); sc.tl.draw_graph(adata); sc.pl.draw_graph(adata, color='paul15_clusters', legend_loc='on data'); The picture showing confused result posted below:; ![Uploading image.png…](). The object information:; >>> adata; AnnData object with n_obs × n_vars = 17885 × 999; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'RNA_snn_res.0.5', 'seurat_clusters', 'pANN_0.25_0.02_752', 'DF.classifications_0.25_0.02_752', 'percent.rp', 'pANN_0.25_0.02_826', 'DF.classifications_0.25_0.02_826', 'group', 'celltype', 'n_counts_all'; var: 'vst.mean', 'vst.variance', 'vst.variance.expected', 'vst.variance.standardized', 'vst.variable', 'n_counts', 'mean', 'std'; uns: 'seurat_clusters_colors', 'log1p', 'pca', 'neighbors', 'draw_graph'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'; varm: 'PCs'; obsp: 'distances', 'connectivities'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1880:1019,variab,variable,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1880,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you!. Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575:190,extend,extend,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575,1,['extend'],['extend']
Modifiability,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468:171,variab,variables,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468,1,['variab'],['variables']
Modifiability,"Hi Fidel, ; Here is the pull request for vmin vmax in dotplot...; I am guessing that there could be similar issues with other plotting functions and other plotting keywords.; In general, it would be best to check plotting methods for plotting keywords and use what is provided in kwds by default rather than setting the variable without checking whether it was provided. ; Thanks for your groups great work!; Tim",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/390:320,variab,variable,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390,1,['variab'],['variable']
Modifiability,"Hi I got this warning when I used sc.pl.scatter:; ```; .../scanpy/plotting/_anndata.py:311: DeprecationWarning: Use obs_vector instead of _get_obs_array, _get_obs_array will be removed in the future.; x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0]); .../anndata/base.py:1618: FutureWarning: In a future version of AnnData, access to `.X` by passing `layer='X'` will be removed. Instead pass `layer=None`.; FutureWarning; ```. I would like to know if the plots generated with this warning are correct.; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/690:256,layers,layers,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/690,1,['layers'],['layers']
Modifiability,"Hi Scanpy devs. Sorry, this isn't an enhancement request, just wan't sure where this fitted. . Just a quick one- when's the next Scanpy release (1.6.0?) scheduled for?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1365:37,enhance,enhancement,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1365,1,['enhance'],['enhancement']
Modifiability,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:1017,flexible,flexible,1017,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,2,"['flexible', 'variab']","['flexible', 'variables']"
Modifiability,"Hi all,. Right now we have two layers in the scanpy API. The top layer consists of the major modules like `pp,pl,tl` as well as the smaller ones like `queries,get,datasets`. In addition, we have some useful functions directly under the scanpy package like `read/read_text/read_mtx` etc. It is obvious that the field is advancing and alternative/better ways to perform fundamental tasks in downstream analysis (e.g. normalization, DE tests, gene selection) are emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:31,layers,layers,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,2,"['extend', 'layers']","['extend', 'layers']"
Modifiability,"Hi all,. this is a re-implementation of the ComBat function in python for batch effect removal by Brent Pedersen at the University of Utah which I slightly modified to work with AnnData objects. I asked Brent for permission and he would be happy with us using this. Originally, the code was written in R for the SVA package:; Jeffrey T. Leek, W. Evan Johnson, Hilary S. Parker, Andrew E. Jaffe; and John D. Storey (). sva: Surrogate Variable Analysis. R package; version 3.4.0. The idea is taken from this paper:; Johnson WE, Rabinovic A, Li C (2007). Adjusting batch effects in microarray; expression data using Empirical Bayes methods. Biostatistics 8:118-127. . Originally, the method was developed to adjust for batch effects in microarray data, however, it is commonly applied to scRNA-seq data nowadays. The method fits linear models to the genes and pools statistical power by means of EB to estimate per gene correction factors. . I understand that @mbuttner also has an implementation of this - maybe we can combine and get the best of both approaches?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398:433,Variab,Variable,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398,1,['Variab'],['Variable']
Modifiability,"Hi authors,. First off, love scanpy. Big fan. . I was just wondering if you have considered including an option in `scanpy.tl.rank_genes_groups` to specify which variables to select for testing, allowing users to select a subset of variables which would or would not be considered in the statistical test. For context, I'm trying to test between groups of cells while ignoring ribosomal / mitochondrial genes, but retain them in the `.var` and `.X` objects for downstream analysis/visualisation. Making a temp object with these variables removed solely for stats testing partially works, but it's confounded by having to further apply the boolean slice to the `.raw` object as well. Thanks, K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1744:162,variab,variables,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1744,3,['variab'],['variables']
Modifiability,"Hi everyone,. Seeing how many new single cell and spatial tools are being developed in Python, and how we are increasingly using it in general and scanpy in particular, at saezlab we decided to re-implement our tools to estimate pathways and Transcription factor (TF) activity ([Dorothea](https://saezlab.github.io/dorothea/) and [Progeny](https://saezlab.github.io/progeny/)) in it. Here's a first draft in Python of our tools:; https://github.com/saezlab/dorothea-py; https://github.com/saezlab/progeny-py. Our tools take gene expression as input and generate matrices of TF and pathway activities. They can be understood as: ; 1) Prior-knowledge dimensionality reduction methods (`obsm`). Examples of usage:; 	* Used as input for NN; 	* Used as input for integration methods; 2) New data assays (`X`). Examples of usage:; 	* Plot feature activities in projections such as PCA or UMAP; 	* Plot feature activities in heat-maps, clustermaps, violin plots, etc; 	* Differences between groups can be modeled to find significant differences. Because of this duality, the integration of our tools into scanpy is not straightforward. If we store the activities in `obsm` they can be used as a dimensonality reduction embedding but then we lose acces to all the fantastic plotting functions based on `X`. Then if we add add our activities to `X`, they have a very different distribution than gene expression plus there would be an overlap of names between genes and TFs. A solution to this would be to have a separate `.layer` to store this matrices but layers must contain the same dimensions as `X`. Another workaround would be to store it in `.raw` but then we force the user to use remove its previous contents, plus it is used in some methods as default which could cause problems. . What would be a smart solution to integrate our tools in your universe?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:1548,layers,layers,1548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,1,['layers'],['layers']
Modifiability,"Hi everyone,. a while back, @giovp asked me to create a pull request that integrates analytical Pearson residuals in scanpy. We already discussed a bit with @LuckyMD and @ivirshup over at berenslab/umi-normalization#1 how to structure it, and now I made a version that should be ready for review. As discussed earlier, this pull request implements two core methods:; - `sc.pp.normalize_pearson_residuals()`, which applies the method to `adata.X`. Overall, the function is very similar in structure to `sc.pp.normalize_total()` (support for layers, inplace operation etc).; - `sc.pp.highly_variable_genes(flavor='pearson_residuals')`, which selects genes based on Pearson residual variance. The ""inner"" function `_highly_variable_pearson_residuals()` is structured similarly to `_highly_variable_seurat_v3()` (support for multiple batches, median ranks for tie breaking). It includes the `chunksize` argument to allow for memory-efficient computation of the residual variance. We discussed quite a lot how to implement a third function that would bundle gene selection, normalization by analytical residuals and PCA. This PR includes the two options that emerged at the end of that discussion, so now we have to choose ;). - `sc.pp.recipe_pearson_residuals()` which does HVG selection and normalization both via Pearson residuals prior to PCA; - `sc.pp.normalize_pearson_residuals_pca()` which applies any HVG selection if the user previously added one to the `adata` object, and then normalizes via Pearson residuals and does PCA. Both functions retain the raw input counts as `adata.X` and add fields for PCA/Normalization/HVG selection results (or return them) as applicable, most importantly the `X_pca` in `adata.obsm['pearson_residuals_X_pca']`. I hope this addresses some of the issues we discussed over at the other repo in a scanpy-y way. Let me know what you think and where you think improvements are needed!. Cheers, Jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715:540,layers,layers,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715,1,['layers'],['layers']
Modifiability,"Hi guys,; I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py; loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. However i would like to open it with scanpy by:. ```py; loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. and i get the following error:. ```pytb; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\L",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:921,layers,layers,921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,2,['layers'],['layers']
Modifiability,"Hi, . Thanks for developing a good tool for analyzing scRNAseq data. In the process of learning scRNAseq analysis with scanpy I have come across a few places in the documentation that left me a bit confused. . The most confusing is that I could not find a description for the variable `n_genes_by_counts` calculated by the function scanpy.pp.calculate_qc_metrics and mentioned in the tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. . I asked a question about this in [the discourse](https://scanpy.discourse.group/t/clarification-of-qc-metrics/295) and was asked to open a github issue. . I guess an explanation for the docs could be something like `n_genes_by_counts: The number of genes with at least 1 count in a cell. Calculated for all cells.` . Since I am writing this I might add that I did not understand exactly how the normalization function [scanpy.pp.normalize_total](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_total.html) works before I stumbled upon the description of the deprecated function with the same purpose: [scanpy.pp.normalize_per_cell](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_per_cell.html) . The deprecated function has a helpful line saying: `Normalize each cell by total counts over all genes, so that every cell has the same total count after normalization.` which at least helped me clear things up. Maybe this line should be added to the new version of the function?. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434:276,variab,variable,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434,1,['variab'],['variable']
Modifiability,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:505,variab,variable,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"Hi, @falexwolf ; Do you have any specific things in mind for `rank_genes_groups` refactoring? What should be done?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723:81,refactor,refactoring,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723,1,['refactor'],['refactoring']
Modifiability,"Hi, I have a Seruat processed dataset, of which I wanted to use scVI for integration. I stored the raw count and cell information then assembled them in scanpy as anndata via method mentioned: `https://smorabit.github.io/tutorials/8_velocyto/`. . So I could get batch HVG function to work without specifying flavor, however, I couldn't get it to work with specifying `flavor=""seurat_v3""`, I wonder how important it is to set this parameter and why does it not work for me?. Thanks a lot!. ```; adata; AnnData object with n_obs × n_vars = 73998 × 13639; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'barcode', 'Celltype2', 'Clusters', 'Sample'; uns: 'log1p'; layers: 'counts'. for i in adatas:; i.layers['counts'] = i.X; adata = ad.concat(adatas); adata.obs_names_make_unique; sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; flavor=""seurat_v3"",; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [197], line 1; ----> 1 sc.pp.highly_variable_genes(; 2 adata_new,; 3 flavor=""seurat_v3"",; 4 layer=""counts"",; 5 batch_key=""Sample"",; 6 subset=True; 7 ); 8 adata_new. File ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:422, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 416 raise ValueError(; 417 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 418 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 419 ); 421 if flavor == 'seurat_v3':; --> 422 return _highly_variable_genes_seurat_v3(; 423 adata,; 424 layer=layer,; 425 n_top_genes=n_top_genes,; 426 batch_key=batch_key,; 427 check_values=check_values,; 428 span=span,; 429 subset=subset,; 430 inplace=inplace,; 431 ); 433 if batch_key is None:; 434 df = _highly_variable_genes_single_batch(; 435 adata,; 436 layer=layer,; (...); 443",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427:662,layers,layers,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427,2,['layers'],['layers']
Modifiability,"Hi, I was doing a dataset integration on quite some datasets. . ```py; adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7] #each is an adata object from scRNA-seq. for i in adatas:; i.layers['counts'] = i.X. adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7]. adata = ad.concat(adatas); adata.obs_names_make_unique. sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; layer=""logcounts"",; batch_key=""Sample"",; subset=True; ). scvi.model.SCVI.setup_anndata(adata, layer=""counts"", batch_key=""Sample""). vae = scvi.model.SCVI(adata, n_hidden=256); vae.train(); adata.obsm[""X_scVI""] = vae.get_latent_representation(); sc.pp.neighbors(adata, use_rep=""X_scVI""); from scvi.model.utils import mde; import pymde; adata.obsm[""X_mde""] = mde(adata.obsm[""X_scVI""]); adata.obsm[""X_normalized_scVI""] = vae.get_normalized_expression(); adata.write_h5ad('Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:171,layers,layers,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['layers'],['layers']
Modifiability,"Hi, the documentation describes running MNN batch correction on a set of highly variable genes - how many genes are recommended? . * I tried running with the top 10,000 variable genes. Is this a good number. * Also, does selecting for these highly variable genes effectively permanently filter the dataset for only these genes?. * I noticed that the minimum gene expression level for some genes was less than zero after batch correction, is this expected?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449:80,variab,variable,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449,3,['variab'],['variable']
Modifiability,"Hi, this is a PR with some changes to Hashsolo, mainly the docstring:; - Rewrite some param descriptions for brevity + clarity; - Explicitly list the obs keys being added to adata; - Copy input adata if inplace=False",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2429:73,Rewrite,Rewrite,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2429,1,['Rewrite'],['Rewrite']
Modifiability,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/318:90,variab,variable,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318,2,['variab'],['variable']
Modifiability,"Hi,. I got negative values after run score_genes with log1P or counts layer like below:; using layers['counts']:; ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:; ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3297:95,layers,layers,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297,2,['layers'],['layers']
Modifiability,"Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me?. Thanks,; Jphe. ```py; adata_raw = adata.copy(); df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None); genes = list(df[0]); genes = [k for k in genes if k in adata.var.index ]; adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20); sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata; ```; ```; AnnData object with n_obs × n_vars = 53165 × 1080 ; obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'; var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap'; varm: 'PCs'; layers: 'counts'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/817:1284,layers,layers,1284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817,1,['layers'],['layers']
Modifiability,"Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2563:155,refactor,refactored,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563,1,['refactor'],['refactored']
Modifiability,"Hi,. Since the update I get this TypeError when running `sc.pp.normalize_total`. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [4], in <cell line: 3>(); 1 import scanpy as sc; 2 adata = sc.datasets.pbmc3k(); ----> 3 sc.pp.normalize_total(adata, target_sum=1e4). File ~/my-conda-envs/sc2022-multiomics/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:200, in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 197 if key_added is not None:; 198 adata.obs[key_added] = counts_per_cell; 199 _set_obs_rep(; --> 200 adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer; 201 ); 202 else:; 203 # not recarray because need to support sparse; 204 dat = dict(; 205 X=_normalize_data(X, counts_per_cell, target_sum, copy=True),; 206 norm_factor=counts_per_cell,; 207 ). File ~/my-conda-envs/sc2022-multiomics/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:25, in _normalize_data(X, counts, after, copy); 23 if issubclass(X.dtype.type, (int, np.integer)):; 24 X = X.astype(np.float32) # TODO: Check if float64 should be used; ---> 25 if isinstance(counts, DaskArray):; 26 counts_greater_than_zero = counts[counts > 0].compute_chunk_sizes(); 27 else:. TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union; ```. I've checked that obs_names, var_names, obs columns names are all unique. Any clue how to solve?. Thanks!. #### Versions. <details>. -----; anndata 0.7.8; scanpy 1.9.0; -----; PIL 9.1.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; entrypoints 0.4; executing 0.8.3; google NA; h5py 3.6.0; hypergeom_ufunc NA; ipykernel 6.12.1; jedi 0.18.1; job",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2210:677,layers,layers,677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2210,1,['layers'],['layers']
Modifiability,"Hi,. This is for #1876 ; I added a new variable `groupby_expand` to dotplot and baseplot to allow the function using two variables from groupby as x and y axis. ; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi,. Using Seurat, in their variable gene function I've had some success using the `equal_frequency` option, where each bin contains an equal number of genes. Would it possible to implement this option in scanpy? . If you'd like I could submit a PR to implement this feature. I think it could be as simple as using `pd.qcut` instead of `pd.cut` or you could use a similar style as in the `cell_ranger` flavor with `pd.cut(df['mean'], np.r_[-np.inf,; np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])`. I don't know how useful it would be, but I could also add the option to have more bins in the `cell_ranger` flavor by replacing `np.arange(10,105,5)` with `np.linspace(10, 100, n_bins - 1)`. Best,; David",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415:28,variab,variable,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415,1,['variab'],['variable']
Modifiability,"Hi,; I couldn't import scanpy due to an error: DLL load failed. I have checked pre-existing issues, but all of them seem to be an h5py issue. My error report seems different from them.; ```; >>> import scanpy as sc; D:\Anaconda\lib\site-packages\dask\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.; data = yaml.load(f.read()) or {}; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\__init__.py"", line 36, in <module>; from . import tools as tl; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\__init__.py"", line 17, in <module>; from ._sim import sim; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\readwrite.py"", line 10, in <module>; import tables; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found. >>> print(sys.version); 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468:251,config,config,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468,1,['config'],['config']
Modifiability,"Hi,; I have two levels in my ```groupby``` variable and was trying to find the differential expression genes between the two levels using the wilcoxon rank sum test. The following two commands will give different pvals and slightly different logfoldchanges; ```; sc.tl.rank_genes_groups(adata, groupby='status', groups=['ALS'], reference='ctrl', n_genes=100000, method='wilcoxon', use_raw=False); sc.tl.rank_genes_groups(adata, groupby='status', groups=['ctrl'], reference='ALS', n_genes=100000, method='wilcoxon', use_raw=False); ```; ```; 	gene	logfoldchanges_ALS_ctrl	pvals_ALS_ctrl	logfoldchanges_ctrl_ALS	pvals_ctrl_ALS; 0	SLC11A1	2.9489155	5.91E-75	-2.9489155	2.08E-73; 1	NEAT1	1.1250153	5.11E-66	-1.1250151	6.82E-64; 2	FKBP5	2.7334108	8.94E-47	-2.7334108	1.78E-45; 3	SPP1	2.1242297	2.27E-42	-2.1242297	2.69E-41; 4	FCGR3A	2.6661332	6.95E-40	-2.6661332	5.37E-39; 5	HAMP	5.394592	1.27E-37	-5.394592	2.27E-36; 6	CD163	3.0886266	9.11E-36	-3.0886264	1.71E-34; 7	RASSF4	2.3211384	2.83E-34	-2.3211384	3.74E-33; 8	DSE	2.8529236	7.43E-33	-2.8529236	7.86E-32; 9	MAFB	2.7013724	3.67E-32	-2.7013724	6.43E-31; 10	DENND3	1.4753485	5.13E-29	-1.4753484	1.19E-27; 11	APOE	1.4111803	1.12E-28	-1.4111804	9.04E-28; 12	C1QB	1.5169998	3.53E-27	-1.5169998	1.68E-25; 13	C3	1.3675922	1.05E-25	-1.3675922	2.62E-25; ```; Am I not doing it right, or because of the tie issue mentioned here?; #698 ; Thanks for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/754:43,variab,variable,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754,1,['variab'],['variable']
Modifiability,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/342:251,layers,layers,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342,3,['layers'],['layers']
Modifiability,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645:1254,variab,variables,1254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645,1,['variab'],['variables']
Modifiability,"Hi,; Is it necessary to use only high variable genes for the downstream analysis ?; If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:38,variab,variable,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,3,['variab'],['variable']
Modifiability,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/188:24,variab,variable,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188,2,['variab'],['variable']
Modifiability,"Hi,; Thank you for your amazing package!. I ran ""sc.tl.rank_genes_groups"" on Jupyter Notebook, and when I did: . `pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names', 'pvals_adj', 'logfoldchanges']})`. I was only able to see 0.0 for p-values and adjusted p-values for all of the 2,000 highly variable genes, while logfoldchanges showed 6 decimal places like 1.816276. . The version of Scanpy that I am using is 1.7.2, and I was wondering if there was a way to see more decimal places for p-values and adjusted p-values, like in the form of 3.642456e-222 in your tutorial. If this is not a formatting problem, do you think the Wilcoxon test gave me 0.0 for all the highly variable genes? ; Based on sc.pl.rank_genes_groups, the scores for 25 genes per cluster are pretty high though (most of them are in a range of 100-200). I am pretty new to python and scanpy, so your advice will be greatly helpful.; Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1869:339,variab,variable,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1869,2,['variab'],['variable']
Modifiability,Highly variable genes for sparse dataset in backed mode,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764,1,['variab'],['variable']
Modifiability,How to add adata.layers information for running scvelo?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/342:17,layers,layers,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342,1,['layers'],['layers']
Modifiability,How to control/normalize for variability among samples?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2155:29,variab,variability,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2155,1,['variab'],['variability']
Modifiability,How to use stacked_violin with variable y-axis limits between rows?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386:31,variab,variable,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386,1,['variab'],['variable']
Modifiability,"I added UMAP support for visualization \o/ Here is how MNIST ""single cells"" look like:. ![image](https://user-images.githubusercontent.com/1140359/36549038-bee9d1c4-17bf-11e8-9383-19a70c9ee018.png). ![image](https://user-images.githubusercontent.com/1140359/36549046-c74cbcb4-17bf-11e8-9d8f-595dc7be3e8c.png). I'm not so familiar with code sytle and variable naming etc. yet, and I haven't fully tested things like additional umap kwargs ~and 3d visuazliation~ etc. but let's keep PR here and resolve things along the way.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/92:350,variab,variable,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/92,1,['variab'],['variable']
Modifiability,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:296,variab,variable,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['variab'],['variable']
Modifiability,"I am trying to get the highly variable genes for a data set. The data set was normalized by fitting a negative binomial model and using the residuals as expression levels. This gives mean gene expression values that can be negative and are very close to 0. When I use the command:; `disp_filter = sc.pp.filter_genes_dispersion(adata.X, min_mean=0, min_disp=0.5)`; I get very few differentially expressed genes. Looking at the dispersions via `disp_filter['dispersions']` shows that many dispersions appear to be NaN. And superficial inspection shows that the genes with negative means have NaN dispersions. This feels like it shouldn't be the case. It is possible to calculate the variance for the genes that have NaN dispersions. Are all negative dispersion values cast to NaN?. Changing the 'mean_mean' parameter to a negative value changes nothing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172,1,['variab'],['variable']
Modifiability,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275:629,variab,variables,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275,3,['variab'],['variables']
Modifiability,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63:128,variab,variable,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63,1,['variab'],['variable']
Modifiability,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:383,variab,variables,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variables']
Modifiability,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762:475,layers,layers,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762,3,['layers'],['layers']
Modifiability,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:623,layers,layers,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,5,['layers'],['layers']
Modifiability,"I have an adata object and multiple CD4+ and CD8+ cells as observations and gene expression as variables. I want to draw a PCA with each dot corresponding to one cell and colored by cell type (e.g. all CD4 cells in blue, CD8 in orange). Each cell have a unique identifier, and I have a list cd4, which contains identifiers of only CD4 cells, and the same for CD8. I tried the following:; `scn.pl.pca(adata_needed, groups={""cd4"":cd4, ""cd8"":cd8}, palette = [""red"", ""blue""])`; But the result is the PCA plot in grey; ![image](https://user-images.githubusercontent.com/45490688/49294731-db036400-f4c4-11e8-9b8f-00b3d20e5c41.png). I also tried to visualize only one sample:; `scn.pl.pca(adata_needed, groups=[""SRR1551000""])`. But got exactly the same result. I decided to use ""color"" parameter in pca plot:; `scn.pl.pca(adata_needed, color=""SRR1551000"")`; But got the following mistake:. > ValueError: key 'SRR1551000' is invalid! pass valid observation annotation, one of [] or a gene name Index(['ENSG....', 'ENSG...', ). The observation names are there (adata_needed.obs_names outputs the observation I try to give to the function). Here's the plot I need, but drawn with pandas. ; ![image](https://user-images.githubusercontent.com/45490688/49296023-3be06b80-f4c8-11e8-8a21-f1aa9e9b4a46.png). How can I draw a PCA plot like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383:95,variab,variables,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383,1,['variab'],['variables']
Modifiability,"I have an anndata object like this . > adata_all:; > AnnData object with n_obs × n_vars = 10000 × 14; > obs: 'sample', 'batch', 'condition'; > var: 'n', 'channel', 'marker', '$PnB', '$PnG', '$PnE', 'signal_type', '$PnR-0', '$PnR-1', '$PnR-2', 'AB'; > uns: 'meta', 'neighbors', 'pca', 'sample_colors', 'umap', 'condition_colors'; > obsm: 'X_pca', 'X_umap', 'X_tsne'; > varm: 'PCs'; > layers: 'original'; > obsp: 'connectivities', 'distances'. The conditions are as follow: conditions = ['a', 'b', 'c'].; How can I draw tSNEs for each marker separated by each condition in a row? As you can see condition is a feature of obstacles and marker is a feature of variables. I want to plot tSNEs for each marker in three different tSNEs based on conditions. Is this possible?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2464:383,layers,layers,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2464,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/10:1132,variab,variable,1132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10,1,['variab'],['variable']
Modifiability,"I noticed that `sc.pl.paga` function has this piece of code. ```py; # compute positions; if pos is None:; adj_tree = None; if layout in {'rt', 'rt_circular', 'eq_tree'}:; adj_tree = adata.uns['paga']['connectivities_tree']; pos = _compute_pos(; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb; >>> sc.pl.paga(data, color='leiden'); ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-138-ed5614508f4e> in <module>(); ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 443 adj_tree = adata.uns['paga']['connectivities_tree']; 444 pos = _compute_pos(; --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); 446 ; 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/483:1674,variab,variable,1674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483,1,['variab'],['variable']
Modifiability,"I ran the newest Scanpy package's ; ```; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,; batch_key='batch'); ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935:251,variab,variable,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935,1,['variab'],['variable']
Modifiability,"I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. I’m less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828:258,layers,layers,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828,1,['layers'],['layers']
Modifiability,"I think we should introduce a standardized “mask” argument to scanpy functions. This would be a boolean array (or reference to a boolean array in `obs`/ `var`) which masks out certain data entries. This can be thought of as a generalization of how highly variable genes is handled. As an example:. ```python; sc.pp.pca(adata, use_highly_variable=True); ```. Would be equivalent to:. ```python; sc.pp.pca(adata, mask=""highly_variable""); # or; sc.pp.pca(adata, mask=adata.obs[""highly_variable""]); ```. One of the big advantages of making this more widespread is that tasks which previously required using `.raw` or creating new anndata objects will be much easier. Some uses for this change:. ### Plotting. A big one is plotting. Right now if you want to show gene expression for a subset of cells, you have to manually work with the Matplotlib Axes:. ```python; ax = sc.pl.umap(pbmc, show=False); sc.pl.umap(; pbmc[pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells',])],; color=""LDHB"",; ax=ax,; ); ```. If a user could provide a mask, this could be reduced, and would make plotting more than one value possible:. ```python; sc.pl.umap(; pbmc,; color=['LDHB', 'LYZ', 'CD79A’],; mask=pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells’,]),; ); ```. ### Other uses. This has come up before in a few contexts:. * Performing normalization on just some variables https://github.com/scverse/scanpy/issues/2142#issuecomment-1046729522; * Selecting a subset of variables for DE tests: https://github.com/scverse/scanpy/issues/1744; * See also https://github.com/scverse/scanpy/issues/748; * Changing use_raw https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988. ## Implementation. I think this could fit quite well into the `sc.get` getter/ validation functions (https://github.com/scverse/scanpy/issues/828#issuecomment-560072919).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234:255,variab,variable,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234,3,['variab'],"['variable', 'variables']"
Modifiability,"I was looking over normalize_total and saw some strange behaviour. Since it's such a common function, I think it's important that it has standard scanpy behaviour. To this end, this PR looks at cleanup up it's code. ### Addition. `layer` argument. A specific layer can now be normalized by itself. ### Deprecations. I've deprecated the `layers` and `layer_norm` argument. Normalizing multiple layers at once seems less useful than normalizing a specific layer. These seem like very specific use cases that are easy for user's to implement themselves, and are not common patterns in scanpy functions. ### TODO:. - [x] Tests for deprecations ; - [x] Scheduling of deprecations (deprecate in 1.8, remove in 1.9)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1667:337,layers,layers,337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1667,2,['layers'],['layers']
Modifiability,"I was trying to get top 1000 variable genes in 1.3M dataset, but every time I ended up with zero genes after the `sc.pp.filter_genes_dispersion(adata, n_top_genes=1000)` call. The reason is that `sc.pp.filter_genes_dispersion(adata, n_top_genes=x)` actually returns `x - num_zero_expression_genes` genes instead of x, where num_zero_expression_genes represents number of genes without any expression. . Here is a small reproducible example:. ![image](https://user-images.githubusercontent.com/1140359/38215015-9f3de66e-36c6-11e8-8c96-9c9a6458741d.png). It's easy to fix with a prior `sc.pp.filter_genes(adata, min_counts=1)` call, but I think filter_genes_dispersion should retrieve n_top_genes, regardless of presence of zero expression genes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/115:29,variab,variable,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/115,1,['variab'],['variable']
Modifiability,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:333,variab,variable,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['variab'],['variable']
Modifiability,"I was wondering if someone who is familiar with sc.pp.regress_out could confirm the following:; I would like to regress out nonlinear effect, e.g. ~1 + a + a^2 + a^3, where a is non-categorical variable. I have looked at the code of regress_out: https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L677; It seems that the code performs the fitting for all specified variables at once, but I am not sure:; https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L701; If the design passed to GLM is combined of all keys passed to the function then I could just create the necessary columns a, a^2, a^3 and pass this as keys. Can someone confirm if I understand this correctly and passing the polynomial columns will do the fitting of a polynom?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839:194,variab,variable,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839,2,['variab'],"['variable', 'variables']"
Modifiability,"I would like to use stacked_violin plot with variable y-axis limits, particularly when swap_axes=True. Examples [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c), particularly code in line 7, show this. How do I do this? When I use it now with my code, it always chooses a uniform y-axis limit for all genes. Which option do I use for variable y-axis limits? Maybe this aspect of scanpy.api.pl.stacked_violin() should be better documented.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386:45,variab,variable,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386,2,['variab'],['variable']
Modifiability,"I'm new to scanpy, and I want to plot umap with some genes. In this tutorial, it's written below. > sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP']). But, I could show only highly variable genes, because other genes were discarded by the code below. > adata = adata[:, adata.var.highly_variable]. So, how can I plot umap with genes without highly variable? How can I leave all genes in anndata?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2098:181,variab,variable,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2098,2,['variab'],['variable']
Modifiability,"I'm not sure if `sc.write` is forgotten or left out on purpose. Leaving it out make sense as it only offers `sc.write('file.csv', adata)` over `adata.write()`. Alternatively, extending `sc.write` functionality to loom and zarr and keeping it in the new API might make it more useful. I was using it just to make the code more symmetric :) i.e. `sc.read` and `sc.write`, but I don't mind if it's removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/423:175,extend,extending,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423,1,['extend'],['extending']
Modifiability,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478:474,variab,variable,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478,1,['variab'],['variable']
Modifiability,"I'm using scanpy 1.8.2, anndata 0.8.0 and h5py 3.1.0. I got this error while reading an h5ad file:. ```; adata=sc.read_h5ad('XXXX.h5ad'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 if zarr and isinstance(elem, (zarr.Group, zarr.Array)):; --> 156 parent = elem.store # Not sure how to always get a name out of this; 157 elif isinstance(elem, SparseDataset):. /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group). /opt/conda/lib/python3.8/enum.py in __getitem__(cls, name); 386 def __getitem__(cls, name):; --> 387 return cls._member_map_[name]; 388 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-15-a2632df74a34> in <module>; ----> 1 adata=sc.read_h5ad('XXXX.h5ad'). /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). /opt/conda/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 parent = elem.file.name; 161 return parent; --> 162 ; 163 ; 164 def report_read_key_on_error(func):. AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:1606,layers,layers,1606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['layers'],['layers']
Modifiability,"I've been working on scRNA from different patients (diseased and healthy) control and was wondering what is the best approach to concatenate these data? It seems like inner join is usually recommended, but given significant individual variability (especially disease versus healthy states) there would be a large proportion of genes lost. Clustering also does not have good result. So I was wondering in this situation, shall I use outer join and fill value to zero? . Another issue is memory use, I'm running this on google colab, and even using TPU, either using combat for batch correction after concatenation or concatetating two subsets of data after batch correction, would take much RAM that it just crashes (there're about 38K cells). Are there any way to limit memory use in this kind of situation? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431:235,variab,variability,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431,1,['variab'],['variability']
Modifiability,"If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works!. I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587:626,plugin,plugin,626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587,1,['plugin'],['plugin']
Modifiability,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/700:254,layers,layers,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700,2,['layers'],['layers']
Modifiability,"In my preprocessing pipeline, I filtered for the most variable genes and also regress out n_counts and cell cycle. However, it seems like every time I restart my Jupyter notebook and rerun the preprocessing pipeline, I get a different neighborhood graph and UMAP, and therefore different clustering. If I use the same preprocessed data, I can reproduce the same PCA (using svd_solver='arpack'), UMAP, and clusters. So I was wondering if there is randomness introduced during filtering process or regress_out methods? What should I do to ensure reproducibility?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313:54,variab,variable,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313,1,['variab'],['variable']
Modifiability,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178:106,flexible,flexible,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178,1,['flexible'],['flexible']
Modifiability,"In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```; ax = fig.add_subplot(gs[1,0]); ```. but I can't seem to overwrite the default axis labels or add new lines as commands like; ```; ax.set_ylabel('new label'); ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks!. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/979:249,enhance,enhance,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979,1,['enhance'],['enhance']
Modifiability,Inherit layer argument in _highly_variable_genes_single_batch when using sc.pp.highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2183:0,Inherit,Inherit,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2183,1,['Inherit'],['Inherit']
Modifiability,Inherit main requirements,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/898:0,Inherit,Inherit,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898,1,['Inherit'],['Inherit']
Modifiability,Insufficient explanation in docs about n_genes_by_counts variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434:57,variab,variable,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434,1,['variab'],['variable']
Modifiability,"Is there anybody meeting the same error with me?; I used de.test.wald() to do differentially expressed genes analysis with totally 47K cells. ### Minimal code sample (that we can copy&paste without having any data). ```python; test_sf = de.test.wald(; data=adata.layers['counts'],; formula_loc=""~ 1 + disease + size_factors"",; factor_loc_totest=""disease"",; as_numeric=['size_factors'],; gene_names=adata.var_names,; sample_description=adata.obs; ); ```. ```pytb; error: 'i' format requires -2147483648 <= number <= 2147483647; ```. #### Versions. <scanpy==1.7.1 anndata==0.7.5 umap==0.4.6 numpy==1.19.2 scipy==1.5.2 pandas==1.1.3 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3>. #### It seems this error happens when cell amount is over 10K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1874:263,layers,layers,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1874,1,['layers'],['layers']
Modifiability,Issue with regress_out() and tagging rather than removing highly variable genes?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667:65,variab,variable,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667,1,['variab'],['variable']
Modifiability,Issue with repeated variables in rank_genes_groups_stacked_violin,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/252:20,variab,variables,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252,1,['variab'],['variables']
Modifiability,"It is common to store raw counts (=unnormalized) of all measured genes under `adata.raw`, while having normalized and unnormalized expression of a subset of genes (might be only protein coding genes, or all genes except ribosomal and mitochondrial etc) at `adata.X` and `adata.layers['counts']` respectively. This however gives rise to a lot of trouble in plotting since visualizing raw counts is not a great idea due to the dynamic range. It is super annoying to pass `use_raw=False` to a lot of functions. Furthermore, weird `rank_genes_groups` outputs as a result of raw counts might go unnoticed because of this. (happened to me many times). I think there was a discussion somewhere about switching to `use_raw=False` by default in all functions, but this may potentially break things since it's a significant behavior change. This might be reasonable for Scanpy 2.0, but not in 1.x I assume. My suggestion is to have a `use_raw` option under `sc.settings` (i.e. the global `ScanpyConfig` instance) which is `None` by default, and can be set to `False` (e.g. `sc.settings.use_raw=False`) which would then affect all the functions with `use_raw` argument. This way we don't break the behavior but still have a reasonable way to turn this thing off :) . Let me know what you think.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798:277,layers,layers,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798,1,['layers'],['layers']
Modifiability,"It looks like we might not be handling non-expressed genes in all of the highly variable genes implementations. For me this was solved by filtering out genes that were not expressed in any cell!; `sc.pp.filter_genes(adata, min_cells=1)`; If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix. _Originally posted by @LisaSikkema in https://github.com/theislab/scanpy/issues/391#issuecomment-870384617_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1910:80,variab,variable,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1910,1,['variab'],['variable']
Modifiability,"It was configurable with the default `k=10` before, now it uses n_neighbors from `sc.tl.neighbors`.; As discussed with @falexwolf .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1110:7,config,configurable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1110,2,['config'],['configurable']
Modifiability,It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618,1,['variab'],['variables']
Modifiability,KeyError: 'dict' … Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:58,layers,layers,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['layers'],['layers']
Modifiability,Layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236,1,['Layers'],['Layers']
Modifiability,Layers support for PCA and regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2588:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2588,1,['Layers'],['Layers']
Modifiability,Layers to scatter,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/555:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/555,1,['Layers'],['Layers']
Modifiability,"Louvain is being difficult to build since a new setuptools release dropped any python2 compatibility https://github.com/vtraag/louvain-igraph/issues/57. We've largely worked around this in #2063, by making louvain dependent tests optional. However, the paul15 PAGA test is difficult to extract louvian from. It checks hardcoded values based on the results of a louvain clustering. To adapt this test to use leiden, we would have to redo the tutorial and create new results. Or louvain building could be fixed, but the package is deprecated anyways.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2065:384,adapt,adapt,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2065,1,['adapt'],['adapt']
Modifiability,Making `scores` parameterized,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1152:16,parameteriz,parameterized,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152,1,['parameteriz'],['parameterized']
Modifiability,More flexible umap,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1132:5,flexible,flexible,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132,1,['flexible'],['flexible']
Modifiability,New plot function to draw the relations between 2 categorical adata.obs variables?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2797:72,variab,variables,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2797,1,['variab'],['variables']
Modifiability,"New version of `uns` as discussed in https://github.com/theislab/anndata/issues/295#issuecomment-596164456. The reason for setting a dummy `library_id` (as ""0"" for instance) is:; * The `library_id` information is in the molecule_info file [explained here](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/molecule_info). Recover it would require to input an additional file path; * All the 10x data I had a look at, both from the same tissue (so 2 slides from same tissue slice) and from different tissues, have `library_id` entry set to ""0"". So I am not really sure how space ranger set that value but it does not appear to be unique (and therefore not a natural `batch_key` value).; * I think it would be more useful if this value is set according to user choice. Only in the context of `adata.concatenate` it should be modified according to `batch_key`. This is also the only point in the analysis where the `library_id` entry matter.; * I agree with respect to maintaining the tree structure before and after concatenation, so the reason for keeping the `library_id` entry and setting it to a dummy variable by default. Looking forward to hear what you think @ivirshup and if agree I'll go on with PRs for `anndata.concatenate`. . Also, let's keep this `spatial` branch open until we really have (almost) everything up and running for spatial analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1105:1143,variab,variable,1143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105,1,['variab'],['variable']
Modifiability,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583:82,flexible,flexible,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583,1,['flexible'],['flexible']
Modifiability,"One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667,1,['variab'],['variable']
Modifiability,PCA fails with batch highly-variable gene correction,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:28,variab,variable,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,1,['variab'],['variable']
Modifiability,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/201:15,variab,variable,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201,1,['variab'],['variable']
Modifiability,"Pass(state.func_ir,; 297 state.typemap,; 298 state.calltypes,; (...); 304 state.metadata,; 305 state.parfor_diagnostics); --> 306 parfor_pass.run(); 308 # check the parfor pass worked and warn if it didn't; 309 has_parfor = False. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:2926, in ParforPass.run(self); 2924 # Validate reduction in parfors.; 2925 for p in parfors:; -> 2926 get_parfor_reductions(self.func_ir, p, p.params, self.calltypes); 2928 # Validate parameters:; 2929 for p in parfors:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3549, in get_parfor_reductions(func_ir, parfor, parfor_params, calltypes, reductions, reduce_varnames, param_uses, param_nodes, var_to_param); 3547 if param_name in used_vars and param_name not in reduce_varnames:; 3548 param_nodes[param].reverse(); -> 3549 reduce_nodes = get_reduce_nodes(param, param_nodes[param], func_ir); 3550 # Certain kinds of ill-formed Python (like potentially undefined; 3551 # variables) in combination with SSA can make things look like; 3552 # reductions except that they don't have reduction operators.; 3553 # If we get to this point but don't find a reduction operator; 3554 # then assume it is this situation and just don't treat this; 3555 # variable as a reduction.; 3556 if reduce_nodes is not None:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3637, in get_reduce_nodes(reduction_node, nodes, func_ir); 3635 defs[lhs.name] = rhs; 3636 if isinstance(rhs, ir.Var) and rhs.name in defs:; -> 3637 rhs = lookup(rhs); 3638 if isinstance(rhs, ir.Expr):; 3639 in_vars = set(lookup(v, True).name for v in rhs.list_vars()). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return lookup(val); 3628 else:; 3629 return var if (varonly or val is None) else",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:9848,variab,variables,9848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['variab'],['variables']
Modifiability,Plot of ranked genes groups with non-raw data or layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438:49,layers,layers,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438,1,['layers'],['layers']
Modifiability,Plotting config files,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:9,config,config,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,1,['config'],['config']
Modifiability,Possible enhancement: multithreaded (via numba) mann-whitney tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060:9,enhance,enhancement,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060,1,['enhance'],['enhancement']
Modifiability,"Pytest supports two test layouts, in-package and outside-of-package. I prefer the outside-of-package mode outlined here: https://docs.pytest.org/en/6.2.x/goodpractices.html. Scanpy currently mixes test utils with tests, but pytest’s test files (`test_*.py` and `conftest.py`) aren’t Python modules one is supposed to import from. To clean things up, we can refactor scanpy to a in-package structure:. - `pyproject.toml`: add `addopts = ['--import-mode=importlib']` to `[tool.pytest.ini_options]`; - `scanpy/tests/__init__.py` during implementation, make it throw an error on import so we can make sure nobody imports things from there, then delete; - `scanpy/tests/**/__init__.py` delete; - `scanpy/test_utils/` or `scanpy/testing/`; - `__init__.py`: leave empty for now, later add public, documented test utils; - `_private.py` add private test utils that can be imported in our tests, such as the `@needs_somepackage` decorators. Later we can decide if we want to keep the in-package layout or switch to the outside-of-package layout",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225:357,refactor,refactor,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225,1,['refactor'],['refactor']
Modifiability,Quite a simple addition meant to fix a bug when one works with specific layers.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2183:72,layers,layers,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2183,1,['layers'],['layers']
Modifiability,"Ref https://github.com/theislab/scanpy/issues/1698#issuecomment-824634639, cut down in https://github.com/theislab/scanpy/issues/1698#issuecomment-826712541. Basically, `gearys_c`, `morans_i` can run into silent numba parallelization errors when all values for a variable are the same. We should specifically address these cases. As suggested by @Hrovatin, `nan` and a warning is probably sufficient for this case. That said, it may take some work to consistently throw a warning. * Depending on the threading backend, this may error (solving the problem of incorrect values); * We may be able to control this, but how the parallel backend interacts with the `error_model` seems unclear; * I don't think we can throw a warning from numba code, let alone parallel numba code; * It would be nice if we could guarantee a `tbb` or `omp` threading backend, since they seem more consistent, but I personally have not been able to `pip install` these for a bit over a year now. This may be macOS specific.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806:263,variab,variable,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806,1,['variab'],['variable']
Modifiability,Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316:0,Refactor,Refactor,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316,1,['Refactor'],['Refactor']
Modifiability,Refactor score_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3170:0,Refactor,Refactor,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3170,1,['Refactor'],['Refactor']
Modifiability,Refactoring t-test default warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2798:0,Refactor,Refactoring,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2798,1,['Refactor'],['Refactoring']
Modifiability,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/442:501,variab,variables,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442,1,['variab'],['variables']
Modifiability,"Right now, if you specify `groupby` to `sc.tl.dendrogram`, but not `dendrogram_key`, storage (and subsequent retrieval) from `adata.uns` is messed up because `groupby` is converted from `str` --> `list` during computation. To give a more concrete example, if my `groupby` variable was ""cell_subtype"", I would expect it to be stored in `adata.uns` as ""dendrogram_cell_subtype"". However, because of the list conversion it's stored as ""dendrogram_['cell_subtype']"" (shown below). ![image](https://user-images.githubusercontent.com/4998310/96769236-d5f77880-13ac-11eb-947f-3dbcf7069d82.png). This PR attempts to address that. I'm not sure if this is the way you want to go about fixing it, but it's one option. Thanks for the great package!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1465:272,variab,variable,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1465,1,['variab'],['variable']
Modifiability,"Running `sc.pl.paga(adata)` in v1.4 returns an error:; ```; Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>; sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:; ```; sc.pl.paga(adata, layout='rt'); sc.pl.paga(adata, layout='rt_circular'); sc.pl.paga(adata, layout='eq_tree'); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/487:410,variab,variable,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487,1,['variab'],['variable']
Modifiability,Running and saving PCA on different anndata layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301:44,layers,layers,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301,1,['layers'],['layers']
Modifiability,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/811:233,Variab,Variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811,1,['Variab'],['Variable']
Modifiability,"See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3239:52,extend,extending,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239,1,['extend'],['extending']
Modifiability,Set up Azure Pipelines with initial configuration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516:36,config,configuration,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516,1,['config'],['configuration']
Modifiability,Seurat v3 VST highly variable gene method,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993:21,variab,variable,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993,1,['variab'],['variable']
Modifiability,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323:1308,Variab,Variables,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323,1,['Variab'],['Variables']
Modifiability,"So this is possibly related to #1136 (pure speculation 😅 ). Basically, on a Vm with ubuntu 18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:357,flexible,flexible,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,2,['flexible'],['flexible']
Modifiability,Some refactoring ahead of key_added,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3182:5,refactor,refactoring,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3182,1,['refactor'],['refactoring']
Modifiability,"Some refactoring:. - single-source AggType in aggregate tests; - fix warnings in aggregate; - fix logs in pca, umap, and tsne",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3186:5,refactor,refactoring,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3186,1,['refactor'],['refactoring']
Modifiability,Support coloring by boolean variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646,1,['variab'],['variables']
Modifiability,"The CLI option is different from the config option …. no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2537:37,config,config,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537,1,['config'],['config']
Modifiability,"The docs for `sc.pl.scatter` say . > The palette can be a valid ListedColormap name ('Set2', 'tab20', …). but setting `palette` to a string throws an error. ```python; adata = sc.datasets.paul15(); sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""); ```. ```pytb; ... storing 'paul15_clusters' as categorical; Trying to set attribute `.uns` of view, copying.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-698-58a5366a0f70> in <module>; 1 adata = sc.datasets.paul15(); ----> 2 sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; --> 128 return _scatter_obs(**args); 129 if (; 130 (x in adata.var.keys() or x in adata.obs.index). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 273 palettes = [palette for _ in range(len(keys))]; 274 for i, palette in enumerate(palettes):; --> 275 palettes[i] = _utils.default_palette(palette); 276 ; 277 if basis is not None:. TypeError: 'str' object does not support item assignment; ```. I get no error if I use any of `sc.pl.palettes`. I also get no error setting `palette=""Set2""` in `sc.pl.umap`, `sc.pl.draw_graph` etc... #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; MulticoreTSNE NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:779,layers,layers,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['layers'],['layers']
Modifiability,"The function `sc.pl.dotplot()` can be used with processed data or raw data by setting `use_raw=True`. The % cells that express a gene is obtained by counting cells with expression above `expression_cutoff` (which by default is 0.0). . However this is likely to produce a wrong result when using scaled and centered data (i.e. after `pp.scale()` ). Unless I'm missing something, the percentage of cells expressing a gene should only be computed from raw data. . Although this is the default usage of the dotplot function in scanpy notebooks, the default use in Seurat is the opposite, and it seems easy to use `sc.pl.dotplot()` in scaled data without noticing this. Maybe a variable similar to `use_raw` to uncouple the use of raw data for the colormap and dotsize could solve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1361:673,variab,variable,673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361,1,['variab'],['variable']
Modifiability,"The scanpy.read_10X_mtx works well for reading in the STARsolo output matrices, which are based on the CellRanger Outputs. . However, it would be nice to have a function or modification of the read_10X_mtx function (e.g. a boolean for STARsolo velocyto) to automate inputting the velocyto matrices that STARsolo outputs and placing them in the appropriate layers. A boolean switch for filtered versus raw matrices would be a good addition as well.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860:356,layers,layers,356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860,1,['layers'],['layers']
Modifiability,"The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |; |--------|--------|; | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3243:117,variab,variables,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243,2,['variab'],['variables']
Modifiability,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661:1160,extend,extend,1160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661,1,['extend'],['extend']
Modifiability,This PR adds `.layers` support for PCA and regress_out.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2588:15,layers,layers,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2588,1,['layers'],['layers']
Modifiability,"This PR adds extra functionality to `pl.violin` adding the option produce stacked violin plots for each `key` passed. The new optional boolean argument `stripplot` was added to add/remove the stripplot on top of the violin plots. An example image is:. ![image](https://user-images.githubusercontent.com/4964309/41411458-3e105336-6fdd-11e8-8e18-07e49fe7c8d1.png). Similarly, I added `pl.heatmap` that plots variables ordered by an observation as follows:. ![image](https://user-images.githubusercontent.com/4964309/41411511-6996f334-6fdd-11e8-93ff-176b89743d83.png). An example notebook using these visualizations is [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175:406,variab,variables,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175,1,['variab'],['variables']
Modifiability,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/164:374,variab,variables,374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164,2,['variab'],['variables']
Modifiability,"This PR adds some utilities for choosing which values to use for `X` inside scanpy functions. These utilities include a getter and setter for representations of the observations and some checks for testing these functions. This approach has been applied to `sc.pp.scale` and `sc.pp.log1p` as demonstrations, and because that code was in need of a refactor. I think it results in cleaner code with more standardized behaviors. The implementation of `sc.pp.scale` is just a hammered out version of what I commented here: https://github.com/theislab/scanpy/pull/1135#issuecomment-608200735. Still todo:. - [x] Figure out how documentation should work. In particular automatic documentation with type variables; - [x] Figure out tab completion for arguments not in the default/ fallback definition. This PR would:. * Close #1089; * Supercede #1135",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1173:347,refactor,refactor,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1173,2,"['refactor', 'variab']","['refactor', 'variables']"
Modifiability,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528:8,extend,extends,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528,1,['extend'],['extends']
Modifiability,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369:649,variab,variable,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369,2,['variab'],['variable']
Modifiability,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619:972,layers,layers,972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619,1,['layers'],['layers']
Modifiability,This collects new features of version 0.3.1 while extending other issues such as https://github.com/theislab/scanpy/issues/45.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/51:50,extend,extending,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/51,1,['extend'],['extending']
Modifiability,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/572:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572,1,['variab'],['variable']
Modifiability,"This fixes the `UnboundLocalError: local variable 'ig_layout' referenced before assignment` exception that happens in following scenario:. ```; sc.tl.louvain(adata); sc.tl.paga(adata); sc.pl.paga(adata); sc.tl.draw_graph(adata); ```. Since there is no else statement for use_paga check, ig_layout is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/124:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/124,1,['variab'],['variable']
Modifiability,"This fixes:; ```; Traceback (most recent call last):; File ""/private/var/folders/df/6xqpqpcd7h73b6jpx9t6cwhw0000gn/T/tmpn9tl9wf0/job_working_directory/000/2/configs/tmp_r9i0cvx"", line 15, in <module>; sc.pl.dpt_timeseries(; File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 171, in dpt_timeseries; timeseries_as_heatmap(; File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_utils.py"", line 206, in timeseries_as_heatmap; pl.colorbar(shrink=0.5); File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/matplotlib/pyplot.py"", line 2188, in colorbar; raise RuntimeError('No mappable was found to use for colorbar '; RuntimeError: No mappable was found to use for colorbar creation. First define a mappable such as an image (with imshow) or a contour set (with contourf).; ```; I see that in other places plt.colobar is used in this module you're; doing the same thing. I believe this broke in; 64f04d8.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1654:157,config,configs,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654,1,['config'],['configs']
Modifiability,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/471:688,variab,variable,688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471,1,['variab'],['variable']
Modifiability,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:199,variab,variability,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['variab'],['variability']
Modifiability,"This issue references the following line of code:. https://github.com/theislab/scanpy/blob/560bd5d348a502d5152eaf20f5f8bef794562a97/scanpy/plotting/_dotplot.py#L185. The documentation accurately describes the `standard_scale='var'` normalization strategy as ; ""Whether or not to standardize the given dimension between 0 and 1, meaning for each variable or group, subtract the minimum and divide each by its maximum."". Something about this normalization has bothered me for a long time, and I finally realized: it's the subtraction of the minimum value. This subtraction means that the minimum valued dot will have color = 0. Imagine a case with only two `groupby` groupings: healthy and disease. In that case, one of the dots will always have color 0, and the other will have color 1. Totally binary, no matter how close the actual values are. I feel that this kind of normalization is very misleading for scRNA-seq data!. A random example follows:. This image makes it look like these genes are very specific to one tissue or another ---------------; ![image](https://user-images.githubusercontent.com/10214815/112070104-6d6f1c00-8b43-11eb-9944-b113da55a567.png). But in reality, if we had scaled by just dividing by the max (and not first subtracting the min), then we'd see -------; ![image](https://user-images.githubusercontent.com/10214815/112070028-4add0300-8b43-11eb-8317-412b74b274aa.png). which is much more realistic, and much closer to what you'd see if you used a log-scaling instead, and made several different plots for genes expressed at different levels overall. I leave it up for discussion, but I would suggest two types of fixes:; 1. delete the above line 185 (and the other places it shows up...); 2. allow the user to specify a custom normalization function (but change the default to `x / max(x)` instead of `(x - min(x)) / max(x)`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757:345,variab,variable,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757,1,['variab'],['variable']
Modifiability,"This may be related to this issue:; https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)); [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/936:476,parameteriz,parameterized,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936,1,['parameteriz'],['parameterized']
Modifiability,This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543,1,['adapt'],['adapted']
Modifiability,"This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s ; after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3294:301,variab,variable,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294,1,['variab'],['variable']
Modifiability,"This will be the discussion that no one ever wants since developers like us tend to bring our mice and keyboards to fight when it comes to questions like these, but hey I thought at some point it should be had anyways haha. @ivirshup recently introduced black as the formatting standard for Scanpy. I am not a fan of blacks formatting, but the idea of consistent formatting for big open source projects is great! So +1 from me. Anyways, currently black formats with 88 characters per line, which makes, especially with black, for lots and lots of line breaks and encourages bad practices like unspecific short variable names etc. Modern Python programming is not C programming from the 80s. Have a read at Linus rant on the 80 character limit in the Linux kernel and why the Linux kernel does **not** enforce it: https://lkml.org/lkml/2020/5/29/1038 . Applying black with a 120 characters limit removes about 1500 lines. That's 1500 lines that you have to scroll less and in my opinion the result is more readable. What do you think?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1694:610,variab,variable,610,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694,1,['variab'],['variable']
Modifiability,"To not repeat ourselves @ivirshup (I think) suggested this. Let’s see if readthedocs supports this. If so, this should soon be visible: https://icb-scanpy.readthedocs-hosted.com/en/inherit-requirements/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/898:181,inherit,inherit-requirements,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898,1,['inherit'],['inherit-requirements']
Modifiability,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/482:1036,variab,variable,1036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482,1,['variab'],['variable']
Modifiability,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:688,variab,variable,688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,Unable to subset a new adata by highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['variab'],['variable']
Modifiability,Use matplotlib 3.1 and adapt tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1020:23,adapt,adapt,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020,1,['adapt'],['adapt']
Modifiability,"We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/938:33,variab,variable,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938,2,['variab'],['variable']
Modifiability,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271:630,config,config,630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271,2,"['config', 'plugin']","['config', 'plugins']"
Modifiability,"What stays the same:. - `pip install scanpy`; - `pip install . `; - `pip install git+https://...`; - you can install your deps with conda; - you can do a dev install. What changes:. - Please check the [install docs](https://scanpy.readthedocs.io/en/flit-for-isaac/installation.html#development-version), in short:; - `pip install -e .[every,single,extra]` → `flit install -s` for dev installs; - `beni pyproject.toml > environment.yml` for conda; - Extremely simple `flit build` and `flit publish`. Maybe install `keyring` to store your publish password, and you know everything you need to.; - `flit build` doesn’t clutter your dev directory with `build/` and `*.egg-info/` junk, it just creates `dist/scanpy-*{.whl,.tar.gz}`.; - No more obscure stuff nobody understands (MANIFEST.in, package_data, …); - Centralized setup configuration in pyproject.toml instead of spread over multiple files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527:824,config,configuration,824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527,1,['config'],['configuration']
Modifiability,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/171:56,variab,variables,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171,3,['variab'],"['variable', 'variables']"
Modifiability,"When running QC on a single-cell dataset with less than 500 features and leaving argument defaults, an index error is raised. This is because the default for `percent_top` assumes genomics data with 500+ genes. Such a number is not necessarily common for other OMICS, like metabolomics (typically 150-300). ```python; adata = anndata.AnnData(np.random.random(100**2).reshape((100, 100))); sc.preprocessing._qc.describe_obs(adata). Traceback (most recent call last):; File ""~/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3398, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-61-7293f2baee95>"", line 1, in <cell line: 1>; sc.preprocessing._qc.describe_obs(adata); File ""~/conda/lib/python3.8/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""~/conda/lib/python3.8/site-packages/scanpy/preprocessing/_qc.py"", line 397, in top_segment_proportions; raise IndexError(""Positions outside range of features.""); IndexError: Positions outside range of features.; ```; (scanpy==1.9.1). In my case, I can not directly specify `percent_top`, because the ScanPy QC is called from a third-party library, which leaves all defaults. Ideally, defaults for unspecified arguments would be compatible with all inputs that the user specifies explicitly. I have some questions:; - Now as the signature's default is None, is there a way to still make the signature in the documentation show the actual default? Or should I also change the docstring's Params section?; - I chose to have the `if` condition in argument order at the top (before `parallel`). Tell me if you prefer me to merge it with the percent_top condition in the middle (line 116), or to refactor the tuple out as a constant.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2346:1778,refactor,refactor,1778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2346,1,['refactor'],['refactor']
Modifiability,"You already set that, so the config isn’t modified at runtime",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204:29,config,config,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204,1,['config'],['config']
Modifiability,"[sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) uses Pearson residuals from “regularized negative binomial regression,” to correct for the sequencing depth. After regressing out total number of UMIs (and other variables if given) it ranks the genes based on their residual variances and therefore also acts as a HVG selection method. This function replaces `sc.pp.normalize_total` and `sc.pp.highly_variable_genes` and requires raw counts in ``adata.X``.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271:252,variab,variables,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271,1,['variab'],['variables']
Modifiability,"\numba\core\lowering.py in lower_function_body(self); 257 bb = self.blkmap[offset]; 258 self.builder.position_at_end(bb); --> 259 self.lower_block(block); 260 self.post_lower(); 261 return entry_block_tail. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 271 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block); 275 . ~\AppData\Local\Continuum\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 750 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 751 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 752 reraise(type(newerr), newerr, tb); 753 ; 754 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\utils.py in reraise(tp, value, tb); 79 if value.__traceback__ is not tb:; 80 raise value.with_traceback(tb); ---> 81 raise value; 82 ; 83 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4079:36: error: '%.2747' defined with type 'i64' but expected 'i32'; %"".2748"" = icmp eq i32 %"".2746"", %"".2747""; ^. File ""..\..\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 412:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. During: lowering ""id=13[LoopNest(index_variable = parfor_index.264, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:13274,config,config,13274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['config'],['config']
Modifiability,"] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; adata = sc.read_csv(dir + 'GSM5226574_C51ctr_raw_counts.csv.gz').T; import scvi ; sc.pp.filter_genes(adata, min_cells = 10); sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); ```. ```pytb; >>> sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\A",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1115,plugin,plugins,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['plugin'],['plugins']
Modifiability,"].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X value of MALAT1 in cell: (0, 0)	142.0. Run 2: after sc.pp.normalize_total: ; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862. Run 3: normalization, specifing argument layer=None; normalizing counts per cell; finished ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2687,layers,layers,2687,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"__dealloc__; KeyError: 0; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-255f06b48663> in <module>(); 1 adata_backed = sc.read(""tmp.h5ad"", backed=""r""); 2 sc.pl.pca(adata_backed, color=""0""); ----> 3 sc.pl.pca(adata_backed[:, :5], color=""0""). /usr/local/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:1868,layers,layers,1868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,"_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 982 return dp; 983 else:; --> 984 dp.make_figure(); 985 savefig_or_show(DotPlot.DEFAULT_SAVE_PREFIX, show=show, save=save); 986 show = settings.autoshow if show is None else show. ~/github/scanpy/scanpy/plotting/_baseplot_class.py in make_figure(self); 606 mainplot_height = len(self.categories) * category_height; 607 mainplot_width = (; --> 608 len(self.var_names) * category_width + self.group_extra_size; 609 ); 610 if self.are_axes_swapped:. AttributeError: 'DotPlot' object has no attribute 'group_extra_size'; ```. First, what's up with the printed error?. Second, I think subsetting the groups and specifying the order can be done at the same time. This is the behaviour of the `groups` kwarg for variable axis of `sc.pl.rank_genes_groups`. This is also the behaviour of `var_names`. I'd noticed some related behaviour I can't quite remember while fixing up #1529. Noticed this specific case while looking at #1914. #### Versions. <details>; <summary> </summary>. ```python; -----; anndata 0.7.7.dev4+g49739eb; scanpy 1.9.0.dev7+g092376d2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.7.dev4+g49739eb; anyio NA; appnope 0.1.0; argon2 20.1.0; asciitree NA; attr 20.3.0; babel 2.8.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli 1.0.9; certifi 2020.06.20; cffi 1.14.0; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.05.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; fsspec 2021.06.0; google NA; h5py 3.2.1; idna 2.10; igraph 0.9.6; ipykernel 5.5.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.6.0; kiwisolver 1.2.0; le",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1915:2291,variab,variable,2291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1915,1,['variab'],['variable']
Modifiability,"_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> 57 yield (key, self[key]); 58 ; 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_man",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:1151,layers,layers,1151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,1,['layers'],['layers']
Modifiability,"`. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()). In [2]: sc.pp.highly_variable_genes(adata); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-2-7727f5f928cd>"", line 1, in <module>; sc.pp.highly_variable_genes(adata). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=fl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:1442,variab,variables,1442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['variab'],['variables']
Modifiability,"```py; %matplotlib inline; import scanpy; ```. when importing scanpy from a jupyter notebook I get this warning, because apparently scanpy calls `matplotlib.use()`. if at all, it should only do that after checking that no backend is already selected. ```; /home/icb/philipp.angerer/.local/lib/python3.5/site-packages/matplotlib/__init__.py:1401: UserWarning: This call to matplotlib.use() has no effect; because the backend has already been chosen;; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,; or matplotlib.backends is imported for the first time. warnings.warn(_use_error_msg); ```. and this, which isn’t actually a `Warning`, is printed to stdout (why?). ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```. in an interactive notebook or other shell, `sett.savefigs` shouldn’t be automatically set to `True`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16:721,variab,variable,721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16,1,['variab'],['variable']
Modifiability,"```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean""); ```. ```; AnnData object with n_obs × n_vars = 11 × 765; obs: 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; layers: 'mean'; ```. ```python; sc.get.aggregate(adata.obsm[""X_umap""], by=adata.obs[""louvain""].array, func=""mean""); ```. ```; {'mean': array([[ -6.18019123, -6.12846152],; [ -3.10995685, 8.4991954 ],; [ 6.30307056, -2.15245383],; [ -4.72268065, -3.24033642],; [-11.94002487, -5.39480163],; [ -1.39242794, 6.6239316 ],; [ 4.3991326 , -0.16749119],; [ 4.847834 , -9.30549509],; [-10.41891144, -1.15700949],; [ -7.91249486, -4.06782072],; [ 1.12418592, -6.94506866]])}; ```. So it returns an `AnnData` when an `AnnData` is passed, but a dict when a less structured object is passed. This is probably because it's `singledispatched` under the hood, but IDK that this behaviour is great. I think it could make more sense for this to either:. * Always return an `AnnData`; * Throw an error if something other than an AnnData is passed in. A third option is that we document this behaviour, but I generally don't love it. There are other places that we do something like this, i.e. return a different type depending on the input. However, I feel like there's more of a loss of information here and less of an obvious return type. Maybe in future this could get a `return_type: type[AnnData] | type[Dict] | type[xr.Dataset] = AnnData` argument that controls what is returned?. WDYT @ilan-gold @Intron7?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2930:276,layers,layers,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2930,1,['layers'],['layers']
Modifiability,"`log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929:502,adapt,adapt,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929,1,['adapt'],['adapt']
Modifiability,`plot_scatter` throws error when sparse layers used for color,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/700:40,layers,layers,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700,1,['layers'],['layers']
Modifiability,a==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - icu=58.2=he6710b0_3; - importlib_metadata=2.0.0=1; - ipykernel=5.3.4=py37h5ca1d4c_0; - ipython=7.20.0=py37hb070fc8_1; - ipython_genutils=0.2.0=pyhd3eb1b0_1; - ipywidgets=7.6.3=pyhd3eb1b0_1; - jedi=0.17.0=py37_0; - jinja2=2.11.3=pyhd3eb1b0_0; - jpeg=9b=h024ee3a_2; - jsonschema=3.2.0=py_2; - jupyter=1.0.0=py37_7; - jupyter_client=6.1.7=py_0; - jupyter_console=6.2.0=py_0; - jupyter_core=4.7.1=py37h06a4308_0; - jupyterlab_pygments=0.1.2=py_0; - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1; - ld_impl_linux-64=2.33.1=h53a641e_7; - libedit=3.1.20191231=h14c3975_1; - libffi=3.3=he6710b0_2; - libgcc-ng=9.1.0=hdf63c60_0; - libpng=1.6.37=hbc83047_0; - libsodium=1.0.18=h7b6447c_0; - libstdcxx-ng=9.1.0=hdf63c60_0; - libuuid=1.0.3=h1bed415_2; - libxcb=1.14=h7b6447c_0; - libxml2=2.9.10=hb55368b_3; - markupsafe=1.1.1=py37h14c3975_1; - mistune=0.8.4=py37h14c3975_1001; - nb_conda=2.2.1=py37_0; - nb_conda_kernels=2.3.1=py37h06a4308_0; - nbclient,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1994,plugin,plugins-base,1994,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['plugin'],['plugins-base']
Modifiability,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375:2036,variab,variable,2036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375,2,['variab'],['variable']
Modifiability,adapted from https://github.com/scverse/anndata/blob/main/.azure-pipelines.yml. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:0,adapt,adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['adapt'],['adapted']
Modifiability,add option to keep genes and store bool array of highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284:56,variab,variable,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284,1,['variab'],['variable']
Modifiability,added support for individual cmaps for continous variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489:49,variab,variables,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489,1,['variab'],['variables']
Modifiability,alizer 2.0.4 pyhd3eb1b0_0 ; chex 0.1.81 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.2.1 py39hca03da5_0 ; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; cython 3.0.10 pypi_0 pypi; debugpy 1.6.7 py39h313beb8_0 ; decorator 5.1.1 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; dm-tree 0.1.7 py39h2666b31_0 conda-forge; docrep 0.3.2 pyh44b312d_0 conda-forge; et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge; etils 1.6.0 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.1 pypi_0 pypi; executing 0.8.3 pyhd3eb1b0_0 ; flax 0.6.1 pyhd8ed1ab_1 conda-forge; fonttools 4.53.0 py39hfea33bf_0 conda-forge; freetype 2.12.1 hadb7bae_2 conda-forge; fsspec 2024.6.1 pyhff2d567_0 conda-forge; future 1.0.0 pyhd8ed1ab_0 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forge; glib 2.80.2 h535f939_0 conda-forge; glib-tools 2.80.2 h4c882b9_0 conda-forge; glpk 5.0 h6d7a090_0 conda-forge; gmp 6.3.0 h7bae524_2 conda-forge; grpc-cpp 1.46.4 hb15be72_9 conda-forge; gst-plugins-base 1.24.4 h8a8f8c8_0 conda-forge; gstreamer 1.24.4 h430e707_0 conda-forge; h5py 3.11.0 nompi_py39h534c8c8_102 conda-forge; hdf5 1.14.3 nompi_hec07895_105 conda-forge; icu 73.2 hc8870d7_0 conda-forge; idna 3.7 py39hca03da5_0 ; igraph 0.10.13 h762ac30_0 conda-forge; importlib-metadata 7.0.1 py39hca03da5_0 ; importlib_metadata 7.0.1 hd3eb1b0_0 ; importlib_resources 6.4.0 pyhd8ed1ab_0 conda-forge; ipykernel 6.28.0 py39hca03da5_0 ; ipython 8.15.0 py39hca03da5_0 ; ipywidgets 8.1.2 py39hca03da5_0 ; jax 0.3.15 pyhd8ed1ab_0 conda-forge; jaxlib 0.3.15 cpu_py39hb5f911d_3 conda-forge; jedi 0.18.1 py39hca03da5_1 ; jinja2 3.1.4 py39hca03da5_0 ; joblib 1.4.2 pyhd8ed1ab_0 conda-forge; json5 0.9.6 pyhd3eb1b0_0 ; jsonschema 4.19.2 py39hca03da5_0 ; jsonschema-specifications 2023.7.1 py39hca03da5_0 ; jupyter 1.0.0 py39hca03da5_9 ; jupyter-lsp 2.2.0 py39hca03da5_0 ; jupyter_client 8.6.0 py39hca03da5_0 ; jupyter_console 6.6.3 py39hca03da5_0 ; jupyter_core 5.7.2 py39hca03da5_0 ; jupyter_even,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:7976,plugin,plugins-base,7976,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['plugin'],['plugins-base']
Modifiability,allowing dotplot to use two variables in groupby as x and y axis,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055,1,['variab'],['variables']
Modifiability,"an this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:2846,variab,variable,2846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,2,['variab'],['variable']
Modifiability,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/809:3846,variab,variable,3846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809,2,['variab'],['variable']
Modifiability,"aster branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). This is exactly the code on https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html using the dataset downloaded per the instructions. The umap step does not produce the correct result. tsne however produces sensible results.; ```python; import numpy as np; import pandas as pd; import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'). results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results. adata = sc.read_10x_mtx(; 'data/filtered_gene_bc_matrices/hg19/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) . adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`. sc.pl.highest_expr_genes(adata, n_top=20, ). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],; jitter=0.4, multi_panel=True). sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'). adata = adata[adata.obs.n_genes_by_counts < 2500, :]; adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:1208,variab,variable,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"atching, some corner cases where numerics might have turned narrow outcomes, so actually 1991/2000. ```py; pmbc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; pbmc.obs[""dummy_tech""] = ""source_1""; pbmc.obs.loc[pbmc.obs.index[500:1000], ""dummy_tech""] = ""source_2""; pbmc.obs.loc[pbmc.obs.index[1000:1500], ""dummy_tech""] = ""source_3""; pbmc.obs.loc[pbmc.obs.index[1500:2000], ""dummy_tech""] = ""source_4""; pbmc.obs.loc[pbmc.obs.index[2000:], ""dummy_tech""] = ""source_5"". # default settings in scanpy are the same as for Seurat; seurat_v3_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3"", batch_key=""dummy_tech"", inplace=False); seurat_v3_paper_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_paper"", batch_key=""dummy_tech"", inplace=False); seurat_v3_implementation_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_implementation"", batch_key=""dummy_tech"", inplace=False). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat_batch = pd.read_csv(""scanpy/scanpy/tests/_scripts/seurat_hvg_v3_batch.csv"", index_col=0). seu = pd.Index(pbmc3k_tutorial_FindVariableGenes_seurat_batch[""x""].values); ```. Matching genes `'seurat_v3'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_hvg[seurat_v3_hvg.highly_variable].index)); ```; `764`. Matching genes `'seurat_v3_paper'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_paper_hvg[seurat_v3_paper_hvg.highly_variable].index)); ```; `1990`. Matching genes `'seurat_v3_implementation'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_implementation_hvg[seurat_v3_implementation_hvg.highly_variable].index)); ```; `1990`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:5772,adapt,adapted,5772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Modifiability,"ath = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""); adata.obs_names_make_unique(); print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse; adata.var[""mt""] = adata.var_names.str.startswith(""MT-""); # ribosomal genes; adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")); # hemoglobin genes; adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True; ). sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter=0.4,; multi_panel=True,; ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data; adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts; sc.pp.normalize_total(adata); # Logarithmize the data; sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""); sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata); sc.tl.umap(adata); sc.tl.leiden(; adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2; ). sc.pl.pca(; adata,; color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],; dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],; ncols=2,; size=2,; ). sc.pp.neighbors(adata). sc.pl.umap(; adata,; color=""sample"",; # Setting a smaller point size to get prevent overlap; size=2,; ). ### runs forever:; sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:2065,layers,layers,2065,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['layers'],['layers']
Modifiability,"ayer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py; import numpy as np; import pandas as pd; import anndata as ad. # Create a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orie",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1355,layers,layers,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['layers'],['layers']
Modifiability,bbknn integrates multiple variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:26,variab,variables,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['variab'],['variables']
Modifiability,"been added.; * **update**: method chaining was added to avoid explosion of parameters as suggested in #956. The caveat is that a different name is needed to keep the current functionality of `sc.pl.dotplot`. Now is possible to do `sc.pl.DotPlot(adata, ....).add_dendrogram(size=1.2).swap_axes().show()`. ![image](https://user-images.githubusercontent.com/4964309/77526545-13ce0600-6e8b-11ea-996a-3d548f52f200.png). ![image](https://user-images.githubusercontent.com/4964309/77529251-8c36c600-6e8f-11ea-8446-c1dd18374cd4.png). Open issues:; * we need a better way to filter the output from rank_genes_groups (and I would also consider a better name). One option is to filter while plotting.; * to correctly plot the results from `sc.tl.rank_genes_groups` is better to report the results for all genes (`n_genes`). The default is to keep the top 100 genes per group. Thus, for many genes, the pvalue, logfoldchange etc in other groups is not saved.; * added new parameters, for some I would be happy to have some feedback. One is `style` that has two options `square color` and `dot color`. **Update**; The following is now possible (notice the method chaining):. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}; sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \; .add_totals(sort='descending', size=1.2) \; .legend(color_title='log(UMI count+1)', width=1.6)\; .style(color_map='RdBu_r', dot_min=0.3)\; .show(); ```; ![image](https://user-images.githubusercontent.com/4964309/77777939-889e6d00-7050-11ea-842e-cf2a11739df6.png). TODO:; - [x] Remove small secondary ticks; - [x] Adapt matrixplot to new object model; - [ ] Adapt heatmap to object model; - [x] Adapt stackedviolin to object model; - [ ] Incorporate modifications from #1116 ; - [x] Fix call from rank_genes_groups; - [x] 'black' code; - [x] Update docstrings; - [ ] Update tests; - [ ] Update tutorial and readthedocs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127:2398,Adapt,Adapt,2398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127,3,['Adapt'],['Adapt']
Modifiability,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1441,variab,variable,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,3,['variab'],['variable']
Modifiability,"bs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata, max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(); sc._settings.ScanpyConfig.n_jobs = os.cpu_count(); sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''; from sklearn.manifold import TSNE; from scanpy.tools._utils import _choose_representation; X = _choose_representation(adata, n_pcs=tsne_n_pcs); X_tsne = TSNE().fit_transform(X.astype(np.float32)); adata.obsm['X_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:2785,variab,variable,2785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,['variab'],['variable']
Modifiability,"by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:1040,layers,layers,1040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,1,['layers'],['layers']
Modifiability,cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.1.3; executing 1.2.0; fastjsonschema 2.18.1; filelock 3.12.4; flake8 6.0.0; Flask 3.0.0; fonttools 4.43.1; fqdn 1.5.1; frozenlist 1.4.0; fsspec 2023.6.0; future 0.18.3; gensim 4.3.2; gitdb 4.0.10; GitPython 3.1.36; gmpy2 2.1.2; greenlet 3.0.0; h5py 3.9.0; holoviews 1.17.1; huggingface-hub 0.17.3; humanfriendly 10.0; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; igraph 0.10.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.11.0;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:4668,Config,ConfigArgParse,4668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['Config'],['ConfigArgParse']
Modifiability,"ckages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 20.0.2 . ubuntu; 20.04. pip list; Package Version Location ; ------------------------- -------------------- -----------------------------; analysaurus 0.0.1 /home/ubuntu/code/analysaurus; anndata 0.7.5 ; ansi2html 1.5.2 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2465,config,config,2465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['config'],['config']
Modifiability,"cked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1004,layers,layers,1004,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,colouring of highly variable genes on pl.filter_genes_dispersion,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:20,variab,variable,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['variab'],['variable']
Modifiability,"conda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2434,layers,layers,2434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['layers'],['layers']
Modifiability,correlation between cell types and continuous variables stored in .obs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:46,variab,variables,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['variab'],['variables']
Modifiability,correlation between cell types and obs variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1826:39,variab,variables,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1826,1,['variab'],['variables']
Modifiability,"crash the kernel, but does bog it down and causes everything to to take much more time than necesarry. ; I am working in a conda env on a Win 10 , 64bit, x64 system; the problem also occurs using the pbmc3k dataset. ### Minimal code sample. ```python; # example with own data, but same happens with pbmc3k data; sc.pp.filter_cells(em_adata, min_genes=200); sc.pp.filter_genes(em_adata, min_cells=3); em_adata.shape; # [out] -> (42753, 21636). sc.pp.calculate_qc_metrics(em_adata, qc_vars=[""mt""], percent_top=None, log1p=False, inplace=True); em_adata.obs[""outlier_mt""] = em_adata.obs.pct_counts_mt > 15; em_adata.obs[""outlier_total""] = em_adata.obs.total_counts > 30000; em_adata.obs[""outlier_ngenes""] = em_adata.obs.n_genes_by_counts > 6000; em_adata = em_adata[~em_adata.obs[""outlier_mt""], :]; em_adata = em_adata[~em_adata.obs[""outlier_total""], :]; em_adata = em_adata[~em_adata.obs[""outlier_ngenes""], :]; sc.pp.filter_genes(em_adata,min_cells=1). sc.pp.scrublet(em_adata); em_adata.layers['counts'] = em_adata.X.copy(); sc.pp.normalize_total(em_adata); sc.pp.log1p(em_adata); sc.pp.highly_variable_genes(em_adata,flavor='seurat'); sc.pl.highly_variable_genes(em_adata); em_adata = em_adata[:, em_adata.var[""highly_variable""]]; em_adata.shape; # [out] -> (41749, 1425); sc.pp.pca(em_adata, n_comps=50); sc.pp.neighbors(em_adata); sc.tl.umap(em_adata); sc.tl.leiden(em_adata,flavor='igraph',n_iterations=2,random_state=1653,directed=False); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32; ```. ### Versions. <details>. ```; conda env:; # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-binding",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:1501,layers,layers,1501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['layers'],['layers']
Modifiability,"cumentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illustrating when and how to use these parameters effectively. 3. **Consistency in Documentation Style**: Ensure that the documentation style is consistent across different parameters, making it easier for users to read and understand. ### Conclusion; Enhancing the documentation of the `MatrixPlot` class will improve the library's usability and user experience. . I am new to open-source contribution and I am eager to contribute to this enhancement, and welcome any additional input or guidance from the project maintainers and community.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:2435,enhance,enhancement,2435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhancement']
Modifiability,don't configure black,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701:6,config,configure,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701,1,['config'],['configure']
Modifiability,dotplot with x axis being one variable and y axis being another variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876,2,['variab'],['variable']
Modifiability,"e /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2467,Layers,Layers,2467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['Layers'],['Layers']
Modifiability,"e a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1544,layers,layers,1544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['layers'],['layers']
Modifiability,"e and there was a two year old thread on the subject (https://github.com/scverse/scanpy/issues/2073), but none of the solutions worked on my machine. I tried version control of all the packages, most importantly numpy as it was mentioned as a problem. I was wondering if this you all have any newer solutions for this issue. Due to my inexperience im really not sure what is causing this issue and therefore what to provide you all with. I also just realized that after a clean install trying to pip3 install scikit-misc returns this . (scanpy_env) user@Mac ~ % pip3 install scikit-misc ; Requirement already satisfied: scikit-misc in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (0.3.1); Requirement already satisfied: numpy>=1.22.3 in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (from scikit-misc) (1.26.4). Still getting the error though... Hardware: M2 max mac, macos 15 beta (could this be it somehow?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:1770,variab,variable,1770,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['variab'],['variable']
Modifiability,"e checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Since a few weeks ago (at least), the tests in `test_metrics.py` started failing because the exact equality tests no longer consistently returned the bit-for-bit same float. Something like it has been observed in https://github.com/scverse/scanpy/pull/1740#discussion_r596827747. #2687 disables the exact comparison, but we should figure out why it’s happening and if we can restore exact precision. ### Minimal code sample. ```console; $ git switch 1.9.5; $ pytest scanpy/tests/test_metrics.py; ```. ### Error output. ```pytb; =================================== FAILURES ===================================; __________________________ test_morans_i_consistency ___________________________. def test_morans_i_consistency():; pbmc = pbmc68k_reduced(); pbmc.layers[""raw""] = pbmc.raw.X.copy(); g = pbmc.obsp[""connectivities""]; ; > assert eq(; sc.metrics.morans_i(g, pbmc.obs[""percent_mito""]),; sc.metrics.morans_i(pbmc, vals=pbmc.obs[""percent_mito""]),; ); E AssertionError: assert False; E + where False = eq(0.13099293222276961, 0.13099293222276967); E + where 0.13099293222276961 = <function morans_i at 0x7f354779d9d0>(<700x700 sparse matrix of type '<class 'numpy.float64'>'\n	with 9992 stored elements in Compressed Sparse Row format>, index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + wher",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2688:1048,layers,layers,1048,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2688,1,['layers'],['layers']
Modifiability,"e sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/wo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1320,layers,layers,1320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,['layers'],['layers']
Modifiability,"e(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance(val, pd.DataFrame):; 92 name = self.attrname.title().rstrip(""s""). ValueError: Value passed for key 'mean' is of incorrect shape. Values of layers must match dimensions ('obs', 'var') of parent. Value had shape (11, 2) while it should have had (11, 765).; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0rc2.dev19+ga6126980; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.3.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; h5py 3.10.0; igraph 0.11.3; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.3; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.1; parso 0.8.3; pexpect 4.9.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 15.0.1; pygments 2.17.2; pyparsing 3.1.1; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:3605,layers,layers,3605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"e**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards favoring the style of `seurat_v3_paper`. Better naming suggestions more than welcome. **Examples**; - Good when no `batch_key` used:; ```py; import numpy as np; import pandas as pd; import scanpy as sc. # load exactly the data from seurat tutorial. pbmc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). print(pbmc). # default settings in scanpy are the same as for Seurat; sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3""). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat = pd.read_csv(""scanpy/scanpy/scanpy/tests/_scripts/seurat_hvg_v3.csv.gz"", index_col=0). # This is used to order and rank the hvg when no batch information used; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance.standardized""],; pbmc.var[""variances_norm""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""mean""],; pbmc.var[""means""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance""],; pbmc.var[""variances""]),; ); ```. - Discrepancy when `batch_key` is used: The flavor `'seurat_v3'` shows 764/2000 genes overlap with Seurat. The new suggested flavors `'seurat_v3_paper'` and `'seurat_v3_implementation'` show a 1990/2000 selected genes overlap with Seurat. The 10 non-matching genes contain 1 gene renamed by Seurat and therefore not found matching, some corner ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:3561,adapt,adapted,3561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Modifiability,"e, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being passed to sanitize anndata, and then say ""please don't pass subsetted anndata objects to plotting functions"" or something like that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3661,variab,variables,3661,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variables']
Modifiability,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328:2123,adapt,adapted,2123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328,1,['adapt'],['adapted']
Modifiability,"e:; 138 self.genlower = self.GeneratorLower(self); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/contextlib.py in __exit__(self, type, value, traceback); 129 value = type(); 130 try:; --> 131 self.gen.throw(type, value, traceback); 132 except StopIteration as exc:; 133 # Suppress StopIteration *unless* it's the same exception that; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 ; ; LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32; ; File ""../../../../miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^; ; During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /Users/depretis.stefano/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py (52); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:9790,config,config,9790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['config'],['config']
Modifiability,"ecent call last); Input In [51], in <module>; 4 adata2.obs['group'] = adata2.obs.index.to_series().str.startswith(""A"").astype(str); 5 fig, axes = plt.subplots(1, 2); ----> 6 sc.pl.violin(adata2, keys=['CD8A', 'CD8B'], groupby=""group"", ax=axes). File /opt/conda/envs/analysis/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:835, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 833 axs = [ax]; 834 for ax, y, ylab in zip(axs, ys, ylabel):; --> 835 ax = sns.violinplot(; 836 x=x,; 837 y=y,; 838 data=obs_tidy,; 839 order=order,; 840 orient='vertical',; 841 scale=scale,; 842 ax=ax,; 843 **kwds,; 844 ); 845 if stripplot:; 846 ax = sns.stripplot(; 847 x=x,; 848 y=y,; (...); 854 ax=ax,; 855 ). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/_decorators.py:46, in _deprecate_positional_args.<locals>.inner_f(*args, **kwargs); 36 warnings.warn(; 37 ""Pass the following variable{} as {}keyword arg{}: {}. ""; 38 ""From version 0.12, the only valid positional argument ""; (...); 43 FutureWarning; 44 ); 45 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); ---> 46 return f(**kwargs). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:2408, in violinplot(x, y, hue, data, order, hue_order, bw, cut, scale, scale_hue, gridsize, width, inner, split, dodge, orient, linewidth, color, palette, saturation, ax, **kwargs); 2405 if ax is None:; 2406 ax = plt.gca(); -> 2408 plotter.plot(ax); 2409 return ax. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:1043, in _ViolinPlotter.plot(self, ax); 1041 def plot(self, ax):; 1042 """"""Make the violin plot.""""""; -> 1043 self.draw_violins(ax); 1044 self.annotate_axes(ax); 1045 if self.orient == ""h"":. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:761, in _ViolinPlotter.draw_violins(self, ax); 759 def draw_violins(self, ax):",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2136:1758,variab,variable,1758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2136,1,['variab'],['variable']
Modifiability,"ed in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scipy/sparse/data.py:132: RuntimeWarning: invalid value encountered in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: overflow encountered in log1p; mean = np.log1p(mean); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: invalid value encountered in log1p; mean = np.log1p(mean); Traceback (most recent call last):; File ""../../scvi/scvi_adata.py"", line 75, in <module>; sc.pp.highly_variable_genes(adata); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 257, in highly_variable_genes; flavor=flavor); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 92, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 233, in cut; ""cannot specify integer `bins` when input data "" ""contains infinity""; ValueError: cannot specify integer `bins` when input data contains infinity; ```. Indeed, if I do `np.expm1(3701)` I get an overflow. I think it will be necessary to come up with a way to calculate highly variable genes without doing `expm1` on the raw counts, due to this overflow issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763:2294,variab,variable,2294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763,1,['variab'],['variable']
Modifiability,"ef map_embedding(self, method):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/umap/umap_.py in transform(self, X); 2715 else:; 2716 epsilon = 0.24 if self._knn_search_index._angular_trees else 0.12; -> 2717 indices, dists = self._knn_search_index.query(; 2718 X, self.n_neighbors, epsilon=epsilon; 2719 ). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pynndescent/pynndescent_.py in query(self, query_data, k, epsilon); 1564 """"""; 1565 if not hasattr(self, ""_search_graph""):; -> 1566 self._init_search_graph(); 1567 ; 1568 if not self._is_sparse:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pynndescent/pynndescent_.py in _init_search_graph(self); 1054 ); 1055 else:; -> 1056 diversify_csr(; 1057 reverse_graph.indptr,; 1058 reverse_graph.indices,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:3528,config,config,3528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,1,['config'],['config']
Modifiability,"en reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; import pandas as pd; import numpy as np; import urllib.request; ; url = ""https://humancellatlas.usegalaxy.eu/datasets/11ac94870d0bb33a8d1d2e6bc37a120e/display?to_ext=h5ad""; # 75 MB anndata; urllib.request.urlretrieve(url, '11ac94.h5ad'). adata = sc.read('11ac94.h5ad'). sc.settings.figdir = '.'. sc.pl.rank_genes_groups_dotplot(; adata,; save='.png',; show=False,; gene_symbols='Symbol',; n_genes=10,; log=False,; use_raw=False,; ); ```. ```pytb; Traceback (most recent call last):; File ""/private/var/folders/23/wwh84vd95rncnf5mx753cpgh0000gp/T/tmpenni0vju/job_working_directory/000/11/configs/tmp4tnm4xxb"", line 15, in <module>; sc.pl.rank_genes_groups_dotplot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 647, in rank_genes_groups_dotplot; return _rank_genes_groups_plot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 426, in _rank_genes_groups_plot; _pl = dotplot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py"", line 913, in dotplot; dp = DotPlot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py"", line 127, in __init__; BasePlot.__init__(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_baseplot_class.py"", line 105, in __init__; self.categories, self.obs_tidy = _prepare_dataframe(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 1845, in _prepare_dataframe; obs_tidy = get.obs_df(; File ""/usr/local/lib/python3.8/site-packages/scanpy/get/ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1796:1107,config,configs,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1796,1,['config'],['configs']
Modifiability,"ent_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; ima",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2202,plugin,plugins,2202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['plugin'],['plugins']
Modifiability,"error message:. ```; computing UMAP; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap; verbose=settings.verbosity > 3,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding; verbose=verbose,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please rep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:1383,parameteriz,parameterized,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['parameteriz'],['parameterized']
Modifiability,"exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (fail",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:1585,variab,variable,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['variab'],['variable']
Modifiability,extend options to aggregate ranks across batches for HVG selection,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2151:0,extend,extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2151,1,['extend'],['extend']
Modifiability,"f all, thank you for your great platform! . When I try to export a SPRING project I get the following error (it seems that the class NeighborsView is not defined; I have a 'neighbors' key in .uns): . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). AnnData object with n_obs × n_vars = 8757 × 20679 ; obs: 'SeqRun', 'Biological replicate', 'nCount_RNA', 'nCount_SCT', 'nFeature_RNA', 'nFeature_SCT', 'novelty', 'orig_ident', 'percent_mt', 'sc_leiden_res_48.75', 'State', 'ImmGen'; var: 'Selected', 'sct_detection_rate', 'sct_gmean', 'sct_residual_mean', 'sct_residual_variance', 'sct_variable', 'sct_variance'; uns: 'Biological replicate_colors', 'ImmGen_colors', 'State_colors', 'leiden', 'neighbors', 'state'; obsm: 'X_pca', 'X_umap'; varm: 'pca_feature_loadings'; layers: 'norm_data', 'scale_data'; obsp: 'connectivities', 'distances'; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-208-9f15be957dd9> in <module>; 1 sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; ----> 2 custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). ~/miniconda3/envs/py36-sc/lib/python3.6/site-packages/scanpy/external/exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260:1198,layers,layers,1198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260,1,['layers'],['layers']
Modifiability,"f six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()). In [2]: sc.pp.highly_variable_genes(adata); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-2-7727f5f928cd>"", line 1, in <module>; sc.pp.highly_variable_genes(adata). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:2129,variab,variable,2129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['variab'],['variable']
Modifiability,flake8 config malformed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1783:7,config,config,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1783,1,['config'],['config']
Modifiability,"fo(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_ses",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3082,config,config,3082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"ft_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actual(. /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 705 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 706 # hackish solution here, no copy should be necessary; --> 707 uns_new = deepcopy(self._adata_ref._uns); 708 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 709 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in _reconstruct(x, memo, func, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:3433,layers,layers,3433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,"genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no error. While running: `adata_orig.X.sum(1)` gives me the error as : ; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-23-951a31c71c45> in <module>; ----> 1 adata_orig.X.sum(1). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```. PS: I downloaded this` .h5ad` file from a published research paper to perform some analysis over it, would be happy to provide you the link to same if required. . Also this is the environment that I am working in:; `scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.1 scipy==1.6.2 pandas==1.2.4 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.6`. Hello @LuckyMD, tagging you for just in case you might be knowing the resolution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:2896,layers,layers,2896,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,"he best trees into a search forest; --> 967 tree_scores = [; 968 score_linked_tree(tree, self._neighbor_graph[0]); 969 for tree in self._rp_forest; 970 ]; 971 if self.verbose:; 972 print(ts(), ""Worst tree score: {:.8f}"".format(np.min(tree_scores))). File ~/miniconda3/envs/test/lib/python3.10/site-packages/pynndescent/pynndescent_.py:968, in <listcomp>(.0); 961 self._search_forest = [; 962 convert_tree_format(tree, self._raw_data.shape[0]); 963 for tree in rp_forest; 964 ]; 965 else:; 966 # convert the best trees into a search forest; 967 tree_scores = [; --> 968 score_linked_tree(tree, self._neighbor_graph[0]); 969 for tree in self._rp_forest; 970 ]; 971 if self.verbose:; 972 print(ts(), ""Worst tree score: {:.8f}"".format(np.min(tree_scores))). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:487, in _DispatcherBase._compile_for_args(self, *args, **kws); 485 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 486 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 487 raise e; 488 finally:; 489 self._types_active_call = []. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:420, in _DispatcherBase._compile_for_args(self, *args, **kws); 418 return_val = None; 419 try:; --> 420 return_val = self.compile(tuple(argtypes)); 421 except errors.ForceLiteralArg as e:; 422 # Received request for compiler re-entry with the list of arguments; 423 # indicated by e.requested_args.; 424 # First, check if any of these args are already Literal-ized; 425 already_lit_pos = [i for i in e.requested_args; 426 if isinstance(args[i], types.Literal)]. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:965, in Dispatcher.compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. File ~/miniconda3/envs/test/lib/python3.10/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:3359,config,config,3359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['config'],['config']
Modifiability,"he environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package?. Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468:2079,enhance,enhance,2079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468,1,['enhance'],['enhance']
Modifiability,high variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/188:5,variab,variable,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188,1,['variab'],['variable']
Modifiability,highly variable genes + batch_key --> reciprocal condition number error,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2669:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2669,1,['variab'],['variable']
Modifiability,"hon-input-2-2626ee07d023> in <module>; ----> 1 sc.read_h5ad('/xchip/beroukhimlab/michelle/jjeang_plgg/sc_labeled.h5ad'). ~/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; leidenalg 0.8.0; llvmlite 0.38.1; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.5.2; mpl_toolkits NA; natsort 8.1.0; numba 0.55.2; numexpr 2.7.1; numpy 1.21.6; packaging 21.3; pandas 1.4.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:2794,layers,layers,2794,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['layers'],['layers']
Modifiability,"hon37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError(""minvalue must be positive""); 1182 ; 1183 def __call__(self, value, clip=None):; ValueError: minvalue must be positive",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3592,extend,extend,3592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['extend'],['extend']
Modifiability,"hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 327 in _main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 273 in wrap_session; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 320 in pytest_cmdline_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 198 in console_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:6591,config,config,6591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['config'],['config']
Modifiability,"https://github.com/theislab/scanpy/blob/2a7fb7cdf56ff44ef6338c7dba3b84b4b32d216c/scanpy/plotting/_tools/__init__.py#L286. `scanpy.pl.rank_genes_groups()` by default presents the `scores` from `adata.uns['rank_genes']['scores']`. However, why is it not possible to change to `scores` `logfoldchanges`. The code at line 286 does not use any modularity or possible parameterization. A possible change would be to introduce a new parameter `plotby`, which corresponds to `adata.uns['rank_genes'][f'{plotby}']` or similar. . I do realize that it is easy to extract the relevant data and plot them by myself. However, there is value in making it easier to choose what to exactly plot from the `rank_genes_groups`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1152:362,parameteriz,parameterization,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152,1,['parameteriz'],['parameterization']
Modifiability,"imal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:1427,variab,variables,1427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['variab'],['variables']
Modifiability,"ine 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3014,config,config,3014,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"ion in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:1444,variab,variable,1444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,1,['variab'],['variable']
Modifiability,"irmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am following the tutorial but everytime I try to run a violin plot the kernel crashes, this doesnt happen with other seaborn graphs. I have tried updating packeges, changing environment, etc, etc & nothing works any help would be great !!. ![image](https://github.com/scverse/scanpy/assets/127498480/b5cc12b1-00af-4919-abd0-7ea99b72cade). ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(; 'data/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`; adata; sc.pl.highest_expr_genes(adata, n_top=20, ); sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'); sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0-4, multi_panel=True); ```. ### Error output. ```pytb; Kernel Restarting; The kernel for Tests/scanpytutorial/Untitled.ipynb appears to have died. It will restart automatically.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840:1111,variab,variable,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"irst concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614:1457,adapt,adapted,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614,1,['adapt'],['adapted']
Modifiability,"ite; cwd: /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/; Complete output (29 lines):; running install; running build; got version from file /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/llvmlite/_version.py {'version': '0.34.0', 'full': 'c5889c9e98c6b19d5d85ebdd982d64a03931f8e2'}; running build_ext; /usr/bin/python /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py; LLVM version... Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 105, in main_posix; out = subprocess.check_output([llvm_config, '--version']); File ""/usr/lib/python3.10/subprocess.py"", line 420, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; File ""/usr/lib/python3.10/subprocess.py"", line 501, in run; with Popen(*popenargs, **kwargs) as process:; File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__; self._execute_child(args, executable, preexec_fn, close_fds,; File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 107, in main_posix; raise RuntimeError(""%s failed executing, please point LLVM_CONFIG ""; RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config; error: command '/usr/bin/python' failed with exit code 1; ----------------------------------------; ```. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:2236,config,config,2236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,3,['config'],['config']
Modifiability,"itle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds); 930 dot_color_df=dot_color_df,; 931 ax=ax,; --> 932 **kwds,; 933 ); 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds); 142 layer=layer,; 143 ax=ax,; --> 144 **kwds,; 145 ); 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds); 111 num_categories,; 112 layer=layer,; --> 113 gene_symbols=gene_symbols,; 114 ); 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope; ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1636:2649,variab,variable,2649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636,1,['variab'],['variable']
Modifiability,"ize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #173; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Docs:; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/release-notes/index.html#version-1-10; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet_simulate_doublets.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pl.scrublet_score_distribution.html. ### How to review this PR. I made tests quantitative before this PR, so note that the only change that modified tests is 42143d88a0d499130fac8e5ca60eef0c19163734. In that PR, I make it so there are no longer any duplicate simulated doublets being created. This is necessary to be able to support any neighborhood detection algorithm. I also feel like it makes more sense. This is the only algorithmic change to upstream. Please use your own judgement to check if this makes sense to you. ### TODO:. - [x] remove unused utils (plotting, preprocessing); - [ ] figure out what remaining utils to replace with ours; - [x] PCA/SVD: https://github.com/scverse/scanpy/blob/bf5f1f9343f5729df6f90f7c68363682022e0480/scanpy/preprocessing/_scrublet/__init__.py#L415-L417; - [x] mean_center, normalize_variance, zscore: small enough to be left alone I think; - [ ] get_knn_graph: no need to have multiple implementations here, but our current implementation automatically calculates connectivities, which this doesn’t need https://github.com/scverse/scanpy/pull/2723; - [ ] refactor so the class API matches the way we use it ; - [x] switch to dataclass for centralized attr defs; - [x] use non-deprecated random state style; - [x] use our logging instead of print statements",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2703:1975,refactor,refactor,1975,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2703,1,['refactor'],['refactor']
Modifiability,"l components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata,max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); ```. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L202",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100:2752,variab,variable,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100,2,['variab'],['variable']
Modifiability,"l tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; adata_dist = AnnData object with n_obs × n_va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1384,config,configfile,1384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['config'],['configfile']
Modifiability,"lace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2240,layers,layers,2240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,log1p warns adata.X is logged when it may not be (when other layers are logged),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333:61,layers,layers,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333,1,['layers'],['layers']
Modifiability,"ls.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actua",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2657,layers,layers,2657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,2,['layers'],['layers']
Modifiability,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(); ```; ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```; def style(; self,; cmap: str = DEFAULT_COLORMAP,; color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,; dot_max: Optional[float] = DEFAULT_DOT_MAX,; dot_min: Optional[float] = DEFAULT_DOT_MIN,; .....; ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633:1659,rewrite,rewrites,1659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633,1,['rewrite'],['rewrites']
Modifiability,"lues; 247 self.extract_function_arguments(); --> 248 entry_block_tail = self.lower_function_body(); 249 ; 250 # Close tail of entry block. ~\anaconda3\lib\site-packages\numba\lowering.py in lower_function_body(self); 271 bb = self.blkmap[offset]; 272 self.builder.position_at_end(bb); --> 273 self.lower_block(block); 274 ; 275 self.post_lower(). ~\anaconda3\lib\site-packages\numba\lowering.py in lower_block(self, block); 286 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block); 290 . ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 723 from numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\lyciansarpedon\anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:12636,config,config,12636,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,"make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Plotting with scanpy.pl gives attribute error. . ### Minimal code sample. ```python; sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[47], line 1; ----> 1 sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True). File ~/anaconda3/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:795, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 790 if multi_panel and groupby is None and len(ys) == 1:; 791 # This is a quick and dirty way for adapting scales across several; 792 # keys if groupby is None.; 793 y = ys[0]; --> 795 g = sns.catplot(; 796 y=y,; 797 data=obs_tidy,; 798 kind=""violin"",; 799 scale=scale,; 800 col=x,; 801 col_order=keys,; 802 sharey=False,; 803 order=keys,; 804 cut=0,; 805 inner=None,; 806 **kwds,; 807 ); 809 if stripplot:; 810 grouped_df = obs_tidy.groupby(x). File ~/anaconda3/lib/python3.11/site-packages/seaborn/categorical.py:2932, in catplot(data, x, y, hue, row, col, kind, estimator, errorbar, n_boot, units, seed, order, hue_order, row_order, col_order, col_wrap, height, aspect, log_scale, native_scale, formatter, orient, color, palette, hue_norm, legend, legend_out, sharex, sharey, margin_titles, facet_kws, ci, **kwargs); 2929 linecolor = plot_kws.pop(""linecolor"", ""auto""); 2930 linecolor = p._complement_color(linecolor, color, p._hue_map); -> 2932 p.plot_violins(; 2933 width=width,; 2934 dodge=dod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:1111,adapt,adapting,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['adapt'],['adapting']
Modifiability,making Cell ranger's normalized dispersion (for highly variable gene selection) not absolute,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705,1,['variab'],['variable']
Modifiability,"map(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in get_distribution(dist); 464 dist = Requirement.parse(dist); 465 if isinstance(dist, Requirement):; --> 466 dist = get_provider(dist); 467 if not isinstance(dist, Distribution):; 468 raise TypeError(""Expected string, Requirement, or Distribution"", dist). /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in get_provider(moduleOrReq); 340 """"""Return an IResourceProvider for the named module or requirement""""""; 341 if isinstance(moduleOrReq, Requirement):; --> 342 return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]; 343 try:; 344 module = sys.modules[moduleOrReq]. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in require(self, *requirements); 884 included, even if they were already activated in this working set.; 885 """"""; --> 886 needed = self.resolve(parse_requ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:2775,config,config,2775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['config'],['config']
Modifiability,"me', 'patient_id'; var: 'feature_types'; uns: 'hvg', 'leiden', 'neighbors', 'pca', 'umap'; obsm: 'X_pca', 'X_pca_harmony', 'X_umap'; layers: 'raw'; ```. After this, when I tried running the command:; `sc.pl.highest_expr_genes(adata_orig, n_top=20, )`. I get the following output:; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-c4ab6dadfa42> in <module>; ----> 1 sc.pl.highest_expr_genes(adata_orig ). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\plotting\_qc.py in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 65 ; 66 # compute the percentage of each gene per cell; ---> 67 norm_dict = normalize_total(adata, target_sum=100, inplace=False); 68 ; 69 # identify the genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no er",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:1947,layers,layers,1947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,monypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; imagecodecs 2024.1.1; imageio 2.34.0; importlib_metadata 7.0.2; importlib_resources 6.1.3; inflect 7.0.0; ipykernel 6.29.3; ipylab 1.0.0; ipython 8.22.2; ipywidgets 8.1.2; isoduration 20.11.0; jedi 0.19.1; Jinja2 3.1.3; joblib 1.3.2; json5 0.9.22; jsonpointer 2.4; jsonschema 4.21.1; jsonschema-specifications 2023.12.1; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyter-events 0.9.0; jupyter-lsp 2.2.4; jupyter_server 2.13.0; jupyter_server_proxy 4.1.0; jupyter_server_terminals 0.5.2; jupyterlab 4.1.4; jupyterlab_pygments 0.3.0; jupyterlab_server 2.25.3; jupyterlab_widgets 3.0.10; kiwisolver 1.4.5; lamin_utils 0.13.0; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; linkify-it-py 2.0.3; llvmlite 0.42.0; locket 1.0.0; louvain 0.8.1; lz4 4.3.3; mapclassify 2.6.1; Markdown 3.5.2; markdown-it-py 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; matplotlib-inline 0.1.6; mdit-py-plugins 0.4.0; mdurl 0.1.2; mistune 3.0.2; msgpack 1.0.7; mudata 0.2.3; multidict 6.0.5; multipledispatch 0.6.0; munkres 1.1.4; muon 0.1.5; natsort 8.4.0; nbclient 0.8.0; nbconvert 7.16.2; nbformat 5.9.2; nbproject 0.10.1; nest_asyncio 1.6.0; networkx 3.2.1; notebook 7.1.1; notebook_shim 0.2.4; numba 0.59.0; numcodecs 0.12.1; numpy 1.24.4; nvtx 0.2.10; omnipath 1.0.8; openpyxl 3.1.2; orjson 3.9.15; overrides 7.7.0; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess 0.7.0; pure-eval 0.2.2; pyarrow 14.0.2; pyarrow-hotfix 0.6; pycparser 2.21; pyct 0.5.0; pydantic 1.10.14; pyee 8.1.0; Pygments 2.17.2; pylibcugraph 24.2.0; pylibraft 24.2.0; pynndescent 0.5.11; pynvml 11.4.1; pyparsing 3.1.2; pyppetee,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:4083,plugin,plugins,4083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['plugin'],['plugins']
Modifiability,"mple. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_recipes.py in; 4 from anndata import AnnData; 5; ----> 6 from . import _simple as pp; 7 from ._deprecated.highly_variable_genes import filter_genes_dispersion, filter_genes_cv_deprecated; 8 from ._normalization import normalize_total. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py in; 8 from typing import Union, Optional, Tuple, Collection, Sequence, Iterable; 9; ---> 10 import numba; 11 import numpy as np; 12 import scipy as sp. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/init.py in; 32; 33 # Re-export decorators; ---> 34 from numba.core.decorators import (cfunc, generated_jit, jit, njit, stencil,; 35 jit_module); 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2126,config,config,2126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,2,"['config', 'extend']","['config', 'extending']"
Modifiability,"mport scanpy as sc; pbmc = sc.datasets.pbmc3k(); log_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch"")>>> pbmc = sc.datasets.pbmc3k(). >>> log_anndata = sc.pp.log1p(pbmc, copy=True); >>> pbmc.layers['log_transformed'] = log_anndata.X.copy(); >>> ; >>> # This errors, because X is not normalized and flavor=""seurat"" requires norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:1480,layers,layers,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variable]; 2 adata. ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in __getitem__(self, index); 1114 def __getitem__(self, index: Index) -> ""AnnData"":; 1115 """"""Returns a sliced view of the object.""""""; -> 1116 oidx, vidx = self._normalize_indices(index); 1117 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1118 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in _normalize_indices(self, index); 1095 ; 1096 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1097 return _normalize_indices(index, self.obs_names, self.var_names); 1098 ; 1099 # TODO: this is not quite complete... ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\index.py in _normalize_indices(index, names0, names1); 34 ax0, ax1 = unpack_index(ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:2035,variab,variable,2035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['variab'],['variable']
Modifiability,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587:1330,layers,layers,1330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587,1,['layers'],['layers']
Modifiability,"n the package for the given example below. The experiment was run on AWS r7i.24xlarge.; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279:1355,variab,variable,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279,1,['variab'],['variable']
Modifiability,"n(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3933,config,config,3933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"n(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger in loggers:; handlers = getattr(logger, 'handlers', []); for handler in handlers:; logger.removeHandler(handler); ```. Next time we come across a reproducible case of this happening, w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:4092,config,config,4092,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"n3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2448,layers,layers,2448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['layers'],['layers']
Modifiability,"n312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:761) if compression == ""gzip"":; [762](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:16735,variab,variable,16735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['variab'],['variable']
Modifiability,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:2015,variab,variable,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['variab'],['variable']
Modifiability,"niconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 176 try:; --> 177 return func(elem, *args, **kwargs); 178 except Exception as e:. ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. ~/miniconda3/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-17-97568eff5295> in <module>; ----> 1 adata=anndata.read_h5ad('./visium_merge_inter_upload.h5ad'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2376:1828,layers,layers,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376,1,['layers'],['layers']
Modifiability,no highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935:10,variab,variable,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935,1,['variab'],['variable']
Modifiability,normalize_total affects layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:24,layers,layers,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"npy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. aggregate throws error when aggregating `obsm` or `varm`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1208,layers,layers,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"ns, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(); 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(); 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(); 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .core.anndata import AnnData, Raw; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(); 46 LayersBase, Layers; 47 ); ---> 48 from .. import h5py; 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView; 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 4 from typing import Optional, Union, KeysView, NamedTuple; 5 ; ----> 6 import h5py; 7 import numpy as np; 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900:1332,Layers,LayersBase,1332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900,2,['Layers'],"['Layers', 'LayersBase']"
Modifiability,"nted yet."". NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2126,layers,layers,2126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['layers'],['layers']
Modifiability,"o a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. kwargs = {""multiplicative_replacement"":""auto""}. sc.pp.normalize(adata, function=clr, **kwargs) # -> addata object with the transformation as one of the layers; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:2621,layers,layers,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['layers'],['layers']
Modifiability,"obs):; 135 ing.map_labels(col, labeling_method[i]). /usr/local/lib/python3.7/dist-packages/scanpy/tools/_ingest.py in neighbors(self, k, queue_size, epsilon, random_state); 469 self._nnd_idx.search_rng_state = rng_state; 470 ; --> 471 self._indices, self._distances = self._nnd_idx.query(test, k, epsilon); 472 ; 473 else:. /usr/local/lib/python3.7/dist-packages/pynndescent/pynndescent_.py in query(self, query_data, k, epsilon); 1564 """"""; 1565 if not hasattr(self, ""_search_graph""):; -> 1566 self._init_search_graph(); 1567 ; 1568 if not self._is_sparse:. /usr/local/lib/python3.7/dist-packages/pynndescent/pynndescent_.py in _init_search_graph(self); 1061 self._distance_func,; 1062 self.rng_state,; -> 1063 self.diversify_prob,; 1064 ); 1065 reverse_graph.eliminate_zeros(). /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. /usr/local/lib/python3.7/dist-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951:2849,config,config,2849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951,1,['config'],['config']
Modifiability,"ocals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 20.0.2 . ubuntu; 20.04. pip list; Package Version Location ; ------------------------- --------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2328,config,config,2328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['config'],['config']
Modifiability,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455:1728,variab,variable,1728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455,2,['variab'],['variable']
Modifiability,"of scanpy. ### What happened?. Hi,. When I try to run the code in https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html# I get an error. ### Minimal code sample. ```python; import numpy as np; import scanpy as sc; import seaborn as sns; from scipy.stats import median_abs_deviation. sc.settings.verbosity = 0; sc.settings.set_figure_params(; dpi=80,; facecolor=""white"",; frameon=False,; ). adata = sc.read_10x_h5(; filename=""filtered_feature_bc_matrix.h5"",; backup_url=""https://figshare.com/ndownloader/files/39546196"",; ). import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython. %%R; library(SoupX). adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp); sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp); sc.pp.neighbors(adata_pp); sc.tl.leiden(adata_pp, key_added=""soupx_groups""). # Preprocess variables for SoupX; soupx_groups = adata_pp.obs[""soupx_groups""]. cells = adata.obs_names; genes = adata.var_names; data = adata.X.T. adata_raw = sc.read_10x_h5(; filename=""raw_feature_bc_matrix.h5"",; backup_url=""https://figshare.com/ndownloader/files/39546217"",; ); adata_raw.var_names_make_unique(); data_tod = adata_raw.X.T. %%R -i data -i data_tod -i genes -i cells -i soupx_groups -o out . # specify row and column names of data; rownames(data) = genes; colnames(data) = cells; # ensure correct sparse format for table of counts and table of droplets; data <- as(data, ""sparseMatrix""); data_tod <- as(data_tod, ""sparseMatrix""). # Generate SoupChannel Object for SoupX ; sc = SoupChannel(data_tod, data, calcSoupProfile = FALSE). # Add extra meta data to the SoupChannel object; soupProf = data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data)); sc = setSoupProfile(sc, soupProf); # Set cluster information in SoupChannel; sc = setClusters(sc, soupx_groups). # Estimate con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685:1245,variab,variables,1245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685,1,['variab'],['variables']
Modifiability,"olcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger in loggers:; handlers = getattr(logger, 'handlers', []); for handler in handlers:; logger.removeHandler(handler); ```. Next time we come across a reproducible case of this happening, we should try this out.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:4225,config,config,4225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"om_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. However i would like to open it with scanpy by:. ```py; loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. and i get the following error:. ```pytb; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:1101,layers,layers,1101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,3,['layers'],['layers']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:2463,variab,variable,2463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call . https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/prepro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:2488,variab,variable,2488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call; https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:2385,variab,variable,2385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata, max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(); sc._settings.ScanpyConfig.n_jobs = os.cpu_count();",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:2517,variab,variable,2517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,['variab'],['variable']
Modifiability,"on3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__(). TypeError: a bytes-like object is required, not 'list'; ```. Conda list output. ```; # packages in environment at /wynton/home/state/alkhairohr/miniconda3/envs/python_env:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; anndata 0.7.3 pypi_0 pypi; attrs 19.3.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; blas 1.0 mkl ; bleach 3.1.5 pyh9f0ad1d_0 conda-forge; blosc 1.19.0 hd408876_0 ; bzip2 1.0.8 h7b6447c_0 ; ca-certificates 2020.1.1 0 ; certifi 2020.6.20 py36_0 ; colorlover 0.3.0 py_0 conda-forge; cycler 0.10.0 py36_0 ; dbus 1.13.6 he372182_0 conda-forge; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; entrypoints 0.3 py36h9f0ad1d_1001 conda-forge; expat 2.2.9 he1b5a44_2 conda-forge; fontconfig 2.13.1 he4413a7_1000 conda-forge; freetype 2.10.2 he06d7ca_0 conda-forge; get-version 2.1 pypi_0 pypi; glib 2.63.1 h3eb4bd4_1 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb31296c_0 ; h5py 2.10.0 pypi_0 pypi; hdf5 1.10.4 hb1b8bf9_0 ; icu 58.2 hf484d3e_1000 conda-forge; importlib-metadata 1.6.1 py36h9f0ad1d_0 conda-forge; importlib_metadata 1.6.1 0 conda-forge; intel-openmp 2020.1 217 ; ipyevents 0.7.1 py_0 conda-forge; ipykernel 5.3.0 py36h95af2a2_0 conda-forge; ipython 7.15.0 py36h9f0ad1d_0 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; ipywidgets 7.5.1 py_0 conda-forge; jedi 0.17.1 py36h9f0ad1d_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.15.1 py_0 ; jpeg 9d h516909a_0 conda-forge; jsonschema 3.2.0 py36h9f0ad1d_1 conda-forge; jupyter 1.0.0 py_2 conda-forge; jupyter_client 6.1.3 py_0 conda-forge; jupyter_console 6.1.0 py_1 conda-forge; jupyter_core 4.6.3 py36h9f0ad1d_1 conda-forge; kiwisolver 1.2.0 py36hfd86e86_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 pypi_0 pypi; leidenalg 0.8.0 pypi_0 pypi; libedit 3.1.20191231 h7b6447c_0 ; libffi 3.3 he6710b0_1 ; libgcc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:6203,plugin,plugins-base,6203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['plugin'],['plugins-base']
Modifiability,"ot args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1887,layers,layers,1887,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,['layers'],['layers']
Modifiability,"ower_normal_function(self, fndesc); 246 # Init argument values; 247 self.extract_function_arguments(); --> 248 entry_block_tail = self.lower_function_body(); 249 ; 250 # Close tail of entry block. ~\anaconda3\lib\site-packages\numba\lowering.py in lower_function_body(self); 271 bb = self.blkmap[offset]; 272 self.builder.position_at_end(bb); --> 273 self.lower_block(block); 274 ; 275 self.post_lower(). ~\anaconda3\lib\site-packages\numba\lowering.py in lower_block(self, block); 286 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block); 290 . ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 723 from numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtrac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:12598,config,config,12598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,pl.pca_loadings fails if `subset=False` when computing highly variables genes.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/315:62,variab,variables,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/315,1,['variab'],['variables']
Modifiability,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728:2232,variab,variable,2232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728,2,['variab'],['variable']
Modifiability,"plotting\_tools\scatterplots.py"", line 542, in umap; return embedding(adata, 'umap', **kwargs); File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 207, in embedding; use_raw=use_raw, gene_symbols=gene_symbols,; File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 865, in _get_color_values; values = adata.raw.obs_vector(value_to_plot); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 413, in obs_vector; idx = self._normalize_indices((slice(None), k)); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 364, in _normalize_indices; var = _normalize_index(var, self.var_names); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 155, in _normalize_index; return name_idx(index); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 142, in name_idx; .format(i)); IndexError: Key ""XKR4"" is not valid observation/variable name/index. ```; However, the gene XKR4 did exist in the var_names:; ```; >>> post_adata.var_names; Index(['XKR4', 'RP1', 'SOX17', 'MRPL15', 'LYPLA1', 'TCEA1', 'RGS20', 'ATP6V1H',; 'OPRK1', 'NPBWR1',; ...; '2700089I24RIK', 'RAB11FIP2', 'E330013P04RIK', 'NANOS1', 'EIF3A',; 'FAM45A', 'SFXN4', 'PRDX3', 'GRK5', 'CSF2RA'],; dtype='object', length=16249); ```. The anndata object looked as below and it was fine when I tried to show the louvain clusters:. ```; >>> post_adata; AnnData object with n_obs × n_vars = 88291 × 16249; obs: 'CellID', 'batch_indices', 'labels', 'local_means', 'local_vars', 'louvain', 'clusters'; var: 'gene_id'; uns: 'neighbors', 'louvain', 'louvain_colors'; obsm: 'X_scVI', 'X_umap'. >>> sc.pl.umap(post_adata, color=['louvain']); ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. #### Versions:; scanpy==1.4.5.post3 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039:1398,variab,variable,1398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039,1,['variab'],['variable']
Modifiability,"pt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 327 in _main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 273 in wrap_session; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 320 in pytest_cmdline_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 198 in console_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:6471,config,config,6471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['config'],['config']
Modifiability,pytest can also be configured in TOML!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1375:19,config,configured,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1375,1,['config'],['configured']
Modifiability,"python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole annd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3398,variab,variable,3398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variable']
Modifiability,"r branch of scanpy. ### What happened?. I'm trying to use `sc.pl.spatial` with the dataset that is available on 10X Visium with the sample ID `CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma`. I can open and do some basic QC just fine, but when I try to plot, I get the error `TypeError: can't multiply sequence by non-int of type 'float`. ### Minimal code sample. ```python; import scanpy as sc; import anndata as an; import pandas as pd; import numpy as np; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import scanorama. sc.set_figure_params(facecolor=""white"", figsize=(8, 8)); sc.settings.verbosity = 3. # Loading dataset; adata = sc.read_visium(; path=r""\external"",; count_file=""CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5"",; load_images=True,; source_image_path=r""\spatial"",; ). adata.var_names_make_unique(). # Quality control; adata.var[""mito""] = adata.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mito""], percent_top=None, log1p=False, inplace=True; ); sc.pl.spatial(adata); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""\scanpy\plotting\_tools\scatterplots.py"", line 1002, in spatial; File ""\plotting\_tools\scatterplots.py"", line 391, in embedding; # if user did not set alpha, set alpha to 0.7; File ""\scanpy\plotting\_utils.py"", line 1107, in circles; if scale_factor != 1.0:; TypeError: can't multiply sequence by non-int of type 'float'; ```; The json file on the spatial folder with the scale factors is as follows:. ```json; {; ""regist_target_img_scalef"": 0.16836435,; ""tissue_hires_scalef"": 0.056121446,; ""tissue_lowres_scalef"": 0.016836435,; ""fiducial_diameter_fullres"": 384.18505640709947,; ""spot_diameter_fullres"": 256.12337093806633; }; ```. `tissue_hires_scalef` is being passed as `scale_factor` variable and hence why it's throwing the error; ### Versions. <details>. ```; scanpy 1.9.6; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778:2144,variab,variable,2144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778,1,['variab'],['variable']
Modifiability,"ranch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; It's very smooth to subset the adata by HVGs when doing `adata = adata[:, adata.var.highly_variable]` in the Scanpy pipeline. But when using the same coding to subeset a new raw adata, it generate errors. Could you please help me to check this issue?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; ACT_sub2 = sc.read('C:/Users/Park_Lab/Documents/ACT_sub2.h5ad') # Scanpy proceeded data; ACT_sub2; AnnData object with n_obs × n_vars = 2636 × 5000; obs: 'leiden', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:1480,layers,layers,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['layers'],['layers']
Modifiability,rank_genes_groups refactoring,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723:18,refactor,refactoring,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723,1,['refactor'],['refactoring']
Modifiability,rank_genes_groups refactoring 2nd try,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156:18,refactor,refactoring,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156,1,['refactor'],['refactoring']
Modifiability,"ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call . https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:2756,variab,variable,2756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,read_10x_mtx() with annData 0.10.4 only reads one variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2825:50,variab,variable,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2825,1,['variab'],['variable']
Modifiability,"recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1624,layers,layers,1624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"rized tab (Thienpont_T-cell_v4_R_fixed.loom). Here are the relevant parts of my conda environment; ```; loompy 2.0.15 <pip>; python 3.6.6 h5001a0f_0 conda-forge; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'),",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/320:1252,layers,layers,1252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320,1,['layers'],['layers']
Modifiability,"rning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 15:; @jit(float32[:](float32[:, :]), nogil=True); def l2_norm(in_matrix):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: Code running in object mode won't allow parallel execution despite nogil=True.; @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""l2_norm"" failed type inference due to: Invalid use of Function(<function norm at 0x7f637ad4ca70>) with argument(s) of type(s): (axis=Literal[int](1), x=array(float32, 2d, A)); * parameterized; In definition 0:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; In definition 1:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<function norm at 0x7f637ad4ca70>); [2] During: typing of call at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (16). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 16:; def l2_norm(in_matrix):; return np.linalg.norm(x=in_matrix, axis=1); ^. @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:3854,parameteriz,parameterized,3854,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,7,['parameteriz'],['parameterized']
Modifiability,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`).; ### Minimal code sample (that we can copy&paste without having any data). ```python; analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False); ```. ```pytb; /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide; residuals = diff / np.sqrt(mu + mu**2 / theta); ```; `analytic_pearson[""X""]` ; Output:; ```py; array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,; -0.08709449, -0.16926342],; [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,; -0.09342913, -0.18157157],; [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,; 12.02085262, 6.06603257],; ...,; [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,; -0.03033674, -0.05896328],; [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,; -0.03077326, -0.05981169],; [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,; -0.03157547, -0.06137084]]); ```. ```py; adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]; adata.layers[""analytic_pearson_residuals""].sum(1); ```; Output:; `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496:1817,layers,layers,1817,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496,2,['layers'],['layers']
Modifiability,"rovement; While the overall structure of the documentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illustrating when and how to use these parameters effectively. 3. **Consistency in Documentation Style**: Ensure that the documentation style is consistent across different parameters, making it easier for users to read and understand. ### Conclusion; Enhancing the documentation of the `MatrixPlot` class will improve the library's usability and user experience. . I am new to open-source contribution and I am eager to contribute to this enhancement, and welcome any additional input or guida",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:1522,enhance,enhancements,1522,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhancements']
Modifiability,"rror with sc.tl.rank_genes_groups that my reference needs to be one of groupby=[cats], but as you can see in the traceback below, it appears identical. It has something to do with the data types, I've been dealing with this for a few weeks now and on some data sets I am eventually able to figure it out and get it to run, on some I am not. I think this is more of a pandas issue than scanpy, so I am wondering what version of pandas you recommend for anndata 0.7.6? Or if you know a simple workaround to make sure that the datatypes match? This has been a massive headache. I use this list(zip()) syntax in my code frequently, have never had an issue with datatypes inside of a zipped object... so this may be a simple pythonic question, but if there is a different method of accomplishing the same thing thing that doesn't introduce this error I would be happy to hear that as well. In the example below, in my adata.obs I have a column 'condition' that is a categorical variable of biological condition for differential expression, and so all cells belong to either group 0 or 1. I have tried seemingly every combination of having these, and the leiden clusters column be integers, strings, objects, before converting to the categorical dtype in line 7 below, always get the same error, and the reference = item always appears identical to the first item in the groupby = list. ### Minimal code sample (that we can copy&paste without having any data). ```python; cluster_method='leiden'; n_genes=1000; g1n='Control'; adata.obs['condition']=adata.obs['condition'].astype('category'); adata.obs[cluster_method]=adata.obs[cluster_method].astype('category'); pairs = list(zip(adata.obs['condition'], adata.obs[cluster_method])); adata.obs['pairs_'+cluster_method]=pairs; adata.obs['pairs_'+cluster_method]=adata.obs['pairs_'+cluster_method].astype('category'); pairs_set = list(set(pairs)); s = sorted(pairs_set); half = int((len(s)/2)); list1 = s[:half]; list2 = s[half:]; lz_cluster_method = list(zip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:1263,variab,variable,1263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['variab'],['variable']
Modifiability,"s #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1310,variab,variable,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['variab'],['variable']
Modifiability,"s, some valuese get lost. . I wasn't able to make a minimal reproducible example, but I obfuscated the `obs` table of my real data and can share it here: ; https://www.dropbox.com/scl/fi/jsbrb2ulki7mmih2242kc/adata_aggregate_bug.h5ad?rlkey=qczuaf2v5vlwb00zyuxmzjkix&dl=1. ### Minimal code sample. ```python; >>> test_adata = sc.read_h5ad(""adata_aggregate_bug.h5ad""). >>> test_adata.obs[""patient_id""].nunique(); 69. >>> test_adata.obs.isnull().sum(); patient_id 0; timepoint 0; external_batch_id 0; dtype: int64. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 15. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:1418,variab,variables,1418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['variab'],['variables']
Modifiability,"s/pytorch_latest/lib/python3.8/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing); 1319 ; 1320 with option_context(""display.max_seq_items"", 10, ""display.width"", 80):; -> 1321 raise KeyError(; 1322 ""Passing list-likes to .loc or [] with any missing labels ""; 1323 ""is no longer supported. "". KeyError: ""Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: CategoricalIndex(['1Ery'], categories=['1Ery', '2Ery', '3Ery', '4Ery', '5Ery', '6Ery', '7MEP', '8Mk', ...], ordered=False, name='paul15_clusters', dtype='category'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike""; ```. #### Versions. <details>. sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.6; autoreload NA; backcall 0.2.0; cffi 1.14.5; configobj 5.0.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; git 3.1.14; gitdb 4.0.7; google NA; gpytorch 1.4.1; h5py 3.2.1; igraph 0.9.6; inferelator NA; ipykernel 5.5.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; matplotlib 3.4.1; mpl_toolkits NA; natsort 7.1.1; numba 0.53.1; numexpr 2.7.3; numpy 1.20.2; packaging 20.9; pandas 1.2.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.18; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pytz 2021.1; scanpy 1.8.2; scipy 1.6.3; seaborn 0.11.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.24.2; smmap 4.0.0; statsmodels 0.12.2; storemagic NA; supirfactor NA; tables 3.6.1; texttable 1.6.3; torch 1.9.0+cu102; tornado 6.1; tqdm 4.60.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; wcwidth 0.2.5; zmq 22.0.3; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2078:6211,config,configobj,6211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2078,1,['config'],['configobj']
Modifiability,"sc.pl.scatter() is a wrapper for _scatter_obs(). It checks to make sure; the variable names the caller is requesting to plot exist in var and/or; obs, but does not take into account whether it should look in raw based; on the use_raw flag, as _scatter_obs() does. This leads to errors when a; user asks to plot variables that are in the raw but not the filtered; matrix of adata. This commit fixes that bug. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,2,['variab'],"['variable', 'variables']"
Modifiability,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731:377,variab,variable,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731,1,['variab'],['variable']
Modifiability,"sc.read_h5ad(results_file); sc.pp.highly_variable_genes(adataMNN, batch_key = 'sample'); var_select = adataMNN.var.highly_variable_nbatches > 1; var_genesMNN = var_select.index[var_select]; datasets = [adataMNN[adataMNN.obs['sample'] == sa].copy() for sa in adataMNN.obs['sample'].cat.categories]; sc.external.pp.mnn_correct(*datasets, var_subset=var_genesMNN, batch_key='sample'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: Code running in object mode won't allow parallel execution despite nogil=True.; @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""l2_norm"" failed type inference due to: Invalid use of Function(<function norm at 0x7f637ad4ca70>) with argument(s) of type(s): (axis=Literal[int](1), x=array(float32, 2d, A)); * parameterized; In definition 0:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; In definition 1:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<function norm at 0x7f637ad4ca70>); [2] During: typing of call at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (16). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 16:; def l2_norm(in_matrix):; return np.linalg.norm(x=in_matrix, axis=1); ^. @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:1410,parameteriz,parameterized,1410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['parameteriz'],['parameterized']
Modifiability,"scanpy version 1.3.7,. I noticed that some figures does not return an ax handle.; Specifically rank_genes_groups_heatmap does not return an axis becouse _rank_genes_groups_plot does not return an axis from the matrix plot. Only tracksplot and stacked_violin seems to return an axis in [_rank_genes_groups_plot](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L258). Also I'm wondering why not all plotting functions return an axes handle (list) always.; It's easy to capture in a variable if not wished to print to screen and easier to keep track of than having to both capture and determine the show=False state, and it would mirror behavior of other figure packages. Any specific reason you do not do this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/419:514,variab,variable,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419,1,['variab'],['variable']
Modifiability,"scanpy version 1.3.7. NOTE!: During typing this issue out i figured out how to solve it and save figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using yo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:802,extend,extended,802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['extend'],['extended']
Modifiability,"se:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. T",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3337,variab,variable,3337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variable']
Modifiability,shared high variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:12,variab,variable,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,1,['variab'],['variable']
Modifiability,small refactor,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2890:6,refactor,refactor,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2890,1,['refactor'],['refactor']
Modifiability,"ster/spatial/integration-scanorama.ipynb. @flying-sheep mentioned that the scanpy tests filter out warnings and indeed you can reproduce these by e.g.,:; ```sh; pytest -W error::FutureWarning -n auto scanpy/tests/test_plotting.py; ```. ### Error output. - [x] `…/scanpy/plotting/_tools/scatterplots.py:401:`. > UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored. - [x] `…/scanpy/plotting/_tools/__init__.py:1269:`. > FutureWarning: The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect. ; > `_ax = sns.violinplot(`. - [x] `…/scanpy/preprocessing/_simple.py:274:`. > ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; > `adata.var[""n_cells""] = number`. - [x] `…/scanpy/plotting/_stacked_violin.py:503: FutureWarning:`. > Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect. ; > `row_ax = sns.violinplot(`. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.10.0.dev191+gf7f5d5c6; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.1.4; parso 0.8.3; pexpect 4.9.0; pluggy 1.3.0; prompt_toolkit 3.0.43; psutil 5.9.7; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pygments 2.17.2; pyparsing 3.1.1; pytest 7.4.4; pytz 2023.3.post1; scipy 1.11.4; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; sitecustomize NA; six 1.16.0; sklearn 1.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839:1531,variab,variable,1531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839,1,['variab'],['variable']
Modifiability,"tations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 932 idcs = idcs[idcs_group]; 933 if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs]); --> 934 else: x += list(adata_X[:, key].X[idcs]); 935 if ikey == 0:; 936 groups += [group for i in range(len(idcs))]. ~\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1301 def __getitem__(self, index):; 1302 """"""Returns a sliced view of the object.""""""; -> 1303 return self._getitem_view(index); 1304 ; 1305 def _getitem_view(self, index):. ~\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1305 def _getitem_view(self, index):; 1306 oidx, vidx = self._normalize_indices(index); -> 1307 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1308 ; 1309 # this is used in the setter for uns, if a view. ~\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 667 if not isinstance(X, AnnData):; 668 raise ValueError('`X` has to be an AnnData object.'); --> 669 self._init_as_view(X, oidx, vidx); 670 else:; 671 self._init_as_actual(. ~\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 724 self._X = None; 725 else:; --> 726 self._init_X_as_view(); 727 ; 728 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 751 shape = (; 752 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 753 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 754 ); 755 if np.isscalar(X):. ~\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 141 return 1; 142 else:; --> 143 return len(idx). TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333:2404,layers,layers,2404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333,1,['layers'],['layers']
Modifiability,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/5:63,config,configuring,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5,3,"['config', 'plugin']","['configuring', 'plugin']"
Modifiability,"tion/pca.py in _fit(self, X); 380 ; 381 X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,; --> 382 copy=self.copy); 383 ; 384 # Handle n_components==None. ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator); 556 "" a minimum of %d is required%s.""; 557 % (n_features, array.shape, ensure_min_features,; --> 558 context)); 559 ; 560 if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:. ValueError: Found array with 0 feature(s) (shape=(44495, 0)) while a minimum of 1 is required.; ```. The `pca` code doesn't error here, because `highly_variable_intersection` makes `'highly_variable' in adata.var.keys()` evaluate to `True`:; ```; if use_highly_variable is True and 'highly_variable' not in adata.var.keys():; raise ValueError('Did not find adata.var[\'highly_variable\']. '; 'Either your data already only consists of highly-variable genes '; 'or consider running `pp.highly_variable_genes` first.'); if use_highly_variable is None:; use_highly_variable = True if 'highly_variable' in adata.var.keys() else False; if use_highly_variable:; logg.info(' on highly variable genes'); adata_comp = adata[:, adata.var['highly_variable']] if use_highly_variable else adata. ```; ```pytb; adata.var.keys(); Index(['mito', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts',; 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts',; 'n_cells', 'highly_variable', 'means', 'dispersions',; 'dispersions_norm', 'highly_variable_nbatches',; 'highly_variable_intersection'],; dtype='object'); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post2 anndata==0.7.1 umap==0.3.10 numpy==1.17.0 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.3 statsmodels==0.11.0dev0+630.g4565348 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:2948,variab,variable,2948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,2,['variab'],['variable']
Modifiability,"tion=0.5, random_state=seed); sc.pl.umap(adata, color=['batch', 'leiden'], alpha=0.3); print(adata.uns['neighbors']['connectivities'].sum()). adata_neigh = adata.copy(); sc.pp.neighbors(adata_neigh, metric='cosine', random_state=seed); sc.tl.umap(adata_neigh, random_state=seed); sc.tl.leiden(adata_neigh, resolution=0.6, random_state=seed); sc.pl.umap(adata_neigh, color=['batch', 'leiden'], alpha=0.3); print(adata_neigh.uns['neighbors']['connectivities'].sum()); ```. Our matrices are the same (the sums are 4918.370372081173 and 5005.088472351332), so the random generation works, but then the UMAPs and clustering solutions are different. For the adata run with `sc.pp.neighbors` (left are batches and right are `leiden` cluster labels):; Mine; ![image](https://user-images.githubusercontent.com/35657291/73087502-e12b7f80-3ed2-11ea-9df9-177cec32d208.png). Hers; ![image](https://user-images.githubusercontent.com/35657291/73087537-f0aac880-3ed2-11ea-8c45-115023be9bff.png). For the adata run with `sce.pp.bbknn`:; Mine; ![image](https://user-images.githubusercontent.com/35657291/73087518-ea1c5100-3ed2-11ea-97de-4b6ce0e0d389.png). Hers; ![image](https://user-images.githubusercontent.com/35657291/73087545-f43e4f80-3ed2-11ea-82b9-a5dcb5d804b7.png). Our PCA decomposition has the same coordinates, so we discard the PCA as the source of variability. Also, since both UMAP and leiden look different, we think the source might come from the neighbor calculation. When running `adata.uns['neighbors']['connectivities'].sum()` I get 801.5580058219996 and 1204.5274490986717 for `adata` and `adata_neigh`. I don't have her values now, but the values using the _real_ dataset were in the order of 5000, and they were of by less than 0.001; so we are confused that with such a small difference on the sum, the results can be so different. I attach the adatas for you to inspect them if you need more info. [adatas.zip](https://github.com/theislab/scanpy/files/4109668/adatas.zip). Thanks for the help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009:2595,variab,variability,2595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009,1,['variab'],['variability']
Modifiability,trackplot enhancement,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1195:10,enhance,enhancement,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1195,1,['enhance'],['enhancement']
Modifiability,"ts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X value of MALAT1 in cell: (0, 0)	142.0. Run 2: after sc.pp.normalize_total: ; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862. Run 3: normalization, specifing argument layer=None; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2921,layers,layers,2921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python; import numpy as np; pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""); # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms; # Wall time: 74.9 ms; to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]; sc.pl.umap(pbmc, color=to_plot, ncols=2); ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:2428,layers,layers,2428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['layers'],['layers']
Modifiability,"typo, `variabes->variables`. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [X] Tests included or not required because:; small change in documentation; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:; minor change in documentation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2793:7,variab,variabes,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2793,2,['variab'],"['variabes', 'variables']"
Modifiability,"ues, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. The arguments I've passed there are whats in the documentation for the function, so I'd figured I'd give them a shot first. I've also tried `host=""www.ensembl.org/biomart""`, but had no such luck. This is after installing the `bioservices` module via `pip` and it creating some config file the first time I tried to run the query.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242:3335,config,config,3335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242,1,['config'],['config']
Modifiability,"v(dir + 'GSM5226574_C51ctr_raw_counts.csv.gz').T; import scvi ; sc.pp.filter_genes(adata, min_cells = 10); sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); ```. ```pytb; >>> sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 422, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip instal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1507,plugin,plugins,1507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['plugin'],['plugins']
Modifiability,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:1642,extend,extending,1642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['extend'],['extending']
Modifiability,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'; ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/593:2069,variab,variables,2069,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593,2,['variab'],['variables']
Modifiability,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/826:180,variab,variable,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826,3,['variab'],['variable']
Modifiability,"wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:1423,adapt,adapting,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['adapt'],['adapting']
Modifiability,"y on my end; I've been able to reproduce the error with my own data and one of the scanpy built-in test datasets). Sorry the code chunks are broken up/a little long; I am using the scran normalization approach outlined in the [single cell tutorial](https://github.com/theislab/single-cell-tutorial). ```python; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_cells = 1). # scran normalization; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after = 1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps = 15); sc.pp.neighbors(adata_pp); sc.tl.leiden(adata_pp, key_added = 'groups', resolution = 0.5); input_groups = adata_pp.obs['groups']; data_mat = adata.X.T; ```; ```python; %%R -i data_mat -i input_groups -o size_factors; size_factors = sizeFactors(computeSumFactors(SingleCellExperiment(list(counts = data_mat)), ; clusters = input_groups, ; min.mean = 0.1)); ```; ```python; del adata_pp; adata.obs['size_factors'] = size_factors; adata.layers['counts'] = adata.X.copy(); adata.X /= adata.obs['size_factors'].values[:,None]; sc.pp.log1p(adata); adata.X = sp.sparse.csr_matrix(adata.X); adata.raw = adata. sc.pp.highly_variable_genes(adata, flavor = 'cell_ranger', n_top_genes = 2000); sc.pp.pca(adata, n_comps = 50, use_highly_variable = True, svd_solver = 'arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata); adata.uns['log1p'] # this produces: {'base': None}; adata.write('test.h5ad'). adata = sc.read('test.h5ad'); adata.uns['log1p'] # the now produces: {}; sc.tl.leiden(adata, key_added='leiden_r1.0'); sc.tl.rank_genes_groups(adata, groupby='leiden_r1.0', key_added='rank_genes_all', method='wilcoxon') # this causes an error; ```. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Input In [13], in <cell line: 2>(); 1 # Calculate marker genes; ----> 2 sc.tl.rank_genes_groups(adata, groupby='leiden_r1.0', key_added='rank_genes_all', method='wilcoxon', use_raw=F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2181:2690,layers,layers,2690,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2181,1,['layers'],['layers']
Modifiability,"y-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1907,variab,variable,1907,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,2,['variab'],['variable']
Modifiability,"| 297 |; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:1390,variab,variable,1390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Performance," 2D DatetimeArray; 1202 result = np.asarray(self._values[key]); -> 1203 disallow_ndim_indexing(result); 1204 return result; 1206 if not isinstance(self.index, MultiIndex):. File ~\AppData\Roaming\Python\Python311\site-packages\pandas\core\indexers\utils.py:341, in disallow_ndim_indexing(result); 333 """"""; 334 Helper function to disallow multi-dimensional indexing on 1D Series/Index.; 335 ; (...); 338 in GH#30588.; 339 """"""; 340 if np.ndim(result) > 1:; --> 341 raise ValueError(; 342 ""Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer ""; 343 ""supported. Convert to a numpy array before indexing instead.""; 344 ). ValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.; ```. ### Versions. <details>. ```-----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; annoy NA; anyio NA; asttokens NA; attr 22.1.0; autograd NA; autograd_gamma NA; babel 2.11.0; backcall 0.2.0; bbknn 1.6.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; brotli NA; certifi 2024.02.02; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.6; entrypoints 0.4; executing 0.8.3; fastjsonschema NA; formulaic 1.0.1; future 0.18.3; gseapy 1.1.2; h5py 3.9.0; hypergeom_ufunc NA; idna 3.4; igraph 0.11.5; interface_meta 1.3.0; invgauss_ufunc NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; lifelines 0.28.0; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.59.1; nume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:6484,bottleneck,bottleneck,6484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['bottleneck'],['bottleneck']
Performance," = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filename, return_ext=True); --> 491 is_present = check_datafile_present_and_download(; 492 filename,; 493 backup_url=backup_url,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in check_datafile_present_and_download(path, backup_url); 745 path.parent.mkdir(parents=True); 746 ; --> 747 download(backup_url, path); 748 return True; 749 . ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in download(url, path); 722 ; 723 path.parent.mkdir(parents=True, exist_ok=True); --> 724 with tqdm(unit='B', unit_scale=True, miniters=1, desc=path.na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:1509,cache,cache,1509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['cache'],['cache']
Performance," CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata). # Then run on a machine with 16 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata). # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:2002,cache,cache,2002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['cache'],['cache']
Performance," I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipywidgets 8.1.2; isoduration NA; jedi 0.19.1; jinja2 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:1073,cache,cache,1073,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['cache'],['cache']
Performance," It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None and max_cells is None else X > 0, axis=0; ^^^^^; TypeError: '>' not supported between instances of 'CSRDataset' and 'int'; ```. ### Versions. <details>. ```; Matplotlib is building the font cache; this may take a moment.; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 10.2.0; colorama 0.4.6; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; h5py 3.10.0; igraph 0.11.4; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.8.3; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numpy 1.26.4; packaging 23.2; pandas 2.2.1; psutil 5.9.8; pyparsing 3.1.1; pytz 2024.1; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.1.post1; texttable 1.7.0; threadpoolctl 3.3.0; typing_extensions NA; wcwidth 0.2.13; -----; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]; Linux-5.4.0-165-generic-x86_64-with-glibc2.31; -----; Session information updated at 2024-03-05 10:07; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:2191,cache,cache,2191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['cache'],['cache']
Performance," ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:1605,cache,cache,1605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,3,['cache'],['cache']
Performance," exists on the main branch of scanpy. ### What happened?. I have a bunch of matrix.mtx, barcode.tsv and genes.tsv. When I want to read in the files with:; `data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1235,cache,cache,1235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance," f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4367,cache,cache,4367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance," files I get an error (code and error attached).; I've tried it on a file for which this command previously worked (but now it no longer does) as well as trying to merge two loom files, which also gives me this error. So I believe it's to do with a package version rather than the files themselves, but I cannot seem to figure out which packages are responsible and which versions I would need instead.; Any help would be greatly appreciated!. My code:; adata_vel = scv.utils.merge(adata, adatal). This is my error:; <img width=""844"" alt=""Screenshot 2023-03-09 at 11 43 11"" src=""https://user-images.githubusercontent.com/85882727/224000508-fb6a0e9a-c49d-4186-a531-88233f3cfc82.png"">. #### Versions. -----; anndata 0.8.0; scanpy 1.9.3; -----; OpenSSL 20.0.1; PIL 8.2.0; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anndata2ri 1.0.6; annoy NA; anyio NA; appnope 0.1.2; asttokens NA; astunparse 1.6.3; attr 21.4.0; babel 2.9.0; backcall 0.2.0; backports NA; boto3 1.26.7; botocore 1.29.7; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cryptography 3.4.7; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; debugpy 1.5.1; decorator 5.0.6; dot_parser NA; dunamai 1.6.0; executing 0.8.2; fbpca NA; flatbuffers NA; fsspec 0.7.4; gast 0.5.3; get_version 3.5; google NA; gprofiler 1.0.0; h5py 3.7.0; idna 2.10; igraph 0.10.2; importlib_resources NA; intervaltree NA; ipykernel 6.8.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.2; jmespath 1.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.7.0; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.7.1; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numpy 1.22.0; opt_einsum v3.3.0; packaging 20.9; pandas 1.2.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2443:1311,bottleneck,bottleneck,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2443,1,['bottleneck'],['bottleneck']
Performance," intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1667,load,loading,1667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance," last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1651,cache,cache,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,2,['cache'],['cache']
Performance," loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.Scanpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1828,load,loading,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance," make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I am running the Scrublet function to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1021,cache,cache,1021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance," message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {};",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1464,cache,cache,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['cache'],['cache']
Performance," or package error. I am getting Segmentation fault error with sc.pp.calculate_qc_metrics with my data. I tried running this on eg. data, and have the same error. I think this might be related to numba issue, but not sure. I did ran python debugger on the script, also attaching the output i got. ; [scanpylog.txt](https://github.com/theislab/scanpy/files/4567012/scanpylog.txt). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; #!/usr/bin/env python; import os, sys ; import scanpy as sc; import scanpy.external as sce; import scipy as sp; import numpy as np; import pandas as pd; os.getcwd(). #sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); #sc.logging.print_versions() #this also give segmentation fault error . #1102 external eg file ; #curl -o pbmc_1k_v2_filtered_feature_bc_matrix.h5 -O http://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v2/pbmc_1k_v2_filtered_feature_bc_matrix.h5. #load file; ext_AD = sc.read_10x_h5('/home/pjb40/scratch/KimCarla_Timeseries_scRNAseq_lung_cancer_organoids_hbc03856_scrathDir/data/TimeSeries_epithelial_ScanpyNotebook/ext_data/pbmc_1k_v2_filtered_feature_bc_matrix.h5', gex_only = True). ext_AD.var_names_make_unique(). print(ext_AD). sc.pp.calculate_qc_metrics(ext_AD, inplace=True). print (ext_AD). ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""timeseriesScanpy.py"", line 65, in <module>; sc.pp.calculate_qc_metrics(ext_AD, inplace=True); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 274, in calculate_qc_metrics; parallel=parallel; File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 102, in describe_obs; proportions = top_segment_proportions(X, percent_top, parallel); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:1090,load,load,1090,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['load'],['load']
Performance," read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:2449,race condition,race condition,2449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance," same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python; adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""); n_genes = 1491; for i in range(10):; sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:; all_unique = list(set(unique_genes)); print(f""total {len(all_unique)} unique genes""); else:; all_unique = list(set(all_unique+unique_genes)); print(f""total {len(all_unique)} unique genes""); ```. ### Error output. ```pytb; total 1491 unique genes; total 1814 unique genes; total 2042 unique genes; total 2163 unique genes; total 2237 unique genes; total 2305 unique genes; total 2356 unique genes; total 2401 unique genes; total 2437 unique genes; total 2453 unique genes; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 8.4.0; asciitree NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.02.1; dateutil 2.8.1; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; django 4.1.3; entrypoints 0.4; executing 0.8.3; fasteners 0.18; fsspec 2023.4.0; google NA; h5py 3.6.0; igraph 0.10.6; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; jupyter_server 1.13.5; kiwisolver 1.2.0; kneed 0.8.3; leidenalg 0.10.1; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mishalpy NA; mpl_toolkits NA; mpmath 1.2.1; msgpack 1.0.2; natsort 8.3.1; nbinom_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numcodecs 0.11.0; numexpr 2.8.1; numpy 1.21.6; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.3; parso 0.8.3; pickleshare 0.7.5; pkg_resources NA; plotly 5.6.0; prompt_toolkit 3.0.20; psutil 5.8.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrenc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2579:1444,bottleneck,bottleneck,1444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579,1,['bottleneck'],['bottleneck']
Performance," sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True); dp.add_totals(size=1.2)\; .legend(color_title='log(UMI count+1)', width=1.6, show_size_legend=True)\; .style(cmap='Blues', dot_edge_color='black', dot_edge_lw=1, size_exponent=1.5)\; .show(); ```; All objects have consistent functions for `legend`, to set up titles and width, `style()` to set visual parameters specific to each plot like colormap, edge color, linewidth. `swap_axes` to transpose the figure, `add_dendrogram` with options to change the with of the dendrogran and `add_total` tho show a bar plot of the total number of cells per category. Also includes options to sort the categories. ![image](https://user-images.githubusercontent.com/4964309/81702505-a9d5e400-946b-11ea-823b-018f5dadac84.png). Here is description of changes:; **all figures**:; * Set a title to the image. ; * Pass an `axe` where to plot the image.; * Return a dictionary of axes for further manipulation; * using `return_fig` the plot object can be used to adjust the proportions to the legend and other visual aspects can be fine tuned.; * a bar plot with the totals per category can be added. This will align at the left of the image if the categories are plotted in rows or at the top if they are plotted in columns.; * legend can be removed; * `groupby` can be a list of categories. . **dotplot**; * Improved the colorbar and size legend for dotplots. Now the colorbar and size have titles, which can be modified using the `colorbar_title` and `size_title` arguments. They also align at the bottom of the image and do not shrink if the dotplot image is smaller.; * Plot genes in rows and categories in columns (swap_axes).; * Using the DotPlot object the dot_edge_color and line width can be set up, a grid added as well as several other features; * `sc.pl.rank_genes_groups_dotplot` can now plot `pvals` and `log fold changes`. **matrixplot**; * added title for colorbar and positioned as in dotplot; * `sc.pl.rank_genes_groups_matrixplo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210:2826,tune,tuned,2826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210,1,['tune'],['tuned']
Performance," scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; with plt.style.context(""seaborn-white""):; fig, ax = plt.subplots(figsize=(8,8)); sc.pl.umap(adata_clr, color=['id_jira'], ax=ax, s=200 ); fig.suptitle(""SOLEXASEQ-1672/1680 | Aitchison Distance [THIS SHOULD BE SUPTITLE]"", fontsize=15); ax.set_title(""SOLEXASEQ-1672/1680 | Aitchison Distance [THIS SHOULD BE TITLE]"", fontsize=15). ```; ![image](https://user-images.githubusercontent.com/9061708/235007925-b7e1c2ae-924c-4fb3-b840-b2d12c15725f.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.3; -----; Bio 1.79; PIL 9.0.1; PyQt5 NA; adjustText NA; appnope 0.1.2; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; cachecontrol 0.12.10; certifi 2022.12.07; cffi 1.15.0; charset_normalizer 2.0.11; colorama 0.4.4; comm 0.1.1; compositional 2022.8.31; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.4; decorator 5.1.1; defusedxml 0.7.1; ensemble_networkx 2023.1.23; entrypoints 0.4; ete3 3.1.2; executing 0.8.2; fastcluster 1.1.26; fontTools 4.29.1; h5py 3.7.0; hdmedians NA; hive_networkx 2021.05.18; hypergeom_ufunc NA; idna 3.3; igraph 0.10.2; ipykernel 6.19.4; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.23.4; kiwisolver 1.3.2; leidenalg 0.9.1; llvmlite 0.38.0; lxml 4.7.1; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; matplotlib_venn 0.11.6; mpl_toolkits NA; msgpack 1.0.3; natsort 8.1.0; nbinom_ufunc NA; networkx 2.6.3; numba 0.55.1; numpy 1.21.5; packaging 21.3; palettable 3.3.0; pandas 1.4.0; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.3.0; prompt_toolkit 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2477:1147,cache,cachecontrol,1147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2477,1,['cache'],['cachecontrol']
Performance, scipy=1.11.2; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.2; - session-info=1.0.0; - setuptools=68.1.2; - simplejson=3.19.2; - six=1.16.0; - snappy=1.1.10; - sniffio=1.3.0; - soupsieve=2.3.2.post1; - stack_data=0.6.2; - statsmodels=0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astun,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:14694,cache,cachetools,14694,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cachetools']
Performance," to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = No",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1073,cache,cache,1073,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,3,['cache'],['cache']
Performance," var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'log1p', 'pca', 'neighbors', 'umap', 'leiden', 'leiden_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'. ```. Ive tried to check whether the data is maybe different or something, but i dont see anything that could be causing these differences, could you please help trying to figure out why the leiden clustering suddenly produces different results? . ### Minimal code sample. ```python; sc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 20); sc.tl.umap(adata); sc.tl.leiden(adata, resolution = 0.2) ; sc.pl.umap(adata, color='leiden'); ```. ### Error output. _No response_. ### Versions. <details>. ```; sc.logging.print_versions(); -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 9.4.0; PyQt5 NA; adjustText 1.0.4; asttokens NA; atomicwrites 1.4.1; bottleneck 1.3.5; brotli NA; bs4 4.12.2; certifi 2024.02.02; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.2.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.8; executing 2.0.1; gseapy 1.1.2; h5py 3.9.0; html5lib 1.1; idna 3.4; igraph 0.11.3; ipykernel 6.29.2; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; kiwisolver 1.4.4; leidenalg 0.10.2; llvmlite 0.42.0; lxml 5.1.0; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mkl 2.4.1; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.4; numpy 1.24.3; packaging 23.1; pandas 2.0.3; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; platformdirs 3.10.0; prompt_toolkit 3.0.42; psutil 5.9.0; pure_eval 0.2.2; pyarrow 11.0.0; pycparser 2.21; pydeseq2 0.4.7; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pynndescent ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956:2914,bottleneck,bottleneck,2914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956,1,['bottleneck'],['bottleneck']
Performance," when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1041,cache,cache,1041,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['cache'],['cache']
Performance,"## Feature type; <!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. ## Request; <!-- Please describe your wishes below: -->; The `scanpy.pp.subsample` function has been really useful for iteration, experimentation, and tutorials. However, I've found that sometimes I don't want to (or simply can't) read the entire `AnnData` object into memory before subsampling. As such, it would be nice to instantiate an `AnnData` object in backed mode, then subsample that directly. ### Current behavior:. ```python; import scanpy as sc. adata = sc.read_h5ad(""matrix/scatlas.h5ad"", backed=""r""); adata_sample = sc.pp.subsample(adata, fraction=0.01, copy=True); ```; yields:; ```; ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`.; ```; ### Workaround:; A workaround I've found is by creating a boolean array and using boolean indexing to grab a random subset of rows from the `AnnData` object:. ```python; import numpy as np. FRACTION = 0.01. np.random.seed(0); random_bool = np.random.choice(; [True, False], size=adata.shape[0], p=[FRACTION, 1 - FRACTION]; ). adata_sample = adata[random_bool, :]; adata_sample_mem = adata_sample.to_memory(); ```. It's easy enough but would be nice to have within `scanpy` itself!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2495:1151,load,load,1151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495,1,['load'],['load']
Performance,"### Minimal code sample (that we can copy&paste without having any data). when I follow your tutorial on https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html; to the ; ```python; cdata = sc.external.pp.mnn_correct(alldata['covid_1'],alldata['covid_15'],alldata['covid_17'],; alldata['ctrl_5'],alldata['ctrl_13'],alldata['ctrl_14'], ; svd_dim = 50, batch_key = 'sample', save_raw = True, var_subset = var_genes); ```; line, then it said; `[1] 1764091 illegal hardware instruction (core dumped`. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.2.0; bottleneck 1.3.2; cffi 1.14.5; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; fsspec 0.9.0; h5py 2.10.0; igraph 0.9.8; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.36.0; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 8.0.0; numba 0.53.1; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; pkg_resources NA; psutil 5.8.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; six 1.15.0; sklearn 0.24.1; sphinxcontrib NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 5.4.1; zope NA; -----; Python 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0]; Linux-5.11.0-38-generic-x86_64-with-glibc2.10; 64 logical CPU cores; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2044:628,bottleneck,bottleneck,628,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2044,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. (extracted from #3167). Utilizing the murine hematopoietic progenitors from [Nestorowa et al., 2016](https://doi.org/10.1182/blood-2016-05-716480), as well as the regev_lab_cell_cycle_genes.txt, one issue is apparent. Currently the code doesn’t produce the expected number of bins of equal or approximately equal size. Bin 24 is empty when n_bins = 25.; ![current_hist](https://github.com/user-attachments/assets/35e5d1a4-fdd0-406e-a2f9-1b53efacc8fa). ### The current ranking system code within score_genes(); ```py; n_items = int(np.round(len(obs_avg) / (n_bins - 1))); obs_cut = obs_avg.rank(method=""min"") // n_items; ```. ### The modified code in #3167; ```py; obs_avg.sort_values(ascending=True, inplace=True); n_items = int(np.ceil(len(obs_avg) / (n_bins))); rank = np.repeat(np.arange(n_bins), n_items)[:len(obs_avg)]; obs_cut = pd.Series(rank, index=obs_avg.index); ```. The modified code performs as expected producing 25 bins containing approximately equal number of genes. The last bin can have up to 24 less than expected because the total number of genes is not perfectly divisible by 25.; ![modified_hist](https://github.com/user-attachments/assets/6ced8eec-b56b-4642-a33c-71acaee98369). ### Minimal code sample. ```python; TODO; ```. ### Error output. _No response_. ### Versions. <details>. ```; Python 3.10.12 ; scanpy==1.10.2 anndata==0.10.8 umap==0.5.6 numpy==1.26.4 scipy==1.14.0 pandas==2.2.2 scikit-learn==1.5.0 statsmodels==0.14.2 igraph==0.11.6 louvain==0.8.2 pynndescent==0.5.13; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3168:1185,perform,performs,1185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3168,1,['perform'],['performs']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi!. I am new to scanpy and I am facing some trouble reading my data in an appropriate way. I noticed that anndata objects in memory require roughly 4x the space they require on disk, so working with large datasets (>50GB on disk) is prohibitive in most scenarios. The USP of h5 files, however, is that you can index and slice them on disk as if they were in memory. This way I could greatly reduce the data size before loading it into memory. However, when I attempt to filter on a backed anndata object, I encounter a TypeError. The case of gene filtering should be just a column-sum, comparing it against a threshold and then saving it as a boolean index mask. It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:709,load,loading,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['load'],['loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I recently started using scanpy and python. I am running into the following error. Any help is much appreciated. ![image](https://github.com/user-attachments/assets/76e1c619-a764-45ac-a475-4bad342854b1). ### Minimal code sample. ```python; sc.pl.umap(adata,color =[""leiden""]); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.6; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; get_annotations NA; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 1.5.3; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215:754,bottleneck,bottleneck,754,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I have a bunch of matrix.mtx, barcode.tsv and genes.tsv. When I want to read in the files with:; `data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:478,cache,cache,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. In #3048 we started raising errors for functions that don’t support backed mode, but seems like a tutorial used `dendrogram` in backed mode: https://scverse-tutorials.readthedocs.io/en/latest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:659,load,loaded,659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['load'],['loaded']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Loading data in backed mode, I get an AxisError when trying to calculate QC metrics. Problem has happened on three different datasets but doesn't happen when I read the data into memory. ### Minimal code sample. ```python; sc.datasets.pbmc3k(); pbmc = sc.read_h5ad('data/pbmc3k_raw.h5ad', backed = 'r+'); pbmc.var['mt'] = pbmc.var_names.str.startswith('MT-'); pbmc.var['ribo'] = pbmc.var_names.str.startswith((""RPS"", ""RPL"")); sc.pp.calculate_qc_metrics(pbmc, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AxisError Traceback (most recent call last); Cell In[8], line 3; 1 pbmc.var['mt'] = pbmc.var_names.str.startswith('MT-'); 2 pbmc.var['ribo'] = pbmc.var_names.str.startswith((""RPS"", ""RPL"")); ----> 3 sc.pp.calculate_qc_metrics(pbmc, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True). File ~/miniconda3/envs/parse_sepsis/lib/python3.12/site-packages/scanpy/preprocessing/_qc.py:315, in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 312 if isinstance(qc_vars, str):; 313 qc_vars = [qc_vars]; --> 315 obs_metrics = describe_obs(; 316 adata,; 317 expr_type=expr_type,; 318 var_type=var_type,; 319 qc_vars=qc_vars,; 320 percent_top=percent_top,; 321 inplace=inplace,; 322 X=X,; 323 log1p=log1p,; 324 ); 325 var_metrics = describe_var(; 326 adata,; 327 expr_type=expr_type,; (...); 331 log1p=log1p,; 332 ); 334 if not inplace:. File ~/miniconda3/envs/parse_sepsis/lib/python3.12/site-packages/scanpy/preprocessing/_qc.py:109, in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, lay",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3004:289,Load,Loading,289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3004,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When i previously performed leiden clustering on my data, the shape of the UMAP changed, as expected. . However, when i now try to reproduce my results, I suddenly am only able to get the leiden clustering that follows the distribution of the unclustered umap . Unclustered UMAP; ![UMAP_ADvsCT_3-18-2024](https://github.com/scverse/scanpy/assets/127406679/2fed0a0c-b20e-425a-a99f-7b1c61f64242). Clustered UMAP: ; ![UMAP_ADvsCT](https://github.com/scverse/scanpy/assets/127406679/a1272099-fde4-4025-ad61-392d1342cd0c). the dataset with which i produced the clustered UMAP: ; ```; adata3; Out[505]: ; AnnData object with n_obs × n_vars = 13243 × 10850; obs: 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'sample', 'group', 'disease_status', 'leiden'; var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'disease_status_colors', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'sample_colors', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'connectivities', 'distances'; ```. the dataset with which i produced the unclustered UMAP: ; ```; adata; Out[518]: ; AnnData object with n_obs × n_vars = 13243 × 10850; obs: 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'sample', 'group', 'disease_status', 'leiden'; var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956:307,perform,performed,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956,1,['perform'],['performed']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I just copied the code of official example, and changed the path to my own documents. But it seems that it doesn't work. One folder of the total containing ""barcodes"", ""features"" and ""matrix"" has been attached below and the entire raw data comes from here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139324. ### Minimal code sample. ```python; import scanpy as sc. path = r'RAW'; adata = sc.read_10x_mtx('RAW/HD PBMC_1/',var_names='gene_symbols', cache=True) ; adata.var_names_make_unique(); sc.pp.filter_cells(adata, min_genes=200) ; sc.pp.filter_genes(adata, min_cells=3); ```. ### Error output. ```pytb; IndexError: index (2444) out of range; ```. ### Versions. <details>. ```. ```. </details>. [HD PBMC_1.zip](https://github.com/scverse/scanpy/files/13404294/HD.PBMC_1.zip); ![2](https://github.com/scverse/scanpy/assets/147734739/0db8909d-84e4-41c5-8beb-09480d7adc3a)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2759:749,cache,cache,749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2759,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I tried to import scanpy and got an error. ### Minimal code sample. ```python; import scanpy as sc; ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>; import scanpy as sc; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>; from ._utils import check_versions; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>; from anndata import AnnData, __version__ as anndata_version; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>; from ._core.anndata import AnnData; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>; import h5py; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>; from . import version; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>; from . import h5 as _h5; File ""h5py\h5.pyx"", line 1, in init h5py.h5; ImportError: DLL load failed while importing defs: The specified procedure could not be found.; ```. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2542:1456,load,load,1456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542,1,['load'],['load']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I'm trying to use `sc.pl.spatial` with the dataset that is available on 10X Visium with the sample ID `CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma`. I can open and do some basic QC just fine, but when I try to plot, I get the error `TypeError: can't multiply sequence by non-int of type 'float`. ### Minimal code sample. ```python; import scanpy as sc; import anndata as an; import pandas as pd; import numpy as np; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import scanorama. sc.set_figure_params(facecolor=""white"", figsize=(8, 8)); sc.settings.verbosity = 3. # Loading dataset; adata = sc.read_visium(; path=r""\external"",; count_file=""CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5"",; load_images=True,; source_image_path=r""\spatial"",; ). adata.var_names_make_unique(). # Quality control; adata.var[""mito""] = adata.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mito""], percent_top=None, log1p=False, inplace=True; ); sc.pl.spatial(adata); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""\scanpy\plotting\_tools\scatterplots.py"", line 1002, in spatial; File ""\plotting\_tools\scatterplots.py"", line 391, in embedding; # if user did not set alpha, set alpha to 0.7; File ""\scanpy\plotting\_utils.py"", line 1107, in circles; if scale_factor != 1.0:; TypeError: can't multiply sequence by non-int of type 'float'; ```; The json file on the spatial folder with the scale factors is as follows:. ```json; {; ""regist_target_img_scalef"": 0.16836435,; ""tissue_hires_scalef"": 0.056121446,; ""tissue_lowres_scalef"": 0.016836435,; ""fiducial_diamet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778:899,Load,Loading,899,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:327,load,load,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,3,"['cache', 'load']","['cache', 'load']"
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Installation using pip & installation from github repository. ### Minimal code sample. ```python; pip install scanpy; ```. ### Error output. ```pytb; Installing collected packages: tbb, distlib, asciitree, stdlib-list, setuptools-scm, pbr, numcodecs, nodeenv, natsort, igraph, identify, filelock, fasteners, docutils, cfgv, array-api-compat, accessible-pygments, zarr, virtualenv, sphinx, session-info, pytest-nunit, pytest-mock, profimp, mdit-py-plugins, leidenalg, sphinxext-opengraph, sphinx-design, sphinx-copybutton, sphinx-autodoc-typehints, scanpydoc, pynndescent, pydata-sphinx-theme, pre-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryorn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:957,cache,cache,957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. RGBA colors that are provided as tuples or lists in `adata.uns[f""{key}_colors""]` are converted to numpy arrays when saving and loading the adata. This leads plots (e.g. `pl.pca` or `pl.umap`) to run into an error (see below). ### Minimal code sample. ```python; adata = sc.datasets.pbmc3k_processed(); adata = adata[adata.obs[""louvain""].isin([""Dendritic cells"",""Megakaryocytes""])].copy(); adata.uns[""louvain_colors""] = [[1, 0, 0, 1], [0, 0, 1, 1]] # or [(1, 0, 0, 1), (0, 0, 1, 1)]; sc.pl.pca(adata, color=""louvain""). adata.write(""./data/pbmc3k_processed_rgba.h5ad""). adata = sc.read(""./data/pbmc3k_processed_rgba.h5ad""); print(type(adata.uns[""louvain_colors""][0])) # --> numpy.ndarray; sc.pl.pca(adata, color=""louvain""); ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); ...; 8 adata = sc.read(""./data/pbmc3k_processed_rgba.h5ad""); 9 print(type(adata.uns[""louvain_colors""][0])) # --> numpy.ndarray; ---> 10 sc.pl.pca(adata, color=""louvain""). File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:893, in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 845 """"""\; 846 Scatter plot in PCA coordinates.; 847 ; (...); 890 pp.pca; 891 """"""; 892 if not annotate_var_explained:; --> 893 return embedding(; 894 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 895 ); 896 else:; 897 if 'pca' not in adata.obsm.keys() and 'X_pca' not in adata.obsm.keys():. File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:267, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2730:418,load,loading,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2730,1,['load'],['loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydev",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:965,cache,cache,965,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,2,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When running phenograph from the external module, and specifying `method='leiden'`, the output says that: . Running Louvain modularity optimization ; After 1 runs, maximum modularity is Q = 0.794615; After 16 runs, maximum modularity is Q = 0.796318; Louvain completed 36 runs in 2.867233991622925 seconds. `Louvain` in the above example should be changed to `Leiden`. . ### Minimal code sample. ```python; from scanpy import external. df = pd.DataFrame(np.zeros(1000,10)); phenograph = external.tl.phenograph ; cluster_ph = phenograph(df.values, k=60, method='leiden')[0]; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.4; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 1.0.0; functions NA; google NA; h5py 3.9.0; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.3.2; jupyter_server 1.18.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.1rc1; markupsafe 2.1.1; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; plotly 5.14.1; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 9.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.11.2; seaborn 0.12.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2653:426,optimiz,optimization,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653,1,['optimiz'],['optimization']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. According to the `pp.neighbors()` [docs we have](https://github.com/scverse/scanpy/blob/4642cf8e2e51b257371792cb4fcb9611c0a81123/scanpy/neighbors/__init__.py#L96):; ```; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; ```. However, the adjacency represented by `adata.uns['neighbors']['connectivities_key']` shows many more neighbors than `n_neighbors` when `knn=True`. ### Minimal code sample. ```python; import urllib.request; import scanpy as sc. # load the data; h5_data = ""https://datasets.cellxgene.cziscience.com/6ff309fa-e9f6-405d-b24e-3c35528f154e.h5ad""; urllib.request.urlretrieve(h5_data, ""/tmp/data.h5ad"") ; adata = sc.read_h5ad(""/tmp/data.h5ad""). # compute the adjacency thresholded at k=10; k=10; sc.pp.neighbors(adata, n_neighbors=k, n_pcs=40, random_state=42,knn=True); adjacency = (adata.obsp[adata.uns['neighbors']['connectivities_key']].todense() > 0).astype(np.int32); print(f""adjacency matrix (k={k}) shape: {adjacency.shape}""). # check to see if we got a threshold; max_neighbors = np.max(adjacency.sum(axis=0)); print(f""Max neighbors={max_neighbors}""); ```. ### Error output. ```pytb; adjacency matrix (k=10) shape: (1011, 1011); Max neighbors=91; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.9.8; -----; Bio 1.83; MOODS NA; PIL 10.2.0; absl NA; anyio NA; argcomplete NA; arrow 1.3.0; asttokens NA; astunparse 1.6.3; attr 23.2.0; attrs 23.2.0; babel 2.14.0; biothings_client 0.3.1; bpnetlite 0.6.0; cattr NA; cattrs NA; certifi 2024.02.02; cffi 1.16.0; charset_normal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3014:941,load,load,941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3014,1,['load'],['load']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi! I am not sure if this is a bug... ; Every time I rescale the `pbmc3k_processed` matrix using it as input in the scanpy `sc.pp.scale` function, I get a very slightly different matrix in output, enough to generate a different UMAP with each run. But if I rewrite it using numpy in a simple function called `my_scale_function` it outputs the exact same matrix as the input, generating the same UMAP down the line... Could someone explain to me what is happening?; (Note: The matrix is not sparse). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; def my_scale_function(X, clip=False):; mean, var = mean_var(X, axis=0); X -= mean; std = np.sqrt(var); std[std == 0] = 1; X /= std; if clip:; X = np.clip(X, -10, 10); return np.matrix(X). ### Scanpy scale vs my_scale_function; mtx = adata.X; from scipy.sparse import issparse; print(""mtx is parse="" + str(issparse(np.matrix(mtx))) + ""\n""); print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(mtx); print((mtx == mtx_rescaled).all()); print(""Rescaled with scanpy:""); mtx_rescaled = sc.pp.scale(mtx, zero_center=True, max_value=None, copy=True); print(str((np.matrix(mtx) == mtx_rescaled).all()) + ""\n""); print(""\nOriginal matrix:""); print(mtx); print(""\nMatrix rescaled with scanpy:""); print(mtx_rescaled); ```. ### Error ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629:871,Load,Loading,871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The test suite keeps failing with a segfault on the `python=3.9` build. I haven't been able to reproduce locally. Interestingly, I haven't seen it error when I rerun the check. It looks like this always happens during the call to `nn_approx`. ### Minimal code sample. ```python; NA; ```. ### Error output. ```pytb; platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/hostedtoolcache/Python/3.9.18/x64/bin/python; cachedir: .pytest_cache; rootdir: /home/vsts/work/1/s; configfile: pyproject.toml; testpaths: scanpy; plugins: nunit-1.0.6, mock-3.12.0; [1mcollecting ... [0mcollected 1474 items. scanpy/_utils/compute/is_constant.py::scanpy._utils.compute.is_constant.is_constant [32mPASSED[0m[32m [ 0%][0m; scanpy/datasets/_ebi_expression_atlas.py::scanpy.datasets._ebi_expression_atlas.ebi_expression_atlas [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pl.py::scanpy.external.pl.phate [33mSKIPPED[0m (needs modul...)[32m [ 0%][0m; scanpy/external/pp/_bbknn.py::scanpy.external.pp._bbknn.bbknn [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_harmony_integrate.py::scanpy.external.pp._harmony_integrate.harmony_integrate [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_hashsolo.py::scanpy.external.pp._hashsolo.hashsolo [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_magic.py::scanpy.external.pp._magic.magic [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_scanorama_integrate.py::scanpy.external.pp._scanorama_integrate.scanorama_integrate Fatal Python error: Illegal instruction. Thread 0x00007f00347c4640 (most recent call first):; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/threading.py"", line 316 in wait; File ""/opt/hostedtoolcache/Python/3.9.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:720,cache,cachedir,720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['cache'],['cachedir']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Tried to run this function:; sc.tl.leiden(test, resolution = 0.1, restrict_to = ('leiden', ['5'])). and instead it is subsetting cluster 5 into over 400 new subsets, even with my resolution set to 0.1. I've also tried different resolutions and none of them work, it ignores the resolution altogether. ; ![leiden](https://github.com/scverse/scanpy/assets/88872118/7fa7114e-d8ae-4e91-94b6-ece1f9505594). ### Minimal code sample. ```python; sc.tl.leiden(test, resolution = 0.1, restrict_to = ('leiden', ['5'])); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.0.1; appnope 0.1.2; asttokens NA; attr 23.1.0; bottleneck 1.3.5; brotli NA; celltypist 1.6.2; certifi 2023.11.17; cffi 1.16.0; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2022.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; dill 0.3.7; docrep 0.3.2; entrypoints 0.4; exceptiongroup 1.2.0; executing 0.8.3; fsspec 2023.10.0; h5py 3.7.0; idna 3.4; igraph 0.10.8; inflect NA; ipykernel 6.28.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.3; joblib 1.3.2; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.7; numpy 1.26.3; omnipath 1.0.8; packaging 23.1; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prompt_toolkit 3.0.43; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pyda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906:964,bottleneck,bottleneck,964,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.ingest` uses `pkg_version('anndata')`, which errors out using the latest version of `anndata`. ### Minimal code sample. ```python; from scanpy._compat import pkg_version; pkg_version(""anndata""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); ----> 1 pkg_version(""anndata""). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/scanpy/_compat.py:80, in pkg_version(package); 76 @cache; 77 def pkg_version(package):; 78 from importlib.metadata import version as v; ---> 80 return version.parse(v(package)). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:52, in parse(version); 43 def parse(version: str) -> ""Version"":; 44 """"""Parse the given version string.; 45 ; 46 >>> parse('1.0.dev1'); ref='/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:0'>0</a>;32m (...); 50 :raises InvalidVersion: When the version string is not a valid version.; 51 """"""; ---> 52 return Version(version). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:195, in Version.__init__(self, version); 184 """"""Initialize a Version object.; 185 ; 186 :param version:; ...; --> 195 match = self._regex.search(version); 196 if not match:; 197 raise InvalidVersion(f""Invalid version: '{version}'""). TypeError: expected string or bytes-like object; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cairo 1.23.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; colorlog NA; comm 0.1.4; cupy 12.2.0; cupy_backends NA; cupyx NA; c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978:790,cache,cache,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The exception happened when try to run scanpy `highly_variable_genes` with sparse dataset loaded in backed mode. ### Minimal code sample. ```python; # read backed; adata = anndata.read_h5ad(file_path, backed='r'); X = adata.raw.X if adata.raw is not None else adata.X; # dataset must be sparse there; print(issparse(X[0])); # calculate dispersions; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, inplace=False); ```; True; ```. ### Error output. ```pytb; loop of ufunc does not support argument 0 of type SparseDataset which has no callable expm1 method!; ```. goes from https://github.com/scverse/scanpy/blob/bc349b999be62196aa51b59db6e2daa37f428322/scanpy/preprocessing/_highly_variable_genes.py#L206. ### Versions. <details>. ```; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.3.0; appnope 0.1.3; asttokens NA; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; igraph 0.10.2; ipykernel 6.17.1; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.0; llvmlite 0.39.1; louvain 0.8.0; matplotlib 3.6.2; mpl_toolkits NA; natsort 8.2.0; numba 0.56.4; numpy 1.23.5; packaging 21.3; pandas 1.2.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.5.4; plotly 5.11.0; prompt_toolkit 3.0.33; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.6; scipy 1.9.3; session_info 1.0.0; setuptools 62.3.2; sitecustomize NA; six 1.16.0; sklearn 1.1.3; stack_data 0.6.1; texttable 1.6.6; threadpoolctl 3.1.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764:381,load,loaded,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764,1,['load'],['loaded']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hello, . I cannot produce normal looking paga plot. Whether I am using my own data or datasets provided through scanpy, the output for PAGA looks weird. . Please see below. Is this something to do with figure margins/axes?. Thanks in advance. ![Screenshot 2023-09-13 at 20 48 43](https://github.com/scverse/scanpy/assets/40166451/c7313231-a2dc-49cb-a3df-21e97d86d278). ### Minimal code sample. ```python; adata2 = sc.datasets.pbmc3k_processed(); sc.tl.paga(adata2, groups='louvain'); sc.pl.paga(adata2); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.5; -----; PIL 8.0.1; backcall 0.2.0; bottleneck 1.3.7; cellrank 1.5.1; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; decorator 5.1.1; docrep 0.3.2; google NA; h5py 3.8.0; igraph 0.10.4; importlib_resources NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 3.0.3; joblib 1.2.0; kiwisolver 1.3.0; leidenalg 0.9.1; llvmlite 0.34.0; lz4 3.1.10; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; numba 0.51.2; numexpr 2.8.5; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 4.2.0; prompt_toolkit 3.0.8; psutil 5.7.2; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pygpcca 1.0.4; pyparsing 2.4.7; python_utils NA; pytz 2020.1; ruamel NA; scipy 1.10.1; scvelo 0.2.5; seaborn 0.11.0; session_info 1.0.0; six 1.15.0; sklearn 1.2.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.1.0; tlz 0.12.1; toolz 0.11.1; tornado 6.1; tqdm 4.50.2; traitlets 5.0.5; typ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2665:933,bottleneck,bottleneck,933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2665,1,['bottleneck'],['bottleneck']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:258,load,loading,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,6,"['load', 'optimiz', 'scalab']","['loading', 'optimized', 'scalability']"
Performance,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation; 2. Use already existing mean-var; 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3226:601,perform,performance,601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226,1,['perform'],['performance']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hi, thanks for your efforts for maintaining this wonderful framework! I was wondering if there is an implementaion of Seurat FindConservedMarkers function in Scanpy, which can perform DGE analysis per group(e.g datasets) then combine them. It could be extreme useful especially when batch effects exist.; Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3117:338,perform,perform,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3117,1,['perform'],['perform']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization?. My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :; replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while; Individual y might have replicate 1 in library 1 but replicate 2 in library 4.; so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3309:255,perform,perform,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309,1,['perform'],['perform']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. I am using `regress_out` and it is painfully slow. Even on a system where I set `n_jobs=36` and `sc.settings.n_jobs = 36`, each core of which has 36Gib of memory, I find that . ```; sc.pp.regress_out(adata, ['total_counts', ], n_jobs=n_jobs); ```. is practically unusable. At the moment that calculation is at `985` minutes. Looking at `htop` while the memory is certainly allocated (`191 Gib / 1.48Tb`), it _feels_ like setting `n_jobs` at all actually hinders performance.... I've checked the [source code](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L580) so I know that [`n_jobs`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L631C5-L631C55); should be set correctly. ```python; n_jobs = sett.n_jobs if n_jobs is None else n_jobs # NOTE: sett.n_jobs defaults to 1. # ... res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```. Looking at [`_regress_out_chunk`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L692), there really doesn't seem to be anything necessarily bottlenecking the performance except for either setting the [chunk length](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L662C5-L662C14) in `regress_out`, or just the size of the dataset... ```python; len_chunk = np.ceil(min(1000, X.shape[1]) / n_jobs).astype(int); ```. Am I missing something?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781:624,perform,performance,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781,3,"['bottleneck', 'perform']","['bottlenecking', 'performance']"
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. If I run scanpy.external.pp.dca with default parameters, do I still need to perform normalization, scaling, and log1p afterward? How can I obtain the non-normalized values?. ```py; scanpy.external.pp.dca(adata, mode='denoise', ae_type='nb-conddisp', normalize_per_cell=True, scale=True, log1p=True, hidden_size=(64, 32, 64), hidden_dropout=0.0, batchnorm=True, activation='relu', init='glorot_uniform', network_kwds=mappingproxy({}), epochs=300, reduce_lr=10, early_stop=15, batch_size=32, optimizer='RMSprop', random_state=0, threads=None, learning_rate=None, verbose=False, training_kwds=mappingproxy({}), return_model=False, return_info=False, copy=False); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2626:238,perform,perform,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. currently `pp.scale` with a `mask_obs` with a sparse matrix and with `zero_center== False` takes a really long time to update the sparse matrix. This also takes up a lot of memory because of the parity calculations. I would suggest a numba kernel that just swaps out the data. This works really well for rapids-singlecell and greatly improves performance and reduces the memory overhead.; I would open a PR with this kernel. ------; Performance for 90k cells and 25k genes:; without mask:; CPU 645 ms | GPU 37 ms | 20x; with mask:; CPU 22 s | GPU 50 ms | 460x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2941:505,perform,performance,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2941,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I'm currently implementing a function that takes in an anndata and then subsamples a given representation using https://github.com/jmschrei/apricot. This generally serves the purpose of semi-optimally picking a reduced number of points that's still representative of the latent space. . Is this sth within the scope of scanpy? When it's done it wouldn't be too much effort to polish it up for a PR. The dependency load seems fairly low.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:594,load,load,594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['load'],['load']
Performance,### What kind of feature would you like to request?. Other?. ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data); - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2555:108,cache,cache,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555,2,['cache'],['cache']
Performance,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Currently, `score_genes()`/`score_genes_cell_cycle()` do not have any examples and specifically no documentation of what the input transformation of the data should be, just a link to a (very old) notebook. The notebook says ""Log-transformation of data and scaling should always be performed before scoring"" but I suspect many users (like me) have missed this (especially the scaling step!). It would be great if this was stated in the function documentation with an example.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2909:376,perform,performed,376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2909,1,['perform'],['performed']
Performance,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. There are two levels of shared parameters that people could benefit from:. 1. Multiple Notebooks that work with the same data and could benefit from shared configuration, e.g. labels and color maps defined for a certain axis/annotation; 2. Plotting using the same labels, color map or so. This could be achieved using object oriented plotting (todo issue number). Having shared configuration files could be achieved either by direct support in the plotting functions (`x='cell type'`) or by adding a convenience function that loads a config object.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:620,load,loads,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,1,['load'],['loads']
Performance,"### 📄 `_make_forest_dict()` in `scanpy/neighbors/__init__.py`. 📈 Performance improved by **`77%`** (**`0.77x` faster**). ⏱️ Runtime went down from **`5670.84μs`** to **`3195.82μs`**; ### Explanation and details. I have used `numpy.array` and `numpy.concatenate` for your sizes and dat object which are much faster than `numpy.fromiter` and assignation respectively, especially when dealing with a large dataset. The sizes of your data_list are computed only once and used where needed. Which results in runtime improvements compared to previous code, where data sizes were computed multiple times in different parts of the code. ### Correctness verification. The new optimized code was tested for correctness. The results are listed below. #### ✅ 8 Passed − 🌀 Generated Regression Tests; <details>; <summary>(click to show generated tests)</summary>. ```python; # imports; import numpy as np; import pytest. # function to test; # (The function definition is omitted as it was provided in the original prompt). # helper class to create mock trees with properties; class MockTree:; def __init__(self, hyperplanes, offsets, children, indices):; self.hyperplanes = np.array(hyperplanes); self.offsets = np.array(offsets); self.children = np.array(children); self.indices = np.array(indices). # unit tests. # Test with a single tree with one-dimensional properties; def test_single_tree_one_dimensional():; tree = MockTree(hyperplanes=[1, 2], offsets=[3], children=[4, 5], indices=[6, 7]); forest = [tree]; result = _make_forest_dict(forest); assert result[""hyperplanes""][""start""][0] == 0; assert result[""offsets""][""start""][0] == 0; assert np.array_equal(result[""hyperplanes""][""data""], tree.hyperplanes); assert np.array_equal(result[""offsets""][""data""], tree.offsets). # Test with multiple trees with two-dimensional properties; def test_multiple_trees_two_dimensional():; tree1 = MockTree(hyperplanes=[[1, 2], [3, 4]], offsets=[5, 6], children=[[7, 8], [9, 10]], indices=[[11, 12], [13, 14]]); tree2 = Moc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2971:65,Perform,Performance,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2971,2,"['Perform', 'optimiz']","['Performance', 'optimized']"
Performance,"(0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-normalizer=3.3.2; - colorama=0.4.6; - colorcet=3.1.0; - colorful=0.5.6; - comm=0.2.2; - contourpy=1.2.1; - cycler=0.12.1; - debugpy=1.8.1; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.8; - dnspython=2.6.1; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.2.0; - executing=2.0.1; - filelock=3.14.0; - fonttools=4.53.0; - fqdn=1.5.1; - freetype=2.12.1; - get-annotations=0.1.2; - gffpandas=1.2.2; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.5; - h11=0.14.0; - h2=4.1.0; - h5py=3.11.0; - hdf5=1.14.3; - hpack=4.0.0; - httpcore=1.0.5; - httpx=0.27.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.7; - igraph=0.10.12; - importlib-metadata=7.1.0; - importlib_metadata=7.1.0; - importlib_resources=6.4.0; - ipykernel=6.29.4; - ipython=8.25.0; - isoduration=20.11.0; - jedi=0.19.1; - jinja2=3.1.4; - joblib=1.4.2; - json5=0.9.25; - jsonpointer=2.4; - jsonschema=4.22.0; - jsonschema-specifications=2023",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:3169,cache,cached-property,3169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cached-property']
Performance,"(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); 10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21515,cache,cache,21515,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; leidenalg 0.8.0; llvmlite 0.38.1; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.5.2; mpl_toolkits NA; natsort 8.1.0; numba 0.55.2; numexpr 2.7.1; numpy 1.21.6; packaging 21.3; pandas 1.4.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.4; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:3047,bottleneck,bottleneck,3047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['bottleneck'],['bottleneck']
Performance,")); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op); 1517 raise TypeError(""Categorical is not ordered for operation {op}\n""; 1518 ""you can use .as_ordered() to change the ""; -> 1519 ""Categorical to an ordered one\n"".format(op=op)); 1520 ; 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one; ```. I was confused for two reason",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:3771,perform,perform,3771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['perform'],['perform']
Performance,"); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'var' of <class 'h5py._hl.files.File'> from /. ```. Am I doing something wrong with this data? Or should I be using a different command?. #### Versions. <details>. anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.2.0; anndata2ri 1.0.6; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; backports NA; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; dunamai 1.6.0; fsspec 0.9.0; get_version 3.5; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.35.0; markupsafe 1.1.1; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.52.0; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; rpy2 3.4.5; samalg 0.8.6;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:6533,bottleneck,bottleneck,6533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['bottleneck'],['bottleneck']
Performance,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/14:504,cache,caches,504,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14,1,['cache'],['caches']
Performance,"**The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**; ```; sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean; ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**; Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905:919,perform,performed,919,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905,1,['perform'],['performed']
Performance,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286:822,cache,cache,822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286,1,['cache'],['cache']
Performance,", but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1632,cache,cache,1632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,2,['cache'],['cache']
Performance,", in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, esca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21362,cache,cache,21362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,", n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 221 ) # 0 is not a valid value for rapids, unlike original umap; 222 X_contiguous = np.ascontiguousarray(X, dtype=np.float32); --> 223 umap = UMAP(; 224 n_neighbors=n_neighbors,; 225 n_components=n_components,. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/cuml/internals/api_decorators.py in inner_f(*args, **kwargs); 792 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); 793 ; --> 794 return func(**kwargs); 795 ; 796 # Set this flag to prevent auto adding this decorator twice. cuml/manifold/umap.pyx in cuml.manifold.umap.UMAP.__init__(). TypeError: %d format: a number is required, not str; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; 2f7ece400a652629565c523b34ee61b04afa385c NA; PIL 8.1.2; absl NA; anndata 0.7.6; anyio NA; astunparse 1.6.3; attr 20.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; cachetools 4.2.2; cellrank 1.3.1; certifi 2020.12.05; cffi 1.14.4; chardet 4.0.0; click 7.1.2; cloudpickle 1.6.0; colorama 0.4.4; cudf 0.20.0a+294.gfbb9a988fa; cugraph 0.20.0a+65.g924f6782.dirty; cuml 0.20.0a+110.gab47f2e11; cupy 9.0.0; cupy_backends NA; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dask_cuda 0+unknown; dask_cudf 0.20.0a+294.gfbb9a988fa; dateutil 2.8.1; decorator 4.4.2; distributed 2021.04.0; docrep 0.3.2; fastrlock 0.5; flatbuffers NA; fsspec 2021.04.0; future_fstrings NA; gast NA; get_version 2.1; google NA; h5py 3.1.0; heapdict NA; idna 2.10; igraph 0.9.1; ipykernel 5.4.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.4.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; nbclassic NA; nb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:2182,cache,cachetools,2182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['cache'],['cachetools']
Performance,", values_key). ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_default_colors_for_categorical_obs(adata, value_to_plot); 468 ); 469 ; --> 470 _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length]); 471 ; 472 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_colors_for_categorical_obs(adata, value_to_plot, palette); 428 colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]; 429 ; --> 430 adata.uns[value_to_plot + '_colors'] = colors_list; 431 ; 432 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/compat/_overloaded_dict.py in __setitem__(self, key, value); 104 self.overloaded[key].set(value); 105 else:; --> 106 self.data[key] = value; 107 ; 108 def __delitem__(self, key):. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in __setitem__(self, idx, value); 32 stacklevel=2,; 33 ); ---> 34 with self._update() as container:; 35 container[idx] = value; 36 . ~/miniconda3/envs/csp/lib/python3.8/contextlib.py in __enter__(self); 111 del self.args, self.kwds, self.func; 112 try:; --> 113 return next(self.gen); 114 except StopIteration:; 115 raise RuntimeError(""generator didn't yield"") from None. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in _update(self); 38 def _update(self):; 39 adata_view, attr_name, keys = self._view_args; ---> 40 new = adata_view.copy(); 41 attr = getattr(new, attr_name); 42 container = reduce(lambda d, k: d[k], keys, attr). ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/anndata.py in copy(self, filename); 1526 ; 1527 if filename is None:; -> 1528 raise ValueError(; 1529 ""To copy an AnnData object in backed mode, ""; 1530 ""pass a filename: `.copy(filename='myfilename.h5ad')`. "". ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`. ```. #### Versions. Scanpy: 1.8.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2401:4486,load,load,4486,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2401,1,['load'],['load']
Performance,"- [ X] I have checked that this issue has not already been reported.; - [ X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.umap(adata); sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP']); sc.tl.leiden(adata); sc.pl.umap(adata, color=['leiden', 'CST3', 'NKG7']). ```. ```pytb; The output does not match the clustering of the PBMC3k tutorial. I've downloaded the PBMC notebook from github and my output has clusters 4 and 5 switched., see my output below. How can I rectify this?; ```. #### Versions. <details>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.0.1; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.02.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; executing 0.8.3; fsspec 2022.02.0; google NA; h5py 3.6.0; igraph 0.10.4; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.13.5; kiwisolver 1.3.2; leidenalg 0.9.1; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; pkg_resources NA; plotly 5.6.0; prompt_toolkit 3.0.20; psutil 5.8.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.8; pyparsing 3.0.4; pythoncom NA; pytz 2021.3; pywintypes NA; ruamel NA; scipy 1.7.3; seaborn 0.11.2; session_info 1.0.0; setuptools 61.2.0; six",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2471:873,bottleneck,bottleneck,873,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471,1,['bottleneck'],['bottleneck']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; #Perform a clustering for scran normalization in clusters; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps=15); sc.pp.neighbors(adata_pp); sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5); ```. ```pytb; ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-785c54721f17> in <module>; 8 sc.pp.pca(adata_pp, n_comps=15); 9 sc.pp.neighbors(adata_pp); ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy); 135 weights = None; 136 if flavor == 'vtraag':; --> 137 import louvain; 138 if partition_type is None:; 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1566:500,Perform,Perform,500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566,1,['Perform'],['Perform']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### This is the code I'm using which was copied from the tutorial:. ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.log1p(adata); sc.pp.neighbors(adata, n_neighbors=20, use_rep='X', method='gauss'); sc.tl.diffmap(adata); sc.tl.dpt(adata, n_branchings=1, n_dcs=10); sc.pl.diffmap(adata, color=['dpt_pseudotime', 'dpt_groups', 'paul15_clusters']); ```; The results I got are:; ![image](https://user-images.githubusercontent.com/33322882/168080076-f6767970-0b3b-4623-ab25-e9e38201e6ef.png). Which is totally different than in the tutorial:; ![image](https://user-images.githubusercontent.com/33322882/168081015-cddf77eb-7b83-4802-8890-3b0d87a5755d.png). Can anyone run into this problem ever? Thanks for your help. #### Versions. <details>; -----. anndata 0.8.0. scanpy 1.9.1. -----. PIL 8.4.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.2. cffi 1.15.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.5.1. decorator 4.4.2. defusedxml 0.6.0. google NA. h5py 3.6.0. ipykernel 6.4.1. ipython_genutils 0.2.0. jedi 0.18.1. joblib 0.17.0. kiwisolver 1.3.1. llvmlite 0.38.0. ... Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]. Linux-3.16.0-11-amd64-x86_64-with-glibc2.19. -----. Session information updated at 2022-05-12 14:59. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2254:1273,bottleneck,bottleneck,1273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2254,1,['bottleneck'],['bottleneck']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Hi Scanpy,; I noticed the error was kind of mnnpy related and after checking the issues there and updating two suggested packages, still getting the error. . ```python; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_mnn, _, _ = sc.external.pp.mnn_correct(*adata_list, batch_key=""sample""); ```. ```pytb; Performing cosine normalization...; Starting MNN correct iteration. Reference batch: 0; Step 1 of 4: processing batch 1; Looking for MNNs...; Computing correction vectors...; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-49-f894e9f745f6> in <module>; ----> 1 adata_mnn, _, _ = sc.external.pp.mnn_correct(*adata_list, batch_key=""sample""). /projects/da_workspace/Users/amoussavi/Software/Anaconda_python3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1367:874,Perform,Performing,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367,1,['Perform'],['Performing']
Performance,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Hi, . I'm processing the `loom` object from the Cao et al 2020 (Dataset [GSE156793](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE156793)), so I can save it as an `anndata` for downstream analyses. Below are the steps I'm taking to do this: . - Load modules. ```python; import anndata; import numpy as np; import pandas as pd; import scanpy as sc; from scipy.sparse import csr_matrix, csc_matrix; ```; - Read loom object. Takes ~ 4 hrs. . ```python; gex_matrix = sc.read_loom('GSE156793_S3_gene_count.loom'); gex_matrix; ```; - Read in metadata ; ```python; gex_metadata = pd.read_csv('GSE156793_S1_metadata_cells.txt.gz', sep = ',', index_col = 0); gex_matrix.obs = gex_metadata; gex_matrix.obs; ```; - Transform to `CSR` matrix; ```python; gex_matrix.X = csr_matrix(gex_matrix.X); gex_matrix.X; ```; - Save object; ```python; gex_matrix.write('GSE156793_GEX_ctl210604.raw.h5ad'); ```. However, I get the following error. Any ideas what this may be related to? . ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:741,Load,Load,741,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['Load'],['Load']
Performance,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:494,load,loading,494,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample . ```python; import scanpy as sc; import numpy as np; import pandas as pd; adata = sc.datasets.pbmc68k_reduced(); sc.pl.pca_loadings(adata, components = '1,2,3'). ```. ```pytb; ValueError: Axis limits cannot be NaN or Inf; ```; **Note**: recalculating pca solves the problem . #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.8.0.dev78+gc488909a; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appnope 0.1.0; attr 19.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2.20.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.33.0+1.g022ab0f; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.2.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.7; numba 0.50.1; numexpr 2.7.1; numpy 1.18.5; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pvectorc NA; pygments 2.6.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; scanpy 1.8.0.dev78+gc488909a; scipy 1.5.0; seaborn 0.10.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.3; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 2.1.5; notebook 6.0.3; -----; Python 3.8.3 (default, Jul 2 2020, 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1816:698,bottleneck,bottleneck,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1816,1,['bottleneck'],['bottleneck']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html).; I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.read(results_file); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names', 'pvals']}).head(5); sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'); sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20); ```. ```pytb; KeyError Traceback (most recent call last); Input In [57], in <cell line: 1>(); ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds); 580 adata.uns[key_added] = {}; 581 adata.uns[key_added]['params'] = dict(; 582 groupby=groupby,; 583 reference=reference,; (...); 587 corr_method=corr_method,; 588 ); --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, la",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2497:632,load,loading,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Trying to run through Pearson residual but when trying to perform 'Plot quality control metrics' produced an error saying it could not find keys in adata.obs or in data.var_names. ### Minimal code sample (that we can copy&paste without having any data). ```python; # adata_control = sc.read_csv('/Users/csb/Desktop/SevenBridge_custom reference remapping_2022.7.21/Sample_sample_Control_WTA/Control_new.csv'); adata_control.uns[""name""] = ""zfish_Control"". for adata in adata_control:; adata.var_names_make_unique(); print(adata.uns[""name""], "": data shape"", adata.shape); sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=100). for adata in adata_control:; adata.var['mt'] = adata.var_names.str.startswith('mt-'); sc.pp.calculate_qc_metrics(; adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True. for adata in adata_control:; print(adata.uns[""name""], "":""); sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter =0.4,; multi_panel=True,; ); ```. ```pytb; zfish_Control :; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /var/folders/bh/yzgycbkj1jn1bd9p52t5s8s40000gn/T/ipykernel_17867/842143982.py in <module>; 1 for adata in adata_control:; 2 print(adata.uns[""name""], "":""); ----> 3 sc.pl.violin(; 4 adata,; 5 [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 779 ); 780 else:; --> 781 obs_df = get.obs_df(adata, keys=keys, layer=layer, use_raw=use_raw); 782 if groupby is None:; 783 obs_tidy ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305:287,perform,perform,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305,1,['perform'],['perform']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hi,. My understanding of the ""groups"" argument in sc.tl.rank_genes_groups is that it subsets the data and then performs the differential expression testing. I.e. if I have clusters 1 to 10, and I set groups=[1,2], the output will give me the genes differentially expressed in cluster 1 as compared to cluster 2 (and 2 vs 1).; However, the current function still compares to all other clusters (see below). ; Is that the intention? If so, we should update the readthedocs I think.; If this is a bug, let's try to fix it :) I can take a look myself in that case. Just wanted to check if I misinterpreted the readthedocs. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc. # load data; adata = sc.datasets.pbmc68k_reduced(); # cluster; sc.tl.leiden(adata, key_added=""clusters"", resolution=0.5); print(""Clusters:"", sorted(set(adata.obs[""clusters""]))); # do test with groups=""all""; sc.tl.rank_genes_groups(adata, groupby=""clusters"", groups=""all""); # store results, sorting genes by logfc; genes_cluster_0_vs_all = [; (name, logfc); for logfc, name in sorted(; zip(; adata.uns[""rank_genes_groups""][""logfoldchanges""][""0""],; adata.uns[""rank_genes_groups""][""names""][""0""],; ),; reverse=True,; ); ]; # do test with groups=[""0"",""1""], i.e. only a subset of the clusters; sc.tl.rank_genes_groups(adata, groupby=""clusters"", groups=[""0"", ""1""]); # store result; genes_cluster_0_vs_1 = [; (name, logfc); for logfc, name in sorted(; zip(; adata.uns[""rank_genes_groups""][""logfoldchanges""][""0""],; adata.uns[""rank_genes_groups""][""names""][""0""],; ),; reverse=True,; ); ]; # print top 5 genes and logfcs for both,; # they're the same and should not be; print(""Top genes cluster 0 versus all:\n"", genes_cluster_0_vs_all[:5]); print(""Top genes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519:340,perform,performs,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519,2,"['load', 'perform']","['load', 'performs']"
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am not 100% this is a bug, so please correct me if I'm doing something wrong... I am using scanpy with a very large scRNAseq dataset (SEA-AD, the sparse h5ad is ~35GB); When I use the read_h5ad file, even when running in backed mode, I get an out of memory exception. I expect it to be due to the fact that parts of the dataset are still read to memory, even in backed mode.; As you can see in the stack trace below, eventually anndata's `read_sparse()` function is called (in `_io/specs/methods.py`; But this method has the following implementation in the latest version:; ```python; def read_sparse(elem):; return SparseDataset(elem).to_memory(); ```; Thus, loading (part of) the dataset to memory. Like I said, I am not sure whether this is a bug, or supposed to happen. But to me it seems odd that backed mode still loads large portions of the dataset to memory. ### Workaround. For my own project, I got around this issue, by removing the call to `.to_memory()` within the source of anndata. I am not sure whether this breaks any other functionality, but I can use the dataset the way I need it right now. ### Minimal code sample (that we can copy&paste without having any data); (this cannot be run without any data, because the problem is that it fails with a big dataset. I included an aws download command to the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:891,load,loading,891,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. To reproduce this issue:; 1. download the public 10x dataset here (https://cf.10xgenomics.com/samples/cell-exp/2.1.0/hgmm_12k/hgmm_12k_raw_gene_bc_matrices_h5.h5) ; 2. run the following. ```python; import scanpy as sc. adata_human = sc.read_10x_h5('hgmm_12k_raw_gene_bc_matrices_h5.h5', genome='hg19'); adata_mouse = sc.read_10x_h5('hgmm_12k_raw_gene_bc_matrices_h5.h5', genome='mm10'). assert (adata_human.X != adata_mouse.X).sum() > 0, 'these count matrices are equal'; ```. which produces the assertion error. We see that the loaded data is the same regardless of `'genome'` argument. A look at the file itself shows this is not the case (notice the number of gene names, which are different for hg19 and mm10):. ![image](https://user-images.githubusercontent.com/10214815/165848884-0ef5c172-83f9-4ead-9687-0acadb496e87.png). #### Versions. Also I think I can say confidently that this was working fine as of scanpy 1.8.1. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.1.0; appnope 0.1.2; backcall 0.2.0; cached_property 1.5.2; cellbender NA; cffi 1.14.5; colorcet 3.0.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 5.0.9; fontTools 4.33.3; h5py 3.2.0; igraph 0.9.10; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.10; llvmlite 0.38.0; lxml 4.8.0; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.55.1; numexpr 2.7.3; numpy 1.19.2; packaging 20.9; pandas 1.2.3; param 1.12.1; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.0; pynndescent 0.5.6; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.1; seaborn 0.11.2; session_info 1.0.0; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246:758,load,loaded,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246,1,['load'],['loaded']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; ---. Hello, I am trying to run `sc.pp.highly_variable_genes` with `flavor='seurat_v3'` on some data, but it is giving me an error. The data has been run through Kallisto Bustools and is being loaded in as an .h5ad file; it should be raw count data. . ### Minimal code sample. ```python; zf_48 = anndata.read_h5ad(""data.h5ad""); zf_48.var[""gene_ids""] = zf_48.var.index.values. t2g = pd.read_csv(""transcripts_to_genes_no_rm.txt"", header=None, names=[""tid"", ""gene_id"", ""gene_name""], sep=""\t""); t2g.index = t2g.gene_id; t2g = t2g.loc[~t2g.index.duplicated(keep='first')]; zf_48.var[""gene_name""] = zf_48.var.gene_ids.map(t2g[""gene_name""]); zf_48.var.index = zf_48.var[""gene_name""] . zf_48.var_names_make_unique(); sc.pp.filter_cells(zf_48, min_genes=550); sc.pp.filter_genes(zf_48, min_cells=10); zf_48; #AnnData object with n_obs × n_vars = 887 × 13180; # obs: 'n_genes'; # var: 'gene_name', 'gene_ids', 'n_cells'. zf_48.var['mt'] = zf_48.var_names.str.startswith('mt-') ; sc.pp.calculate_qc_metrics(zf_48, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1); ```; Here is the error:; ```pytb; ValueError Traceback (most recent call last); <ipython-input-170-37cd37b7326e> in <module>; 16 zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; 17 zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 41",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:334,load,loaded,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['load'],['loaded']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; ---; Hello, I am trying to save my .h5ad files to my desktop from scanpy, but adata.write is giving me an error. The data has been run through Kallisto Bustools and is being loaded in as an .h5ad file, but the error appears even if I try to save it immediately after loading it in, without any modifications. Unfortunately, the error does not happen when I try it on other data, such as the pbmc3k data. ### Minimal code sample . ```python; zf_48 = anndata.read_h5ad(""/Users/julius/Desktop/48hpf_adata.h5ad""); zf_48.var[""gene_ids""] = zf_48.var.index.values. t2g = pd.read_csv(""/Users/julius/Desktop/zf_transcripts_to_genes_no_rm.txt"", header=None, names=[""tid"", ""gene_id"", ""gene_name""], sep=""\t""); t2g.index = t2g.gene_id; t2g = t2g.loc[~t2g.index.duplicated(keep='first')]; zf_48.var[""gene_name""] = zf_48.var.gene_ids.map(t2g[""gene_name""]); zf_48.var.index = zf_48.var[""gene_name""] . zf_48.write_h5ad(""/Users/julius/Desktop/zf_48.h5ad""); ```; Here is the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:316,load,loaded,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,2,['load'],"['loaded', 'loading']"
Performance,------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1231,load,loading,1231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files there:; ```; ls /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/; barcodes.tsv features.tsv matr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1800,cache,cache,1800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"-> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex; ```. </details>. I don't get an error from this on master, but I do get these warnings. ```; *c* argument looks like a single numeric RGB or RGBA sequence, whi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:4428,Cache,CachedProperty,4428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['Cache'],['CachedProperty']
Performance,. </details>. Luke's environment: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 h10d778d_5 conda-forge; ca-certificates 2024.2.2 h8857fd0_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.3.0 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.49.0 pypi_0 pypi; fsspec 2024.2.0 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.3.2 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h73e2aa4_0 conda-forge; libffi 3.4.2 h0d85af4_5 conda-forge; libsqlite 3.45.2 h92b6c6a_0 conda-forge; libzlib 1.2.13 h8a1eda9_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.3 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 h93d8f39_2 conda-forge; networkx 3.2.1 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.0 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd75f5a5_0 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.2.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pynndescent 0.5.11 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 4.1.0 pypi_0 pypi; pytest-m,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:33112,load,loader,33112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,"... Hello, I hope I'm in the correct place. I was wondering, in Spatial Transcriptomics analyses, after loading data `adata = sc.read_visium`, where are the spot co-ordinates stored?. Example; spot x y; 1x17x20	17	20	; 1x17x21	17	21",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2253:104,load,loading,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2253,1,['load'],['loading']
Performance,.0; plotly 5.17.0; pluggy 1.3.0; ply 3.11; pooch 1.7.0; prometheus-client 0.17.1; prompt-toolkit 3.0.39; Protego 0.3.0; psutil 5.9.5; ptyprocess 0.7.0; PuLP 2.7.0; pure-eval 0.2.2; py-cpuinfo 9.0.0; pyarrow 13.0.0; pyasn1 0.5.0; pyasn1-modules 0.3.0; pycodestyle 2.10.0; pycosat 0.6.6; pycparser 2.21; pyct 0.4.6; pycurl 7.45.1; pydantic 1.10.13; pydeseq2 0.4.1; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0.3; pyflakes 3.0.1; Pygments 2.16.1; PyJWT 2.8.0; pylint 2.17.5; pylint-venv 3.0.2; pyls-spyder 0.4.0; pynndescent 0.5.10; pyodbc 4.0.39; pyOpenSSL 23.2.0; pyparsing 3.1.1; PyQt5-sip 12.11.0; PySocks 1.7.1; pytest 7.4.2; python-dateutil 2.8.2; python-dotenv 1.0.0; python-json-logger 2.0.7; python-lsp-black 1.3.0; python-lsp-jsonrpc 1.1.2; python-lsp-server 1.7.2; python-slugify 8.0.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz_comms 3.0.0; PyWavelets 1.4.1; pyxdg 0.28; PyYAML 6.0.1; pyzmq 25.1.1; QDarkStyle 3.1; qstylizer 0.2.2; QtAwesome 1.2.3; qtconsole 5.4.4; QtPy 2.4.0; queuelib 1.6.2; referencing 0.30.2; regex 2023.10.3; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; reretry 0.11.8; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:8619,queue,queuelib,8619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['queue'],['queuelib']
Performance,".index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:1761,cache,cache,1761,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"/install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/base.py"", line 21, in <module>; from scipy.sparse.sputils import IndexMixin; ImportError: cannot import name 'IndexMixin'; ```. ```bash; #pip3 install works any better?; pip3 install scanpy --target $PYTHONPATH --upgrade; pip3 install scanpy-scripts --target $PYTHONPATH --upgrade",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:1406,load,load,1406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,1,['load'],['load']
Performance,"/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:1952,cache,cached,1952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1.6.8; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.7; - dnspython=2.4.2; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.1.3; - executing=1.2.0; - expat=2.5.0; - ffmpeg=6.0.0; - filelock=3.12.2; - font-ttf-dejavu-sans-mono=2.37; - font-ttf-inconsolata=3.000; - font-ttf-source-code-pro=2.038; - font-ttf-ubuntu=0.83; - fontconfig=2.14.2; - fonts-conda-ecosystem=1; - fonts-conda-forge=1; - fonttools=4.42.1; - fqdn=1.5.1; - freetype=2.12.1; - fribidi=1.0.10; - get-annotations=0.1.2; - gettext=0.21.1; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.2; - gnutls=3.7.8; - graphite2=1.3.13; - h11=0.14.0; - h2=4.1.0; - h5py=3.9.0; - harfbuzz=7.3.0; - hdf5=1.14.1; - hpack=4.0.0; - httpcore=0.18.0; - hyperfra,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:9258,cache,cached-property,9258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cached-property']
Performance,0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; dill 0.3.7; gmpy2 2.1.2; google NA; h5py 3.9.0; igraph 0.11.3; joblib 1.3.2; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvm,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:14945,load,loader,14945,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['load'],['loader']
Performance,"0972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2284,cache,cache,2284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,1.1.1o-h166bdaf_0. Proceed ([y]/n)? y. Downloading and Extracting Packages; keyutils-1.6.1 | 115 KB | ############################################################################# | 100% ; h5py-3.6.0 | 1.4 MB | ############################################################################# | 100% ; cached_property-1.5. | 11 KB | ############################################################################# | 100% ; c-ares-1.18.1 | 113 KB | ############################################################################# | 100% ; anndata-0.8.0 | 151 KB | ############################################################################# | 100% ; libev-4.33 | 104 KB | ############################################################################# | 100% ; libnghttp2-1.46.0 | 680 KB | ############################################################################# | 100% ; libcurl-7.82.0 | 342 KB | ############################################################################# | 100% ; libssh2-1.10.0 | 233 KB | ############################################################################# | 100% ; cached-property-1.5. | 4 KB | ############################################################################# | 100% ; openssl-1.1.1o | 2.1 MB | ############################################################################# | 100% ; certifi-2022.5.18.1 | 150 KB | ############################################################################# | 100% ; hdf5-1.12.1 | 3.5 MB | ############################################################################# | 100% ; libedit-3.1.20191231 | 121 KB | ############################################################################# | 100% ; ca-certificates-2022 | 144 KB | ############################################################################# | 100% ; krb5-1.19.3 | 1.4 MB | ############################################################################# | 100% ; Preparing transaction: done; Verifying transaction: done; Executing transaction: done. ```. Any ideas??,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:3184,cache,cached-property-,3184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['cache'],['cached-property-']
Performance,"108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filename, return_ext=True); --> 491 is_present = check_datafile_present_and_download(; 492 filename,; 493 backup_url=backup_url,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in check_datafile_present_and_download(path, backup_url); 745 path.parent.mkdir(parents=True); 746 ; --> 747 download(backup_url, path); 748 return True; 749 . ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in download(url, path); 722 ; 723 path.parent.mkdir(parents=True, exist_ok=True); --> 724 with tqdm(unit='B', unit_scale=True, miniters=1, desc=path.name) as t:; 725 def update_to(b=1, bsize=1, tsize=None):; 726 if tsize is not None:. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs); 206 unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1; 207 total = self.total * unit_scale if self.total else self.total; --> 208 self.container = self.status_printer(; 209 se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:1914,cache,cache,1914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['cache'],['cache']
Performance,10x Genomics dataset loading feature,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1264:21,load,loading,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1264,1,['load'],['loading']
Performance,"2. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1816,cache,cache,1816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"200 np.concatenate([doublet_latent_rep, np.log(doublet_lib_size)], axis=1); 201 ); 202 doublet_adata.obs[LABELS_KEY] = ""doublet""; --> 204 full_adata = latent_adata.concatenate(doublet_adata); 205 cls.setup_anndata(full_adata, labels_key=LABELS_KEY); 206 return cls(full_adata, **classifier_kwargs). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/anndata/_core/anndata.py:1808, in AnnData.concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1799 pat = rf""-({'|'.join(batch_categories)})$""; 1800 out.var = merge_dataframes(; 1801 [a.var for a in all_adatas],; 1802 out.var_names,; 1803 partial(merge_outer, batch_keys=batch_categories, merge=merge_same),; 1804 ); 1805 out.var = out.var.iloc[; 1806 :,; 1807 (; -> 1808 out.var.columns.str.extract(pat, expand=False); 1809 .fillna(""""); 1810 .argsort(kind=""stable""); 1811 ),; 1812 ]; 1814 return out. File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/accessor.py:224, in CachedAccessor.__get__(self, obj, cls); 221 if obj is None:; 222 # we're accessing the attribute of the class, i.e., Dataset.geo; 223 return self._accessor; --> 224 accessor_obj = self._accessor(obj); 225 # Replace the property with the accessor object. Inspired by:; 226 # https://www.pydanny.com/cached-property.html; 227 # We need to use object.__setattr__ because we overwrite __setattr__ on; 228 # NDFrame; 229 object.__setattr__(obj, self._name, accessor_obj). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:181, in StringMethods.__init__(self, data); 178 def __init__(self, data) -> None:; 179 from pandas.core.arrays.string_ import StringDtype; --> 181 self._inferred_dtype = self._validate(data); 182 self._is_categorical = is_categorical_dtype(data.dtype); 183 self._is_string = isinstance(data.dtype, StringDtype). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:235, in StringMethods._validate(data); 23",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2474:1876,Cache,CachedAccessor,1876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2474,1,['Cache'],['CachedAccessor']
Performance,"30 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; batchglm v0.7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; keras 2.9.0; kiwisolver 1.4.3; llvmlite 0.38.1; magic 3.0.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.1; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pcurve NA; pexpect 4.8.0; phate 1.0.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:10709,bottleneck,bottleneck,10709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['bottleneck'],['bottleneck']
Performance,"74 return index.get_loc(indexer) # int; 75 elif isinstance(indexer, (Sequence, np.ndarray, pd.Index, spmatrix, np.matrix)):; 76 if hasattr(indexer, ""shape"") and (. /software/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2646 return self._engine.get_loc(key); 2647 except KeyError:; -> 2648 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2649 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2650 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'SPP1'. ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.4.0; backcall 0.1.0; bottleneck 1.2.1; cffi 1.11.5; cloudpickle 0.5.3; colorama 0.3.9; cycler 0.10.0; cython_runtime NA; cytoolz 0.9.0.1; dask 0.17.5; dateutil 2.7.3; decorator 4.3.0; fa2 NA; flaskext NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.9.7; ipykernel 4.8.2; ipython_genutils 0.2.0; ipywidgets 7.2.1; jedi 0.12.0; joblib 0.13.2; kiwisolver 1.0.1; legacy_api_wrap 1.2; leidenalg 0.8.8; llvmlite 0.34.0; louvain 0.6.1; lxml NA; matplotlib 3.3.4; mpl_toolkits NA; natsort 6.0.0; networkx 2.5.1; numba 0.51.2; numexpr 2.6.5; numpy 1.19.5; packaging 21.0; pandas 1.1.5; parso 0.2.0; pexpect 4.5.0; pickleshare 0.7.4; pkg_resources NA; prompt_toolkit 1.0.15; psutil 5.4.5; ptyprocess 0.5.2; pycparser 2.18; pygments 2.2.0; pynndescent 0.5.0; pyparsing 2.2.0; pytz 2018.4; ruamel NA; scipy 1.4.1; scvelo 0.2.4; setuptools_scm NA; simplegeneric NA; six 1.11.0; sklearn 0.24.2; sphinxcontrib NA; storemagic NA; tables 3.4.3; texttable 1.6.2; toolz 0.9.0; tornado 5.0.2; tqdm 4.32.1; traitlets 4.3.2; typing_extensions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2018:4281,bottleneck,bottleneck,4281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2018,1,['bottleneck'],['bottleneck']
Performance,"79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:2729,race condition,race condition,2729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files there:; ```; ls /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/; barcodes.tsv features.tsv matrix.mtx",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:2142,cache,cache,2142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,2,['cache'],['cache']
Performance,"972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1972,cache,cache,1972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://clou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2128,cache,cache,2128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE20097",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2440,cache,cache,2440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"; ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1826,load,load,1826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['load'],['load']
Performance,"; 1092 # A collection of keys; -> 1093 keyarr, indexer = self._get_listlike_indexer(key, axis); 1094 return self.obj._reindex_with_indexers(; 1095 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. <path>/lib/python3.9/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis); 1312 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr); 1313 ; -> 1314 self._validate_read_indexer(keyarr, indexer, axis); 1315 ; 1316 if needs_i8_conversion(ax.dtype) or isinstance(. <path>/lib/python3.9/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis); 1375 ; 1376 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()); -> 1377 raise KeyError(f""{not_found} not in index""); 1378 ; 1379 . KeyError: ""['GGAACCCTCTCCCAGC-batch1'] not in index""; ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.4.0; annoy NA; anyio NA; attr 21.2.0; autoreload NA; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; brotli NA; certifi 2021.10.08; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fastjsonschema NA; fsspec 2021.08.1; h5py 3.3.0; idna 3.2; igraph 0.10.2; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.1.2; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.23.3; jupyterlab_server 2.8.2; kiwisolver 1.3.1; leidenalg 0.9.0; llvmlite 0.37.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.2.0; nbclassic 0.4.8; nbformat 5.7.0; nbinom_ufunc NA; notebook_shim NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.11.0; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pv",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2377:4724,bottleneck,bottleneck,4724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2377,1,['bottleneck'],['bottleneck']
Performance,"; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20); sc.tl.umap(adata); return adata. adata = sc.read_csv(""./myfile.csv"", first_column_names=True); adata = pp(adata); ```. My computer is Mac book Intel i5. Thanks!; #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; OpenSSL 22.0.0; PIL 9.2.0; PyObjCTools NA; absl NA; appnope 0.1.2; astunparse 1.6.3; attr 21.4.0; backcall 0.2.0; bcrypt 3.2.0; beta_ufunc NA; binom_ufunc NA; boto3 1.24.28; botocore 1.27.28; bottleneck 1.3.5; brotli NA; certifi 2022.09.24; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; chex 0.1.5; cloudpickle 2.0.0; colorama 0.4.5; contextlib2 NA; cryptography 37.0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; dill 0.3.4; docrep 0.3.2; entrypoints 0.4; etils 0.8.0; flax 0.6.1; fsspec 2022.7.1; google NA; graphviz 0.20; h5py 3.7.0; idna 3.4; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.23; jaxlib 0.3.22; jedi 0.18.1; jinja2 2.11.3; jmespath 0.10.0; joblib 1.1.1; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.8.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; matplotlib_inline 0.1.6; ml_collections NA; mpl_toolkits NA; msgpack 1.0.3; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.56.3; numexpr 2.8.3; numpy 1.22.4; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:2485,bottleneck,bottleneck,2485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['bottleneck'],['bottleneck']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; .../usr/local/lib/python3.6/site-packages/joblib/externals/loky/backend/semaphore_tracker.py:198: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown; len(cache)). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...gene_trends = d.palantir.presults.compute_gene_trends(pr_res, ; ...: d.imp_df.iloc[:, 0:1000], ['RG']) ; ...: . ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...Segmentation fault (core dumped); ```; it made me out of the python environment.; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; Scanpy version: 1.4.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1223:272,cache,cache,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1223,1,['cache'],['cache']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; I have upgraded to scanpy 1.4.6 in a conda environment. Since then I cannot load the package into python, as it gives me the following error: `AttributeError: module 'cairo' has no attribute 'version_info'`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2349, in <module>; switch_backend(rcParams[""backend""]); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/__init__.py"", line 833, in __getitem__; plt.switch_backend(rcsetup._auto_backend_sentinel); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 204, in switch_backend; switch_backend(candidate); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, leve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166:150,load,load,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166,1,['load'],['load']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; I've had hard time in figuring this out. This is not a problem of scanpy directly but apparently is related to [scikit-learn 0.21 series](https://github.com/scikit-learn/scikit-learn/issues/14485) which is a dependency of latest scanpy version (1.4.6). Also related to [this comment in pytorch](https://github.com/pytorch/pytorch/issues/2575#issuecomment-523657178). My issue is that I'm using, in addition to scanpy, another library performing a dl_import with static TLS. ; So if I issue; ```python; import scanpy as sc; import graph_tool.all as gt; ```; I get. ```python; ImportError: dlopen: cannot load any more object with static TLS ; ```; error and I'm not able to use the second library. Reversing the order of the imports raises the same error and I'm not able to use `scanpy`. The issue is solved installing scikit-learn 0.20.4 (the last of 0.20 series). What are the exact scikit-learn 0.21.2 dependecies in scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:508,perform,performing,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,2,"['load', 'perform']","['load', 'performing']"
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; Loading data using `adata = sc.datasets.visium_sge('V1_Human_Lymph_Node')` with Anndata<0.7rc1 leads to error `'AnnData' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:74,Load,Loading,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['Load'],['Loading']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; The bug is just like the title of issue, _AttributeError: module 'scanpy' has no attribute 'anndata'_, for I just wanna to load a h5ad file from Tabula-Muris dataset; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. data = sc.anndata.read_h5ad(''tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'); ```; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):. File ""<ipython-input-91-67a79760d7cb>"", line 1, in <module>; sc.anndata.read_h5ad('tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'). AttributeError: module 'scanpy' has no attribute 'anndata'; ```; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.3 anndata==0.6.19 umap==0.3.10 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1322:197,load,load,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1322,1,['load'],['load']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.recipe_seurat and scanpy.pp.recipe_zheng17 indicate that they expect non log-transformed data. This leads both functions to do by default the highly variable gene (HVG) selection on non log-transformed data. This seems contrary to the scanpy and seurat clustering tutorials, which perform HVG selection after log-transform. It also seems contrary to the new function scanpy.pp.highly_variable_genes which expects log-transformed inputs. scanpy version : 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251:365,perform,perform,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251,1,['perform'],['perform']
Performance,"<!-- What kind of feature would you like to request? -->. It would be very useful for the GPU data science and research community if Scanpy were able to perform end to end workflows on the GPU, using either Cupy, CuDF or both. An initial iteration of this feature could include simply swapping out the numpy imports for cupy. . - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177:153,perform,perform,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177,1,['perform'],['perform']
Performance,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:642,perform,perform,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,2,['perform'],"['perform', 'performed']"
Performance,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; In function read_10x_mtx there could be an option to search for non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:970,cache,cache,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:623,perform,perform,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,2,['perform'],['perform']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2762; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process change. All changes were automatic, except for:. - removing unused imports or replacing them with `__all__`. Much more uncontroversial than AnnData as scanpy’s public modules were more well defined from the start. There were no ambiguous cases except for `sc.get` re-exporting `""_check_mask""`, `""_get_obs_rep""`, and `""_set_obs_rep""`. But since those aren’t documented, we can decide over their fate at a later date.; - Fixing circular imports like `sc.{pp,tl}.pca`. I only needed to create a `neighbors/_doc.py` file for shared neighbors/tools doc parts, and put the `pca` import in `tools` in `__getattr__` (supported since 3.7); - replacing some `with open(p) as f: x = json.loads(f.read())`s with `x = json.loads(Path(p).read_text())`. All in all surprisingly easy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2761:1149,load,loads,1149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2761,2,['load'],['loads']
Performance,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2969; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Seems like this code is super performance sensitive: Having a Python implementation of `getrandbits` in 8572ecb1b38616f98f2af6462aa4fe5a3a8871ae resulted in a slowdown:. | Change | Before [0d4554b4] | After [1b2d9dd5] | Ratio | Benchmark (Parameter) |; |----------|----------------------|---------------------|---------|------------------------------------------|; | + | 15.2±0.03ms |	31.7±0.1ms |	2.09 | preprocessing.time_highly_variable_genes |,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3041:516,perform,performance,516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3041,1,['perform'],['performance']
Performance,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2973 ; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Visium HD stores its coordinates in a `.parquet` file. This loads said file.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992:547,load,loads,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992,1,['load'],['loads']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3051; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] release notes; - [x] some added text explaining things; - [x] run internet tests, implement caching for datasets. Optional:. 1. continue to not run the internet tests in CI. A side effect of this PR is that our tests get less flaky by not running the flaky `ebi_expression_atlas` doctest; 2. run internet tests in CI; 1. add caching to CI; 2. make sure the dataset functions don’t download already-downloaded data; 3. validate cached data instead; 4. run the internet tests (with caching) in CI. ## [Rendered](https://icb-scanpy--3060.com.readthedocs.build/en/3060/api/datasets.html)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3060:926,cache,cached,926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3060,1,['cache'],['cached']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this; 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already!; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: edited",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3306:905,load,load,905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306,1,['load'],['load']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->. In our analyses we wanted to try SCTransform normalization instead of default log-norm. I have done it quite crudely, but it works: I run SCT in Seurat and dump the counts on disk to load in scanpy.; While verifying that this approach worked, we encountered slight inconsistencies between clustering using (1) vanilla log-norm scanpy (2) SCT imported scanpy and (3) SCT in Seurat.; After investigation, it appears that vanilla scanpy sometimes better picks up some clusters than SCT+scanpy, despite the latter having more relevant genes in its HVG list. Here is the investigation: https://github.com/mxposed/notebooks/blob/master/sct-scanpy.ipynb. And here are the main questions that remain:; 1. Why Vanilla scanpy could resolve those populations, despite operating on less marker HVGs?; 2. What is the difference between kNN graph construction and clustering between Seurat and scanpy?; 3. How to be sure we did not undercluster and miss some smaller cell populations?. I would be glad for any feedback or input, and of course if someone knows the answers, that's great!. Best wishes,; Nick. PS. Thank you for scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531:360,load,load,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531,1,['load'],['load']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ... Hi,. I notice that even if I set n_jobs=1 in sc.pp.regress_out, all cpus are utilized. This also happens if I set n_jobs to other numbers. Basically there isn't a noticeable difference in cpu usage no matter what number of n_jobs I set. I'm using CentOS6.8 on a machine with 28 physical cores and hyper threading on (appears as 56 cores in the os). Is this an intended behavior, or just my installation? I understand that some numpy functions are naturally multi-threaded because of the setup of BLAS libraries. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396:638,multi-thread,multi-threaded,638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396,1,['multi-thread'],['multi-threaded']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Since I'm writing an extension that performs some analysis on neighbor graph I'm a bit confused about the current status of scanpy's way to store that information. Will connectivity matrix always be stored into `adata.obsp` making `adata.uns['neighbors']['connectivities']` deprecated? I always need to access the connectivity sparse matrix, as far as I understand the path will be. ```python; conn_key = adata.uns[neighbor_key][f'{neighbor_key}_connectivities']; adata.obsp[neighbor_key][conn_key]; ```. except when no `neighbor_key` is given and matrix is `adata.obsp['connectivities']`, correct?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1176:218,perform,performs,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1176,1,['perform'],['performs']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; Hi, ; I am little confused about the parameter in pl.ump, use_raw=False. ; When you set raw=False, it takes normalized, log transformed but not corrected gene expression, while when you set user_raw=True, it takes scaled and corrected gene expression. What does corrected gene expression means here? . From tutorial it reads as below:; ""As we set the .raw attribute of adata, the previous plots showed the “raw” (normalized, logarithmized, but uncorrected) gene expression. You can also plot the scaled and corrected gene expression by explicitly stating that you don’t want to use .raw."". Trying to get some clarification on my results, in case, where i performed DE with t-test, and get top 5 genes, When i want to look them in clusters and plot ump, I do not see them with pl.ump with user_raw=True but can see them with user_raw=False. ; Any clarification will be great. . thanks, ; Preeti ; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1266:832,perform,performed,832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266,1,['perform'],['performed']
Performance,"= False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex. ```. </details>. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; backcall 0.2.0; bb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:4727,Cache,CachedProperty,4727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['Cache'],['CachedProperty']
Performance,"= sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2085,cache,cached-property,2085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['cache'],['cached-property']
Performance,">> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. **The problem is the error in importing gene names both when using id and when using symbolic labeling. All genes have the same name. if you use `anndata=0.10.3` instead of `anndata=0.10.4`, then everything works correctly.**. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; import matplotlib; import seaborn as sns. path='<path_to_files>'. adata = sc.read_10x_mtx(; path, ; var_names='gene_symbols', ; cache=True). adata.var_names_make_unique(). adata.var; ```. ### Error output. ```pycon; >>> # then anndata=0.10.4; >>> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. ### Expected. ```pycon; >>> # then anndata=0.10.3; >>> print(adata.var); gene_ids feature_types; 4933401J01Rik ENSMUSG00000102693 Gene Expression; Gm26206 ENSMUSG00000064842 Gene Expression; Xkr4 ENSMUSG00000051951 Gene Expression; Gm18956 ENSMUSG00000102851 Gene Expression; Gm37180 ENSMUSG0000010",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:7176,cache,cache,7176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cache']
Performance,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/453:386,cache,cache,386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453,2,['cache'],['cache']
Performance,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526:347,load,loaded,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526,1,['load'],['loaded']
Performance,Add pip cache to CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1620:8,cache,cache,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620,1,['cache'],['cache']
Performance,"Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default.; 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865:328,perform,performance,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865,1,['perform'],['performance']
Performance,"After @ivirshup's pytables PR (#2064) we started having issues with loading h5 files with scalar datasets, such as those created by CellBender (https://github.com/broadinstitute/CellBender/issues/128). It is currently not an issue for the 10X h5 files for now since they don't have any scalars, however it'd be good to just handle scalars as well as arrays for two reasons, 1) to fix the cellbender file loading problem 2) to fix potential problems we might end up having if 10X h5 format includes scalar datasets. I am not an HDF/h5py/tables person though, so please review carefully :) (although it's a tiny PR) Also let me know what you think, @sjfleming!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2344:68,load,loading,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2344,2,['load'],['loading']
Performance,"Any ideas what's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:133,cache,cached,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194:709,multi-thread,multi-threaded,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194,1,['multi-thread'],['multi-threaded']
Performance,Backport PR #2248 on branch 1.9.x (Fix legacy 10x loader when more than one genome exists),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2342:50,load,loader,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2342,1,['load'],['loader']
Performance,Backport PR #2248: Fix legacy 10x loader when more than one genome exists,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2342:34,load,loader,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2342,1,['load'],['loader']
Performance,Backport PR #3177 on branch 1.10.x (Cache data for subsequent test runs),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3179:36,Cache,Cache,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3179,1,['Cache'],['Cache']
Performance,Backport PR #3177: Cache data for subsequent test runs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3179:19,Cache,Cache,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3179,1,['Cache'],['Cache']
Performance,"C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")). #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; bottleneck 1.3.2; cairo 1.20.0; cffi 1.14.3; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2.30.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; joblib 0.17.0; kiwisolver 1.3.0; legacy_api_wrap 0.0.0; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.1.3; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.1; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; traitlets 5.0.5; typing_exten",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1742:2297,bottleneck,bottleneck,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742,1,['bottleneck'],['bottleneck']
Performance,Cache data for subsequent test runs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3177:0,Cache,Cache,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3177,1,['Cache'],['Cache']
Performance,"Cache datasets so notebook tests can run without requiring an external server, since they cover realistic use cases and a good amount of the API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/717:0,Cache,Cache,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/717,1,['Cache'],['Cache']
Performance,Cannot load EBI Expression Datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2449:7,load,load,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449,1,['load'],['load']
Performance,Could not use Scanpy 0.4 to load AnnData file generated by v0.2.8,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56,1,['load'],['load']
Performance,"Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805:52,load,loadings,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805,1,['load'],['loadings']
Performance,"DENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:986,cache,cache,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,1,['cache'],['cache']
Performance,Defer loading of umap to speed up import,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704:6,load,loading,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704,1,['load'],['loading']
Performance,Do not use the cached rp_forest,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1558:15,cache,cached,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1558,1,['cache'],['cached']
Performance,"Every time I build the docs locally, they do a complete rebuild. This is painfully slow (especially with our examples that run on each build) and really discourages editing the docs. This is happening because the sphinx sees the config being modified. There are two causes of this:. * The version being set dynamically – at each commit the version string changes.; * `scanpydoc.elegant_typehints` sets some properties of the config after it's loaded. E.g.:. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```. ### Solution. Version being set dynamically does really add that much value for us, so I just removed that part of the version string. `scanpydoc.elegant_typehints` does make the doc-strings nicer, but it is not worth a five minute build to update the docs. Ideally it can be implemented in a way that doesn't make sphinx think the config has changed, but I am disabling it until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2199:443,load,loaded,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2199,1,['load'],['loaded']
Performance,Fix legacy 10x loader when more than one genome exists,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2248:15,load,loader,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2248,1,['load'],['loader']
Performance,"Fixes #1697. I'll leave any major backwards compatibility changes (i.e. updating to use python 3.7+ features) to the future. CI took a really long time to install the dependencies on the first run. Hopefully that will be cached? Not sure what triggers a saved cache. At the moment it looks like building `louvain` is taking a while, which we should actually just get around to deprecating.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1897:221,cache,cached,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1897,2,['cache'],"['cache', 'cached']"
Performance,"Fixes #1806. New behaviour for Moran's I and Geary's C. If one of the variables passed has constant values, the score for that variable is `nan` and the function warns the user about this. Previously, the presence of this variable would silently fail, corrupting the other outputs as well. Adds a new utility `is_constant` to check if values in an array are constant. * Could have less code repetition, since now there's some logic that is applied to any case which is 2d, but the conditional isn't structured this way.; * Performance hit pretty minor, as computing the metric itself is expensive.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:523,Perform,Performance,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,1,['Perform'],['Performance']
Performance,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358:101,optimiz,optimized,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358,1,['optimiz'],['optimized']
Performance,Get errors when performing sc.pp.highly_variable_genes!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:16,perform,performing,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['performing']
Performance,Getting errors when loading loom files,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247:20,load,loading,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247,1,['load'],['loading']
Performance,"Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. . <img width=""749"" alt=""Screenshot 2019-07-27 at 12 49 49"" src=""https://user-images.githubusercontent.com/37718031/61993507-06971800-b06d-11e9-815e-acf667f818a5.png"">; <img width=""730"" alt=""Screenshot 2019-07-27 at 12 41 58"" src=""https://user-images.githubusercontent.com/37718031/61993416-edda3280-b06b-11e9-9a4f-7d4a1259cd47.png"">. This happens in the first loop to load all the datasets. If I run only one dataset the same error `(unsupported operand type(s) for +: 'int' and 'str')` showed up when I plot some data quality summary plots:. For instance:; `p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac')`; `adata = adata[adata.obs['mt_frac'] < 0.2]; print('Number of cells after MT filter: {:d}'.format(adata.n_obs))`; `sc.pp.filter_cells(adata, min_genes = 700); print('Number of cells after gene filter: {:d}'.format(adata.n_obs))`. I am using data generated by 10x V3 and CellRanger v3.0.2. I really do not know where the problem is. . I really appreciate any advice/help to solve this issue. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/751:194,load,load,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751,2,['load'],['load']
Performance,"Great work! . Python (64bit) throws a memory error when projecting ~1 million cells into 3D UMAP with 40 features (PCs). I suppose that matrix products are super big, but I'm performing it on 256G RAM.; Is there a way to decrease memory usage?; I would appreciate your advice!; Thanks ahead!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/710:175,perform,performing,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710,1,['perform'],['performing']
Performance,"Hello - I am trying to use scanpy to load visium ST data. The tissue_positions.csv file is located in the spatial folder - I renamed it to tissue_positions_list.csv per the expectation of scanpy but I continue to get the following error. I have confirmed that the folder structure and pathing is correct. Can you think of what else could be the issue with it not reading/finding this file? . ```py; >>> import os; >>> # p = os.path.join( ""path to outs location""); >>> print(p); ""path to outs location""; >>> print(os.path.exists(p)); True; >>> ad = sc.read_visium(p); ```. ```pytb; Traceback (most recent call last):. Cell In[6], line 1; ad = sc.read_visium(p). File ~\anaconda3\lib\site-packages\scanpy\readwrite.py:390 in read_visium; raise OSError(f""Could not find '{f}'""). OSError: Could not find 'path to outs location\spatial\tissue_positions_list.csv'; ```. ### Session/scanpy info:; Software versions; Python 3.10.9 64bit [MSC v.1916 64 bit (AMD64)]; IPython 8.10.0; OS Windows 10 10.0.22621 SP0; scanpy 1.9.3; Sun May 14 14:59:57 2023 Eastern Daylight Time",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488:37,load,load,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488,1,['load'],['load']
Performance,"Hello world!; I've read in many papers that when performing a re-clustering of some populations, like T cells or B cells, prior to the step of integration and so on, they re-calculate the HVGs but excluding the TCR- or BCR-related genes, because they are donor-specific, especially when talking about BCR. Can you help me how to remove the TCR- or BCR-related genes before computing the HVGs selection, but without removing them from the .var of the anndata, since I want to evaluate their expression during the step of cell annotation?. The code that I use to calculate the HVGs is the following:; sc.pp.highly_variable_genes(adata,; n_top_genes = 4000, flavor = ""seurat_v3"",; layer = ""raw"", batch_key = 'sample_id',; subset = False). Thanks a lot!; Paolo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2895:49,perform,performing,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2895,1,['perform'],['performing']
Performance,"Hello, . I am using scanpy rank genes groups, and rank genes group filter for differential expression analysis after using a classifier. I often receive errors because statistics cannot be calculated on these types of low count groups. The workaround I have found is to drop these cells from the adata object, and then continue with differential expression. Is there an existing solution for this that is better? Could we consider adding this as a flag to the function call? What I have in mind is a flag like ""ignore_low = True"". The flag would operate by taking the passed adata object, applying the 2 cell filtration internally, and performing differential expression as normal on this internal object. It would then append the relevant uns categories to the original adata object before exiting. The threshold could even be passable to make this more general. . What do we think? Is this too niche for this scale of a repository? In principle, I think that forcing these observations to be dropped is not best practice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3118:636,perform,performing,636,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3118,1,['perform'],['performing']
Performance,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680:317,perform,perform,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680,2,['perform'],['perform']
Performance,"Hello,. It would appear that `louvain-igraph` has been obsoleted in favour of `leidenalg`, and the author makes a [persuasive case](https://arxiv.org/abs/1810.08473) as to the superiority of the new approach. To my untrained eye, the algorithm is conceptually similar to the Louvain modification used by Seurat, but introduces an extra collapsed network refinement step. it should be easy to support this in Scanpy - the syntax appears to be identical to the old `louvain` innards, and I was able to construct a very minimal dummy function for testing by taking the key bits of `sc.tl.louvain()` and replacing `louvain.` with `leidenalg.`:. ```py; import leidenalg; import numpy as np; import pandas as pd; from scanpy import utils; from natsort import natsorted. def leiden(adata, use_weights=False, resolution=1, iterations=-1):; 	g = utils.get_igraph_from_adjacency(adata.uns['neighbors']['connectivities'], directed=True); 	weights = None; 	if use_weights:; 		weights = np.array(g.es[""weight""]).astype(np.float64); 	part = leidenalg.find_partition(; 		g, leidenalg.RBConfigurationVertexPartition, ; 		resolution_parameter = resolution, weights = weights, ; 		n_iterations = iterations,; 	); 	groups = np.array(part.membership); 	adata.obs['louvain'] = pd.Categorical(; 		values=groups.astype('U'),; 		categories=natsorted(np.unique(groups).astype('U')),; 	); ```. As such, replacing any `louvain.` with `leidenalg.` in `sc.tl.louvain()` would do most of the work. Probably the only new thing that would need support would the the `n_iterations` parameter in `leidenalg.find_partition()`. The default value is 2, positive values control how many passes of the algorithm are performed. -1 just makes it run until it fails to improve the clustering.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350:1677,perform,performed,1677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350,1,['perform'],['performed']
Performance,"Hello,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33:676,perform,perform,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33,1,['perform'],['perform']
Performance,"Hey!. Here's the downsample function I wrote to downsample count matrices. Now the function is also loaded via the api. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/100:100,load,loaded,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100,1,['load'],['loaded']
Performance,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739:65,load,load,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739,1,['load'],['load']
Performance,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734:40,load,loaded,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:; ```py; ad = sc.read_h5ad('scdataset.h5ad', backed='r+'); ad2 = sc.read_h5ad('scdataset.h5ad'); ```; and; ```py; random_genes = list(ad.var_names.to_series().sample(100)); ```; this works perfectly:; ```py; sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42); ```; but, this:; ```py; sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42); ```; yields the following error:; ```pytb; -----------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-113-9cb28e089b25> in <module>; ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 90 else:; 91 obs_avg = pd.Series(; ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes; 93 ; 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims); 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims); 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims); --> 951 avg = _divide_by_count(tot, cnt, out=out); 952 ; 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out); 216 else:; 217 if out is None:; --> 218 return a.dtype.type(a / b); 219 else:; 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence.; ```. thanks; Mark",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/883:67,load,load,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883,1,['load'],['load']
Performance,Hi . We have purchased the Nadia dolomite machine which is the automated version of dropseq. I am using the dropseqpipe pipeline for demultiplexing and generate count matrix. Analysis with Seurat is fine but how do you load a count matrix in scanpy?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/366:219,load,load,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366,1,['load'],['load']
Performance,"Hi @ALL,; I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request?; Any advice would be appreciated.; Best,; hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650:120,perform,perform,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650,1,['perform'],['perform']
Performance,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:291,cache,cache,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,6,"['Cache', 'cache']","['Cache', 'cache', 'cached']"
Performance,"Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`; `adata.var_names_make_unique()`; `sc.pp.filter_cells(adata, min_genes=200)`; `sc.pp.filter_genes(adata, min_cells=3)`; `mito_genes = adata.var_names.str.startswith('MT-')`; `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`; `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem?. Thank you in advance,; Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/977:306,cache,cache,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977,1,['cache'],['cache']
Performance,"Hi all,. Right now we have two layers in the scanpy API. The top layer consists of the major modules like `pp,pl,tl` as well as the smaller ones like `queries,get,datasets`. In addition, we have some useful functions directly under the scanpy package like `read/read_text/read_mtx` etc. It is obvious that the field is advancing and alternative/better ways to perform fundamental tasks in downstream analysis (e.g. normalization, DE tests, gene selection) are emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:360,perform,perform,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,1,['perform'],['perform']
Performance,"Hi everyone,; at the moment, pie charts for paga are a bit brittle, see https://github.com/theislab/cellrank/issues/25.; This pull request is an attempt to fix it. Rather than using `ax.pie`, it's just a bunch of scatterplots with custom markers.; I've tested the performance; ```python; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; sc.pl.paga(adata, color=foo, colorbar=False); ```; ![currect](https://user-images.githubusercontent.com/46717574/77180766-ad339b80-6aca-11ea-9a85-617ad122d140.png). Performancewise, it takes about ~14 seconds to produce the plot with the proposed changes,; ~4s, but I consider that the worst-case scenario.; More importantly, current version doesn't produce a correct plot, see below:; ![buggy](https://user-images.githubusercontent.com/46717574/77180621-7f4e5700-6aca-11ea-8c78-25fbba8f7c98.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123:264,perform,performance,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123,2,"['Perform', 'perform']","['Performancewise', 'performance']"
Performance,"Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it.; I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856:1308,load,loading,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856,1,['load'],['loading']
Performance,"Hi there! Thanks for adding the ingest method to scanpy!; I was wondering what would be the suggested approach when the reference data contains batch effects that should be removed before actually performing the asymmetric integration with a query dataset. I tried to use the neighbors structure returned by BBKNN (which correctly adjusts for batch effects), but the 'metric' object is missing. Here the error:. ```; KeyError Traceback (most recent call last); <ipython-input-22-a805d117788e> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='time', embedding_method='umap'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 115 labeling_method = labeling_method * len(obs); 116 ; --> 117 ing = Ingest(adata_ref); 118 ing.fit(adata); 119 . /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in __init__(self, adata); 268 ; 269 if 'neighbors' in adata.uns:; --> 270 self._init_neighbors(adata); 271 ; 272 if 'X_umap' in adata.obsm:. /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_neighbors(self, adata); 229 else:; 230 dist_args = (); --> 231 dist_func = named_distances[adata.uns['neighbors']['params']['metric']]; 232 self._random_init, self._tree_init = make_initialisations(dist_func, dist_args); 233 self._search = make_initialized_nnd_search(dist_func, dist_args). KeyError: 'metric'. ```; I'm running scanpy version 1.4.5.post2. Any help would be highly appreciated!! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1108:197,perform,performing,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1108,1,['perform'],['performing']
Performance,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/606:1354,bottleneck,bottleneck,1354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606,1,['bottleneck'],['bottleneck']
Performance,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:108,load,load-failed-while-file-is-in-working-directory,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,2,['load'],"['load', 'load-failed-while-file-is-in-working-directory']"
Performance,"Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using ‘cellranger’ software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In ‘Seurat’, I can read the data from my three treatments separated, do quality control, and then integrate them using ‘FindIntegrationAnchors’ and ‘IntegrateData’ functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in ‘Scanpy’ to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that?. In ‘Scanpy’ I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type?. Thank you very much!. Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:156,perform,perform,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,5,['perform'],['perform']
Performance,"Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`; `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help!. Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/668:320,perform,perform,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668,1,['perform'],['perform']
Performance,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:107,cache,cache,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,7,['cache'],['cache']
Performance,"Hi, I found that using the sc.tl.rank_genes_groups to perform differential gene expression analysis return the following error. ---; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'); ```. ```pytb; ranking genes; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-92-a8f4e965724c> in <module>; 1 adata = sc.datasets.pbmc68k_reduced(); ----> 2 sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). /mnt/data4/weixu/miniconda3_R_4.0/envs/celloracle_env/lib/python3.6/site-packages/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds); 607 for col in test_obj.stats.columns.levels[0]:; 608 adata.uns[key_added][col] = test_obj.stats[col].to_records(; --> 609 index=False, column_dtypes=dtypes[col]; 610 ); 611 . TypeError: to_records() got an unexpected keyword argument 'column_dtypes'; ```; I was wondering that its associate with my pandas version? or other issues?; my pandas version 0.23.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1478:54,perform,perform,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478,1,['perform'],['perform']
Performance,"Hi,. I am getting an error when loading my loom files, which did not happen before and I am not capable of understanding the error output to try to fix it. . ![screen shot 2018-08-29 at 10 58 23](https://user-images.githubusercontent.com/42487820/44760841-9b527680-ab7b-11e8-9e85-0d0235cee6db.png). Your help will be much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247:32,load,loading,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247,1,['load'],['loading']
Performance,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:372,queue,queue,372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,2,"['multi-thread', 'queue']","['multi-threaded', 'queue']"
Performance,"Hi,. I found there is 'use_rep' for tool.tsne(), but not for tool.umap().; Is there any solution to perform umap on a selected anndata's obsm?. Thanks in advance,; BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/689:100,perform,perform,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689,1,['perform'],['perform']
Performance,"Hi,. I have few queries regarding scanpy. 1. As scanpy is using Louvain Leiden algorithms for clustering which optimize modularity 'Q', so how we can access and print modularity funciton?. 2. Resolution parameter gave us different number of clusters when we iterated between the best suggested 0.6-1.2. So how we know best number of cluster and how we can choose the optimum value of parameter 'resolution'?. Thanks,; Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670:111,optimiz,optimize,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670,1,['optimiz'],['optimize']
Performance,"Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872:175,perform,perform,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872,1,['perform'],['perform']
Performance,"Hi,; I couldn't import scanpy due to an error: DLL load failed. I have checked pre-existing issues, but all of them seem to be an h5py issue. My error report seems different from them.; ```; >>> import scanpy as sc; D:\Anaconda\lib\site-packages\dask\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.; data = yaml.load(f.read()) or {}; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\__init__.py"", line 36, in <module>; from . import tools as tl; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\__init__.py"", line 17, in <module>; from ._sim import sim; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\readwrite.py"", line 10, in <module>; import tables; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found. >>> print(sys.version); 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468:51,load,load,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468,7,"['Load', 'load']","['Loader', 'load']"
Performance,"Hi,; I really like using scanpy for our large scale single cell aging project. I am uploading several libraries from different age mice (8 time points) and concatinate them together (see example cmd I used below). I would like to control the color display so that I have a gradient color that changes with age and visualise in umap plot for example sc.pl.umap(adata, color= ['Age','louvain']). How could I do that? Thank you very for your help and for this wonderful package. path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/13_mouse_IL33_8w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_13_mouse_IL33_8w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_13_mouse_IL33_8w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_13_mouse_IL33_8w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_13_mouse_IL33_8w_X.obs['Tissue'] = 'X'; adata_13_mouse_IL33_8w_X.obs['Age'] = '8Weeks'; adata_13_mouse_IL33_8w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_13_mouse_IL33_8w_X, min_genes=250). path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/1_mouse_IL33_16w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_1_mouse_IL33_16w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_1_mouse_IL33_16w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_1_mouse_IL33_16w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_1_mouse_IL33_16w_X.obs['Tissue'] = 'X'; adata_1_mouse_IL33_16w_X.obs['Age'] = '16Weeks'; adata_1_mouse_IL33_16w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_1_mouse_IL33_16w_X, min_genes=250). etc............ adata = adata_13_mouse_IL33_8w_X.concatenate([adata_1_mouse_IL33_16w_X, etc ....)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/185:649,cache,cache,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185,2,['cache'],['cache']
Performance,"Hi,; I was reading some mtx file from here: ; https://www.ebi.ac.uk/gxa/sc/experiments/E-HCAD-4/downloads. `adata = sc.read_mtx(""./data/mtx/E-HCAD-4.aggregated_filtered_counts.mtx"")`; `AnnData object with n_obs × n_vars = 25052 × 606606; ` ; `sc.__version__`; `'1.7.1'`. when loading the mtx file the obs and vars are mixed up. ; That happened with another mtx file before. I was wondering if already a fix exists to specify the obs and vars (or switch them if necessary). . Thanks . </details>; ![image](https://user-images.githubusercontent.com/7283790/112545551-a19f4280-8db8-11eb-8e0d-7d56ee0443b5.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1761:276,load,loading,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761,1,['load'],['loading']
Performance,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645:71,perform,perform,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645,2,['perform'],['perform']
Performance,"Hi,; Scanpy are designed to handle big datasets, while how about the performance on small datasets ? (such as as few as 50 cells from early embryo)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764:69,perform,performance,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764,1,['perform'],['performance']
Performance,"Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/985:12,perform,performed,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985,2,['perform'],['performed']
Performance,"Hi. I am recently transformed Seurat object to scanpy and use it for further pseudotime analysis (PAGA) and it performs really well. . But I have three question here:. 1) I am wondering if anybody here knows how to make PAGA connectivity score heatmap (ref: Popescu et al, 2019, Nature) which shows connections strength between partitions. I've tried dendrogram in scanpy (pl.coorelation.matrix) but we'd like to try more. . 2) And also if anyone knows if we could perform differential expression on the partitions by PAGA to find the marker gene along the potential path?. 3) PAGA generated a pie chart in every partition But does anyone know whether I could acquire the real percentage of the pie representing different Seurat cluster I input?. Thanks in advance for everyone's help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1133:111,perform,performs,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133,2,['perform'],"['perform', 'performs']"
Performance,"Hi. I have successfully installed scanpy but ; ImportError Traceback (most recent call last); <ipython-input-5-99fcf407c387> in <module>; ----> 1 import scvelo as scv; 2 import scanpy as sc; 3 import numpy as np. ~/anaconda3/lib/python3.7/site-packages/scvelo/__init__.py in <module>; 14 del version; 15 ; ---> 16 from .read_load import AnnData, read, read_loom, load, read_csv, get_df, DataFrame; 17 from .preprocessing.neighbors import Neighbors; 18 from .tools.run import run_all, test. ~/anaconda3/lib/python3.7/site-packages/scvelo/read_load.py in <module>; 10 from scipy.sparse import issparse; 11 from anndata import AnnData; ---> 12 from scanpy import read, read_loom; 13 ; 14 . ImportError: cannot import name 'read' from 'scanpy' (unknown location). Would you please help me to fix this problem. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1433:363,load,load,363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1433,1,['load'],['load']
Performance,"Hi.; I have a problem to install fa2 (pip install fa2) in windows 10 operating system and I am using python 3.7 version? . Using Conda env . **sc.tl.draw_graph(ds, init_pos='paga'),**; **drawing single-cell graph using layout 'fa'; WARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).**. installation error. Collecting fa2; Using cached fa2-0.3.5.tar.gz (435 kB); Requirement already satisfied: numpy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.18.4); Requirement already satisfied: scipy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.4.1); Requirement already satisfied: tqdm in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (4.46.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\xkalaj\AppData\Local\Temp\pip-wheel-yjh93oit'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running bdist_wheel; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:438,cache,cached,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['cache'],['cached']
Performance,"How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data.; Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/495:252,load,loaded,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495,1,['load'],['loaded']
Performance,"I am fairly new with using scanpy, and so I may be performing this incorrectly. I encountered an error when trying to create a backed AnnData object from an h5ad file, and then logarithmizing the data matrix within the object using scanpy.pp.log1p. However I get an error within the AnnData object code because the preprocessing/_simple.py script is not passing a filename in the copy() function. Right now my current workaround is to create the AnnData object as non-backed, do the log1p, and then create a ""filename"" property to the AnnData object afterwards to make it backed for other scanpy functions. ### Example; ```python; import scanpy as sc. dataset_path = ""/path/to/test/data.h5ad"" # Subbing out actual filenames for data; adata = sc.read_h5ad(dataset_path, backed='r'); print(adata) # To ensure there is a backed filepath. adata.raw = sc.pp.log1p(adata, copy=True) # Error is here; ```. #### Error output; ```pytb; # I printed the AnnData object to ensure it was backed; AnnData object with n_obs × n_vars = 4166 × 16852 backed at '/tmp/1b12dde9-1762-7564-8fbd-1b07b750505f.h5ad'; obs: 'cell_type', 'barcode', 'tSNE_1', 'tSNE_2', 'replicate', 'louvain', 'n_genes', 'percent_mito', 'n_counts'; var: 'gene_symbol', 'n_cells'; obsm: 'X_tsne'. # Actual error after calling log1p; Traceback (most recent call last):; File ""log1p_test.cgi"", line 129, in <module>; main(); File ""log1p_test.cgi"", line 81, in main; adata.raw = sc.pp.log1p(adata, copy=True); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 292, in log1p; data = data.copy(); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/anndata/_core/anndata.py"", line 1457, in copy; ""To copy an AnnData object in backed mode, ""; ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`.; ```. #### Versions:; scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.16.3 scipy==1.4.1 pandas==0.24.2 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1153:51,perform,performing,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1153,1,['perform'],['performing']
Performance,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:200,perform,perform,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['perform']
Performance,"I am trying to load some datasets with `sc.read_h5ad(file_name)`. Frequently, I get the below error. When I re-run the code multiple times or at different times it sometimes works, but often I get the error (using the same code and data). This happens when reading different h5ad datasets (e.g. is not specific to one dataset). At all times there seems to be enough free RAM / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error messag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:15,load,load,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['load'],['load']
Performance,"I am wondering about the motivation that went into subtracting the min when performing standardisation of the scale between genes. I find that it leads a misleading visualisation when the genes expressed by all clusters so I am now copying and modifying your function for my work. Do you think it would be justified to remove min subtraction step or make it optional?; Thanks, and please let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1451:76,perform,performing,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1451,1,['perform'],['performing']
Performance,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275:428,cache,cache,428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275,1,['cache'],['cache']
Performance,"I have a dataset with around 400K observations -- I wanted to perform batch correction using sc.pp.combat, but I'm getting out of memory errors after running for a couple hours with > 2 TB of memory. My understanding was that combat used a dense matrix, which requires a lot of memory.; Why is this? Are there suggestions for workarounds here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1977:62,perform,perform,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977,1,['perform'],['perform']
Performance,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/66:466,cache,cache,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66,2,['cache'],['cache']
Performance,"I have a similar issue to [this comment](https://github.com/theislab/scanpy/issues/1916#issuecomment-927497782). `Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids')`. Switching to `gene_symbols` didn't work. Error messages:; ```; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise V",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:287,cache,cache,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,2,['cache'],['cache']
Performance,"I just have scanpy 0.2.7 and am trying to produce bpmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:647,cache,cache,647,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,1,['cache'],['cache']
Performance,"I know this (quite ancient) pull request has been open (#403), but I wasn't sure on its status. I think the consensus was to wait for sklearn to integrate the necessary changes? If that's still the case, then please feel free to remove this PR. Here I make use of scipy's extremely nifty [LinearOperator](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html) class to customize the dot product functions for an input sparse matrix. In this case, the 'custom' dot product performs implicit mean centering. In my benchmarks, performing implicit mean centering in this way does not affect the runtime whatsoever. However, this approach has to use [svds](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html), for which randomized SVD is not implemented. So we have to use 'arpack', which can be significantly slower (but not intractably so.... in my hands, I could still do PCA on datasets of 200k+ cells in minutes, and it sure beats densifying the data, if you want more thorough benchmarks I am happy to generate them!). The way I incorporated this functionality into scanpy/preprocessing/_simple.py might be questionable, and would love any suggestions or advice on how to better integrate this if there is interest in pushing this PR through. Let me know!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066:511,perform,performs,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066,2,['perform'],"['performing', 'performs']"
Performance,"I think we should introduce a standardized “mask” argument to scanpy functions. This would be a boolean array (or reference to a boolean array in `obs`/ `var`) which masks out certain data entries. This can be thought of as a generalization of how highly variable genes is handled. As an example:. ```python; sc.pp.pca(adata, use_highly_variable=True); ```. Would be equivalent to:. ```python; sc.pp.pca(adata, mask=""highly_variable""); # or; sc.pp.pca(adata, mask=adata.obs[""highly_variable""]); ```. One of the big advantages of making this more widespread is that tasks which previously required using `.raw` or creating new anndata objects will be much easier. Some uses for this change:. ### Plotting. A big one is plotting. Right now if you want to show gene expression for a subset of cells, you have to manually work with the Matplotlib Axes:. ```python; ax = sc.pl.umap(pbmc, show=False); sc.pl.umap(; pbmc[pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells',])],; color=""LDHB"",; ax=ax,; ); ```. If a user could provide a mask, this could be reduced, and would make plotting more than one value possible:. ```python; sc.pl.umap(; pbmc,; color=['LDHB', 'LYZ', 'CD79A’],; mask=pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells’,]),; ); ```. ### Other uses. This has come up before in a few contexts:. * Performing normalization on just some variables https://github.com/scverse/scanpy/issues/2142#issuecomment-1046729522; * Selecting a subset of variables for DE tests: https://github.com/scverse/scanpy/issues/1744; * See also https://github.com/scverse/scanpy/issues/748; * Changing use_raw https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988. ## Implementation. I think this could fit quite well into the `sc.get` getter/ validation functions (https://github.com/scverse/scanpy/issues/828#issuecomment-560072919).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234:1337,Perform,Performing,1337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234,1,['Perform'],['Performing']
Performance,"I updated anndata to 0.8.0 and was not able to load my scanpy 1.8.2 properly. Any ideas?. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_31935/912249142.py in <module>; ----> 1 import scanpy as sc. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2264:47,load,load,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2264,1,['load'],['load']
Performance,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270:655,optimiz,optimized,655,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270,1,['optimiz'],['optimized']
Performance,"I upgraded anndata to 0.8.0 and couldn't load my scanpy 1.8.2 anymore. Error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_31935/912249142.py in <module>; ----> 1 import scanpy as sc. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```. The messages when updating anndata:; ```; The following packages will be REMOVED:. pytables-3.6.1-py38h9f153d1_1. The following packages will be UPDATED:. anndata 0.7.6-py38h578d9bd_0 --> 0.8.0-py38h578d9bd_0; ca-certificates pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0; h5py 2.10.0-nompi_py38h513d04c_102 --> 3.6.0-nompi_py38hfbb2109_100; hdf5 1.10.5-nompi_h5b725eb_1114 --> 1.12.1-nompi_h2750804_100. The following packages will be SUPERSEDED by a higher-priority channel:. certifi pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:41,load,load,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['load'],['load']
Performance,"I was going crazy but I have since reproduced this finding using the minimal 3000 PBMC dataset clustering example. Essentially I run the same code either on a virtual machine with 8 CPUs or one with 16 CPUs and I get non-identical PCA results. It doesn't seem to matter if I use the arpack or the randomized solver even though using the randomized solver gives the warning:. `Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.`. I'd like to just attach the jupyter notebook but it won't seem to let me do that so I'm copying the code below.; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; # First run on a machine with 8 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata). # Then run on a machine with 16 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normaliz",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:1140,cache,cache,1140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['cache'],['cache']
Performance,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146:208,load,loading,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146,2,['load'],"['load', 'loading']"
Performance,"I was wondering if someone who is familiar with sc.pp.regress_out could confirm the following:; I would like to regress out nonlinear effect, e.g. ~1 + a + a^2 + a^3, where a is non-categorical variable. I have looked at the code of regress_out: https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L677; It seems that the code performs the fitting for all specified variables at once, but I am not sure:; https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L701; If the design passed to GLM is combined of all keys passed to the function then I could just create the necessary columns a, a^2, a^3 and pass this as keys. Can someone confirm if I understand this correctly and passing the polynomial columns will do the fitting of a polynom?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839:388,perform,performs,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839,1,['perform'],['performs']
Performance,"I'd be happy to add this once I could figure it out. I've been able to load my tabular text files, store them as h5ad, then load them back again. I cannot see how to iterate over rows, then columns so that I can access all values of the dataframe with an awareness of which row/column each belongs to. My wishful code example:. ```py; adata = sc.read_h5ad(filename); selected = adata[:, adata.var_names.isin({'AAR2', 'ECT2'})]. ## this line spews information on the columns like:; # Empty DataFrameView; # Columns: []; # Index: [Cancer--Cell_1, Cancer--Cell_10, Cancer--Cell_100, Cancer--Cell_1000, Cancer--Cell_1001; #print(selected.obs). ## this line gives the row information:; # Empty DataFrameView; # Columns: []; #Index: [AAR2, ECT2]; #print(selected.var); ; # Nothing happens here at all; #for i, row in selected.obs.iteritems():; # print(i, row). for gene_name, row in selected.var.iterrows():; # this prints like: Series([], Name: AAR2, dtype: float64); print(row). # Nothing happens here; for cell_name, val in row.iteritems():; print(""{0}\t{1}\t{2}"".format(gene_name, cell_name, val)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/70:71,load,load,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70,2,['load'],['load']
Performance,"I'm not sure if this is a bug or not. It might just be something that I need clarification on, so apologies if adding it here is inappropriate. I've found that the `.obsp['distances']` matrix output by `sc.pp.neighbors()` is non-symmetric, which doesn't make sense to me. I don't see any parameters in the function for calculating directed vs undirected graph, which might have otherwise led to asymmetry. What am I missing?; Is there some special treatment of the matrix being performed to optimise downstream processing? Or is there something wrong causing this behaviour? . Many thanks. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sp; import pandas as pd. data = sp.datasets.pbmc3k(). sp.pp.normalize_total(data,target_sum=10000); sp.pp.log1p(data); sp.pp.highly_variable_genes(data, n_top_genes=2000) ; sp.pp.scale(data). sp.tl.pca(data, svd_solver='arpack', ); sp.pp.neighbors(data, n_neighbors=20). num_nonzeros = {}; for i in range(data.obsp['distances'].shape[0]):; num_nonzeros[i]= (data.obsp['distances'][i,:].count_nonzero(), data.obsp['distances'][:,i].count_nonzero()). df_nonzeros = pd.DataFrame.from_dict(num_nonzeros, orient = 'index', columns = ['row','column']). print((data.obsp['distances'].A == data.obsp['distances'].A.transpose()).all()) #demonstration that matrix is not symmetric. print(df_nonzeros) #number of non-zero entries in each row and column; #each row has 20 non-zero entries (when adding 1 for self-loops), which is the k parameter used in sp.pp.neighbors; #each column has a varying number of non-zero entries. print(df_nonzeros.column.mean()) #there is still an average of 20 connections in each column; ```. ```pytb; No error; ```. #### Versions. <details>. -----; anndata 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2381:478,perform,performed,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381,1,['perform'],['performed']
Performance,"I'm trying to import some data I downloaded from GEO using the read_10x_mtx() function. Since this data was generated with an older version of Cellranger, there is no features.tsv.gz file. I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. I am pasting the error message below: . ```pytb; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=Non",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:375,cache,cache,375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,2,['cache'],['cache']
Performance,"I'm trying to load the GSE164690 data using sc.read_10x_h5(), for which I'm including the path to the folders which contain the barcodes.tsv, features.tsv and matrix.mtx but I'm getting the IsADirectoryError every time I run the function. . `; adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'); reading GSE164690_RAW/GSM5017021_HN01_PBL/; ---------------------------------------------------------------------------; IsADirectoryError Traceback (most recent call last); Input In [3], in <cell line: 1>(); ----> 1 adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'). File ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py:180, in read_10x_h5(filename, genome, gex_only, backup_url); 178 if not is_present:; 179 logg.debug(f'... did not find original file {filename}'); --> 180 with h5py.File(str(filename), 'r') as f:; 181 v3 = '/matrix' in f; 182 if v3:. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:507, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds); 502 fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,; 503 locking, page_buf_size, min_meta_keep, min_raw_keep, **kwds); 504 fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,; 505 fs_persist=fs_persist, fs_threshold=fs_threshold,; 506 fs_page_size=fs_page_size); --> 507 fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); 509 if isinstance(libver, tuple):; 510 self._libver = libver. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:220, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 218 if swmr and swmr_support:; 219 flags |= h5f.ACC_SWMR_READ; --> 220 fid = h5f.open(name, flags, fapl=fapl); 221 elif mode == 'r+':; 222 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). File h5py/_objects.pyx:54, in h5py._objects.with_ph",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2328:14,load,load,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2328,1,['load'],['load']
Performance,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:242,load,load,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['load'],['load']
Performance,Import performance part 3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756:7,perform,performance,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756,1,['perform'],['performance']
Performance,ImportError: DLL load failed while importing utilsextension,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:17,load,load,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['load'],['load']
Performance,ImportError: dlopen: cannot load any more object with static TLS,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,1,['load'],['load']
Performance,"In DropSeq experiments cell names are encoded by 12nt barcodes. It seems that no name check is performed when merging multiple datasets in ScanPy. . ```python; >>> from collections import Counter; >>> import scanpy.api as sc. >>> f = sc.read(""data1.txt"").transpose(); >>> g = sc.read(""data2.txt"").transpose(); >>> c = f.concatenate(g). >>> len(c.obs_names); 7932; >>> len(set(c.obs_names)); 7890. >>> cc = Counter(c.obs_names); >>> cc.most_common(10); [('AAAAAAAAAAAA', 2), ('TCCTGTCTCTTA', 2), ('CGCAAGGGAAAG', 2), ('ACCCGTCTATGT', 2), ('CTCCTGTCTCTT', 2), ('TTCCTGTCTCTT', 2), ('CCCTGTCTCTTA', 2), ('CCGCTGTCTCTT', 2), ('GACAAACCTACC', 2), ('ACACTGTCTCTT', 2)]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55:95,perform,performed,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55,1,['perform'],['performed']
Performance,"In a recent paper, we found the brute force KNN computation to become very expensive as the data sizes increase. I’ve noticed the kNN graph computed during the “neighbors” computation can be cached and reused when Umap-learn is called downstream but when Cuml UMAP is used, the kNN graph is recomputed each time. . In cuml 0.13 we added an optional `knn_graph` argument to umap’s training and inference methods to allow it to accept pre-computed kNN graph. This will allow the kNN graph to be computed once and reused when `n_neighbors` has not been changed. I think this would further accelerate the exploratory data analysis and visualization process with scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1279:191,cache,cached,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1279,1,['cache'],['cached']
Performance,"In pl.pca_loagings(), there should be an option to limit the number of points plotted (basically n_points from ranking). Why: I recently used the AnnData/scanpy suite to perform some analysis on a low number of genes (less than 30, amplified by qRT-PCR).; As the number of features is less than 30 (30 being the default value for n_points in ranking(adata,*args,**kwargs), the loadings appear twice on the sc.pl.loadings() graph.; (the slices [0:15] and 5:20] are overlapping, in case you have only 20 genes. definition should be:; ```; def pca_loadings(; adata: AnnData,; components: Union[str, Sequence[int], None] = None,; n_points=30,; include_lowest: bool = True,; show: Optional[bool] = None,; save: Union[str, bool, None] = None,; ):; ```. and later in implementation; ```; ranking(; adata,; 'varm',; 'PCs',; npoints=npoints,; indices=components,; include_lowest=include_lowest,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2059:170,perform,perform,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2059,3,"['load', 'perform']","['loadings', 'perform']"
Performance,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178:203,perform,perform,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178,1,['perform'],['perform']
Performance,Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:45,perform,perform,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,1,['perform'],['perform']
Performance,"Is it because I have too many cells? But no memory error is reported. ```pycon; >>> adata; AnnData object with n_obs × n_vars = 1493240 × 4489; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'nCount_nFeature_ratio', 'brain_area', 'batch'; var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'brain_area_colors', 'hvg', 'pca'; obsm: 'X_pca'; varm: 'PCs'; ```. ```python; topPC = 40; n_neigbor = 15; resolution = 0.3; sc.pp.neighbors(adata, n_neighbors=n_neigbor, n_pcs=topPC); ```. ```pytb; computing neighbors; using 'X_pca' with n_pcs = 40; Segmentation fault (core dumped); ### error file: core.212911; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.4.0; anndata 0.7.8; asciitree NA; attr 21.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; dask 2021.11.1; dateutil 2.8.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; encodings NA; entrypoints 0.3; fasteners NA; fsspec 2021.11.0; genericpath NA; h5py 3.4.0; idna 3.1; igraph 0.9.8; ipykernel 6.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.3; joblib 1.1.0; jsonschema 4.2.1; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.3; mpl_toolkits NA; natsort 8.0.0; nbformat 5.1.3; nbinom_ufunc NA; ntpath NA; numba 0.53.1; numcodecs 0.9.1; numexpr 2.7.3; numpy 1.21.4; opcode NA; packaging 21.0; pandas 1.3.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; posixpath NA; prometheus_client NA; prompt_toolkit 3.0.22; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 9.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydoc_data NA; pyexpat NA; pygments 2.10.0; pyparsing 3.0.6; py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361:969,concurren,concurrent,969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361,1,['concurren'],['concurrent']
Performance,Is there any interest in writing a function to load any of the public datasets offered by 10x genomics?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1264:47,load,load,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1264,1,['load'],['load']
Performance,It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdown 0.11; docutils 0.18.1; entrypoints 0.4; et-xmlfile 1.1.0; executing 0.8.3; fastjsonschema 2.16.2; filelock 3.9.0; flake8 6.0.0; Flask 2.2.2; fonttools 4.25.0; frozenlist 1.3.3; fsspec 2023.4.0; futur,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:2083,Bottleneck,Bottleneck,2083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['Bottleneck'],['Bottleneck']
Performance,Loading from `.h5ad` taking much more memory than loading same dataset from 10x `.h5`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146:0,Load,Loading,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146,2,"['Load', 'load']","['Loading', 'loading']"
Performance,"Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(); ```; ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```; def style(; self,; cmap: str = DEFAULT_COLORMAP,; color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,; dot_max: Optional[float] = DEFAULT_DOT_MAX,; dot_min: Optional[float] = DEFAULT_DOT_MIN,; .....; ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://gith",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633:609,tune,tune,609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633,1,['tune'],['tune']
Performance,"My issue is...; 1. I read the data into anndata and performed all preprocessed steps and then used data matrix to perform non-linear dimensionality reduction (DR).; 2. I performed DR and k-means clustering.; 3. I added back the data into one of the components (X_tsne); 4. Added KM labels also. Now, I am not able to select the clusters based on the cluster.; I want to select one of the clusters and perform clustering on that. Please find the code snippets below. mlle_3d_data=pd.read_csv(""C:/Users/saite/source/df.csv""); mlle_3dc_data=pd.read_csv(""C:/Users/saite/source/dfc.csv""). adata.obsm['X_tsne']=np.asanyarray(mlle_3d_data); adata.obs['km']=list(mlle_3dc_data['clusters']); adata.obs['km']=adata.obs['km'].astype('category'); sc.pl.tsne(adata,color=['km']). ------------------; I am trying to select based on the cluster number how we do with leiden or louvain clustering, but I am not seeing any data.; Please help me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1452:52,perform,performed,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452,4,['perform'],"['perform', 'performed']"
Performance,Nadia single cell loading in scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/366:18,load,loading,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366,1,['load'],['loading']
Performance,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/300:491,perform,perform,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300,1,['perform'],['perform']
Performance,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583:127,perform,performed,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583,1,['perform'],['performed']
Performance,OSError: [WinError 123] When importing 10x data with cache=TRUE,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:53,cache,cache,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['cache'],['cache']
Performance,"On master (37851434b2) from the base of the repo, I haven't seen the following code finish running:. ```python; import scanpy.api as sc; adata = sc.read(""./data/pbmc3k_raw.h5ad""); %time sc.pp.downsample_counts(adata, 1500); ```. This PR implements an optimized version of the same thing, which gives:. ```python; %time sc.pp.downsample_counts(adata, 1500) ; CPU times: user 2.25 s, sys: 44.7 ms, total: 2.29 s; Wall time: 2.32 s; ```. ## What's changed. * I've rewritten the function to use numba along with fewer allocations; * Added a test for the function; * Added argument `replace`, which indicates whether subsampling should happen with replacement. ## Notes. To me, it makes more sense to sample without replacement, since for small changes in total counts you'll have more similar profiles. However, I've set the default for replacement to `True` to preserve the current behavior. Neither this or the previous method scale well with sampling depth, and it's maybe worth using a call to sample a multinomial or multivariate hypergeometric distribution instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340:251,optimiz,optimized,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340,1,['optimiz'],['optimized']
Performance,PMBC3k tutorial - issue loading saved object from .h5ad file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2497:24,load,loading,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497,1,['load'],['loading']
Performance,Pca loadings n points patch,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075:4,load,loadings,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075,1,['load'],['loadings']
Performance,Performance improvements for Regress Out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781:0,Perform,Performance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781,1,['Perform'],['Performance']
Performance,Performance: Investigate `pp.scale` with sparse matrices,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2986:0,Perform,Performance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2986,1,['Perform'],['Performance']
Performance,"Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed).; I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"".; Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/516:269,perform,performed,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516,1,['perform'],['performed']
Performance,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:58,multi-thread,multi-threading,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,2,['multi-thread'],['multi-threading']
Performance,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56:86,load,load,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56,2,['load'],['load']
Performance,Run static analysis concurrently,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/846:20,concurren,concurrently,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846,1,['concurren'],['concurrently']
Performance,"Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/846:21,concurren,concurrently,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846,1,['concurren'],['concurrently']
Performance,"Scanpy 1.5.0; Out of curiousity I set min_fold_change to different values but it didn't work. Even when I set min_fold_change=100 it doesn't shorten the gene list. So is it because the filters are working in an OR logic? If that's the case, I don't think the default values for each filter should be the ones you set - they should be a very harsh condition to make sure customer-specified parameters are the bottleneck.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1447:408,bottleneck,bottleneck,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1447,1,['bottleneck'],['bottleneck']
Performance,Show PCA loadings for genes with lowest loadings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805:9,load,loadings,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805,2,['load'],['loadings']
Performance,"Sometimes, it can happen when downloading 10x files from e.g. GEO that they are not organized in; folders but instead, they have a sample-specific prefix. E.g. . ```console; sturm@zeus [SSH] processed % ll; total 156M; -rw-r--r-- 1 dbadmin dbadmin 29K May 21 2018 GSM3148575_BC09_TUMOR1_barcodes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 259K May 21 2018 GSM3148575_BC09_TUMOR1_genes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 34M May 21 2018 GSM3148575_BC09_TUMOR1_matrix.mtx.gz; -rw-r--r-- 1 dbadmin dbadmin 28K May 21 2018 GSM3148576_BC09_TUMOR2_barcodes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 259K May 21 2018 GSM3148576_BC09_TUMOR2_genes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 33M May 21 2018 GSM3148576_BC09_TUMOR2_matrix.mtx.gz; ```. This PR adds a keyword argument `prefix` to `read_10x_mtx` which enables to load these files ; without manual renaming and moving, e.g. ; ```python; adata = sc.read_10x_mtx(""path/to/files"", prefix=""GSM3148575_BC09_TUMOR1_""); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1250:802,load,load,802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1250,1,['load'],['load']
Performance,Submodular optimization using apricot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:11,optimiz,optimization,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['optimiz'],['optimization']
Performance,"The ""outs"" from `spaceranger count` v2 differ from v1. Specifically, `tissue_positions_list.csv` has been renamed `tissue_positions.csv` and now has headers. Fortunately the column names exactly match those used by `scanpy`. See https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/spatial (search for `tissue_positions.csv`). This PR adjusts `sc.read_visium()` so that it can load the outputs of `spaceranger count` v2. The API / method signature is unchanged, so no changes to the documentation are required. The version is inferred from the presence or otherwise of the old `tissue_positions_list.csv` (implying v1) and then the files in `spatial/` are handled accordingly, including differential handling of headers / column names. @linhuawang; @sopvdl; @TaopengWang",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2458:413,load,load,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2458,1,['load'],['load']
Performance,"The original set of default parameters used by SAM were geared more towards smaller datasets. As scRNAseq throughput is ever-increasing, I tweaked the default parameters to be better suited for large datasets.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540:106,throughput,throughput,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540,1,['throughput'],['throughput']
Performance,The previous `sc.pl.umap` etc. had an option to export legend positions via 'on data export'. We need a solution in the docs... Presumably just by exposing the cached positions to the user.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/335:160,cache,cached,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/335,1,['cache'],['cached']
Performance,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:414,cache,cache-ing,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,2,['cache'],"['cache', 'cache-ing']"
Performance,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462:619,cache,cached,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462,1,['cache'],['cached']
Performance,"This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844:96,cache,cacheing,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844,1,['cache'],['cacheing']
Performance,This would fix #2941 . I created some numba.njit() kernels that perform in-place substitutions based on the assumption that we only change existing values and don't add new ones (where all the scipy overhead comes from). . Benchmarks for 90k cells and 25k genes:; CSR:; old 23 s | new 1 s | 23x; CSC:; old 61 s | 1.6 s | 36x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942:64,perform,perform,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942,1,['perform'],['perform']
Performance,Tracking operations performed on AnnData objects,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472:20,perform,performed,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472,1,['perform'],['performed']
Performance,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:297,cache,cached,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,3,"['cache', 'perform']","['cached', 'performing']"
Performance,Update:. Docs don't build with sphinx 4.1.0 due to a error triggered by `scanpydoc`. Sphinx will be pinned until this is solved (which is when this issue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentatio,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:421,load,loading,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,7,['load'],['loading']
Performance,Use cacheing and parallelism for qc metrics,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844:4,cache,cacheing,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844,1,['cache'],['cacheing']
Performance,"We are very impressed with the scalability of scanpy. We are interested in performing gene co-expression clustering on large single-cell RNAseq datasets. This typically involves calculating pairwise correlations between genes, then using these correlations as distance metrics for hierarchical and k-means clustering. Does scanpy already support these kinds of analyses?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72:31,scalab,scalability,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72,2,"['perform', 'scalab']","['performing', 'scalability']"
Performance,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191:16,perform,performance,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191,2,['perform'],['performance']
Performance,"We often identify a subset of cells as irrelevant noise cells and hope to discard them during the analysis. I am a bit confused about how to perform such operations in Scanpy. It's a common practice in other analysis tool like Seurat to do ScaleData across cells so that the relative expression level is adjusted without uninteresting cells' influences. . Such operation is supported by Seurat by providing multiple ""Assay"", such as `counts`, `data`, and `scale.data`, which stores the raw UMI counts, column normalized data (across genes, log1p), and row normalized data (across samples, zscores). . I noticed the scaled data are stored in `adata.X` in scanpy. With this design, how can we access the raw UMI count and do re-scaling if I hope to subset the data?. Looking forward your reply. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089:141,perform,perform,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089,1,['perform'],['perform']
Performance,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:29,queue,queue,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,4,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/171:143,load,loading,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171,1,['load'],['loading']
Performance,"When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace?; Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472:273,perform,performed,273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472,1,['perform'],['performed']
Performance,"When loading scanpy through `import scanpy.api as sc` the plotting style is changed. This prevents my from using it in my general workflow because I have particular plotting styles I use. This used to be a issue with seaborn, but was changed in version 0.8, but scanpy still seems to restyle the entire plotting environment?. If styles are applied locally in particular plotting functions, that is fine, but it shouldn't affect the users other plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/108:5,load,loading,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108,1,['load'],['loading']
Performance,When reading in a .xlsx file with sc.read_excel() it says the sheet I asked to load in does not exist.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:79,load,load,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,1,['load'],['load']
Performance,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223:142,optimiz,optimize,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223,2,['optimiz'],['optimize']
Performance,"\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1925,cache,cache,1925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['cache'],['cache']
Performance,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3289,race condition,race condition,3289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3569,race condition,race condition,3569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 73 return indexer; 74 elif isinstance(indexer, str):; ---> 75 return index.get_loc(indexer) # int; 76 elif isinstance(indexer, (Sequence, np.ndarray, pd.Index, spmatrix, np.matrix)):; 77 if hasattr(indexer, ""shape"") and (. D:\anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 3629 return self._engine.get_loc(casted_key); 3630 except KeyError as err:; -> 3631 raise KeyError(key) from err; 3632 except TypeError:; 3633 # If we have a listlike key, _check_indexing_error will raise. KeyError: 'CST3'. #### Versions; scanpy==1.9.2 anndata==0.8.0 umap==0.5.3 numpy==1.21.6 scipy==1.9.1 pandas==1.4.4 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.10.4 pynndescent==0.5.8; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.1.2; matplotlib 3.5.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.6; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2430:4487,bottleneck,bottleneck,4487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430,1,['bottleneck'],['bottleneck']
Performance,"]['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works; adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2565,cache,cache,2565,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['cache'],['cache']
Performance,"_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\Python38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\Python38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```. #### Versions. <details>. Package Version; ------------------- ---------; anndata 0.7.8; anyio 2.2.0; argon2-cffi 20.1.0; async-generator 1.10; attrs 21.2.0; Babel 2.9.1; backcall 0.2.0; bleach 4.1.0; Bottleneck 1.3.2; brotlipy 0.7.0; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.4; colorama 0.4.4; cryptography 36.0.0; cycler 0.11.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fonttools 4.25.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; importlib-metadata 4.8.2; ipykernel 6.4.1; ipython 7.29.0; ipython-genutils 0.2.0; jedi 0.18.0; Jinja2 3.0.2; joblib 1.1.0; json5 0.9.6; jsonschema 3.2.0; jupyter-client 7.1.0; jupyter-core 4.9.1; jupyter-server 1.4.1; jupyterlab 3.2.1; jupyterlab-pygments 0.1.2; jupyterlab-server 2.10.2; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.37.0; MarkupSafe 2.0.1; matplotlib 3.5.0; matplotlib-inline 0.1.2; mistune 0.8.4; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; mock 4.0.3; munkres 1.1.4; natsort 8.0.2; nbclassic 0.2.6; nbclient 0.5.3; nbconvert 6.1.0; nbformat 5.1.3; nest-asyncio 1.5.1; networkx 2.6.3; notebook 6.4.6; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; olefile 0.46; packaging 21.3; pandas 1.3.5; pandocfilters 1.4.3; pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:3375,Bottleneck,Bottleneck,3375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['Bottleneck'],['Bottleneck']
Performance,"_float; from skimage.util import invert; from scipy.spatial import ConvexHull, convex_hull_plot_2d; from multiprocessing import Pool; import time; import math; from collections import Counter; import scanpy as sc; import networkx as nx; import pandas as pd; import numpy as np; import itertools; import random; from scipy.stats import mannwhitneyu; import os; import warnings; import pickle; import matplotlib.pyplot as plt; import pandas as pd; import gseapy as gp; from matplotlib import pyplot as plt; from matplotlib_venn import venn2; import mygene; import seaborn as sns; from gseapy import barplot, dotplot; import random; import matplotlib.pyplot as plt; import numpy as np; import decoupler as dc; from numpy.random import default_rng; from copy import deepcopy; import scanpy as sc; import squidpy as sq; import time; from neighborhood_enrichment import neighborhood_enrichment; import schist as scs; with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:; pickle_= pickle.load(f); pickle_=dict(list(pickle_.items())[7:8]); for i in pickle_:; cnt+=1; adata=pickle_[i]; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3); sc.tl.leiden(adata); scs.inference.planted_model(adata); sc.pp.scale(adata); sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'); adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']); ```. ### Error output. ```pytb; It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2586:3170,load,load,3170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586,1,['load'],['load']
Performance,"_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:1379,cache,cache,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,1,['cache'],['cache']
Performance,"_get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex. ```. </details>. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; backcall 0.2.0; bbknn NA; bottleneck 1.3.2; cairo 1.19.1; cffi 1.14.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.2.0; legacy_api_wrap 0.0.0; leidenalg 0.8.2; llvmlite 0.34.0; lxml 4.6.1; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.0.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; palantir 1.0.0; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.2; scipy 1.5.2; scvelo 0.2.3; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tornado 6.0.4; traitlets 5.0.5; typing_extensions NA; umap ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:5743,bottleneck,bottleneck,5743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['bottleneck'],['bottleneck']
Performance,"_groups(adata, min_fold_change=1); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 1.5828611850738525 seconds ---; ```. with version 1.6.0:; ```python; import time; start_time = time.time(); sc.tl.rank_genes_groups(adata, groupby = 'leiden', method = 'wilcoxon'); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 49.53031611442566 seconds ---. start_time = time.time(); sc.tl.filter_rank_genes_groups(adata, min_fold_change=1); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 600.4000315666199 seconds ---; ```; I also noticed that it was using up 98% of my CPU while running `filter_rank_genes_groups`. #### Versions. <details>. `scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.19.1 scipy==1.5.2 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1`. and. ```; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; Bio 1.77; PIL 7.2.0; adjustText NA; anndata 0.7.4; annoy NA; backcall 0.2.0; bbknn NA; brotli NA; cachecontrol 0.12.6; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.1; changeo 1.0.0; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dandelion 0.0.15; dateutil 2.8.0; decorator 4.4.2; descartes NA; distance NA; get_version 2.1; h5py 2.10.0; hdmedians NA; idna 2.10; igraph 0.8.2; importlib_metadata 1.7.0; ipykernel 5.3.3; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; markupsafe 1.1.1; matplotlib 3.3.0; mizani 0.7.1; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; networkx 2.4; numba 0.50.1; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; palettable 3.3.0; pandas 1.0.5; parso 0.7.1; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotnine 0.6.0; polyleven NA; presto 0.6.1; prompt_toolkit 3.0.6; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pyparsing 2.4.7; pytz 2019.2; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; scrublet NA; seaborn 0.10.1; setuptools_scm NA; sinf",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1449:1802,cache,cachecontrol,1802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1449,1,['cache'],['cachecontrol']
Performance,"_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py"", line 45, in <module>; @numba.njit(cache=True); File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function 'sparse_mean_var_minor_axis': no locator available for file '/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py'. ```. I would highly appreciate if you could please point out how to fix this issue. . Thank you in advance!. Best wishes,; Abdelrahman . ```. #### Versions. <details>. numba==0.53.1; scanpy==1.8.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:3352,cache,cache,3352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,2,['cache'],['cache']
Performance,"`_sparse_nanmean` makes two copies of the data matrix and performs a set index operation on a sparse array. It could be much faster by not doing this things. Noticed while reviewing #1890. <details>; <summary> possible solution </summary>. ```python; from numba import njit, prange; import numpy as np. @njit(parallel=True); def nanmean_lowlevel(data, indices, indptr, shape):; N, M = shape; sums = np.zeros(N, dtype=np.float64); nans = np.zeros(N, dtype=np.int64); for i in prange(N):; start = indptr[i]; stop = indptr[i+1]; window = data[start:stop]; n_nan = np.int64(0); i_sum = np.float64(0.); for j_val in window:; if np.isnan(j_val):; n_nan += 1; else:; i_sum += j_val; sums[i] = i_sum; nans[i] = n_nan; sums /= (M - nans); return sums; ```. Has more error from dense reference compared to current solution, not sure why. Something about the sums being different. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1894:58,perform,performs,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1894,1,['perform'],['performs']
Performance,"```; adata=sc.read('./CONFIDENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:543,cache,cache,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,3,['cache'],['cache']
Performance,"```pytb; >>> adata.write(""./result.h5ad""); >>> bdata = sc.read(""./result.h5ad). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-15-d90a365327a0> in <module>(); 1 adata.write(“./results.h5ad”); ----> 2 bdata = sc.read(“./results.h5ad""). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed); 347 # load everything into memory; 348 d = _read_h5ad(filename=filename); --> 349 return AnnData(d); 350 ; 351 . /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode, asview, oidx, vidx); 632 obsm=obsm, varm=varm, raw=raw,; 633 dtype=dtype, single_col=single_col,; --> 634 filename=filename, filemode=filemode); 635 ; 636 def _init_as_view(self, adata_ref, oidx, vidx):. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode); 741 raise ValueError(; 742 'If `X` is a dict no further arguments must be provided.'); --> 743 X, obs, var, uns, obsm, varm, raw = self._from_dict(X); 744 ; 745 # init from AnnData. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _from_dict(ddata); 1591 d_true_keys['obs'][k_stripped] = pd.Categorical.from_codes(; 1592 codes=d_true_keys['obs'][k_stripped].values,; -> 1593 categories=v); 1594 if k_stripped in d_true_keys['var']:; 1595 d_true_keys['var'][k_stripped] = pd.Categorical.from_codes(. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in from_codes(cls, codes, categories, ordered); 616 ""codes need to be convertible to an arrays of integers""); 617 ; --> 618 categories = CategoricalDtype._validate_categories(categories); 619 ; 620 if len(codes) and (codes.max() >= len(categories) or codes.min() < -1):. /fastdata/chris/bin/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102:447,load,load,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102,1,['load'],['load']
Performance,"```pytb; package=scanpy; pversion=1.5.1 #found in docs/release-latest.rst after git download; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; cd /usr/common/src; git clone https://github.com/theislab/scanpy.git; cd scanpy; module load python3-libraries #for PYTHONPATH; python3 ./setup.py install \; --install-scripts=$TOPDIR/bin --prefix /usr/common \; 2>&1 | tee ../install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:268,load,load,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,2,['load'],['load']
Performance,"```python; import scanpy as sc; ```. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_22924/912249142.py in <module>; ----> 1 import scanpy as sc. ~\Anaconda3\lib\site-packages\scanpy\__init__.py in <module>; 13 Verbosity,; 14 ) # start with settings as several tools are using it; ---> 15 from . import tools as tl; 16 from . import preprocessing as pp; 17 from . import plotting as pl. ~\Anaconda3\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\Anaconda3\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\Anaconda3\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\Anaconda3\lib\site-packages\tables\__init__.py in <module>; 40 # Import the user classes from the proper modules; 41 from .exceptions import *; ---> 42 from .file import File, open_file, copy_file; 43 from .node import Node; 44 from .group import Group. ~\Anaconda3\lib\site-packages\tables\file.py in <module>; 21 import numpy as np; 22 ; ---> 23 from . import hdf5extension; 24 from . import utilsextension; 25 from . import parameters. ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173:1656,load,load,1656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173,1,['load'],['load']
Performance,``biomart_annotations`` still creates cache when ``use_cache=False``,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2861:38,cache,cache,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2861,1,['cache'],['cache']
Performance,"`sc.external.pp.harmony_integrate(adata, 'Batch')` only adds an obsm key `'X_pca_harmony'`. In order to analyze values for a specific gene after harmony, you would need to multiply `X_pca_harmony` by a loadings matrix, but `adata.varm` doesn't seem to get a `harmony_PCs` to accompany vanilla `adata.varm['PCs']`. . Is there a method I'm missing to analyze a harmony-corrected counts matrix in scanpy? If so, I'd love to see it documented. If not, I'd love to see it added.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314:202,load,loadings,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314,1,['load'],['loadings']
Performance,"`score_genes` does not work on a dataset loaded with `backed=""r+""`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/883:41,load,loaded,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883,1,['load'],['loaded']
Performance,"a-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.13 noxft_h4845f30_101 conda-forge; tqdm 4.66.1 pyhd8ed1ab_0 conda-forge; tzdata 2023d h0c530f3_0 conda-forge; umap-learn 0.5.5 py311h38be061_0 conda-forge; wheel 0.42.0 pyhd8ed1ab_0 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zstd 1.5.5 hfc55251_0 conda-forge; ```. 2) I imported the scanpy, seaborn, pandas, numpy and matplotlib libraries. Then I called the `read_10x_mtx()` function. The code is given below. ```pycon; >>> import scanpy as sc; >>> import pandas as pd; >>> import numpy as np; >>> import matplotlib; >>> import seaborn as sns; >>> !ls -lh ./H004/; -rwxrwxrwx. 1 nikolay nikolay 49K Mar 25 2021 barcodes.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 424K Mar 25 2021 features.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 101M Mar 25 2021 matrix.mtx.gz; >>> adata = sc.read_10x_mtx(; ... './H004/', ; ... var_names='gene_symbols', ; ... cache=True); >>> adata.var_names_make_unique(); >>> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. **The problem is the error in importing gene names both when using id and when using symbolic labeling. All genes have the same name. if you use `anndata=0.10.3` instead of `anndata=0.10.4`, then everything works correctly.**. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; import matplotlib; import seaborn as sns. path='<path_to_files>'. adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:6107,cache,cache,6107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cache']
Performance,"a/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); [190](file:///C:/Program%20Files/Python312/Lib/gzip.py:190) mode += 'b'; [191](file:///C:/Program%20Files/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:19950,cache,cache,19950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the memory usage is due to something else than the data not being backed? It is only 8000 cells and 15000 genes. Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:2410,Perform,Perform,2410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['Perform'],['Perform']
Performance,added example for PCA loadings plot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1815:22,load,loadings,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1815,1,['load'],['loadings']
Performance,"analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; In function read_10x_mtx there could be an option to search for non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1130,cache,cache,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"anceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. when I run; ```; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:; @numba.njit(fastmath=True, nogil=True, parallel=True); def euclidean_random_projection_split(data, indices, rng_state):; ^. self.func_ir.loc)); /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913:3669,bottleneck,bottleneck,3669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913,1,['bottleneck'],['bottleneck']
Performance,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1315,perform,performance,1315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['perform'],['performance']
Performance,"apper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad..callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\methods.py:500, in read_sparse(elem, _reader); 495 @_REGISTRY.register_read(H5Group, IOSpec(""csc_matrix"", ""0.1.0"")); 496 @_REGISTRY.register_read(H5Group, IOSpec(""csr_matrix"", ""0.1.0"")); 497 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csc_matrix"", ""0.1.0"")); 498 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csr_matrix"", ""0.1.0"")); 499 def read_sparse(elem, _reader):; --> 500 return SparseDataset(elem).to_memory(). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_core\sparse_dataset.py:379, in SparseDataset.to_memory(self); ...; 189 f""Above error raised while reading key {elem.name!r} of ""; 190 f""type {type(elem)} from {parent}.""; 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type from /.; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.3; -----; PIL 10.0.0; PyQt5 NA; anyio NA; arrow 1.2.3; asttokens NA; attr 23.1.0; attrs 23.1.0; babel 2.12.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.7; brotli 1.0.9; certifi 2023.07.22; cffi 1.15.1; charset_normalizer 3.2.0; colorama 0.4.6; comm 0.1.4; cvxopt 1.3.1; cycler 0.10.0; cython_runtime NA; ...; Python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]; Windows-10-10.0.19045-SP0; -----; Session information updated at 2023-08-04 10:17; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2592:2340,bottleneck,bottleneck,2340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592,1,['bottleneck'],['bottleneck']
Performance,"as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3456 if self.columns.nlevels > 1:; 3457 return self._getitem_multilevel(key); -> 3458 indexer = self.columns.get_loc(key); 3459 if is_integer(indexer):; 3460 indexer = [indexer]. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:; -> 3363 raise KeyError(key) from err; 3364 ; 3365 if is_scalar(key) and isna(key) and not self.hasnans:. KeyError: 1; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:1825,cache,cache,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['cache'],['cache']
Performance,"as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734:1201,cache,cache,1201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734,2,['cache'],['cache']
Performance,"aster branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pytz 2023.3.post1; ruamel NA; scipy 1.11.3; session_info 1.0.0; setuptools 68.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.3.0; sphinxcontrib NA; stack_data 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:1242,bottleneck,bottleneck,1242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,1,['bottleneck'],['bottleneck']
Performance,"aster branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). This is exactly the code on https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html using the dataset downloaded per the instructions. The umap step does not produce the correct result. tsne however produces sensible results.; ```python; import numpy as np; import pandas as pd; import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'). results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results. adata = sc.read_10x_mtx(; 'data/filtered_gene_bc_matrices/hg19/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) . adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`. sc.pl.highest_expr_genes(adata, n_top=20, ). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],; jitter=0.4, multi_panel=True). sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'). adata = adata[adata.obs.n_genes_by_counts < 2500, :]; adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:1247,cache,cache,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,1,['cache'],['cache']
Performance,"aster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1500,cache,cache,1500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,3,['cache'],['cache']
Performance,"at32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; six 1.16.0; sklearn 0.24.2; storemagic NA; tables 3.6.1; texttable 1.6.4; tornado 6.1; traitlets 5.0.5; wcwidth 0.2.5; zipp NA; zmq 22.1.0; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) [GCC 9.3.0]; Linux-4.18.0-240.22.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:4384,bottleneck,bottleneck,4384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['bottleneck'],['bottleneck']
Performance,"ata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as ad; import dask.array as da; import scanpy as sc. # write data to zarr file; rel_zarr_path = 'data/pbmc3k_processed.zarr'; adata = sc.datasets.pbmc3k_processed(); adata.write_zarr(f'./{rel_zarr_path}', chunks=[adata.shape[0], 5]); zarr.consolidate_metadata(f'./{rel_zarr_path}'). # read data from zarr file with X as a dask array; def read_dask(store):; f = zarr.open(store, mode=""r""). def callback(func, elem_name: str, elem, iospec):; if iospec.encoding_type in (; ""dataframe"",; ""csr_matrix"",; ""csc_matrix"",; ""awkward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.from_zarr(elem); else:; return func(elem). adata = ad.experimental.read_dispatched(f, callback=callback). return adata; adata_dask = read_dask(f'./{rel_zarr_path}'). # perform preprocessing; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pp.scale(adata_dask, max_value=10); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[1], line 39; 37 sc.pp.log1p(adata); 38 # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); ---> 39 sc.pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:2836,perform,perform,2836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['perform'],['perform']
Performance,"b ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; batchglm v0.7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:10172,Perform,Performance,10172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"bs(); adata.raw = adata; sc.tl.score_genes(adata, ['0']); ```. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-c835b3ad221d> in <module>; 2 adata = sc.datasets.blobs(); 3 adata.raw = adata; ----> 4 sc.tl.score_genes(adata, ['0']). ~/miniconda3/envs/spols200618/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 174 elif len(gene_list) == 1:; 175 if _adata[:, gene_list].X.ndim == 2:; --> 176 vector = _adata[:, gene_list].X.toarray()[:, 0] # new anndata; 177 else:; 178 vector = _adata[:, gene_list].X # old anndata. AttributeError: 'numpy.ndarray' object has no attribute 'toarray'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.1.2; anndata 0.7.4; backcall 0.2.0; bottleneck 1.3.2; cairo 1.19.1; cffi 1.14.0; cloudpickle 1.3.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.18.1; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; importlib_metadata 1.6.1; ipykernel 5.3.0; ipyparallel 6.3.0; ipython_genutils 0.2.0; jedi 0.17.0; joblib 0.15.1; kiwisolver 1.2.0; legacy_api_wrap 0.0.0; leidenalg 0.8.0; llvmlite 0.32.1; louvain 0.7.0; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.0.1; numba 0.49.1; numexpr 2.7.1; numpy 1.17.5; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.6.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; storemagic NA; tables 3.6.1; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.4; yaml 5.3.1; zipp NA; zmq 19.0.1; -----; IPython 7.15.0; jupyter_client",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1395:1480,bottleneck,bottleneck,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395,1,['bottleneck'],['bottleneck']
Performance,"busercontent.com/26215587/118845227-7ec67180-b8c3-11eb-9bce-a877ef26b47a.png). If i don't use harmony and just ran with `n_pcs = 50`, the result is similar.; ```python; adatax = adata.copy(); sc.pp.neighbors(adatax, n_pcs = 50); sc.tl.umap(adatax, min_dist = .3); sc.pl.umap(adatax, color = 'sample'); ```; ![image](https://user-images.githubusercontent.com/26215587/118845811-01e7c780-b8c4-11eb-8ca9-3ea639d97684.png). What i don't get is, why does the following work then?; ```python; # my current workaround; adata2 = adata.copy(); sc.external.pp.harmony_integrate(adata2, key = 'sample', adjusted_basis='X_pca'); sc.pp.neighbors(adata2, n_pcs = 20); sc.tl.umap(adata2); sc.pl.umap(adata2, color = 'sample'); ```; ![image](https://user-images.githubusercontent.com/26215587/118843486-04492200-b8c2-11eb-8e2b-6aaf59564721.png). #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; Bio 1.78; PIL 8.2.0; adjustText NA; anndata 0.7.5; annoy NA; backcall 0.2.0; brotli NA; cachecontrol 0.12.6; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.5; changeo 1.0.2; chardet 4.0.0; cloudpickle 1.6.0; cycler 0.10.0; cython_runtime NA; dandelion 0.1.2; dask 2021.03.0; dateutil 2.8.1; decorator 5.0.6; descartes NA; distance NA; get_version 2.1; google NA; h5py 2.10.0; harmonypy NA; hdmedians NA; idna 2.10; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.34.0; matplotlib 3.3.4; mizani 0.7.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.20.2; packaging 20.9; palettable 3.3.0; pandas 1.2.4; parso 0.8.2; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotnine 0.7.1; polyleven NA; presto 0.6.2; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pytoml NA; pytz 2021.1; pywt 1.1.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.3; scrublet ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1846:2248,cache,cachecontrol,2248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1846,1,['cache'],['cachecontrol']
Performance,"c.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 422, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. #### Versions. <details>; scanpy 1.9.1; [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>; I don't know how to deal with this problem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1669,load,load,1669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['load'],['load']
Performance,c3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.0.1; tomlkit 0.12.1; toolz 0.12.0; toposort 1.10; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; transformers 4.34.0; truststore 0.8.0; Twisted 22.10.0; types-python-dateutil 2.8.19.14; typing_extensions 4.8.0; typing-utils 0.1.0; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.8.0; umap-learn 0.5.4; uri-template 1.3.0; urllib3 1.26.15; virtualenv 20.24.5; w3lib 2.1.2; watchdog 3.0.0; wcwidth 0.2.8; webcolors 1.13; webencodings 0.5.1; websocket-client 1.6.4; Werkzeug 3.0.0; whatthepatch 1.0.5; wheel 0.38.4; widgetsnbextension 4.0.9; wrapt 1.15.0; wurlitzer 3.0.3; xarray 2023.9.0; xxhash 3.4.1; xyzservices 2023.10.0; yapf 0.24.0; yarl 1.9.2; yte 1.5.1; zict 3.0.0; zipp 3.17.0; zope.interface 6.1; zstandard 0.21.0. ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:9936,throttle,throttler,9936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['throttle'],['throttler']
Performance,"c88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f7411679000-7f741168d000 r--p 00075000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168d000-7f741168e000 r--p 00088000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168e000-7f741168f000 rw-p 00089000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168f000-7f7411691000 rw-p 00000000 00:00 0; 7f7411691000-7f741169a000 r--p 00000000 fd:03 339598995 /home/dgc88/miniconda3/envs/scvi/lib/libtiff.so.6.0.0; 7f741169a000-7f74116e8000 r-xp 00009000 fd:03 339598995 /home/dgc88/miniconda3/envs/scvi/lib/libtiff.so.6.0.0Aborted; ```. #### Versions. <details>. python: /home/dgc88/miniconda3/envs/scvi/bin/python; libpython: /home/dgc88/miniconda3/envs/scvi/lib/libpython3.10.so; pythonhome: /home/dgc88/miniconda3/envs/scvi:/home/dgc88/miniconda3/envs/scvi; version: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; numpy: /home/dgc88/miniconda3/envs/scvi/lib/python3.10/site-packages/numpy; numpy_version: 1.23.5. python versions found:; /home/dgc88/miniconda3/bin/python3; /home/dgc88/miniconda3/bin/python; /home/dgc88/miniconda3/envs/scvi/bin/python. R version 4.2.3 (2023-03-15); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Red Hat Enterprise Linux. Matrix products: default; BLAS: /opt/R/4.2.3/lib64/R/lib/libRblas.so; LAPACK: /opt/R/4.2.3/lib64/R/lib/libRlapack.so. locale:; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C; [9] LC_ADDRESS=C LC_TELEPHONE=C; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C. attached base packages:; [1] stats graphics grDevices utils datasets methods base. other attached packages:; [1] reticulate_1.28. loaded via a namespace (and not attached):; [1] compiler_4.2.3 Matrix_1.5-4 Rcpp_1.0.10 grid_4.2.3 jsonlite_1.8.4; [6] png_0.1-8 lattice_0.21-8. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2479:9538,load,loaded,9538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2479,1,['load'],['loaded']
Performance,"cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2019,cache,cache,2019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,3,['cache'],['cache']
Performance,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3849,race condition,race condition,3849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,5,"['cache', 'race condition']","['cache', 'race condition']"
Performance,cannot load scanpy after updating to 1.4.6 via miniconda,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166:7,load,load,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166,1,['load'],['load']
Performance,"canpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2030,cache,cache,2030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,1,['cache'],['cache']
Performance,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:2595,perform,performing,2595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['performing']
Performance,"canpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; lxml 4.6.3; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 21.0; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.9.0; pylab NA; pynndescent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3944,bottleneck,bottleneck,3944,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['bottleneck'],['bottleneck']
Performance,"cb_kw = {k: v for k, v in kw.items() if k not in NON_COLORBAR_KEYS}; -> 2238 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2239 ; 2240 self.sca(current_ax). [/usr/local/lib/python3.7/dist-packages/matplotlib/colorbar.py](https://localhost:8080/#) in colorbar_factory(cax, mappable, **kwargs); 1683 cb = ColorbarPatch(cax, mappable, **kwargs); 1684 else:; -> 1685 cb = Colorbar(cax, mappable, **kwargs); 1686 ; 1687 cid = mappable.callbacksSM.connect('changed', cb.on_mappable_changed). [/usr/local/lib/python3.7/dist-packages/matplotlib/colorbar.py](https://localhost:8080/#) in __init__(self, ax, mappable, **kw); 1228 kw['alpha'] = mappable.get_alpha(); 1229 ; -> 1230 ColorbarBase.__init__(self, ax, **kw); 1231 ; 1232 def on_mappable_changed(self, mappable):. TypeError: __init__() got an unexpected keyword argument 'location'; ```. #### Versions. <details>. anndata 0.8.0; scanpy 1.9.0. PIL 7.1.2; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:3729,bottleneck,bottleneck,3729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['bottleneck'],['bottleneck']
Performance,"ck (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21074,cache,cache,21074,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"class 'numpy.int32'>`. I've tried numpy versions 1.26.3 and 1.24.4 as well. I've also tried uninstalling and reinstalling scanpy, jupyter notebook, and (consequently) other packages used in the tutorials (igraph, leidenalg, etc.). `numpy.random.test(label='full', verbose=2)` returned True, but noted 1322 passed, 6 skipped in 30.78s. ### Minimal code sample. ```python; sc.tl.leiden(; adata, ; resolution=0.9,; random_state=0,; flavor=""igraph"",; n_iterations=2,; directed=False,; ); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 1423, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32; ```. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; bottleneck 1.3.7; brotli 1.0.9; certifi 2024.02.02; cffi 1.16.0; charset_normalizer 2.0.4; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.11.0; idna 3.4; ipykernel 6.29.3; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.4.0; json5 0.9.25; jsonpointer 2.1; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.0; jupyterlab_server 2.26.0; kiwisolver 1.4.5; legacy_api_wrap NA; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.8.4; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; numba 0.59.1; numexpr 2.8.7; numpy 1.26.4; overrides NA; packaging 23.1; pandas 2.2.1; parso 0.8.4; pickleshare 0.7.5; platformdirs 3.10.0; prometheus_client NA; prompt_toolkit 3.0.42; psutil 5.9.8; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028:2131,bottleneck,bottleneck,2131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028,1,['bottleneck'],['bottleneck']
Performance,"cted components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`; - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path); - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:3801,optimiz,optimization,3801,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['optimiz'],['optimization']
Performance,"ction to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1333,cache,cache,1333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"d_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:2669,cache,cache,2669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['cache'],['cache']
Performance,"dered=None.; --> 273 dtype = CategoricalDtype(categories, ordered); 274 ; 275 return dtype. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered); 158 ; 159 def __init__(self, categories=None, ordered: Ordered = False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:4133,Cache,CachedProperty,4133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['Cache'],['CachedProperty']
Performance,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:3615,optimiz,optimizer,3615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,2,"['Optimiz', 'optimiz']","['Optimizer', 'optimizer']"
Performance,"disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.8; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.8; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.3; pyparsing 2.4.7; pyrsistent NA; pytz 2020.4; rpy2 3.3.6; scanpy 1.6.0; scipy 1.5.4; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; skmisc 0.1.3; sphinxcontrib NA; statsmodels 0.12.1; storemagic NA; tables 3.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2830,bottleneck,bottleneck,2830,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['bottleneck'],['bottleneck']
Performance,"dy been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am not 100% this is a bug, so please correct me if I'm doing something wrong... I am using scanpy with a very large scRNAseq dataset (SEA-AD, the sparse h5ad is ~35GB); When I use the read_h5ad file, even when running in backed mode, I get an out of memory exception. I expect it to be due to the fact that parts of the dataset are still read to memory, even in backed mode.; As you can see in the stack trace below, eventually anndata's `read_sparse()` function is called (in `_io/specs/methods.py`; But this method has the following implementation in the latest version:; ```python; def read_sparse(elem):; return SparseDataset(elem).to_memory(); ```; Thus, loading (part of) the dataset to memory. Like I said, I am not sure whether this is a bug, or supposed to happen. But to me it seems odd that backed mode still loads large portions of the dataset to memory. ### Workaround. For my own project, I got around this issue, by removing the call to `.to_memory()` within the source of anndata. I am not sure whether this breaks any other functionality, but I can use the dataset the way I need it right now. ### Minimal code sample (that we can copy&paste without having any data); (this cannot be run without any data, because the problem is that it fails with a big dataset. I included an aws download command to the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory); ```pytb; Traceback (most recent call last):; Fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:1051,load,loads,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['load'],['loads']
Performance,"e sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.2; pkg_resources NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; tables 3.6.1; tqdm 4.62.1; typing_extensions NA; yaml 5.4.1; zipp NA; -----; Python 3.7.11 (default, Jul 27 2021, 07:03:16) [Clang 10.0.0 ]; Darwin-20.4.0-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-09-15 10:41. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:1708,bottleneck,bottleneck,1708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['bottleneck'],['bottleneck']
Performance,"e to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3009,race condition,race condition,3009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"e ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/anndata/_core/anndata.py:1808, in AnnData.concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1799 pat = rf""-({'|'.join(batch_categories)})$""; 1800 out.var = merge_dataframes(; 1801 [a.var for a in all_adatas],; 1802 out.var_names,; 1803 partial(merge_outer, batch_keys=batch_categories, merge=merge_same),; 1804 ); 1805 out.var = out.var.iloc[; 1806 :,; 1807 (; -> 1808 out.var.columns.str.extract(pat, expand=False); 1809 .fillna(""""); 1810 .argsort(kind=""stable""); 1811 ),; 1812 ]; 1814 return out. File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/accessor.py:224, in CachedAccessor.__get__(self, obj, cls); 221 if obj is None:; 222 # we're accessing the attribute of the class, i.e., Dataset.geo; 223 return self._accessor; --> 224 accessor_obj = self._accessor(obj); 225 # Replace the property with the accessor object. Inspired by:; 226 # https://www.pydanny.com/cached-property.html; 227 # We need to use object.__setattr__ because we overwrite __setattr__ on; 228 # NDFrame; 229 object.__setattr__(obj, self._name, accessor_obj). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:181, in StringMethods.__init__(self, data); 178 def __init__(self, data) -> None:; 179 from pandas.core.arrays.string_ import StringDtype; --> 181 self._inferred_dtype = self._validate(data); 182 self._is_categorical = is_categorical_dtype(data.dtype); 183 self._is_string = isinstance(data.dtype, StringDtype). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:235, in StringMethods._validate(data); 232 inferred_dtype = lib.infer_dtype(values, skipna=True); 234 if inferred_dtype not in allowed_types:; --> 235 raise AttributeError(""Can only use .str accessor with string values!""); 236 return inferred_dtype. AttributeError: Can only use .str accessor with string values!; ```; I did try to eleminate e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2474:2174,cache,cached-property,2174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2474,1,['cache'],['cached-property']
Performance,"e-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:1889,cache,cache,1889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['cache'],['cache']
Performance,"e_total; ' The following highly-expressed genes are not considered during '; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = Index(['0', '1', '2'], dtype='object'); key = dask.array<invert, shape=(3,), dtype=bool, chunksize=(3,), chunktype=numpy.ndarray>. def __getitem__(self, key):; """"""; Override numpy.ndarray's __getitem__ method to work as desired.; ; This function adds lists and Series as valid boolean indexers; (ndarrays only supports ndarray with dtype=bool).; ; If resulting ndim != 1, plain ndarray is returned instead of; corresponding `Index` subclass.; ; """"""; getitem = self._data.__getitem__; ; if is_integer(key) or is_float(key):; # GH#44051 exclude bool, which would return a 2d ndarray; key = com.cast_scalar_indexer(key, warn_float=True); return getitem(key); ; if isinstance(key, slice):; # This case is separated from the conditional above to avoid; # pessimization com.is_bool_indexer and ndim checks.; result = getitem(key); # Going through simple_new for performance.; return type(self)._simple_new(result, name=self._name); ; if com.is_bool_indexer(key):; # if we have list[bools, length=1e5] then doing this check+convert; # takes 166 µs + 2.1 ms and cuts the ndarray.__getitem__; # time below from 3.8 ms to 496 µs; # if we already have ndarray[bool], the overhead is 1.4 µs or .25%; key = np.asarray(key, dtype=bool); ; > result = getitem(key); E IndexError: too many indices for array: array is 1-dimensional, but 3 were indexed. ../../venvs/single-cell/lib/python3.8/site-packages/pandas/core/indexes/base.py:5055: IndexError; ```. #### Versions. <details>. -----; anndata 0.9.0rc2.dev18+g7771f6ee; scanpy 1.10.0.dev50+g3e3427d0; -----; PIL 9.1.1; asciitree NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.2.1; cycler 0.10.0; cython_runtime NA; dask 2023.3.2; dateutil 2.8.2; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.17.3; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; jinja2 3.1.2; joblib 1.1.0; kiwisolve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2465:3331,perform,performance,3331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2465,1,['perform'],['performance']
Performance,"eat tutorials. Recently, I've been encountering an error after I import ScanPy that causes the kernel I'm using to die and restart. After this happens, I can no longer call sc.- commands and have to import the library again. And then the kernel quickly dies again. . ### Minimal code sample. ```python; import numpy as np. import pandas as pd. import scanpy as sc. sc.settings.verbosity = 3 . sc.logging.print_header(). sc.settings.set_figure_params(dpi=80, facecolor='white'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); /var/folders/jt/m9v9k7dd4f31m5pl8v_g8m3m0000gn/T/ipykernel_3816/1026803476.py in <module>; ----> 1 sc.settings.set_figure_params(dpi=80, facecolor='white'). NameError: name 'sc' is not defined; ```. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.4; -----; PIL 9.2.0; PyObjCTools NA; appnope 0.1.2; astunparse 1.6.3; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.7; entrypoints 0.4; fsspec 2022.7.1; gmpy2 2.1.2; google NA; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.1; ipykernel 6.15.2; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; mpl_toolkits NA; mpmath 1.2.1; natsort 7.1.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.1; numexpr 2.8.4; numpy 1.21.6; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pyarrow 13.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675:1367,bottleneck,bottleneck,1367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675,1,['bottleneck'],['bottleneck']
Performance,"eatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards favoring the style of `seurat_v3_paper`. Better naming suggestions more than welcome. **Examples**; - Good when no `batch_key` used:; ```py; import numpy as np; import pandas as pd; import scanpy as sc. # load exactly the data from seurat tutorial. pbmc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). print(pbmc). # default settings in scanpy are the same as for Seurat; sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3""). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat = pd.read_csv(""scanpy/scanpy/scanpy/tests/_scripts/seurat_hvg_v3.csv.gz"", index_col=0). # This is used to order and rank the hvg when no batch information used; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance.standardized""],; pbmc.var[""variances_norm""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""mean""],; pbmc.var[""means""],; ). # Another quantity reported by both; assert ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:3109,load,load,3109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['load'],['load']
Performance,"ectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1662,cache,cache,1662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"er/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:1845,cache,cached,1845,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,"ers, fill_value, copy, allow_dups); 4887 fill_value=fill_value,; 4888 allow_dups=allow_dups,; -> 4889 copy=copy,; 4890 ); 4891 # If we've made a copy once, no need to make another one. ~/.local/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 668 # some axes don't allow reindexing with dups; 669 if not allow_dups:; --> 670 self.axes[axis]._validate_can_reindex(indexer); 671 ; 672 if axis >= self.ndim:. ~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in _validate_can_reindex(self, indexer); 3783 # trying to reindex on an axis with duplicates; 3784 if not self._index_as_unique and len(indexer):; -> 3785 raise ValueError(""cannot reindex from a duplicate axis""); 3786 ; 3787 def reindex(. ValueError: cannot reindex from a duplicate axis. ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.9.1; -----; PIL 7.2.0; backcall 0.1.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.3.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2.13.0; dateutil 2.8.1; decorator 4.4.2; google NA; h5py 2.10.0; igraph 0.9.7; ipykernel 5.1.4; ipython_genutils 0.2.0; jedi 0.15.2; joblib 0.17.0; kiwisolver 1.1.0; leidenalg 0.8.8; llvmlite 0.39.1; louvain 0.7.0; matplotlib 3.5.3; mpl_toolkits NA; natsort 7.0.1; nbinom_ufunc NA; numba 0.56.2; numexpr 2.7.3; numpy 1.21.6; packaging 20.3; pandas 1.3.4; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.4; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.6; pyteomics NA; pytz 2019.3; scipy 1.7.1; session_info 1.0.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tblib 1.6.0; texttable 1.6.3; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth NA; yaml 5.3.1; zipp NA; zmq 17.1.2; -----; IPython 7.13.0; jupyter_client 6.1.2; ju",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:5731,bottleneck,bottleneck,5731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,1,['bottleneck'],['bottleneck']
Performance,"ession,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcore 0.2.0; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; frozendict 2.3.8; h5py 3.9.0; ikarus NA; importlib_resources NA; ipykernel 6.24.0; ipython_genutils 0.2.0; jedi 0.18.2; jinja2 3.1.2; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; packaging 23.1; pandas 2.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2652,cache,cache,2652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,"from the doc of sc.tl.tsne, I knew it was implemented by scikit, but I can't get the same result between sc.tl.tsne and scikit.TSNE，my test code is below; ```; #!/usr/bin/env python3; # -*- coding: utf-8 -*-; """""". from sklearn import datasets; import scanpy as sc; import numpy as np; import random; import matplotlib.pyplot as plt; from sklearn.manifold import TSNE; np.random.seed(1); random.seed(1). iris = datasets.load_iris(); X = iris.data; label=iris.target. adata=sc.AnnData(X); adata.obs[""celltype""]=label.astype(int).astype(str); sc.tl.tsne(adata,random_state=0,use_fast_tsne=False); axis=sc.pl.tsne(adata,color=[""celltype""],size=100,show=False); sc_tsne=adata.obsm[""X_tsne""]; #print(ax). print(np.min(sc_tsne[:,0])); print(np.max(sc_tsne[:,0])); print(np.min(sc_tsne[:,1])); print(np.max(sc_tsne[:,1])); print(""====================""). # import pickle; # #with open(); # file = open('/Users/xiaokang/Desktop/data/tsne.pkl', 'rb'); # tsne2=pickle.load(file). target=label; tsne = TSNE(learning_rate=1000,init='random', random_state=0); X_transformed = tsne.fit_transform(X); fig=plt.figure(); for label in np.unique(target):; plt.scatter(X_transformed[label==target,0], X_transformed[label==target,1],label=label); plt.legend(loc=""upper left""); plt.show(); #print(X_transformed); print(np.min(X_transformed[:,0])); print(np.max(X_transformed[:,0])); print(np.min(X_transformed[:,1])); print(np.max(X_transformed[:,1])). print(""==================""); params_sklearn = dict(; perplexity=30,; random_state=0,; verbose=False,; early_exaggeration=12,; learning_rate=1000,; ); from sklearn.manifold import TSNE; # unfortunately, sklearn does not allow to set a minimum number; # of iterations for barnes-hut tSNE; tsne3 = TSNE(**params_sklearn); X_transformed=tsne3.fit_transform(X); print(np.min(X_transformed[:,0])); print(np.max(X_transformed[:,0])); print(np.min(X_transformed[:,1])); print(np.max(X_transformed[:,1])); ```; I get the result; ![image](https://user-images.githubusercontent.com/5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1759:956,load,load,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759,1,['load'],['load']
Performance,"g intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1496,load,loading,1496,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"g intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1745,load,loading,1745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no error. While running: `adata_orig.X.sum(1)` gives me the error as : ; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-23-951a31c71c45> in <module>; ----> 1 adata_orig.X.sum(1). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```. PS: I downloaded this` .h5ad` file from a published research paper to perform some analysis over it, would be happy to provide you the link to same if required. . Also this is the environment that I am working in:; `scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.1 scipy==1.6.2 pandas==1.2.4 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.6`. Hello @LuckyMD, tagging you for just in case you might be knowing the resolution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:3348,perform,perform,3348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['perform'],['perform']
Performance,"gorical; ... storing 'feature_types-180905-3' as categorical; ... storing 'feature_types-180905-4' as categorical; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-72-19c7ca58c3a2> in <module>; ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2089,load,load,2089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['load'],['load']
Performance,"he ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata = AnnData(random(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:1027,queue,queue,1027,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['queue'],['queue']
Performance,"he_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2; ```. Versions of all packages:. `scanpy==1.4.5.pos",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:2781,cache,cache,2781,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/744:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/744,2,['perform'],['perform']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/744:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/744,1,['perform'],['perform']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> pbmc; AnnData object with n_obs × n_vars = 700 × 765; **obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'**; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_. i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/745:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/745,1,['perform'],['perform']
Performance,"https://stackoverflow.com/questions/51593527/oserror-unable-to-open-file-unable-to-open-file. /edit: that was not it. I’ll set up a Python 3.7 venv and look if I can reproduce it locally. /edit: nope, cleaning caches …",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1113:210,cache,caches,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113,1,['cache'],['caches']
Performance,"ib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcore 0.2.0; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; frozendict 2.3.8; h5py 3.9.0; ikarus NA; importlib_resources NA; ipykernel 6.24.0; ipython_genutils 0.2.0; jedi 0.18.2; jinja2 3.1.2; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; packaging 23.1; pandas 2.0.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.1; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pydev_ipython",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2888,cache,cache,2888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,iling env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 con,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:29787,load,loader,29787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,"in(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453580_P5_exp.txt.gz').T; lung_P8 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453583_P8_exp.txt.gz').T; lung_P16 = sc.read_text('D:\课题\博士课题\单细胞",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2908,cache,cache,2908,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"in(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453580_P5_exp.txt.gz').T; lung_P8 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453583_P8_exp.txt.gz').T; lung_P16 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453591_P16_exp.txt.gz').T; lung_P24 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453599_P24_exp.txt.gz').T; lung_P28 = sc.read_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:3064,cache,cache,3064,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"index=new_index). ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 322 @wraps(func); 323 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 324 return func(*args, **kwargs); 325 ; 326 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4770 kwargs.pop(""axis"", None); 4771 kwargs.pop(""labels"", None); -> 4772 return super().reindex(**kwargs); 4773 ; 4774 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""]). ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4817 # perform the reindex on the axes; 4818 return self._reindex_axes(; -> 4819 axes, level, limit, tolerance, method, fill_value, copy; 4820 ).__finalize__(self, method=""reindex""); 4821 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4596 if index is not None:; 4597 frame = frame._reindex_index(; -> 4598 index, method, copy, level, fill_value, limit, tolerance; 4599 ); 4600 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4618 copy=copy,; 4619 fill_value=fill_value,; -> 4620 allow_dups=False,; 4621 ); 4622 . ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4887 fill_value=fill_value,; 4888 allow_dups=allow_dups,; -> 4889 copy=copy,; 4890 ); 4891 # If we've made a copy once, no need to make another one. ~/.local/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:3921,perform,perform,3921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,1,['perform'],['perform']
Performance,"input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key +",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2914,load,load,2914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['load'],['load']
Performance,"irmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am following the tutorial but everytime I try to run a violin plot the kernel crashes, this doesnt happen with other seaborn graphs. I have tried updating packeges, changing environment, etc, etc & nothing works any help would be great !!. ![image](https://github.com/scverse/scanpy/assets/127498480/b5cc12b1-00af-4919-abd0-7ea99b72cade). ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(; 'data/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`; adata; sc.pl.highest_expr_genes(adata, n_top=20, ); sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'); sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0-4, multi_panel=True); ```. ### Error output. ```pytb; Kernel Restarting; The kernel for Tests/scanpytutorial/Untitled.ipynb appears to have died. It will restart automatically.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840:1150,cache,cache,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840,2,['cache'],['cache']
Performance,kernel 6.29.3 pypi_0 pypi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0 ; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0 ; jupyter_core 5.5.0 py311haa95532_0 ; jupyter_events 0.8.0 py311haa95532_0 ; jupyter_server 2.10.0 py311haa95532_0 ; jupyter_server_terminals 0.4.4 py311haa95532_1 ; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.25.1 py311haa95532_0 ; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0 ; libsodium 1.0.18 h62dcd97_0 ; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2 ; m2w64-bzip2 1.0.6 6 ; m2w64-expat 2.1.1 2 ; m2w64-fftw 3.3.4 6 ; m2w64-flac 1.3.1 3 ; m2w64-gcc-libgfortran 5.3.0 6 ; m2w64-gcc-libs 5.3.0 7 ; m2w64-gcc-libs-core 5.3.0 7 ; m2w64-gettext 0.19.7 2 ; m2w64-gmp 6.1.0 2 ; m2w64-gsl 2.1 2 ; m2w64-libiconv 1.14 6 ; m2w64-libjpeg-turbo 1.4.2 3 ; m2w64-libogg 1.3.2 3 ; m2w64-libpng 1.6.21 2 ; m2w64-libsndfile 1.0.26 2 ; m2w64-libsodium 1.0.10 2 ; m2w64-libtiff 4.0.6 2 ; m2w64-libvorbis 1.3.5 2 ; m2w64-libwinpthread-git 5.0.0.4634.697f757 2 ; m2w64-libxml2 2.9.3 4 ; m2w64-mpfr 3.1.4 4 ; m2w64-openblas 0.2.19 1 ; m2w64-pcre 8.38 2 ; m2w64-speex 1.2rc2 3 ; m2w64-speexdsp 1.2rc3 3 ; m2w64-tcl 8.6.5 3 ; m2w64-tk 8.6.5 3 ; m2w64-tktable 2.10 5 ; m2w64-wineditline 2.101 5 ; m2w64-xz 5.2.2 2 ; m2w64-zeromq 4.1.4 2 ; m2w64-zlib 1.2.8 10 ; markupsafe 2.1.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:6920,load,loader,6920,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['load'],['loader']
Performance,"l, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 632 ; 633 if isinstance(asheetname, str):; --> 634 sheet = self.get_sheet_by_name(asheetname); 635 else: # assume an integer if not a string; 636 sheet = self.get_sheet_by_index(asheetname). ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py in get_sheet_by_name(self, name); 543 ; 544 def get_sheet_by_name(self, name: str):; --> 545 self.raise_if_bad_sheet_by_name(name); 546 return self.book[name]; 547 . ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in raise_if_bad_sheet_by_name(self, name); 568 def raise_if_bad_sheet_by_name(self, name: str) -> None:; 569 if name not in self.sheet_names:; --> 570 raise ValueError(f""Worksheet named '{name}' not found""); 571 ; 572 def parse(. ValueError: Worksheet named 'expression' not found]; ```. #### '1.9.1'. <details>. [-----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; PyObjCTools NA; appnope 0.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.0; llvmlite 0.38.0; louvain 0.8.0; lxml 4.9.1; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; mkl 2.4.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.5; openpyxl 3.0.10; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pytz 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:3886,bottleneck,bottleneck,3886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,1,['bottleneck'],['bottleneck']
Performance,"le writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scipy 1.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6607,bottleneck,bottleneck,6607,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['bottleneck'],['bottleneck']
Performance,"lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1518, in _format_strings; return list(self.get_result_as_array()); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1482, in get_result_as_array; formatted_values = format_values_with(float_format); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1456, in format_values_with; values = format_with_na_rep(values, formatter, na_rep); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1427, in format_with_na_rep; [; File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1428, in <listcomp>; formatter(val) if not m else na_rep; ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.2; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; cffi 1.14.6; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; encodings NA; genericpath NA; get_version 3.5; h5py 3.4.0; joblib 1.0.1; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; llvmlite 0.37.0; matplotlib 3.4.3; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.0; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.3; pkg_resources NA; posixpath NA; pycparser 2.20; pyexpat NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 1.0; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tables 3.6.1; -----; Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) [GCC 9.4.0]; Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.31; 24 logical CPU cores, x86_64; -----; Session information updated at 2021-10-01 14:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008:4425,concurren,concurrent,4425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008,1,['concurren'],['concurrent']
Performance,limit number of loadings in pl.pca_loadings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2059:16,load,loadings,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2059,1,['load'],['loadings']
Performance,"m/h5py/h5py/issues/1732. Also, I've encoutered this bug when coming up with example (seems unrelated):; ```python; import scanpy as sc. adata = sc.datasets.paul15(). sc.read('data/paul15/paul15.h5'); ```; The last line raises:. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-24-3d2f3a02bf09> in <module>; ----> 1 sc.read('data/paul15/paul15.h5'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank 1.0.0; cffi 1.14.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jax 0.2.5; jaxlib 0.1.56; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; lapack NA; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:1959,cache,cache,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['cache'],['cache']
Performance,"most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applica",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:1475,cache,cache,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,3,['cache'],['cache']
Performance,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587:2160,load,load,2160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587,1,['load'],['load']
Performance,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/153:1428,cache,cache,1428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153,2,['cache'],['cache']
Performance,"n(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2752,cache,cache,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"nd:; 419 self.add_legend_data(ax, func, common_kws, semantic_kws=semantic_kws); --> 420 handles, _ = ax.get_legend_handles_labels(); 421 if handles:; 422 ax.legend(title=self.legend_title). AttributeError: 'NoneType' object has no attribute 'get_legend_handles_labels'; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.6.0; aiohttp 3.8.6; aioitertools 0.11.0; aiosignal 1.3.1; alabaster 0.7.13; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.0; anaconda-cloud-auth 0.1.4; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anndata 0.10.1; anndata 0.10.0rc1; annoy 1.17.2; anyio 4.0.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:3981,cache,cache,3981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['cache'],['cache']
Performance,"nd; 272 # ordered=None.; --> 273 dtype = CategoricalDtype(categories, ordered); 274 ; 275 return dtype. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered); 158 ; 159 def __init__(self, categories=None, ordered: Ordered = False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for Mu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:4436,Cache,CachedProperty,4436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['Cache'],['CachedProperty']
Performance,"nda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3456 if self.columns.nlevels > 1:; 3457 return self._getitem_multilevel(key); -> 3458 indexer = self.columns.get_loc(key); 3459 if is_integer(indexer):; 3460 indexer = [indexer]. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3361 return self",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:1470,cache,cache,1470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['cache'],['cache']
Performance,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:2947,cache,cache,2947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['cache'],['cache']
Performance,"ned?. In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1308,cache,cachedir,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['cache'],['cachedir']
Performance,"nes, method=method). File ""/home/smith/anaconda3/envs/scanpy18/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 574, in rank_genes_groups; raise ValueError(. ValueError: reference = (0, 0) needs to be one of groupby = [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 14), (0, 15), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15)].; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.1; -----; 2f7ece400a652629565c523b34ee61b04afa385c NA; ACWS_filterCells NA; PIL 8.3.1; PyQt5 NA; absl NA; anndata 0.7.6; appdirs 1.4.4; astunparse 1.6.3; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.0; cloudpickle 1.6.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.0; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; encodings NA; et_xmlfile 1.0.1; flatbuffers NA; fsspec 2021.07.0; gast NA; genericpath NA; google NA; gprofiler 1.0.0; h5py 3.3.0; idna 3.1; igraph 0.9.6; imagecodecs 2021.6.8; imageio 2.9.0; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.36.0; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.5; ntpath NA; numba 0.53.1; numexpr 2.7.3; numpy 1.21.1; opcode NA; openpyxl 3.0.7; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pooch v1.4.0; posixpath NA; prompt_toolkit 3.0.19; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pydoc_data NA; pyexpat NA; pygments 2.9.0; pynndescent 0.5.4; pyparsing 2.4.7; pytz 2021.1; requests 2.26.0; scanpy 1.8.1; scipy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:4160,concurren,concurrent,4160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['concurren'],['concurrent']
Performance,"nes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 ); 66 ; ---> 67 df['means'], df['variances'] = _get_mean_var(X); 68 ; 69 if batch_key is None:. ~/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X, axis); 6 def _get_mean_var(X, *, axis=0):; 7 if sparse.issparse(X):; ----> 8 mean, var = sparse_mean_variance_axis(X, axis=axis); 9 else:; 10 mean = np.mean(X, axis=axis, dtype=np.float64). ~/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py in sparse_mean_variance_axis(mtx, axis); 40 ); 41 else:; ---> 42 return sparse_mean_var_minor_axis(mtx.data, mtx.indices, *shape, np.float64); 43 ; 44 . SystemError: CPUDispatcher(<function sparse_mean_var_minor_axis at 0x7fcea12550e0>) returned a result with an error set; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 6.2.0; absl NA; attr 19.2.0; backcall 0.1.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.2.1; cffi 1.12.3; cloudpickle 1.2.2; colorama 0.4.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.0; dask 2.5.2; dateutil 2.8.0; decorator 4.4.0; deprecate 0.3.0; fsspec 2021.08.1; google NA; h5py 2.10.0; ipykernel 5.1.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.15.1; joblib 0.13.2; kiwisolver 1.1.0; llvmlite 0.29.0; matplotlib 3.4.3; more_itertools NA; mpl_toolkits NA; natsort 7.0.1; nbinom_ufunc NA; numba 0.45.1; numexpr 2.7.0; numpy 1.21.2; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.2; parso 0.5.1; pexpect 4.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 2.0.10; psutil 5.6.3; ptyprocess 0.6.0; pycparser 2.19; pygments 2.10.0; pyparsing 2.4.2; pyro 1.7.0; pytorch_lightning 1.3.8; pytz 2019.3; rich NA; scipy 1.7.1; scvi 0.13.0; seaborn 0.9.0; setuptools 41.4.0; setuptools_scm NA; simplejson 3.17.2; six 1.12.0; sklearn 0.24.2; skmisc 0.1.4; sphinxcontrib NA; statsmodels 0.10.1; storemagic NA; tables 3.5.2; tblib 1.4.0; tensorboard 2.6.0; th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1995:2540,bottleneck,bottleneck,2540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1995,1,['bottleneck'],['bottleneck']
Performance,"ngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); <ipython-input-20-26443e0aed95> in <module>; ----> 1 rnaseq1 = sc.read_10x_mtx(""GSE145328_RAW""). ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 479 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 480 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 481 adata = read(; 482 str(path),; 483 var_names=var_names,. ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 560 else:; 561 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 562 adata.var['feature_types'] = genes[2].values; 563 adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[; 564 0. ~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3022 if self.columns.nlevels > 1:; 3023 return self._getitem_multilevel(key); -> 3024 indexer = self.columns.get_loc(key); 3025 if is_integer(indexer):; 3026 indexer = [indexer]. ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3080 return self._engine.get_loc(casted_key); 3081 except KeyError as err:; -> 3082 raise KeyError(key) from err; 3083 ; 3084 if tolerance is not None:. KeyError: 2; ```. #### Versions; scanpy==1.8.0 anndata==0.7.6 umap==0.5.1 numpy==1.19.2 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.2. <details>. [Paste the output of scanpy.logging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916:1675,cache,cache,1675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916,1,['cache'],['cache']
Performance,"ns, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(); 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(); 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(); 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .core.anndata import AnnData, Raw; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(); 46 LayersBase, Layers; 47 ); ---> 48 from .. import h5py; 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView; 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 4 from typing import Optional, Union, KeysView, NamedTuple; 5 ; ----> 6 import h5py; 7 import numpy as np; 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900:2256,load,load,2256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900,1,['load'],['load']
Performance,"nv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, rele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1573,load,loading,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"nvs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:1373,load,load,1373,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,1,['load'],['load']
Performance,"o my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipywidgets 8.1.2; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.12.5; jupyterlab_server 2.25.3; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.40.0; louvain 0.8.0; lz4 4.3.3; markupsafe 2.1.5; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nt NA; numba 0.57.1; numexpr 2.8.7; numpy 1.23.0; overrides NA; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pickleshare 0.7.5; pkg_resou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:1570,bottleneck,bottleneck,1570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['bottleneck'],['bottleneck']
Performance,"o the update of the anndata package to 0.10.4 (January 14, 2024); (?Error reading the file *features.tsv.gz*). The launch was carried out on the following data: ; https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM5733023. You can also download files using my google drive:; https://drive.google.com/drive/folders/1p6ilbsJX_cYZb4HG0OSbLHAwQObqmncW?usp=sharing. # **My actions**:. 1) I have installed the latest version of `scanpy=1.9.6` using conda:; ```console; $ conda --version; conda 23.10.0; $ conda install scanpy; # Channels:; # - conda-forge; # - bioconda; # - defaults; # Platform: linux-64. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.4 pyhd8ed1ab_0 conda-forge; array-api-compat 1.4 pyhd8ed1ab_0 conda-forge; brotli 1.1.0 hd590300_1 conda-forge; brotli-bin 1.1.0 hd590300_1 conda-forge; bzip2 1.0.8 hd590300_5 conda-forge; c-ares 1.25.0 hd590300_0 conda-forge; ca-certificates 2023.11.17 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.11.17 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; contourpy 1.2.0 py311h9547e67_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.0 pyhd8ed1ab_2 conda-forge; fonttools 4.47.2 py311h459d7ec_0 conda-forge; freetype 2.12.1 h267a509_2 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forge; h5py 3.10.0 nompi_py311hebc2b07_101 conda-forge; hdf5 1.14.3 nompi_h4f84152_100 conda-forge; icu 73.2 h59595ed_0 conda-forge; joblib 1.3.2 pyhd8ed1ab_0 conda-forge; keyutils 1.6.1 h166bdaf_0 conda-forge; kiwisolver 1.4.5 py311h9547e67_1 conda-forge; krb5 1.21.2 h659d440_0 conda-forge; lcms2 2.16 hb7c19ff_0 conda-forge; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; lerc 4.0.0 h27087fc_0 conda-forge; libaec 1.1.2 h59595ed_1 conda-forge; libblas 3.9.0 20_linux64_openblas conda-forge; libbrotlicommon 1.1.0 hd590300_1 conda-forge; libbrotlidec 1.1.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:1345,cache,cached-property,1345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cached-property']
Performance,"oin(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2596,cache,cache,2596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"ort anndata as ad; from scipy.sparse import csr_matrix; print(ad.__version__). mtx = np.array([[1.2,2.1,3.9],[2.01,3.99,4.23],[4.21,5.12,6.87],[0,20.12,100.96]]). adata = sc.AnnData(mtx); adata.raw = adata; print(adata); print(adata.X). sc.pp.normalize_total(adata,target_sum=1e4); sc.pp.log1p(adata); print(adata.X) . print(adata.raw.X[0:10,0:10]); ```; I get following result; <img width=""525"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/59059267/5eec641b-3542-471b-be22-51ef8e8f31a8"">. It sems strange for me? Shouldn't I save raw data for float data? Could you give some suggestions? My environment is; <img width=""647"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/59059267/2267345f-1a2b-4708-90f9-d1892adfb42f"">. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.1; scanpy 1.9.5; -----; CoreFoundation NA; Foundation NA; PIL 9.4.0; PyObjCTools NA; anyio NA; appnope 0.1.2; asttokens NA; attr 22.1.0; babel 2.11.0; backcall 0.2.0; bottleneck 1.3.5; brotli NA; certifi 2023.07.22; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.6; entrypoints 0.4; executing 0.8.3; fastjsonschema NA; gmpy2 2.1.2; h5py 3.9.0; idna 3.4; igraph 0.10.8; ipykernel 6.25.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.40.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numexpr 2.8.4; numpy 1.24.3; numpydoc 1.5.0; objc 10.0; packaging 23.1; pandas 2.0.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prometheus_client NA; prompt_toolkit 3.0.36; psutil 5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2748:2062,bottleneck,bottleneck,2062,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2748,1,['bottleneck'],['bottleneck']
Performance,"own installation of scanpy but it seems like a version issue with networkx2.6.2 (which is the latest of this date).; Minimal working example:; ```python; import scanpy as sc; paul15 = sc.datasets.paul15(); sc.pp.recipe_zheng17(paul15); sc.pp.neighbors(paul15, n_neighbors=4, n_pcs=20); sc.tl.paga(paul15, groups='paul15_clusters'); sc.pl.paga(paul15, labels=None, color='paul15_clusters'). gexf=sc.pl.paga(paul15, labels=None, color='paul15_clusters', export_to_gexf=True). ```. ```pytb. File ""anaconda3\lib\site-packages\scanpy\plotting\_tools\paga.py"", line 860, in _paga_graph; nx_g_solid.node[count]['label'] = str(node_labels[count]). AttributeError: 'Graph' object has no attribute 'node'```. ```; #### Versions; scanpy 1.8.1; networkx 2.6.2; matplotlib 3.4.3. <details>. sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.1; -----; PIL 8.0.1; PyQt5 NA; anndata 0.7.6; autoreload NA; backcall 0.2.0; bottleneck 1.3.2; bs4 4.9.3; cairo 1.20.1; cffi 1.14.3; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2.30.0; dateutil 2.8.1; decorator 4.4.2; h5py 2.10.0; html5lib 1.1; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.1; joblib 1.0.1; kiwisolver 1.3.0; leidenalg 0.8.7; llvmlite 0.34.0; lxml 4.6.1; matplotlib 3.4.3; mpl_toolkits NA; natsort 7.1.1; networkx 2.6.2; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.1; numpy 1.20.3; packaging 20.4; pandas 1.3.2; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pyarrow 0.16.0; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.2; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.8.1; scipy 1.5.2; sinfo 0.3.1; sip NA; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; sphinxcontrib NA; spyder 4.1.5; spyder_kernels 1.9.4; spydercustomize NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.4; tlz 0.11",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1997:1082,bottleneck,bottleneck,1082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1997,1,['bottleneck'],['bottleneck']
Performance,"pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.8; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.8; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.3; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2544,load,loading,2544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,2,['load'],"['loaded', 'loading']"
Performance,"pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.4; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 2.1.5; notebook 6.0.3; -----; Python 3.8.3 (default, Jul 2 2020, 16:21:59) [GCC 7.3.0]; Linux-3.10.0-1160.76.1.el7.x86_64-x86_64-with-glibc2.10; 52 logical CPU cores, x86_64; -----; </details>. The following is my package version list. <details>. -----; anndata 0.8.0; scanpy 1.7.0; sinfo 0.3.4; -----; OpenSSL 21.0.0; PIL 8.4.0; absl NA; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bioservices 1.9.0; bottleneck 1.3.2; brotli NA; bs4 4.10.0; bson NA; cairo NA; cattr NA; certifi 2021.10.08; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; colorlog NA; cryptography 3.4.8; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; deprecate 0.3.1; dill 0.3.5.1; docutils 0.17.1; dunamai 1.11.1; easydev 0.12.0; entrypoints 0.3; fsspec 2021.08.1; get_version 3.5.4; google NA; gridfs NA; gseapy 0.10.8; h5py 3.6.0; html5lib 1.1; idna 3.2; igraph 0.9.11; importlib_metadata NA; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; itsdangerous 2.0.1; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.9; llvmlite 0.36.0; lxml 4.6.3; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:4581,bottleneck,bottleneck,4581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['bottleneck'],['bottleneck']
Performance,"pmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:980,cache,cache,980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,2,['cache'],['cache']
Performance,"ps://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:2925,cache,cache,2925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,put:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1315,load,loading,1315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,pydoc`. Sphinx will be pinned until this is solved (which is when this issue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1051,load,loading,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3079 try:; -> 3080 return self._engine.get_loc(casted_key); 3081 except KeyError as err:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); <ipython-input-20-26443e0aed95> in <module>; ----> 1 rnaseq1 = sc.read_10x_mtx(""GSE145328_RAW""). ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 479 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 480 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 481 adata = read(; 482 str(path),; 483 var_names=var_names,. ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 560 else:; 561 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 562 adata.var['feature_types'] = genes[2].values; 563 adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[; 564 0. ~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3022 if self.columns.nlevels > 1:; 3023 return self._getitem_multilevel(key); -> 3024 indexer = self.columns.get_loc(key); 3025 if is_integer(indexer):; 3026 indexer = [indexer]. ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3080 return self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916:1318,cache,cache,1318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916,1,['cache'],['cache']
Performance,"qual(adata1.obsm['X_pca'], adata2.obsm['X_pca'])); print(adata1.obsm['X_pca'] - adata2.obsm['X_pca']); ```. ```pytb; env: PYTHONHASHSEED=0; False; [[-4.7683716e-07 -3.5762787e-07 -1.1920929e-07 ... -2.7803779e-03; -5.5277348e-04 8.6665154e-05]; [ 9.5367432e-07 4.7683716e-07 6.5565109e-07 ... 3.1501055e-03; -3.6475658e-03 -1.0871887e-04]; [ 1.6689301e-06 0.0000000e+00 -1.1920929e-07 ... 1.7441511e-03; 6.9665909e-04 4.2915344e-04]; ...; [-5.9604645e-07 4.7683716e-07 -3.5762787e-07 ... 1.5980005e-04; -1.0134950e-03 -1.2260675e-04]; [-2.0861626e-07 1.4305115e-06 -8.3446503e-07 ... -7.1705580e-03; -2.4490356e-03 2.4688244e-04]; [ 5.3644180e-07 -5.9604645e-07 -6.8545341e-07 ... -1.5603602e-03; 7.9727173e-04 8.6218119e-04]]; ```. #### Versions. I am running scanpy in this singularity container: https://singularity-hub.org/collections/5095. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; constants NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; highs_wrapper NA; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749:1843,bottleneck,bottleneck,1843,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749,1,['bottleneck'],['bottleneck']
Performance,"r non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1500,cache,cache,1500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"r/miniconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/readwrite.py"", line 10, in <module>; import tables. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/__init__.py"", line 62, in <module>; from .file import File, open_file, copy_file. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/file.py"", line 33, in <module>; from . import hdf5extension. ImportError: /home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/hdf5extension.cpython-37m-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_direct; ```; Removing anndata2ri with `conda remove anndata2ri` fixes the issue. Installing with `pip install anndata2ri` prevents the issue. #### Versions; After fixing the issue.; <details>; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 9.0.1; PyQt5 NA; anndata 0.7.8; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.10.0; fsspec 2022.01.0; get_version 3.5.4; h5py 2.10.0; igraph 0.9.9; ipykernel 6.4.1; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; leidenalg 0.8.9; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; packaging 21.3; pandas 1.3.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.4; pytz 2021.3; scanpy 1.7.2; scipy 1.7.3; setuptools_sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172:2094,bottleneck,bottleneck,2094,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172,1,['bottleneck'],['bottleneck']
Performance,"raph_k'] = k. path_to_h5ad_file = '~/test.h5ad'; adata.write_h5ad(path_to_h5ad_file) # works. # but if I run; sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'); rcParams['figure.figsize'] = 4,4; rcParams['axes.grid'] = True; sc.pl.rank_genes_groups(adata); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works; adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2187,cache,cache,2187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,3,['cache'],['cache']
Performance,rking env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 con,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:26422,load,loader,26422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,rmdirs 3.10.0; plotly 5.9.0; pluggy 1.0.0; ply 3.11; poyo 0.5.0; prometheus-client 0.14.1; prompt-toolkit 3.0.36; Protego 0.1.16; psutil 5.9.0; ptyprocess 0.7.0; pure-eval 0.2.2; py-cpuinfo 8.0.0; pyarrow 11.0.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycodestyle 2.10.0; pycosat 0.6.4; pycparser 2.21; pyct 0.5.0; pycurl 7.45.2; pydantic 1.10.8; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0; pyflakes 3.0.1; Pygments 2.15.1; PyJWT 2.4.0; pylint 2.16.2; pylint-venv 2.3.0; pyls-spyder 0.4.0; pyodbc 4.0.34; pyOpenSSL 23.2.0; pyparsing 3.0.9; PyQt5-sip 12.11.0; pyrsistent 0.18.0; PySocks 1.7.1; pytest 7.4.0; python-dateutil 2.8.2; python-dotenv 0.21.0; python-json-logger 2.0.7; python-lsp-black 1.2.1; python-lsp-jsonrpc 1.0.0; python-lsp-server 1.7.2; python-slugify 5.0.2; python-snappy 0.6.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz-comms 2.3.0; PyWavelets 1.4.1; pyxdg 0.27; PyYAML 6.0; pyzmq 23.2.0; QDarkStyle 3.0.2; qstylizer 0.2.2; QtAwesome 1.2.2; qtconsole 5.4.2; QtPy 2.2.0; queuelib 1.5.0; regex 2022.7.9; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; responses 0.13.3; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rope 1.7.0; Rtree 1.0.1; ruamel.yaml 0.17.21; ruamel-yaml-conda 0.17.21; s3fs 2023.4.0; safetensors 0.3.2; scikit-image 0.20.0; scikit-learn 1.3.0; scikit-learn-intelex 20230426.111612; scipy 1.11.1; Scrapy 2.8.0; seaborn 0.12.2; SecretStorage 3.3.1; Send2Trash 1.8.0; service-identity 18.1.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 5.2.1; sniffio 1.2.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.4; Sphinx 5.0.2; sphinxcontrib-applehelp 1.0.2; sphinxcontrib-devhelp 1.0.2; sphinxcontrib-htmlhelp 2.0.0; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.3; sphinxcontrib-serializinghtml 1.1.5; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 1.4.39; stack-data 0.2.0; statsmodels 0.14.0; sympy 1.11.1; tables 3.8.0; tabulate 0.8.10; TBB 0.2; tblib 1.7.0; TELR 1.0; tenacity 8.2.2; terminado 0.17.1; text-unid,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:6200,queue,queuelib,6200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['queue'],['queuelib']
Performance,"rogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1489,cache,cache,1489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"s pl. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\Python38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\Python38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```. #### Versions. <details>. Package Version; ------------------- ---------; anndata 0.7.8; anyio 2.2.0; argon2-cffi 20.1.0; async-generator 1.10; attrs 21.2.0; Babel 2.9.1; backcall 0.2.0; bleach 4.1.0; Bottleneck 1.3.2; brotlipy 0.7.0; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.4; colorama 0.4.4; cryptography 36.0.0; cycler 0.11.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fonttools 4.25.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; importlib-metadata 4.8.2; ipykernel 6.4.1; ipython 7.29.0; ipython-genutils 0.2.0; jedi 0.18.0; Jinja2 3.0.2; joblib 1.1.0; json5 0.9.6; jsonschema 3.2.0; jupyter-client 7.1.0; jupyter-core 4.9.1; jupyter-server 1.4.1; jupyterlab 3.2.1; jupyterlab-pygments 0.1.2; jupyterlab-server 2.10.2; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.37.0; MarkupSafe 2.0.1; matplotlib 3.5.0; matplotlib-inline 0.1.2; mistune 0.8.4; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; mock 4.0.3; munkres 1.1.4; nat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:3125,load,load,3125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['load'],['load']
Performance,s plt; 13 from anndata import AnnData; ---> 14 from pandas.api.types import is_categorical; 16 from ..preprocessing._utils import _get_mean_var; 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py); ```. ### Versions. <details>. ```; conda list; # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:; #; # Name Version Build Channel; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; appnope 0.1.2 py39hca03da5_1001 anaconda; asttokens 2.0.5 pyhd3eb1b0_0 anaconda; backcall 0.2.0 pyhd3eb1b0_0 anaconda; blosc 1.21.4 hc338f07_0 conda-forge; brotli 1.0.9 h1a8c8d9_9 conda-forge; brotli-bin 1.0.9 h1a8c8d9_9 conda-forge; brotli-python 1.0.9 py39h23fbdae_9 conda-forge; bzip2 1.0.8 h3422bc3_4 conda-forge; c-ares 1.19.1 hb547adb_0 conda-forge; c-blosc2 2.10.0 h068da5f_0 conda-forge; ca-certificates 2022.4.26 hca03da5_0 anaconda; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2022.6.15 py39hca03da5_0 anaconda; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; curl 8.1.2 hc52a3a8_1 conda-forge; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; debugpy 1.5.1 py39hc377ac9_0 anaconda; decorator 5.1.1 pyhd3eb1b0_0 anaconda; dunamai 1.18.0 pyhd8ed1ab_0 conda-forge; entrypoints 0.4 py39hca03da5_0 anaconda; executing 0.8.3 pyhd3eb1b0_0 anaconda; fonttools 4.41.0 py39h0f82c59_0 conda-forge; freetype 2.12.1 hd633e50_1 conda-forge; get_version 3.5.4 pyhd8ed1ab_0 conda-forge; gettext 0.21.1 h0186832_0 conda-forge; git 2.41.0 pl5321h46e2b6d_0 conda-forge; h5py 3.9.0 nompi_py39he9c2634_101 conda-forge; hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; ipykernel 6.9.1 py39hca03da5_0 anaconda; ipython 8.3.0 py39,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:2704,cache,cached-property,2704,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['cache'],['cached-property']
Performance,"s/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20245,cache,cache,20245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"s=keys, layer=layer, use_raw=use_raw); 782 if groupby is None:; 783 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 270 alias_index = None; 271 ; --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(; 273 adata.obs,; 274 var.index,. ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw); 165 not_found.append(key); 166 if len(not_found) > 0:; --> 167 raise KeyError(; 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in""; 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['n_genes_by_counts', 'pct_counts_mt', 'total_counts']' in columns of `adata.obs` or in adata.var_names.""; ]; ```. #### Versions. <details>. anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.4.0; anyio NA; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; brotli NA; certifi 2022.06.15; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fastjsonschema NA; fsspec 2021.08.1; google NA; h5py 3.2.1; idna 3.2; igraph 0.9.11; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; leidenalg 0.8.10; llvmlite 0.37.0; louvain 0.7.1; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.4.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305:2935,bottleneck,bottleneck,2935,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305,1,['bottleneck'],['bottleneck']
Performance,"s_with_different_data_types():; tree1 = MockTree(hyperplanes=[[1.0, 2.0]], offsets=[3], children=[[4, 5]], indices=[[6, 7]]); tree2 = MockTree(hyperplanes=[[8, 9]], offsets=[10], children=[[11, 12]], indices=[[13, 14]]); forest = [tree1, tree2]; result = _make_forest_dict(forest); assert result[""hyperplanes""][""data""].dtype == np.float64. # Test with trees that have properties with NaN or inf values; @pytest.mark.parametrize(""value"", [np.nan, np.inf, -np.inf]); def test_trees_with_special_values(value):; tree = MockTree(hyperplanes=[[value, value]], offsets=[value], children=[[value, value]], indices=[[value, value]]); forest = [tree]; result = _make_forest_dict(forest); assert np.isnan(result[""hyperplanes""][""data""]).all() or np.isinf(result[""hyperplanes""][""data""]).all(). # Test with a large number of trees; def test_large_number_of_trees():; trees = [MockTree(hyperplanes=[[i, i+1]], offsets=[i+2], children=[[i+3, i+4]], indices=[[i+5, i+6]]) for i in range(1000)]; forest = trees; result = _make_forest_dict(forest); assert len(result[""hyperplanes""][""start""]) == 1000; assert result[""hyperplanes""][""data""].shape == (2000, 2). # Test with trees missing one of the expected properties; def test_trees_missing_properties():; class IncompleteTree:; def __init__(self, hyperplanes, children, indices):; self.hyperplanes = np.array(hyperplanes); self.children = np.array(children); self.indices = np.array(indices); tree = IncompleteTree(hyperplanes=[[1, 2]], children=[[3, 4]], indices=[[5, 6]]); forest = [tree]; with pytest.raises(AttributeError):; _make_forest_dict(forest); ```; </details>. This optimization was discovered by [Codeflash AI](https://codeflash.ai) ⚡️. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] Tests included",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2971:4459,optimiz,optimization,4459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2971,1,['optimiz'],['optimization']
Performance,sonpy-ykxq6c1e/meson-python-native-file.ini; Preparing metadata (pyproject.toml) did not run successfully.; ```. ### Error output. _No response_. ### Versions. <details>. ```; # Name Version Build Channel; absl-py 2.1.0 pyhd8ed1ab_0 conda-forge; anndata 0.10.8 pypi_0 pypi; anyio 4.2.0 py39hca03da5_0 ; appnope 0.1.2 py39hca03da5_1001 ; argon2-cffi 21.3.0 pyhd3eb1b0_0 ; argon2-cffi-bindings 21.2.0 py39h1a28f6b_0 ; arpack 3.9.1 nompi_h593882a_101 conda-forge; array-api-compat 1.7.1 pyhd8ed1ab_0 conda-forge; asttokens 2.0.5 pyhd3eb1b0_0 ; async-lru 2.0.4 py39hca03da5_0 ; attrs 23.1.0 py39hca03da5_0 ; babel 2.11.0 py39hca03da5_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; beautifulsoup4 4.12.3 py39hca03da5_0 ; blas 1.0 openblas ; bleach 4.1.0 pyhd3eb1b0_0 ; blosc2 2.0.0 pypi_0 pypi; brotli 1.1.0 hb547adb_1 conda-forge; brotli-bin 1.1.0 hb547adb_1 conda-forge; brotli-python 1.0.9 py39h313beb8_8 ; bzip2 1.0.8 h80987f9_6 ; c-ares 1.28.1 h93a5062_0 conda-forge; ca-certificates 2024.7.4 hf0a4a13_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2024.6.2 py39hca03da5_0 ; cffi 1.16.0 py39he153c15_0 conda-forge; charset-normalizer 2.0.4 pyhd3eb1b0_0 ; chex 0.1.81 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.2.1 py39hca03da5_0 ; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; cython 3.0.10 pypi_0 pypi; debugpy 1.6.7 py39h313beb8_0 ; decorator 5.1.1 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; dm-tree 0.1.7 py39h2666b31_0 conda-forge; docrep 0.3.2 pyh44b312d_0 conda-forge; et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge; etils 1.6.0 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.1 pypi_0 pypi; executing 0.8.3 pyhd3eb1b0_0 ; flax 0.6.1 pyhd8ed1ab_1 conda-forge; fonttools 4.53.0 py39hfea33bf_0 conda-forge; freetype 2.12.1 hadb7bae_2 conda-forge; fsspec 2024.6.1 pyhff2d567_0 conda-forge; future 1.0.0 pyhd8ed1ab_0 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forg,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:6788,cache,cached-property,6788,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['cache'],['cached-property']
Performance,"sphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1411,load,loading,1411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"st version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I am running the Scrublet function to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1177,cache,cache,1177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"stall \; --install-scripts=$TOPDIR/bin --prefix /usr/common \; 2>&1 | tee ../install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/base.py"", line 21, in <module>; from scipy.sparse.sputils import IndexMixin; ImportError: cannot import name 'IndexMixin'; ```. ```bash; #pip3 install works any better?; pip3 install scanpy --target $PYT",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:1310,load,load,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,1,['load'],['load']
Performance,"sts on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1173,cache,cache,1173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,2,['cache'],['cache']
Performance,sue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1125,load,loading,1125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"t time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1180,cache,cache,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['cache'],['cache']
Performance,t-SNE optimization using scikit-learn-intelex,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:6,optimiz,optimization,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,4,['optimiz'],['optimization']
Performance,"ta1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\P",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20822,cache,cache,20822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,te 'get_legend_handles_labels'; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.6.0; aiohttp 3.8.6; aioitertools 0.11.0; aiosignal 1.3.1; alabaster 0.7.13; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.0; anaconda-cloud-auth 0.1.4; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anndata 0.10.1; anndata 0.10.0rc1; annoy 1.17.2; anyio 4.0.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:4209,cache,cached-property,4209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['cache'],['cached-property']
Performance,"test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2108,cache,cachetools,2108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['cache'],['cachetools']
Performance,"tl.dpt with no branching events works:; ```; sc.tl.dpt(adata, n_branchings=0, n_dcs=10, min_group_size=0.01, allow_kendall_tau_shift=True); yields; performing Diffusion Pseudotime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:148,perform,performing,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['perform'],['performing']
Performance,"to;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; .xl65; 	{text-align:center;}; .xl66; 	{text-align:center;; 	border:.5pt solid windowtext;}; .xl67; 	{color:red;; 	text-align:center;; 	border:.5pt solid windowtext;}; .xl68; 	{font-weight:700;; 	text-align:center;; 	border:.5pt solid windowtext;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">.   | Right UMAP | Wrong UMAP; -- | -- | --; Package | Version | Version; Anaconda | 2.1.0 | 2.1.0; Python | 3.6.13 | 3.6.13; anndata | 0.7.6 | 0.7.6; anyio | 2.2.0 | 2.2.0; argon2-cffi | 20.1.0 | 20.1.0; async-generator | 1.1 | 1.1; attrs | 21.2.0 | 21.2.0; Babel | 2.9.1 | 2.9.1; backcall | 0.2.0 | 0.2.0; bleach | 4.0.0 | 4.0.0; brotlipy | 0.7.0 | 0.7.0; cached-property | 1.5.2 | 1.5.2; certifi | 2021.5.30 | 2021.5.30; cffi | 1.14.6 | 1.14.6; charset-normalizer | 2.0.4 | 2.0.4; colorama | 0.4.4 | 0.4.4; contextvars | 2.4 | 2.4; **cryptography | 3.4.7 | 35.0.0**; cycler | 0.10.0 | 0.11.0; dataclasses | 0.8 | 0.8; decorator | 4.4.2 | 4.4.2; defusedxml | 0.7.1 | 0.7.1; entrypoints | 0.3 | 0.3; get-version | 2.1 | 2.1; h5py | 3.1.0 | 3.1.0; idna | 3.2 | 3.2; igraph | 0.9.8 | 0.9.8; immutables | 0.16 | 0.16; importlib-metadata | 4.8.1 | 4.8.1; ipykernel | 5.3.4 | 5.3.4; ipython | 7.16.1 | 7.16.1; ipython-genutils | 0.2.0 | 0.2.0; jedi | 0.17.0 | 0.17.0; **Jinja2 | 3.0.1 | 3.0.2**; joblib | 1.1.0 | 1.1.0; json5 | 0.9.6 | 0.9.6; jsonschema | 3.2.0 | 3.2.0; jupyter-client | 7.0.1 | 7.0.1; jupyter-core | 4.8.1 | 4.8.1; jupyter-server | 1.4.1 | 1.4.1; **jupyterlab | 3.1.7 | 3.2.1**; jupyterlab-pygments | 0.1.2 | 0.1.2; jupyterlab-server | 2.8.2 | 2.8.2; kiwisolver | 1.3.1 | 1.3.1; legacy-api-wrap | 1.2 | 1.2; leidenalg | 0.8.8 | 0.8.8; llvmlite | 0.36.0 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045:3432,cache,cached-property,3432,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045,1,['cache'],['cached-property']
Performance,"ts.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1951,Perform,Perform,1951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['Perform'],['Perform']
Performance,"ues, cast_type, column); 1807 values = astype_nansafe(values, cast_type,; -> 1808 copy=True, skipna=True); 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna); 701 # Explicit copy, or required since NumPy can't view from / to object.; --> 702 return arr.astype(dtype, copy=True); 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:1270,cache,cache,1270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,1,['cache'],['cache']
Performance,"uild_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:2789,cache,cache,2789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['cache'],['cache']
Performance,"ule>; from . import notation; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/notation.py"", line 8, in <module>; from ..util.exceptions import ParameterError; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/__init__.py"", line 83, in <module>; from .utils import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py"", line 1848, in <module>; def __shear_dense(X, factor=+1, axis=-1):; File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/prepr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:2299,cache,cache,2299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['cache'],['cache']
Performance,"un_setup(); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 145, in run_setup; exec(compile(code, __file__, 'exec'), locals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2155,load,load,2155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['load'],['load']
Performance,updated Scanpy to support Weighted sampled data to perform clustering…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630:51,perform,perform,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630,1,['perform'],['perform']
Performance,"ups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:2538,cache,cache,2538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['cache'],['cache']
Performance,"velope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:5675,cache,cache,5675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"veshell.py"", line 3442, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-8-1616c771d0da>"", line 1, in <module>; sc.tl.dendrogram(adata, 'cat_key', use_rep='X'); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scanpy/tools/_dendrogram.py"", line 139, in dendrogram; corr_condensed = distance.squareform(1 - corr_matrix); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scipy/spatial/distance.py"", line 2354, in squareform; is_valid_dm(X, throw=True, name='X'); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scipy/spatial/distance.py"", line 2429, in is_valid_dm; raise ValueError(('Distance matrix \'%s\' must be '; ValueError: Distance matrix 'X' must be symmetric.```. #### Versions. <details>. sc.logging.print_versions() ; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.4.0; PyQt5 NA; anyio NA; asttokens NA; attr 22.2.0; babel 2.11.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.6; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.1.1; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.1.1; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; fastcluster 1.2.6; fastjsonschema NA; gepdynamics NA; h5py 3.8.0; hypergeom_ufunc NA; idna 3.4; igraph 0.10.4; invgauss_ufunc NA; ipykernel 6.21.1; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonschema 4.17.3; jupyter_events 0.5.0; jupyter_server 2.2.1; jupyterlab_server 2.19.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; louvain 0.8.0; markupsafe 2.1.2; matplotlib 3.6.3; mpl_toolkits NA; natsort 8.2.0; nbformat 5.7.3; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numexpr 2.8.3; numpy 1.23.5; packaging 23.0; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.6.2; prometheus_client NA; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2418:2261,bottleneck,bottleneck,2261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2418,1,['bottleneck'],['bottleneck']
Performance,"write.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6508,cache,cache,6508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"x([ b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', ..., dtype='object', length=640); ```; When using `h5py==2.10.0`, it works as expected (i.e. the index type is str). The same happens for `.var_names` (I haven't check further). I'd recommend updating the requirements.txt oto `h5py<=2.10.0` for now. Related h5py issue: https://github.com/h5py/h5py/issues/1732. Also, I've encoutered this bug when coming up with example (seems unrelated):; ```python; import scanpy as sc. adata = sc.datasets.paul15(). sc.read('data/paul15/paul15.h5'); ```; The last line raises:. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-24-3d2f3a02bf09> in <module>; ----> 1 sc.read('data/paul15/paul15.h5'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:1613,cache,cache,1613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['cache'],['cache']
Performance,"x.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scell",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1446,cache,cache,1446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,"xists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:2650,cache,cache,2650,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"yhd3eb1b0_0; - prompt-toolkit=3.0.8=py_0; - prompt_toolkit=3.0.8=0; - ptyprocess=0.7.0=pyhd3eb1b0_2; - pycparser=2.20=py_2; - pygments=2.7.4=pyhd3eb1b0_0; - pyparsing=2.4.7=pyhd3eb1b0_0; - pyqt=5.9.2=py37h05f1152_2; - pyrsistent=0.17.3=py37h7b6447c_0; - python=3.7.9=h7579374_0; - python-dateutil=2.8.1=pyhd3eb1b0_0; - pyzmq=20.0.0=py37h2531618_1; - qt=5.9.7=h5867ecd_1; - qtconsole=4.7.7=py_0; - qtpy=1.9.0=py_0; - readline=8.1=h27cfd23_0; - send2trash=1.5.0=pyhd3eb1b0_1; - setuptools=52.0.0=py37h06a4308_0; - sip=4.19.8=py37hf484d3e_0; - six=1.15.0=py37h06a4308_0; - sqlite=3.33.0=h62c20be_0; - terminado=0.9.2=py37h06a4308_0; - testpath=0.4.4=pyhd3eb1b0_0; - tk=8.6.10=hbc83047_0; - tornado=6.1=py37h27cfd23_0; - traitlets=5.0.5=pyhd3eb1b0_0; - wcwidth=0.2.5=py_0; - webencodings=0.5.1=py37_1; - wheel=0.36.2=pyhd3eb1b0_0; - widgetsnbextension=3.5.1=py37_0; - xz=5.2.5=h7b6447c_0; - zeromq=4.3.3=he6710b0_3; - zipp=3.4.0=pyhd3eb1b0_0; - zlib=1.2.11=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:4448,cache,cached-property,4448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['cache'],['cached-property']
Performance,ypi; ipykernel 6.29.3 pypi_0 pypi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0; jupyter_core 5.5.0 py311haa95532_0; jupyter_events 0.8.0 py311haa95532_0; jupyter_server 2.10.0 py311haa95532_0; jupyter_server_terminals 0.4.4 py311haa95532_1; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0; jupyterlab_server 2.25.1 py311haa95532_0; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0; libsodium 1.0.18 h62dcd97_0; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2; m2w64-bzip2 1.0.6 6; m2w64-expat 2.1.1 2; m2w64-fftw 3.3.4 6; m2w64-flac 1.3.1 # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1 ; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-bindings 21.2.0 py311h2bbff1b_0 ; array-api-compat 1.5.1 pypi_0 pypi; arrow 1.3.0 pypi_0 pypi; asttokens 2.4.1 pypi_0 pypi; async-lru 2.0.4 py311haa95532_0 ; attrs 23.2.0 pypi_0 pypi; babel 2.14.0 pypi_0 pypi; beautifulsoup4 4.12.3 pypi_0 pypi; bleach 6.1.0 pypi_0 pypi; brotli-python 1.0.9 py311hd77b12b_7 ; bzip2 1.0.8 h2bbff1b_5 ; ca-certificates 2023.12.12 haa95532_0 ; certifi 2024.2.2 py311haa95532_0 ; cffi 1.16.0 py311h2bbff1b_0 ; chardet 5.2.0 pypi_0 pypi; charset-normalizer 3.3.2 pypi_0 pypi; colorama 0.4.6 py311haa95532_0 ; comm 0.2.2 pypi_0 pypi; conto,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:4477,load,loader,4477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['load'],['loader']
Performance,"ysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2306,perform,performing,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['perform'],['performing']
Safety," a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code; See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs; I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it can’t be ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:1585,avoid,avoid,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,1,['avoid'],['avoid']
Safety," genes that are detected in less than 5 cells; filtered out 5905 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6390 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5974 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 7136 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5971 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6234 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6442 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5593 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6412 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6662 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6039 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6036 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5923 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_u",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2080:2404,detect,detected,2404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2080,1,['detect'],['detected']
Safety," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3471,detect,detects,3471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['detect'],['detects']
Safety," vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. [1] During: lowering ""$80for_iter.1 = iternext(value=$phi80.0)"" at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (107); @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); Adjusting variance...; Applying correction...; Step 2 of 11: processing batch 2; Looking for MNNs...; Computing correction vectors...; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""compute_correction"" failed type inference due to: non-precise type pyobject; [1] During: typing of argument at /home/a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:24774,detect,detected,24774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['detect'],['detected']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Dear `scanpy` developers, . I was exploring the new features in the latest version of Scanpy, but encountered a prolonged pause when running the `sc.pp.scrublet(adata)`. Initially I thought the problem was due to the large size (~100k cells) of the dataset I was exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:786,detect,detected,786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['detect'],['detected']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipyw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:744,avoid,avoid,744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['avoid'],['avoid']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. While I am using `sc.pp.calculate_qc_metrics(ad, inplace=True)` to get QC metrics, its reported that a error occoured. ; Error message as below. ; ﻿﻿; It might just be because there's something wrong with my data. Does anyone else have a similar situation?. ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; import anndata. # ad = anndata.read_h5ad('mypath'). def scrublet_by_sample(ad, key='samplename'):; """""" do doublet prediction by batch/sample """"""; """""" ad = anndata object """"""; """""" key = sample or batch in ad.obs""""""; sc.pp.calculate_qc_metrics(ad, inplace=True); ads = []; samplenames = ad.obs[key].unique(); for i in samplenames:; adx = ad[ad.obs[key].isin([i])].copy(); print(i,adx.n_obs); sc.external.pp.scrublet(adx,n_prin_comps=min(30,adx.shape[0]-1)); ads.append(adx); adata = ads[0].concatenate(tuple(ads[1:]), join='outer'); return adata. if np.array_equal(arr, np.round(arr)):; ad = scrublet_by_sample(ad, 'sample_ID'); ad.write(qc_h5); qc_md5 = generate_file_md5(qc_h5); print(""QC MD5 Hash:"", qc_md5); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""<stdin>"", line 5, in scrublet_by_sample; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 306, in calculate_qc_metrics; obs_metrics = describe_obs(; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 401, in top_segment_proportions; return top_segment_proportions_sparse_csr(mtx.d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2758:740,predict,prediction,740,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2758,1,['predict'],['prediction']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. **In fact, that's not a real bug report, but a suggestion about safe design to plot with scanpy.**. It may happen when our celltype name is too long to show in the plotting figure.; The problem happens here:. https://github.com/scverse/scanpy/blob/a20334f02e6f2a0b56dd6dd862b07d5bdd4d879e/scanpy/plotting/_baseplot_class.py#L1059-L1061. It cut off the string in functions `dotplot`/`matrixplot(var_group_labels=)` , as the function `_plot_var_groups_brackets(group_labels=)`. So, when we use code like the sample, the veryvery long labels will be cut off & we got an Error; Because Var `celltype_order` is a list, and function `_plot_var_groups_brackets` (PATH: [scanpy/plotting/_baseplot_class.py](https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_baseplot_class.py)) will affect the string in list,; then if affect the `celltype_order` itself, so. when called with the `categories_order`, it has been changed by `_plot_var_groups_brackets`, then the Error happened. I suggest to add one copy, for parameter `group_labels` in function `_plot_var_groups_brackets`; It may help someone are not so skillful on coding, to solve the problem maybe happen. For example: add the code `group_labels = copy.deepcopy(group_labels)` at the top of function `_plot_var_groups_brackets`. Thank you very much for your attention. ### Minimal code sample. ```python; adata: any anndata; markers: gene list include in var_names; group: obs key; celltype_order = ['short', 'veryveryverylong_name', 'others', ...]. sc.pl.dotplot(; 	adata, markers, group, show=False, swap_axes=True,; 	categories_order=celltype_order, var_group_labels=celltype_order, var_group_positions=pos_markers,; ); ```. ### Error output. ```pytb; K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3081:353,safe,safe,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3081,1,['safe'],['safe']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi,; since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught.; Cheers,; Jesko. ### Minimal code sample. ```python; import anndata as ad; import scanpy as sc; import numpy as np; import pandas as pd; adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})); sc.pp.regress_out(adata, ""a""); ```. ### Error output. ```pytb; .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified; warnings.warn(msg, category=PerfectSeparationWarning); ```. ### Versions. <details>. ```; anndata 0.10.4; scanpy 1.9.6; statsmodels 0.14.0; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3260:1439,predict,prediction,1439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260,2,"['detect', 'predict']","['detected', 'prediction']"
