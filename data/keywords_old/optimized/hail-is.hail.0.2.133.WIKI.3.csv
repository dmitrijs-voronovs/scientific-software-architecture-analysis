quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"ambda idx_and_e: hl.rbind(; idx_and_e[0],; idx_and_e[1],; lambda idx, e: hl.coalesce(; joined._ref_entries[idx],; hl.or_missing((e.__contig_idx == joined.__contig_idx) & (e.END >= pos), e),; ),; ).drop('__contig_idx'); ),; ),; ); ); dense = dense.filter(dense._include_locus).drop('_interval_dup', '_include_locus', '__contig_idx'). # at this point, 'dense' is a table with dense rows of reference blocks, keyed by locus. refl_filtered = refl.annotate(**{interval_field: intervals[refl.locus]._interval_dup}). # remove rows that are not contained in an interval, and rows that are the start of an; # interval (interval starts come from the 'dense' table); refl_filtered = refl_filtered.filter(; hl.is_defined(refl_filtered[interval_field]) & (refl_filtered.locus != refl_filtered[interval_field].start); ). # union dense interval starts with filtered table; refl_filtered = refl_filtered.union(dense.transmute(_ref_entries=dense.dense_ref)). # rewrite reference blocks to end at the first of (interval end, reference block end); refl_filtered = refl_filtered.annotate(; interval_end=refl_filtered[interval_field].end.position - ~refl_filtered[interval_field].includes_end; ); refl_filtered = refl_filtered.annotate(; _ref_entries=refl_filtered._ref_entries.map(; lambda entry: entry.annotate(END=hl.min(entry.END, refl_filtered.interval_end)); ); ). return refl_filtered._unlocalize_entries('_ref_entries', '_ref_cols', list(ref.col_key)). [docs]@typecheck(; vds=VariantDataset,; intervals=Table,; gq_thresholds=sequenceof(int),; dp_thresholds=sequenceof(int),; dp_field=nullable(str),; ); def interval_coverage(; vds: VariantDataset,; intervals: Table,; gq_thresholds=(; 0,; 10,; 20,; ),; dp_thresholds=(0, 1, 10, 20, 30),; dp_field=None,; ) -> 'MatrixTable':; """"""Compute statistics about base coverage by interval. Returns a :class:`.MatrixTable` with interval row keys and sample column keys. Contains the following row fields:; - ``interval`` (*interval*): Genomic interval of interest.; - ``inte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:27277,rewrite,rewrite,27277,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['rewrite'],['rewrite']
Modifiability,"ample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant annotation that is a String representing the chromosome and start position. va.altName = v.contig + "":"" + v.start. Add a new variant annotation that splits a comma-separated string with gene names and keeps the first element of the resulting array. va.geneName = va.geneNames.split("","")[0]. Add a new variant annotation that is the log of an existing annotation. va.logIntensity = log(va.intensity). Add a new global annotation computed from existing global annotations. global.callRate = global.nCalled / global.nGenotypes. Variant Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new variant annotations, a genotype aggregable (gs) represents a row of genotypes in the variant-sample matrix.; The result of evaluating the gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:6045,variab,variable,6045,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"ample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry (struct) – Father entry fields from mt.; proband_entry (struct) – Mother entry fields from mt.; is_female (bool) – True if proband is female.; p_de_novo (float64) – Unfiltered posterior probability; that the event is de novo rather than a missed heterozygous; event in a parent.; confidence (str) Validation confidence. One of: 'HIGH',; 'MEDIUM', 'LOW'. The key of the table is ['locus', 'alleles', 'id'].; The model looks for de novo events in which both parents are homozygous; reference and the proband is a heterozygous. The model makes the simplifying; assumption that when this configuration x = (AA, AA, AB) of calls; occurs, exactly one of the following is true:. d: a de novo mutation occurred in the proband and all calls are; accurate.; m: at least one parental allele is actually heterozygous and; the proband call is accurate. We can then estimate the posterior probability of a de novo mutation as:. \[\mathrm{P_{\text{de novo}}} = \frac{\mathrm{P}(d \mid x)}{\mathrm{P}(d \mid x) + \mathrm{P}(m \mid x)}\]; Applying Bayes rule to the numerator and denominator yields. \[\frac{\mathrm{P}(x \mid d)\,\mathrm{P}(d)}{\mathrm{P}(x \mid d)\,\mathrm{P}(d) +; \mathrm{P}(x \mid m)\,\mathrm{P}(m)}\]; The prior on de novo mutation is estimated from the rate in the literature:. \[\mathrm{P}(d) = \frac{1 \, \text{mutation}}{30{,}000{,}000 \, \text{bases}}\]; The prior used for at least one alternate allele between the parents; depends on the alternate allele frequency:. \[\mathrm{P}(m) = 1 - (1 - AF)^4\]; The likelihoods \(\mathrm{P}(x \mid d)\) and \(\mathrm{P}(x \mid m)\); are computed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:54399,config,configuration,54399,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"an run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPU",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25202,plugin,plugin,25202,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['plugin'],['plugin']
Modifiability,"ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[str]:; raise NotImplementedError. [docs]class VEPConfigGRCh37Version85(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:26231,variab,variable,26231,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['variab'],['variable']
Modifiability,"and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7698,Config,Configure,7698,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['Config'],['Configure']
Modifiability,"and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple; homogeneous populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. simulate_random_mating(mt[, n_rounds, ...]); Simulate random diploid mating to produce new in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:1879,inherit,inherited,1879,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['inherit'],['inherited']
Modifiability,"antDataset` with those chromosomes; removed.; - ``keep_autosomes``: This argument expects the value ``True``, and returns a dataset without; sex and mitochondrial chromosomes. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset.; keep; Keep a specified list of contigs.; remove; Remove a specified list of contigs; keep_autosomes; If true, keep only autosomal chromosomes. Returns; -------; :class:`.VariantDataset`.; """""". n_args_passed = (keep is not None) + (remove is not None) + keep_autosomes; if n_args_passed == 0:; raise ValueError(""filter_chromosomes: expect one of 'keep', 'remove', or 'keep_autosomes' arguments""); if n_args_passed > 1:; raise ValueError(; ""filter_chromosomes: expect ONLY one of 'keep', 'remove', or 'keep_autosomes' arguments""; ""\n In order use 'keep_autosomes' with 'keep' or 'remove', call the function twice""; ). rg = vds.reference_genome. to_keep = []. if keep is not None:; keep = wrap_to_list(keep); to_keep.extend(keep); elif remove is not None:; remove = set(wrap_to_list(remove)); for c in rg.contigs:; if c not in remove:; to_keep.append(c); elif keep_autosomes:; to_remove = set(rg.x_contigs + rg.y_contigs + rg.mt_contigs); for c in rg.contigs:; if c not in to_remove:; to_keep.append(c). parsed_intervals = hl.literal(to_keep, hl.tarray(hl.tstr)).map(; lambda c: hl.parse_locus_interval(c, reference_genome=rg); ); return _parameterized_filter_intervals(vds, intervals=parsed_intervals, keep=True, mode='unchecked_filter_both'). [docs]@typecheck(; vds=VariantDataset,; intervals=oneof(Table, expr_array(expr_interval(expr_any))),; split_reference_blocks=bool,; keep=bool,; ); def filter_intervals(; vds: 'VariantDataset', intervals, *, split_reference_blocks: bool = False, keep: bool = True; ) -> 'VariantDataset':; """"""Filter intervals in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; intervals : :class:`.Table` or :class:`.ArrayExpression` of type :class:`.tint",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:21561,extend,extend,21561,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,2,['extend'],['extend']
Modifiability,"aproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71985,config,configure,71985,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configure']
Modifiability,"ar()[source]¶; True if the call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. Parameters:num_genotypes (int) – number of possible genotypes. Return type:list of int or None. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Call.html:3536,variab,variables,3536,docs/0.1/representation/hail.representation.Call.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Call.html,1,['variab'],['variables']
Modifiability,"aram log: Log path. :param bool quiet: Don't write logging information to standard error. :param append: Write to end of log file instead of overwriting. :param parquet_compression: Level of on-disk annotation compression. :param min_block_size: Minimum file split size in MB. :param branching_factor: Branching factor for tree aggregation. :param tmp_dir: Temporary directory for file merging. :ivar sc: Spark context; :vartype sc: :class:`.pyspark.SparkContext`; """""". @typecheck_method(sc=nullable(SparkContext),; app_name=strlike,; master=nullable(strlike),; local=strlike,; log=strlike,; quiet=bool,; append=bool,; parquet_compression=strlike,; min_block_size=integral,; branching_factor=integral,; tmp_dir=strlike); def __init__(self, sc=None, app_name=""Hail"", master=None, local='local[*]',; log='hail.log', quiet=False, append=False, parquet_compression='snappy',; min_block_size=1, branching_factor=50, tmp_dir='/tmp'):. if Env._hc:; raise FatalError('Hail Context has already been created, restart session '; 'or stop Hail context to change configuration.'). SparkContext._ensure_initialized(). self._gateway = SparkContext._gateway; self._jvm = SparkContext._jvm. # hail package; self._hail = getattr(self._jvm, 'is').hail. Env._jvm = self._jvm; Env._gateway = self._gateway. jsc = sc._jsc.sc() if sc else None. # we always pass 'quiet' to the JVM because stderr output needs; # to be routed through Python separately.; self._jhc = self._hail.HailContext.apply(; jsc, app_name, joption(master), local, log, True, append,; parquet_compression, min_block_size, branching_factor, tmp_dir). self._jsc = self._jhc.sc(); self.sc = sc if sc else SparkContext(gateway=self._gateway, jsc=self._jvm.JavaSparkContext(self._jsc)); self._jsql_context = self._jhc.sqlContext(); self._sql_context = SQLContext(self.sc, self._jsql_context). # do this at the end in case something errors, so we don't raise the above error without a real HC; Env._hc = self. sys.stderr.write('Running on Apache Spark version",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:2453,config,configuration,2453,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['config'],['configuration']
Modifiability,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28043,variab,variable,28043,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variable']
Modifiability,"are converted to None in Python. In [12]:. hc.eval_expr_typed('NA: Int') # missing Int. Out[12]:. (None, Int). In [13]:. hc.eval_expr_typed('NA: Dict[String, Int]'). Out[13]:. (None, Dict[String,Int]). In [14]:. hc.eval_expr_typed('1 + NA: Int'). Out[14]:. (None, Int). You can test missingness with isDefined and isMissing. In [15]:. hc.eval_expr_typed('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:5995,variab,variable,5995,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variable']
Modifiability,"are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; logP (Boolean) – If true, input quantiles are given as log(p). qpois(p: Double, lambda: Double): Int. Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative. range(start: Int, stop: Int, step: Int): Array[Int]. Generate an Array with values in the interval [start, stop) in increments of step.; let r = range(0, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:15459,variab,variable,15459,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability,"argument may be None to indicate no filter.; First 10 rows, all columns:; >>> mt_range.head(10, None).count(); (10, 100). All rows, first 10 columns:; >>> mt_range.head(None, 10).count(); (100, 10). Notes; The number of partitions in the new matrix is equal to the number of; partitions containing the first n_rows rows. Parameters:. n_rows (int) – Number of rows to include (all rows included if None).; n_cols (int, optional) – Number of cols to include (all cols included if None).; n (int) – Deprecated in favor of n_rows. Returns:; MatrixTable – Matrix including the first n_rows rows and first n_cols cols. index_cols(*exprs, all_matches=False)[source]; Expose the column values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.s).pheno). Or equivalently:; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.col_key).pheno). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_cols(cols) is equivalent to cols().index(exprs); or cols()[exprs].; The type of the resulting struct is the same as the type of; col_value(). Returns:; Expression. index_entries(row_exprs, col_exprs)[source]; Expose the entries as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(dataset.row_key, dataset.col_key).GQ). Or equivalently:; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Parameters:. row_exprs (tuple of Expression) – Row index expressions.; col_exprs (tuple of Expression) – Column index expressions. Notes; The type of the resulting struct is the same as the type of; entry(). Note; There is a shorthand syntax for MatrixTable.index_entries() using; square brackets (the Python __getitem__ syntax). This sy",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:41702,variab,variable-length,41702,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"ariates', e, mt._col_indices). # _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_names = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field_names, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # Handle filtering columns with missing values:; mt = mt.filter_cols(hl.array(y + covariates).all(hl.is_defined)). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). ht = mt._localize_entries('entries', 'samples'). # covmat rows are samples, columns are the different covariates; ht = ht.annotate_globals(; covmat=hl.nd.array(ht.samples.map(lambda s: [s[cov_name] for cov_name in cov_field_names])); ). # yvecs is a list of sample-length vectors, one for each dependent variable.; ht = ht.annotate_globals(yvecs=[hl.nd.array(ht.samples[y_name]) for y_name in y_field_names]). # Fit null models, which means doing a logreg fit with just the covariates for each phenotype.; def fit_null(yvec):; def error_if_not_converged(null_fit):; return (; hl.case(); .when(; ~null_fit.exploded,; (; hl.case(); .when(null_fit.converged, null_fit); .or_error(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""Newton iteration failed to converge""; ); ),; ); .or_error(; hl.format(; ""Failed to fit logistic regression null model (standard MLE with covariates only): ""; ""exploded at Newton iteration %d"",; null_fit.n_iterations,; ); ); ). null_fit = logreg_fit(ht.covmat, yvec, None, max_iterations=max_iterations, tolerance=tolerance); return hl.bind(error_if_not_converged, null_fit). ht = ht.annotate_globals(null_fits=ht.yvecs.map(fit_null)). ht = ht.transmute(x=hl.nd.array(mean_impute(ht.entries[x_field_name]))); h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:58804,variab,variable,58804,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"ariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height=400)). 2-D histogram; For visualizing relationships between variables in large datasets (where scatter plots may be less informative since they highlight outliers), the histogram_2d() function will create a heatmap with the number of observations in each section of a 2-d grid based on two variables. [10]:. p = hl.plot.histogram2d(pca_scores.scores[0], pca_scores.scores[1]); show(p). Q-Q (Quantile-Quantile); The qq() function requires either a Python type or a Hail field containing p-values to be plotted. This function also allows for downsampling. [11]:. p = hl.plot.qq(gwas.p_value, n_divisions=None); p2 = hl.plot.qq(gwas.p_value, n_divisions=75). show(gridplot([p, p2], ncols=2, width=400, height=400)). Manhattan; The manhattan() function requires a Hail field containing p-values. [12]:. p = hl.plot.manhattan(gwas.p_value); show(p). We can also pass in a dictionary of fields that we would like to show up as we hover over a data point, and choose not to downsample if the dataset is relatively small. [13]:. hover_fields = dict([('alleles', gwas.alleles)]); p = hl.plot.manhattan(gwas.p_value, hover_fields=hover_fields, n_divisions=None)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:6311,variab,variables,6311,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,2,['variab'],['variables']
Modifiability,"as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:3883,variab,variable,3883,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,2,['variab'],['variable']
Modifiability,"as.export(f'{output_file}.assoc', header=True). hl.export_plink(mt, output_file, fam_id=mt.s, ind_id=mt.s). if __name__ == '__main__':; parser = argparse.ArgumentParser(); parser.add_argument('--vcf', required=True); parser.add_argument('--phenotypes', required=True); parser.add_argument('--output-file', required=True); parser.add_argument('--cores', required=False); args = parser.parse_args(). if args.cores:; hl.init(master=f'local[{args.cores}]'). run_gwas(args.vcf, args.phenotypes, args.output_file). Docker Image; A Python script alone does not define its dependencies such as on third-party packages. For; example, to execute the run_gwas.py script above, Hail must be installed as well as the; libraries Hail depends on. Batch uses Docker images to define these dependencies including; the type of operating system and any third-party software dependencies. The Hail team maintains a; Docker image, hailgenetics/hail, for public use with Hail already installed. We extend this; Docker image to include the run_gwas.py script. Dockerfile; FROM hailgenetics/hail:0.2.37. COPY run_gwas.py /. The following Docker command builds this image:; docker pull hailgenetics/hail:0.2.37; docker build -t 1kg-gwas -f Dockerfile . Batch can only access images pushed to a Docker repository. You have two repositories available to; you: the public Docker Hub repository and your project’s private Google Container Repository (GCR).; It is not advisable to put credentials inside any Docker image, even if it is only pushed to a; private repository.; The following Docker command pushes the image to GCR:; docker tag 1kg-gwas us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas; docker push us-docker.pkg.dev/<MY_PROJECT>/1kg-gwas. Replace <MY_PROJECT> with the name of your Google project. Ensure your Batch service account; can access images in GCR. Batch Script; The next thing we want to do is write a Hail Batch script to execute LD-based clumping of; association results for the 1000 genomes dataset. Functio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:5243,extend,extend,5243,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['extend'],['extend']
Modifiability,"aset = dataset.filter_rows(dataset.auto_or_x_par | dataset.locus.in_x_nonpar()). hom_ref = 0; het = 1; hom_var = 2. auto = 2; hemi_x = 1. # kid, dad, mom, copy, t, u; config_counts = [; (hom_ref, het, het, auto, 0, 2),; (hom_ref, hom_ref, het, auto, 0, 1),; (hom_ref, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: Matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18706,config,config,18706,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['config'],['config']
Modifiability,"ass_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; class_name, word, ', '.join(""'{}'"".format(m) for m in inherited_matches); ); ); elif has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def get_nice_field_error(obj, item):; class_name, _, handler, has_describe = get_obj_metadata(obj). field_names = obj._fields.keys(); dd = defaultdict(lambda: []); for f in field_names:; dd[f.lower()].append(f). item_lower = i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8885,extend,extend,8885,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['extend'],['extend']
Modifiability,"ataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174466,config,configuration,174466,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['config'],['configuration']
Modifiability,"ault = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pnorm"", tfloat64, x, mu, sigma, lower_tail, log_p). [docs]@typecheck(x=expr_float64, n=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pT(x, n, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikipedia.org/wiki/Student%27s_t-distribution>`__ with; `n` degrees of freedom. Examples; --------. >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; -----; If `lower_tail` is true, returns Prob(:math:`X \leq` `x`) where :math:`X` is; a t-distributed random variable with `n` degrees of freedom. If `lower_tail`; is false, returns Prob(:math:`X` > `x`). Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; n : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom of the t-distribution.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`. """"""; return _func(""pT"", tfloat64, x, n, lower_tail, log_p). [docs]@typecheck(x=expr_float64, df1=expr_float64, df2=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pF(x, df1, df2, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `F-distribution; <https://en.wikipedia.org/wiki/F-distribution>`__ with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:73860,variab,variable,73860,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7500,Config,Configure,7500,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['Config'],['Configure']
Modifiability,"ay of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; conseque",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60277,config,configuration,60277,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"aybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:45396,config,config,45396,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],['config']
Modifiability,"bals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46039,config,configuration,46039,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"bic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact.; We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the global model. With \(n\) samples and \(c\) sample covariates, we define:. \(y = n \times 1\) vector of phenotypes; \(X = n \times c\) matrix of sample covariates and intercept column of ones; \(K = n \times n\) kinship matrix; \(I = n \times n\) identity matrix; \(\beta = c \times 1\) vector of covariate coefficients; \(\sigma_g^2 =\) coefficient of genetic variance component \(K\); \(\sigma_e^2 =\) coefficient of environmental variance component \(I\); \(\delta = \frac{\sigma_e^2}{\sigma_g^2} =\) ratio of environmental and genetic variance component coeffici",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97971,config,configuration,97971,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['config'],['configuration']
Modifiability,"bility p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:22660,variab,variable,22660,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"ble formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:3009,polymorphi,polymorphism,3009,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ble() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-allelic sites. Important; The genotype_schema() must be of type TGenotype in order to use this method. This method is much less computationally expensive than; split_multi(), and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. Returns:Dataset with no multiallelic sites, which can; be used for biallelic-only methods. Return type:VariantDataset. filter_samples_expr(expr, keep=True)[source]¶; Filter samples with the expression language.; Examples; Filter samples by phenotype (assumes sample annotation sa.isCase exists and is a Boolean variable):; >>> vds_result = vds.filter_samples_expr(""sa.isCase""). Remove samples with an ID that matches a regular expression:; >>> vds_result = vds.filter_samples_expr('""^NA"" ~ s' , keep=False). Filter samples from sample QC metrics and write output to a new variant dataset:; >>> (vds.sample_qc(); ... .filter_samples_expr('sa.qc.callRate >= 0.99 && sa.qc.dpMean >= 10'); ... .write(""output/filter_samples.vds"")). Notes; expr is in sample context so the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. For more information, see the documentation on data representation, annotations, and; the expression language. Caution; When expr evaluates to missing, the sample will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep samples where expr evaluates to true. Returns:Filtered variant dataset. Retu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:56021,variab,variable,56021,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['variab'],['variable']
Modifiability,"bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38095,config,config,38095,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. This will show a list of all the jobs; in the batch with the current state, exit code, duration, and cost. The possi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10884,config,configuration,10884,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,3,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"ce. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be configured to use cold storage, even if the bucket is not. In the case of public access GCP buckets where the user does not ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1485,config,configuration,1485,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['config'],['configuration']
Modifiability,"ce_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configura",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70907,config,configured,70907,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configured']
Modifiability,"ck_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46415,variab,variable,46415,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['variab'],['variable']
Modifiability,"cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_comma",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28802,config,configuration,28802,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"clude in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.pca(entry_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on numeric columns derived from a; matrix table.; Examples; For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls.; >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; This method does not automatically mean-center or normalize each column.; If desired, such transformation",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:16143,variab,variable,16143,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"coercer_from_dtype(unified_typ); indices, aggs = unify_all(*exprs). func_name = name; if filter_missing:; func_name += '_ignore_missing'; if filter_nan and unified_typ in (tfloat32, tfloat64):; func_name = 'nan' + func_name; return construct_expr(; functools.reduce(lambda l, r: ir.Apply(func_name, unified_typ, l, r), [ec.coerce(e)._ir for e in exprs]),; unified_typ,; indices,; aggs,; ). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def nanmax(*exprs, filter_missing: builtins.bool = True) -> NumericExpression:; """"""Returns the maximum value of a collection or of given arguments, excluding NaN. Examples; --------. Compute the maximum value of an array:. >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:. >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 50.1. Notes; -----; Like the Python builtin ``max`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the maximum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. NaN arguments / array elements are ignored; the maximum value of `NaN` and; any non-`NaN` value `x` is `x`. See Also; --------; :func:`max`, :func:`min`, :func:`nanmin`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """""". return _comparison_func('max', exprs, filter_missing, filter_nan=True). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:125612,variab,variable-length,125612,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"contig (Expression of type tstr); position (Expression of type tint); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must be length 3.; For a PL array [a, b, c], let:. \[a^\prime = 10^{-a/10} \\; b^\prime = 10^{-b/10} \\; c^\prime = 10^{-c/10} \\\]; The genotype dosage is given by:. \[\frac{b^\prime + 2 c^\prime}; {a^\prime + b^\prime +c ^\prime}\]; Examples; >>> hl.eval(hl.pl_dosage([5, 10, 100])); 0.24025307377482674. Parameters:; pl (ArrayNumericExpression of type tint32) – Length 3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:17182,polymorphi,polymorphism,17182,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"cs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(self._jcall.oneHotGenotype(self._jrep, num_genotypes)); . © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:11125,variab,variables,11125,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['variab'],['variables']
Modifiability,"ction must satisfy the following property:; tie_breaker(l, r) == -tie_breaker(r, l).; When multiple nodes have the same degree, this algorithm will order the; nodes according to tie_breaker and remove the largest node.; If keyed is False, then a node may appear twice in the resulting; table. Parameters:. i (Expression) – Expression to compute one endpoint of an edge.; j (Expression) – Expression to compute another endpoint of an edge.; keep (bool) – If True, return vertices in set. If False, return vertices removed.; tie_breaker (function) – Function used to order nodes with equal degree.; keyed (bool) – If True, key the resulting table by the node field, this requires; a sort. Returns:; Table – Table with the set of independent vertices. The table schema is one row; field node which has the same type as input expressions i and j. hail.methods.rename_duplicates(dataset, name='unique_id')[source]; Rename duplicate column keys. Note; Requires the column key to be one field of type tstr. Examples; >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; This method produces a new column field from the string column key by; appending a unique suffix _N as necessary. For example, if the column; key “NA12878” appears three times in the dataset, the first will produce; “NA12878”, the second will produce “NA12878_1”, and the third will produce; “NA12878_2”. The name of this new field is parameterized by name. Parameters:. dataset (MatrixTable) – Dataset.; name (str) – Name of new field. Returns:; MatrixTable. hail.methods.segment_intervals(ht, points)[source]; Segment the interval keys of ht at a given set of points. Parameters:. ht (Table) – Table with interval keys.; points (Table or ArrayExpression) – Points at which to segment the intervals, a table or an array. Returns:; Table. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/misc.html:7185,parameteriz,parameterized,7185,docs/0.2/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/methods/misc.html,1,['parameteriz'],['parameterized']
Modifiability,"ction.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Rele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43054,config,configuration,43054,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"cts each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to export VDS elements to a TSV file, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2566,variab,variables,2566,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variables']
Modifiability,"d Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0.2.69. Added the option to specify either remote_tmpdir or bucket when using the ServiceBackend. Version 0.2.68. Fixed copying a directory from GCS when using the LocalBackend; Fixed writing files to GCS when the bucket name starts with a “g” or an “s”; Fixed the error “Argument list too long” when using the LocalBackend; Fixed an error where memory is set to None when using the LocalBackend. Version 0.2.66. Removed the need for the proje",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4626,variab,variables,4626,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['variab'],['variables']
Modifiability,"d accuracy was not; achieved. Two indicates the round-off error is possibly significant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pnorm"", tfloat64, x, mu, sigma, lower_tail, log_p). [docs]@typecheck(x=expr_float64, n=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pT(x, n, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikiped",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:72499,variab,variable,72499,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"d one alternate allele. :rtype: bool; """""". return self._jcall.isHetRef(self._jrep). [docs] def is_not_called(self):; """"""True if the call is missing. :rtype: bool; """""". return self._jcall.isNotCalled(self._jrep). [docs] def is_called(self):; """"""True if the call is non-missing. :rtype: bool; """""". return self._jcall.isCalled(self._jrep). [docs] def num_alt_alleles(self):; """"""Returns the count of non-reference alleles. This function returns None if the genotype call is missing. :rtype: int or None; """""". return self._jcall.nNonRefAlleles(self._jrep). [docs] @handle_py4j; @typecheck_method(num_alleles=integral); def one_hot_alleles(self, num_alleles):; """"""Returns a list containing the one-hot encoded representation of the called alleles. This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Call(0); het = Call(1); hom_var = Call(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(self._jcall.oneHotAlleles(self._jrep, num_alleles)). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Call(0); het ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:10202,variab,variables,10202,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['variab'],['variables']
Modifiability,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39085,config,configuration,39085,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['config'],['configuration']
Modifiability,"dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7702,variab,variables,7702,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variables']
Modifiability,"define the derivative:. >>> def derivative(x):; ... return 15 * x**2 - 2. and starting at :math:`x_0 = 0`, we'll compute the next step :math:`x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}`; until the difference between :math:`x_{i}` and :math:`x_{i+1}` falls below; our convergence threshold:. >>> threshold = 0.005; >>> def find_root(f, guess, error):; ... converged = hl.is_defined(error) & (error < threshold); ... new_guess = guess - (polynomial(guess) / derivative(guess)); ... new_error = hl.abs(new_guess - guess); ... return hl.if_else(converged, guess, f(new_guess, new_error)); >>> x = hl.experimental.loop(find_root, hl.tfloat, 0.0, hl.missing(hl.tfloat)); >>> hl.eval(x); 0.8052291984599675. Warning; -------; Using arguments of a type other than numeric types and booleans can cause; memory issues if if you expect the recursive call to happen many times. Parameters; ----------; f : function ( (marker, \*args) -> :class:`.Expression`; Function of one callable marker, denoting where the recursive call (or calls) is located,; and many `args`, the loop variables.; typ : :class:`str` or :class:`.HailType`; Type the loop returns.; args : variable-length args of :class:`.Expression`; Expressions to initialize the loop values.; Returns; -------; :class:`.Expression`; Result of the loop with `args` as initial loop values.; """""". loop_name = Env.get_uid(). def contains_recursive_call(non_recursive):; if isinstance(non_recursive, ir.Recur) and non_recursive.name == loop_name:; return True; return any([contains_recursive_call(c) for c in non_recursive.children]). def check_tail_recursive(loop_ir):; if isinstance(loop_ir, ir.If):; if contains_recursive_call(loop_ir.cond):; raise TypeError(""branch condition can't contain recursive call!""); check_tail_recursive(loop_ir.cnsq); check_tail_recursive(loop_ir.altr); elif isinstance(loop_ir, ir.Let):; if contains_recursive_call(loop_ir.value):; raise TypeError(""bound value used in other expression can't contain recursive call!""); check_tail_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:3815,variab,variables,3815,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,2,['variab'],['variables']
Modifiability,"dot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AA) \\; {} + {} &\mathrm{P}(x_{\mathrm{father}} = AA \mid \mathrm{father} = AA); \cdot \mathrm{P}(x_{\mathrm{mother}} = AA \mid \mathrm{mother} = AB); \end{aligned}; \right) \\; &{} \cdot \mathrm{P}(x_{\mathrm{proband}} = AB \mid \mathrm{proband} = AB); \end{aligned}. \]; (Technically, the second factorization assumes there is exactly (rather; than at least) one alternate allele among the parents, which may be; justified on the grounds that it is typically the most likely case by far.); While this posterior probability is a good metric for grouping putative de; novo mutations by validation likelihood, there exist error modes in; high-throughput sequencing data that are not appropriately accounted for by; the phred-scaled genotype likelihoods. To this end, a number of hard filters; are applied in order to assign validation likelihood.; These filters are different for SNPs and insertions/deletions. In the below; rules, the following variables are used:. DR refers to the ratio of the read depth in the proband to the; combined read depth in the parents.; DP refers to the read depth (DP field) of the proband.; AB refers to the read allele balance of the proband (number of; alternate reads divided by total reads).; AC refers to the count of alternate alleles across all individuals; in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:; (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the min_gq paramete",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:56963,variab,variables,56963,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variables']
Modifiability,"dx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to strings, and it assigns a discrete color to each string.; Say we wanted to plot the line with the colored points overlayed on top of it. We could try:. [7]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) +; geom_line() +; geom_point(); ); fig.show(). But that is coloring the line as well, causing us to end up with interlocking blue and orange lines, which isn’t what we want. For that reason, it’s possible to define aesthetics that only apply to certain geoms. [8]:. fig = (ggplot(ht, aes(x=ht.idx, y=ht.squared)) +; geom_line() +; geom_point(aes(color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))); ); fig.show(). All geoms can take in their own aesthetic mapping, which lets them specify aesthetics specific to them. And geom_point still inherits the x and y aesthetics from the mapping defined in ggplot(). Geoms that group; Some geoms implicitly do an aggregation based on the x aesthetic, and so don’t take a y value. Consider this dataset from gapminder with information about countries around the world, with one datapoint taken per country in the years 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, and 2007. [9]:. gp = hl.Table.from_pandas(plotly.data.gapminder()); gp.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'country': str; 'continent': str; 'year': int32; 'lifeExp': float64; 'pop': int32; 'gdpPercap': float64; 'iso_alpha': str; 'iso_num': int32; ----------------------------------------; Key: []; ----------------------------------------. Let’s filter the data to 2007 for our first experiments. [10]:. gp_2007 = gp.filter(gp.year == 2007). If we want to see how many countries from each continent we have, we can use geom_bar, which just ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:4081,inherit,inherits,4081,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['inherit'],['inherits']
Modifiability,"e PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:4555,variab,variable,4555,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"e PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset sch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175756,plugin,plugin,175756,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability,"e aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; This method does not support aggregation in key expressions. Parameters:. exprs (varargs of type str or Expression) – Field names o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:32480,variab,variable-length,32480,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"e allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4174,polymorphi,polymorphism,4174,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"e between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.; def _max_entropy_cdf(min_x, max_x, x, y, e):; def point_on_bound(i, upper):; if i == len(x):; return max_x, 1; else:; yi = y[i] + e if upper else y[i + 1] - e; return x[i], yi. # Result variables:; new_y = np.full_like(x, 0.0, dtype=np.float64); keep = np.full_like(x, False, dtype=np.bool_). # State variables:; # (fx, fy) is most recently fixed point on max-ent cdf; fx, fy = min_x, 0; li, ui = 0, 0; j = 1. def slope_from_fixed(i, upper):; xi, yi = point_on_bound(i, upper); return (yi - fy) / (xi - fx). def fix_point_on_result(i, upper):; nonlocal fx, fy, new_y, keep; xi, yi = point_on_bound(i, upper); fx, fy = xi, yi; new_y[i] = fy; keep[i] = True. min_slope = slope_from_fixed(li, upper=False); max_slope = slope_from_fixed(ui, upper=True). # Consider a line l from (fx, fy) to (x[j], y?). As we increase y?, l first; # bumps into the upper staircase at (x[ui], y[ui] + e), and as we decrease; # y?, l first bumps into the lower staircase at (x[li], y[li+1] - e).; # We track the min and max slopes l can have while staying between the; # staircases, as well as the points li and ui where the line must bend if; # forced too high or too low. while True:; lower_slope = slope_from_fixed(j, upper=False); upper_slope = slope_from_fixed(j, upper=True); if upper_slope < min_slope:; # Line must bend down at x[li]. We know the max-entropy cdf pas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:15509,variab,variables,15509,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['variab'],['variables']
Modifiability,"e for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; return hl.bind(; lambda at: hl.if_else(; at == AlleleType.SNP,; hl.if_else(hl.is_transition(ref, alt), AlleleType.TRANSITION, AlleleType.TRANSVERSION),; at,; ),; numeric_allele_type(ref, alt),; ). [docs]@typecheck(mt=MatrixTable, name=str); def sample_qc(mt, name='sample_qc') -> MatrixTable:; """"""Compute per-sample metrics useful for quality control. .. include:: ../_templates/req_tvariant.rst. Examples; --------. Compute sample QC metrics and rem",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:1428,config,config,1428,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"e header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample ann",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:5149,variab,variable,5149,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"e in builtins.zip(uids, types)]); indices, aggregations = unify_all(*streams); behavior = 'ExtendNA' if fill_missing else 'TakeMinLength'; return construct_expr(; ir.StreamZip([s._ir for s in streams], uids, body_ir, behavior),; tstream(ttuple(*(s.dtype.element_type for s in streams))),; indices,; aggregations,; ). [docs]@typecheck(arrays=expr_array(), fill_missing=bool); def zip(*arrays, fill_missing: bool = False) -> ArrayExpression:; """"""Zip together arrays into a single array. Examples; --------. >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the `fill_missing` parameter. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :obj:`bool`; If ``False``, return an array with length equal to the shortest length; of the `arrays`. If ``True``, return an array equal to the longest; length of the `arrays`, by extending the shorter arrays with missing; values. Returns; -------; :class:`.ArrayExpression`; """"""; return _zip_streams(*(a._to_stream() for a in arrays), fill_missing=fill_missing).to_array(). def _zip_func(*arrays, fill_missing=False, f):; n_arrays = builtins.len(arrays); uids = [Env.get_uid() for _ in builtins.range(n_arrays)]; refs = [; construct_expr(ir.Ref(uid, a.dtype.element_type), a.dtype.element_type, a._indices, a._aggregations); for uid, a in builtins.zip(uids, arrays); ]; body_result = f(*refs); indices, aggregations = unify_all(*arrays, body_result); behavior = 'ExtendNA' if fill_missing else 'TakeMinLength'; return construct_expr(; ir.toArray(ir.StreamZip([ir.toStream(a._ir) for a in arrays], uids, body_resu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:116743,variab,variable-length,116743,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"e is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a); Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; If f returns False for every element, then the result is missing. Parameters:; f (function ( (a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:10045,extend,extend,10045,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['extend'],['extend']
Modifiability,"e numbers, strings, or; numpy arrays.; Types are very important in Hail, because the fields of Table and; MatrixTable objects have data types. Primitive types; Hail’s primitive data types for boolean, numeric and string objects are:. tint; Alias for tint32. tint32; Hail type for signed 32-bit integers. tint64; Hail type for signed 64-bit integers. tfloat; Alias for tfloat64. tfloat32; Hail type for 32-bit floating point numbers. tfloat64; Hail type for 64-bit floating point numbers. tstr; Hail type for text strings. tbool; Hail type for Boolean (True or False) values. Container types; Hail’s container types are:. tarray - Ordered collection of homogenous objects.; tndarray - Ordered n-dimensional arrays of homogenous objects.; tset - Unordered collection of distinct homogenous objects.; tdict - Key-value map. Keys and values are both homogenous.; ttuple - Tuple of heterogeneous values.; tstruct - Structure containing named fields, each with its own; type. tarray; Hail type for variable-length arrays of elements. tndarray; Hail type for n-dimensional arrays. tset; Hail type for collections of distinct elements. tdict; Hail type for key-value maps. ttuple; Hail type for tuples. tinterval; Hail type for intervals of ordered values. tstruct; Hail type for structured groups of heterogeneous fields. Genetics types; Hail has two genetics-specific types:. tlocus; Hail type for a genomic coordinate with a contig and a position. tcall; Hail type for a diploid genotype. When to work with types; In general, you won’t need to mention types explicitly.; There are a few situations where you may want to specify types explicitly:. To specify column types in import_table() if the impute flag does not; infer the type you want.; When converting a Python value to a Hail expression with literal(),; if you don’t wish to rely on the inferred type.; With functions like missing() and empty_array(). Viewing an object’s type; Hail objects have a dtype field that will print their type.; >>",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:2225,variab,variable-length,2225,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['variab'],['variable-length']
Modifiability,"e of the globals struct:; >>> table1.globals.dtype; dtype('struct{global_field_1: int32, global_field_2: int32}'). The number of global fields:; >>> len(table1.globals); 2. Returns:; StructExpression – Struct of all global fields. group_by(*exprs, **named_exprs)[source]; Group by a new key for use with GroupedTable.aggregate().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two u",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:31655,variab,variable-length,31655,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"e table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. index_globals; Return this table's global variables for use in another expression context. join; Join two tables together. key_by; Key table by a new set of fields. multi_way_zip_join; Combine many tables in a zip join. n_partitions; Returns the number of partitions in the table. naive_coalesce; Naively decrease the number of partitions. order_by; Sort by the specified fields, defaulting to ascending order. parallelize; Parallelize a local array of structs into a distributed table. persist; Persist this table in memory or on disk. rename; Rename fields of the table. repartition; Change the number of partitions. sample; Downsample the table by keeping each row with probability p. select; Select existing fields or create new fields by name, dropping the rest. select_globals; Select existing global fields or create new fields by name, dropping the rest. semi_join; Filters the table to rows whose key appears in other. show; Print the first few rows of the table to the console. summarize; Compute and print summary information about th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:4699,variab,variables,4699,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variables']
Modifiability,"e tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in inverse ppois().; log_p (bool or BooleanExpression) – Exponentiate p before testing. Returns:; Expression of type tfloat64. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:27591,variab,variable,27591,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"e {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:31890,config,configuration,31890,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"e(; ir.MatrixMapRows(; mart,; ir.InsertFields(; ir.Ref('va', mart.typ.row_type),; [; (; uid,; ir.Apply(; 'get',; join_table._row_type[uid].value_type,; ir.GetField(ir.GetField(ir.Ref('va', mart.typ.row_type), uid), uid),; ir.MakeTuple([e._ir for e in exprs]),; ),; ); ],; None,; ),; ); ). else:. def joiner(left: MatrixTable):; return MatrixTable(ir.MatrixAnnotateRowsTable(left._mir, right._tir, uid, all_matches)). ast = ir.Join(ir.ProjectedTopLevelReference('va', uid, new_schema), [uid], exprs, joiner); return construct_expr(ast, new_schema, indices, aggregations); elif indices == src._col_indices and not (is_interval and all_matches):; all_uids = [uid]; if len(exprs) == len(src.col_key) and all([exprs[i] is src.col_key[i] for i in range(len(exprs))]):; # key is already correct; def joiner(left):; return MatrixTable(ir.MatrixAnnotateColsTable(left._mir, right._tir, uid)). else:; index_uid = Env.get_uid(); uids = [Env.get_uid() for _ in exprs]. all_uids.append(index_uid); all_uids.extend(uids). def joiner(left: MatrixTable):; prev_key = list(src.col_key); joined = (; src.annotate_cols(**dict(zip(uids, exprs))); .add_col_index(index_uid); .key_cols_by(*uids); .cols(); .select(index_uid); .join(self, 'inner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :cl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:78155,extend,extend,78155,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['extend'],['extend']
Modifiability,"e, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); n (float or Expression of type tfloat64) – Degrees of freedom of the t-distribution.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pF(x, df1, df2, lower_tail=True, log_p=False)[source]; The cumulative probability function of a F-distribution with parameters; df1 and df2.; Examples; >>> hl.eval(hl.pF(0, 3, 10)); 0.0. >>> hl.eval(hl.pF(1, 3, 10)); 0.5676627969783028. >>> hl.eval(hl.pF(1, 3, 10, lower_tail=False)); 0.4323372030216972. >>> hl.eval(hl.pF(1, 3, 10, log_p=True)); -0.566227703842908. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a random variable with distribution \(F`(df1, df2). If `lower_tail\); is false, returns Prob(\(X\) > x). Parameters:. x (float or Expression of type tfloat64); df1 (float or Expression of type tfloat64) – Parameter of the F-distribution; df2 (float or Expression of type tfloat64) – Parameter of the F-distribution; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.ppois(x, lamb, lower_tail=True, log_p=False)[source]; The cumulative probability function of a Poisson distribution.; Examples; >>> hl.eval(hl.ppois(2, 1)); 0.9196986029286058. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is a; Poisson random variable with rate parameter lamb. If lower_tail is false,; returns Prob(\(X\) > x). Parameters:. x (float or Expression of type t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:23612,variab,variable,23612,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"e. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.';",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11217,config,configure,11217,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configure']
Modifiability,"e.element_type for s in streams))),; indices,; aggregations,; ). [docs]@typecheck(arrays=expr_array(), fill_missing=bool); def zip(*arrays, fill_missing: bool = False) -> ArrayExpression:; """"""Zip together arrays into a single array. Examples; --------. >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the `fill_missing` parameter. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :obj:`bool`; If ``False``, return an array with length equal to the shortest length; of the `arrays`. If ``True``, return an array equal to the longest; length of the `arrays`, by extending the shorter arrays with missing; values. Returns; -------; :class:`.ArrayExpression`; """"""; return _zip_streams(*(a._to_stream() for a in arrays), fill_missing=fill_missing).to_array(). def _zip_func(*arrays, fill_missing=False, f):; n_arrays = builtins.len(arrays); uids = [Env.get_uid() for _ in builtins.range(n_arrays)]; refs = [; construct_expr(ir.Ref(uid, a.dtype.element_type), a.dtype.element_type, a._indices, a._aggregations); for uid, a in builtins.zip(uids, arrays); ]; body_result = f(*refs); indices, aggregations = unify_all(*arrays, body_result); behavior = 'ExtendNA' if fill_missing else 'TakeMinLength'; return construct_expr(; ir.toArray(ir.StreamZip([ir.toStream(a._ir) for a in arrays], uids, body_result._ir, behavior)),; tarray(body_result.dtype),; indices,; aggregations,; ). [docs]@typecheck(a=expr_array(), start=expr_int32, index_first=bool); def enumerate(a, start=0, *, index_first=True):; """"""Returns an array of (index, element) tuples. Examples;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:117009,extend,extending,117009,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['extend'],['extending']
Modifiability,"e; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmax(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.nanmax(*exprs, filter_missing=True)[source]; Returns the maximum value of a collection or of given arguments, excluding NaN.; Examples; Compute the maximum value of an array:; >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:; >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 50.1. Notes; Like the Python builtin max function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the maximum value of NaN and; any non-NaN value x is x. See also; max(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.mean(collection, filter_missing=True)[source]; Returns the mean of all values in the collection.; Examples; >>> a = [1, 3, 5, 6, 7, 9]. >>> hl.eval(hl.mean(a)); 5.166666666666667. Note; Missing elements are ignored if filter_missing is True. If filter_missing; is False, then any mis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:12189,variab,variable-length,12189,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['variab'],['variable-length']
Modifiability,"e; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only the default storage policy for the bucket is checked; individual objects in a bucket may be config",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1321,variab,variables,1321,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,2,['variab'],['variables']
Modifiability,"e_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuratio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1907,config,config,1907,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,6,['config'],['config']
Modifiability,"e_locus_interval(); variant_str(); call(); unphased_diploid_gt_index_call(); parse_call(); downcode(); triangle(); is_snp(); is_mnp(); is_transition(); is_transversion(); is_insertion(); is_deletion(); is_indel(); is_star(); is_complex(); is_strand_ambiguous(); is_valid_contig(); is_valid_locus(); contig_length(); allele_type(); numeric_allele_type(); pl_dosage(); gp_dosage(); get_sequence(); mendel_error_code(); liftover(); min_rep(); reverse_complement(). Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. null(t); Deprecated in favor of missing(). is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. Constructors. bool(x); Convert to a Boolean expression. float(x); Convert to a 64-bit floating point expression. float32(x); Convert to a 32-bit floating point expression. float64(x); Convert to a 64-bit floating point expression. int(x); Convert to a 32-bit integer expression. int32(x); Convert to a 32-bit inte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:3432,variab,variable,3432,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['variab'],['variable']
Modifiability,"e`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44603,config,config,44603,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"ean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_ind",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:11957,variab,variable,11957,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"echeck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:222155,variab,variable,222155,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['variab'],['variable']
Modifiability,"ed in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-ps",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4793,polymorphi,polymorphism,4793,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ed over the lazy dog'); 'The quick brown fox jumped over the lazy dog'. Parameters; ----------; other : :class:`.StringExpression`; String to concatenate. Returns; -------; :class:`.StringExpression`; Concatenated string.; """"""; other = to_expr(other); if not other.dtype == tstr:; raise NotImplementedError(""'{}' + '{}'"".format(self.dtype, other.dtype)); return self._bin_op(""concat"", other, self.dtype). def __radd__(self, other):; other = to_expr(other); if not other.dtype == tstr:; raise NotImplementedError(""'{}' + '{}'"".format(other.dtype, self.dtype)); return self._bin_op_reverse(""concat"", other, self.dtype). def __mul__(self, other):; other = to_expr(other); if not other.dtype == tint32:; raise NotImplementedError(""'{}' + '{}'"".format(self.dtype, other.dtype)); return hl.delimit(hl.range(other).map(lambda x: self), delimiter=''). def __rmul__(self, other):; other = to_expr(other); return other * self. def _slice(self, start=None, stop=None, step=None):; if step is not None:; raise NotImplementedError('Variable slice step size is not currently supported for strings'). if start is not None:; start = to_expr(start); if stop is not None:; stop = to_expr(stop); return self._method('slice', tstr, start, stop); else:; return self._method('sliceRight', tstr, start); elif stop is not None:; stop = to_expr(stop); return self._method('sliceLeft', tstr, stop); else:; return self. [docs] def length(self):; """"""Returns the length of the string. Examples; --------. >>> hl.eval(s.length()); 19. Returns; -------; :class:`.Expression` of type :py:data:`.tint32`; Length of the string.; """"""; return apply_expr(lambda x: ir.Apply(""length"", tint32, x), tint32, self). [docs] @typecheck_method(pattern1=expr_str, pattern2=expr_str); def replace(self, pattern1, pattern2):; """"""Replace substrings matching `pattern1` with `pattern2` using regex. Examples; --------. Replace spaces with underscores in a Hail string:. >>> hl.eval(hl.str(""The quick brown fox"").replace(' ', '_')); 'The_quick__brown_f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:65755,Variab,Variable,65755,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['Variab'],['Variable']
Modifiability,"ed); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1704,polymorphi,polymorphism,1704,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.range(rows_in_block).map(build_row). return new_rows. def process_partition(part):; grouped = part.grouped(block_size); return grouped.flatmap(lambda block: process_block(block)._to_stream()). res = ht._map_partitions(process_partition). if not y_is_list:; fields = ['y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; res = res.annotate(**{f: res[f][0] for f in fields}). res = res.select_globals(). temp_file_name = hl.utils.new_temp_file(""_linear_regression_rows_nd"", ""result""); res = res.checkpoint(temp_file_name). return res. [docs]@typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def logistic_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... cova",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:26317,variab,variable,26317,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variable']
Modifiability,"eets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add annotations from datasets specified by name to a relational object. annotate_rows_db(rel, *names)[source]; Add annotations from datasets specified by name to a relational; object.; List datasets with available_datasets.; An interactive query builder is available in the; Hail Annotation Database documentation.; Examples; Annotate a MatrixTable with gnomad_lof_metrics:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:1461,config,configuration,1461,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['config'],['configuration']
Modifiability,"eft):; return MatrixTable(ir.MatrixAnnotateColsTable(left._mir, right._tir, uid)). else:; index_uid = Env.get_uid(); uids = [Env.get_uid() for _ in exprs]. all_uids.append(index_uid); all_uids.extend(uids). def joiner(left: MatrixTable):; prev_key = list(src.col_key); joined = (; src.annotate_cols(**dict(zip(uids, exprs))); .add_col_index(index_uid); .key_cols_by(*uids); .cols(); .select(index_uid); .join(self, 'inner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:78981,variab,variables,78981,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variables']
Modifiability,"einberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101590,config,configuration,101590,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"el, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :meth:`.GroupedTable.aggregate`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:55345,Variab,Variable-length,55345,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Variab'],['Variable-length']
Modifiability,"ele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2221,polymorphi,polymorphism,2221,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"eles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.ve",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102349,config,configuration,102349,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"ep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` conf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:222279,variab,variable,222279,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['variab'],['variable']
Modifiability,"ep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102201,config,config,102201,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,"ep, ...]); Return a table containing the vertices in a near maximal independent set of an undirected graph whose edges are given by a two-column table. rename_duplicates(dataset[, name]); Rename duplicate column keys. segment_intervals(ht, points); Segment the interval keys of ht at a given set of points. hail.methods.grep(regex, path, max_count=100, *, show=True, force=False, force_bgz=False)[source]; Searches given paths for all lines containing regex matches.; Examples; Print all lines containing the string hello in file.txt:; >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in file1.txt and file2.txt:; >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; grep() mimics the basic functionality of Unix grep in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses Java regular expression; patterns.; The RegExr sandbox may be helpful. Parameters:. regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return; show (bool) – When True, show the values on stdout. When False, return a; dictionary mapping file names to lines.; force_bgz (bool) – If True, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; dict of str to list of str. hail.methods.maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True)[source]; Return a table containing the vertices in a near; maximal independent set; of an undirecte",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/misc.html:1842,sandbox,sandbox,1842,docs/0.2/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/methods/misc.html,1,['sandbox'],['sandbox']
Modifiability,"ep.nAlleles(). [docs] @handle_py4j; @typecheck_method(i=integral); def allele(self, i):; """"""Returns the string allele representation for the ith allele. The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:. >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:. >>> v_biallelic.alt == v_biallelic.allele(1). :param int i: integer index of desired allele. :return: string representation of ith allele; :rtype: str; """""". return self._jrep.allele(i). [docs] def num_genotypes(self):; """"""Returns the total number of unique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:4637,polymorphi,polymorphism,4637,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"eparator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:102890,variab,variable,102890,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['variab'],['variable']
Modifiability,"er() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (locus, alleles) polymorphism.; Examples; >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters:. locus (LocusExpression); alleles (ArrayExpression of type tstr). Returns:; StructExpression – A tstruct expression with two fields, locus; (LocusExpression) and alleles; (ArrayExpression of type tstr). hail.expr.functions.reverse_complement(s, rna=False)[source]; Reverses the string and translates base pairs into their complements; .. rubric:: Examples; >>> bases = hl.literal('NNGATTACA'); >>> hl.eval(hl.reverse_complement(bases)); 'TGTAATCNN'. Parameters:. s (StringExpression) – Base string.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:23581,polymorphi,polymorphism,23581,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"erence genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:1964,polymorphi,polymorphism,1964,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"erence genome. locus_interval(contig, start, end[, ...]); Construct a locus interval expression. parse_locus(s[, reference_genome]); Construct a locus expression by parsing a string or string expression. parse_variant(s[, reference_genome]); Construct a struct with a locus and alleles by parsing a string. parse_locus_interval(s[, reference_genome, ...]); Construct a locus interval expression by parsing a string or string expression. variant_str(*args); Create a variant colon-delimited string. call(*alleles[, phased]); Construct a call expression. unphased_diploid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:13709,polymorphi,polymorphism,13709,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['polymorphi'],['polymorphism']
Modifiability,"erence_genome='GRCh37')); False. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.is_valid_locus(contig, position, reference_genome='default')[source]; Returns True if contig and position is a valid site in reference_genome.; Examples; >>> hl.eval(hl.is_valid_locus('1', 324254, 'GRCh37')); True. >>> hl.eval(hl.is_valid_locus('chr1', 324254, 'GRCh37')); False. Parameters:. contig (Expression of type tstr); position (Expression of type tint); reference_genome (str or ReferenceGenome). Returns:; BooleanExpression. hail.expr.functions.contig_length(contig, reference_genome='default')[source]; Returns the length of contig in reference_genome.; Examples; >>> hl.eval(hl.contig_length('5', reference_genome='GRCh37')); 180915260. Parameters:. contig (Expression of type tstr); reference_genome (str or ReferenceGenome). Returns:; Int32Expression. hail.expr.functions.allele_type(ref, alt)[source]; Returns the type of the polymorphism as a string.; Examples; >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes. The possible return values are:; ""SNP""; ""MNP""; ""Insertion""; ""Deletion""; ""Complex""; ""Star""; ""Symbolic""; ""Unknown"". Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; StringExpression. hail.expr.functions.numeric_allele_type(ref, alt)[source]; Returns the type of the polymorphism as an integer. The value returned; is the integer value of AlleleType representing that kind of; polymorphism.; Examples; >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; The values of AlleleType are not stable and thus should not be; relied upon across hail versions. hail.expr.functions.pl_dosage(pl)[source]; Return expected genotype dosage from array of Phred-scaled genotype; likelihoods with uniform prior. Only defined for bi-allelic variants. The; pl argument must ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:16713,polymorphi,polymorphism,16713,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"erted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84689,variab,variables,84689,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1563,config,config,1563,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,3,['config'],['config']
Modifiability,"es as ``False``. See Also; --------; :class:`.CaseBuilder`, :func:`.switch`, :func:`.cond`. Returns; -------; :class:`.CaseBuilder`.; """"""; from .builders import CaseBuilder. return CaseBuilder(missing_false=missing_false). [docs]@typecheck(expr=expr_any); def switch(expr) -> 'hail.expr.builders.SwitchBuilder':; """"""Build a conditional tree on the value of an expression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See Also; --------; :class:`.SwitchBuilder`, :func:`.case`, :func:`.cond`. Parameters; ----------; expr : :class:`.Expression`; Value to match against. Returns; -------; :class:`.SwitchBuilder`; """"""; from .builders import SwitchBuilder. return SwitchBuilder(expr). [docs]@typecheck(f=anytype, exprs=expr_any, _ctx=nullable(str)); def bind(f: Callable, *exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. Examples; --------. >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. :func:`.bind` also can take multiple arguments:. >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters; ----------; f : function ( (args) -> :class:`.Expression`); Function of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""; args = []; uids = []; irs = []. for expr in exprs:; uid = Env.get_uid(base=_ctx); args.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uids.append(uid); irs.append(expr._ir). lambda_result = to_expr(f(*args)); if _ctx:; indices, aggregations = unify_all(lambda_result) # FIXME: hacky. May drop field refs from errors?; else:; indices, aggregations = unify_all(*exprs, lambda_result). res_ir = lambda_result._ir; for uid, value_ir in builtins.zip(uids, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:17062,variab,variable,17062,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table. Returns; -------; :class:`.Table`; """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt = matrix_table_source('logistic_regresion_rows/x', x); raise_unless_entry_indexed('logistic_regresion_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""). y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:56820,variab,variable,56820,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"ests in `EPACTS`_. For each variant, Hail; imputes missing input values as the mean of non-missing input values,; whereas EPACTS subsets to those samples with called genotypes. Hence,; Hail and EPACTS results will currently only agree for variants with no; missing genotypes. .. _EPACTS: http://genome.sph.umich.edu/wiki/EPACTS#Single_Variant_Tests. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; test : {'wald', 'lrt', 'score', 'firth'}; Statistical test.; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a :class:`.BooleanExpression` will be implicitly converted to; a :class:`.Float64Expression` with this property.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; max_iterations : :obj:`int`; The maximum number of iterations.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if max_iterations is None:; max_iterations = 25 if test != 'firth' else 100. if hl.current_backend().requires_lowering:; return _logistic_regression_rows_nd(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('logistic regression requires at least one covariate expression'). mt ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:36375,variab,variable,36375,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"et, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTable.; We can use rows along with select to pull out 5 variants. The select method takes either a string refering to a field name in the table, or a Hail Expression. Here, we leave the arguments blank to keep only the row key fields, locus and alleles.; Use the show method to display the variants. [6]:. mt.rows().select().show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing top 5 rows. Alternatively:. [7]:. mt.row_key.show(5). locusalleleslocus<GRCh37>array<str>; 1:904165[""G"",""A""]; 1:909917[""G"",""A""]; 1:986963[""C"",""T""]; 1:1563691[""T"",""G""]; 1:1707740[""T"",""G""]; showing to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:3134,variab,variable,3134,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['variab'],['variable']
Modifiability,"eta=hl.missing(hl.tfloat64),; chi_sq_stat=hl.missing(hl.tfloat64),; p_value=hl.missing(hl.tfloat64),; firth_null_fit=hl.missing(firth_improved_null_fit.dtype),; fit=hl.missing(firth_fit.dtype),; ); return (; hl.case(); .when(; firth_improved_null_fit.converged,; hl.case(); .when(; firth_fit.converged,; hl.struct(; beta=firth_fit.b[firth_fit.b.shape[0] - 1],; chi_sq_stat=firth_chi_sq,; p_value=firth_p,; firth_null_fit=firth_improved_null_fit,; fit=firth_fit,; ),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit, fit=firth_fit)),; ); .default(blank_struct.annotate(firth_null_fit=firth_improved_null_fit)); ). return hl.bind(cont2, firth_fit). return hl.bind(cont, firth_improved_null_fit). @typecheck(; test=enumeration('wald', 'lrt', 'score', 'firth'),; y=oneof(expr_float64, sequenceof(expr_float64)),; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=nullable(int),; tolerance=nullable(float),; ); def _logistic_regression_rows_nd(; test, y, x, covariates, pass_through=(), *, max_iterations: Optional[int] = None, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:47210,variab,variable,47210,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],['variable']
Modifiability,"ets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. However, not all operations will scale this way. Certain complicated operations; like pca or BlockMatrix multiplies do not scale linearly. When doing small time estimates, it can sometimes be helpful to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:1431,config,configurable,1431,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['config'],['configurable']
Modifiability,"etting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:14650,polymorphi,polymorphism,14650,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,2,['polymorphi'],['polymorphism']
Modifiability,"eturn Table(ir.MatrixEntriesTable(self._mir)). [docs] def index_globals(self) -> Expression:; """"""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(ir.MatrixRowsTable(self._mir)), self.globals.dtype). [docs] def index_rows(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:. >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_rows(exprs)`` is equivalent to ``rows().index(exprs)``; or ``rows()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.row_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.rows()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable row key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_cols(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the column values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_cols(pheno =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:91208,variab,variable-length,91208,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability,"eturn x(elt). else:. def f(elt, x):; return elt == x. return hl.bind(lambda a: hl.range(0, a.length()).filter(lambda i: f(a[i], x)).first(), self). [docs] @typecheck_method(item=expr_any); def append(self, item):; """"""Append an element to the array and return the result. Examples; --------. >>> hl.eval(names.append('Dan')); ['Alice', 'Bob', 'Charlie', 'Dan']. Note; ----; This method does not mutate the caller, but instead returns a new; array by copying the caller and adding `item`. Parameters; ----------; item : :class:`.Expression`; Element to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17202,extend,extend,17202,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['extend'],['extend']
Modifiability,"eturns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:4458,variab,variables,4458,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variables']
Modifiability,"eviation across all samples. Missing values NA may result (for example, due to division by zero) and are handled properly ; in filtering and written as “NA” in export modules. The empirical standard deviation is computed; with zero degrees of freedom. Parameters:root (str) – Variant annotation root for computed struct. Returns:Annotated variant dataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Requi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174112,config,config,174112,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['config'],['config']
Modifiability,"expr._type, expr._indices, expr._aggregations)); uids.append(uid); irs.append(expr._ir). lambda_result = to_expr(f(*args)); if _ctx:; indices, aggregations = unify_all(lambda_result) # FIXME: hacky. May drop field refs from errors?; else:; indices, aggregations = unify_all(*exprs, lambda_result). res_ir = lambda_result._ir; for uid, value_ir in builtins.zip(uids, irs):; if _ctx == 'agg':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=False); elif _ctx == 'scan':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=True); else:; res_ir = ir.Let(uid, value_ir, res_ir). return construct_expr(res_ir, lambda_result.dtype, indices, aggregations). [docs]def rbind(*exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """""". *args, f = exprs; args = [expr_any.check(arg, 'rbind', f'argument {index}') for index, arg in builtins.enumerate(args)]. return hl.bind(f, *args, _ctx=_ctx). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def chi_squared_test(c1, c2, c3, c4) -> StructExpression:; """"""Performs chi-squared test of independence on a 2x2 contingency table. Examples; --------. >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odds_ratio=1.0). >>> hl.eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; -----; The odds ratio is given by ``(c1 / c2) / (c3 / c4)``. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:18649,variab,variable-length,18649,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'). Create, then overwrite, then drop a global annotation:. >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS""]'); >>> vds = vds.annotate_global_expr('global.pops = [""FIN"", ""AFR"", ""EAS"", ""NFE""]'); >>> vds = vds.annotate_global_expr('global.pops = drop(global, pops)'). The expression namespace contains only one variable:. - ``global``: global annotations. :param expr: Annotation expression; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateGlobalExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(path=strlike,; annotation=anytype,; annotation_type=Type); def annotate_global(self, path, annotation, annotation_type):; """"""Add global annotations from Python objects. **Examples**. Add populations as a global annotation:; ; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). **Notes**. This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given ``annotation``; parameter. :param str path: annotatio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:12021,variab,variable,12021,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['variab'],['variable']
Modifiability,"f Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175728,plugin,plugin,175728,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability,"f get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e in exprs:; if not e._ir.is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12144,extend,extend,12144,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['extend'],['extend']
Modifiability,"f the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.annotate_rows(**scores[mt.rsid]); >>> flip = hl.case().when(mt.allele == mt.alleles[0], True).when(; ... mt.allele == mt.alleles[1], False).or_missing(); >>> mt = mt.annotate_rows(flip=flip); >>> mt = mt.annotate_rows(; ... prior=2 * hl.if_else(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1])); >>> mt = mt.annotate_cols(; ... prs=hl.agg.sum(; ... mt.score * hl.coalesce(; ... hl.if_else(mt.flip, 2 - mt.GT.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:13488,flexible,flexible,13488,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['flexible'],['flexible']
Modifiability,"f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; rais",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:11850,config,config,11850,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,12,['config'],"['config', 'configurations']"
Modifiability,"f, het, hom_ref, auto, 0, 1),; (het, het, het, auto, 1, 1),; (het, hom_ref, het, auto, 1, 0),; (het, het, hom_ref, auto, 1, 0),; (het, hom_var, het, auto, 0, 1),; (het, het, hom_var, auto, 0, 1),; (hom_var, het, het, auto, 2, 0),; (hom_var, het, hom_var, auto, 1, 0),; (hom_var, hom_var, het, auto, 1, 0),; (hom_ref, hom_ref, het, hemi_x, 0, 1),; (hom_ref, hom_var, het, hemi_x, 0, 1),; (hom_var, hom_ref, het, hemi_x, 1, 0),; (hom_var, hom_var, het, hemi_x, 1, 0),; ]. count_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:18977,config,config,18977,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['config'],['config']
Modifiability,"failing_job.status(), 'log': failing_job.log()}; raise FatalError(yamlx.dump(message)). annotations = hl.import_table(; f'{temp_output_directory}/annotations/*',; types={'variant': hl.tstr, 'vep': vep_typ, 'part_id': hl.tint, 'block_id': hl.tint},; force=True,; ). annotations = annotations.annotate(; vep_proc_id=hl.struct(part_id=annotations.part_id, block_id=annotations.block_id); ); annotations = annotations.drop('part_id', 'block_id'); annotations = annotations.key_by(**hl.parse_variant(annotations.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:36804,config,config,36804,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],['config']
Modifiability,"ficant. """"""; if max_iterations is None:; max_iterations = hl.literal(10_000); if min_accuracy is None:; min_accuracy = hl.literal(1e-5); return _func(""pgenchisq"", PGENCHISQ_RETURN_TYPE, x - mu, w, k, lam, sigma, max_iterations, min_accuracy). [docs]@typecheck(x=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False) -> Float64Expression:; """"""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""pnorm"", tfloat64, x, mu, sigma, lower_tail, log_p). [docs]@typecheck(x=expr_float64, n=expr_float64, lower_tail=expr_bool, log_p=expr_bool); def pT(x, n, lower_tail=True, log_p=False) -> Float64Expression:; r""""""The cumulative probability function of a `t-distribution; <https://en.wikipedia.org/wiki/Student%27s_t-distribution>`__ with; `n` degrees of freedom. Examples;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:72546,variab,variable,72546,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:49336,variab,variable-length,49336,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"fig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return datase",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44153,config,config,44153,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25505,variab,variables,25505,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['variab'],['variables']
Modifiability,"final_fields, indices). if final_fields == select_fields + list(insertions):; # don't clog the IR with redundant field names; s = base_struct.select(*select_fields).annotate(**insertions); else:; s = base_struct.select(*select_fields)._annotate_ordered(insertions, final_fields). assert list(s) == final_fields; return s. def check_annotate_exprs(caller, named_exprs, indices, agg_axes):; from hail.expr.expressions import analyze. protected_key = set(indices.protected_key); for k, v in named_exprs.items():; analyze(f'{caller}: field {k!r}', v, indices, agg_axes, broadcast=True); check_keys(caller, k, protected_key); check_collisions(caller, list(named_exprs), indices); return named_exprs. def process_joins(obj, exprs):; all_uids = []; left = obj; used_joins = set(). for e in exprs:; joins = e._ir.search(lambda a: isinstance(a, hail.ir.Join)); for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; if j.idx not in used_joins:; left = j.join_func(left); all_uids.extend(j.temp_vars); used_joins.add(j.idx). def cleanup(table):; remaining_uids = [uid for uid in all_uids if uid in table._fields]; return table.drop(*remaining_uids). return left, cleanup. def divide_null(num, denom):; from hail.expr import if_else, missing; from hail.expr.expressions.base_expression import unify_types_limited. typ = unify_types_limited(num.dtype, denom.dtype); assert typ is not None; return if_else(denom != 0, num / denom, missing(typ)). def lookup_bit(byte, which_bit):; return (byte >> which_bit) & 1. def timestamp_path(base, suffix=''):; return ''.join([base, '-', datetime.datetime.now().strftime(""%Y%m%d-%H%M""), suffix]). def upper_hex(n, num_digits=None):; if num_digits is None:; return ""{0:X}"".format(n); else:; return ""{0:0{1}X}"".format(n, num_digits). def escape_str(s, backticked=False):; sb = StringIO(). rewrite_dict = {'\b': '\\b', '\n': '\\n', '\t': '\\t', '\f': '\\f', '\r': '\\r'}. for ch in s:; chNum = ord(ch); if chNum > 0x7F:; sb.write(""\\u"" + upper_hex(chNum",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:14808,extend,extend,14808,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['extend'],['extend']
Modifiability,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76454,config,configuration,76454,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['config'],['configuration']
Modifiability,"float64) – Probability.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:26675,variab,variable,26675,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"floor(x)[source]. hail.expr.functions.ceil(x)[source]. hail.expr.functions.sqrt(x)[source]. hail.expr.functions.sign(x)[source]; Returns the sign of a numeric value, array or ndarray.; Examples; >>> hl.eval(hl.sign(-1.23)); -1.0. >>> hl.eval(hl.sign([-4, 0, 5])); [-1, 0, 1]. >>> hl.eval(hl.sign([0.0, 3.14])); [0.0, 1.0]. >>> hl.eval(hl.sign(float('nan'))); nan. Notes; The sign function preserves type and maps nan to nan. Parameters:; x (NumericExpression, ArrayNumericExpression or NDArrayNumericExpression). Returns:; NumericExpression, ArrayNumericExpression or NDArrayNumericExpression. hail.expr.functions.min(*exprs, filter_missing=True)[source]; Returns the minimum element of a collection or of given numeric expressions.; Examples; Take the minimum value of an array:; >>> hl.eval(hl.min([1, 3, 5, 6, 7, 9])); 1. Take the minimum value of arguments:; >>> hl.eval(hl.min(1, 50, 2)); 1. Notes; Like the Python builtin min function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmin(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.nanmin(*exprs, filter_missing=True)[source]; Returns the minimum value of a collection or of given arguments, excluding NaN.; Examples; Compute the minimum value of an array:; >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:; >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:8889,variab,variable-length,8889,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['variab'],['variable-length']
Modifiability,"for an alternate allele is [0, 1]. Thus, with the; following variables:. .. testcode::. num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. :param int num_alleles: number of possible alternate alleles; :rtype: list of int or None; """"""; return jiterable_to_list(from_option(self._jrep.oneHotAlleles(num_alleles))). [docs] @handle_py4j; @typecheck_method(num_genotypes=integral); def one_hot_genotype(self, num_genotypes):; """"""Returns a list containing the one-hot encoded representation of the genotype call. A one-hot encoding is a vector with one '1' and many '0' values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:. .. testcode::. num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:. .. testcode::. hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotype(num_genotypes) == [0, 0, 1]. This function returns None if the genotype call is missing. :param int num_genotypes: number of possible genotypes; :rtype: list of int or None; """""". return jiterable_to_list(from_option(self._jrep.oneHotGenotype(num_genotypes))). [docs] @handle_py4j; @typecheck_method(theta=numeric); def p_ab(self, theta=0.5):; """"""Returns the p-value associated with finding the given allele depth ratio. This function uses a one-tailed binomial test. This function returns None if the allelic depth (ad) is missing. :param float theta: null reference probability for binomial model; :rtype: float; """""". return from_option(self._jrep.pAB(theta)). [docs] def fraction_reads_ref(self):; """"""R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html:6304,variab,variables,6304,docs/0.1/_modules/hail/representation/genotype.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/genotype.html,1,['variab'],['variables']
Modifiability,"form. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; hailctl dataproc; Reading from Google Cloud Storage; Requester Pays; Variant Effect Predictor (VEP). Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Google Cloud Platform. View page source. Google Cloud Platform; If you’re new to Google Cloud in general, and would like an overview, linked; here.; is a document written to onboard new users within our lab to cloud computing. hailctl dataproc; As of version 0.2.15, pip installations of Hail come bundled with a command-line; tool, hailctl. This tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:1062,config,configured,1062,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['config'],['configured']
Modifiability,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42395,config,config,42395,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,6,['config'],"['config', 'configuration']"
Modifiability,"gen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. GT (tcall) – The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; GP (tarray o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:10452,parameteriz,parameterized,10452,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability,"ghly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175582,plugin,plugin,175582,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability,"gion of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal position (1-based). Return type:int. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx usi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4390,polymorphi,polymorphism,4390,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"gs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_genome.name]; Env.backend().remove_liftover(self.name, dest_reference_genome.name). [docs] @typecheck_method(chain_file=str, dest_reference_genome=reference_genome_type); def add_l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12538,config,config,12538,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['config'],['config']
Modifiability,"h a; syntax similar to Python’s dict syntax. Struct fields are; accessed using the . syntax. In [42]:. print(hc.eval_expr_typed('{gene: ""ACBD"", function: ""LOF"", nHet: 12}')). (Struct{u'function': u'LOF', u'nHet': 12, u'gene': u'ACBD'}, Struct{gene:String,function:String,nHet:Int}). In [43]:. hc.eval_expr_typed('let s = {gene: ""ACBD"", function: ""LOF"", nHet: 12} in s.gene'). Out[43]:. (u'ACBD', String). In [44]:. hc.eval_expr_typed('let s = NA: Struct { gene: String, function: String, nHet: Int} in s.gene'). Out[44]:. (None, String). Genetic Types¶; Hail contains several genetic types: -; Variant -; Locus -; AltAllele -; Interval -; Genotype -; Call; These are designed to make it easy to manipulate genetic data. There are; many built-in functions for asking common questions about these data; types, like whether an alternate allele is a SNP, or the fraction of; reads a called genotype that belong to the reference allele. Demo variables¶; To explore these types and constructs, we have defined five; representative variables which you can access in eval_expr:. In [45]:. # 'v' is used to indicate 'Variant' in Hail; hc.eval_expr_typed('v'). Out[45]:. (Variant(contig=16, start=19200405, ref=C, alts=[AltAllele(ref=C, alt=G), AltAllele(ref=C, alt=CCC)]),; Variant). In [46]:. # 's' is used to refer to sample ID in Hail; hc.eval_expr_typed('s'). Out[46]:. (u'NA12878', String). In [47]:. # 'g' is used to refer to the genotype in Hail; hc.eval_expr_typed('g'). Out[47]:. (Genotype(GT=1, AD=[14, 0, 12], DP=26, GQ=60, PL=[60, 65, 126, 0, 67, 65]),; Genotype). In [48]:. # 'sa' is used to refer to sample annotations; hc.eval_expr_typed('sa'). Out[48]:. (Struct{u'cohort': u'1KG', u'covariates': Struct{u'PC2': -0.61512, u'PC3': 0.3166666, u'age': 34, u'PC1': 0.102312, u'isFemale': True}},; Struct{cohort:String,covariates:Struct{PC1:Double,PC2:Double,PC3:Double,age:Int,isFemale:Boolean}}). The above output is a bit wordy. Let’s try 'va':. In [49]:. # 'va' is used to refer to variant annota",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:12147,variab,variables,12147,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,2,['variab'],['variables']
Modifiability,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:8988,config,configuration,8988,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"han the min_child_ab parameter, if the depth ratio between the; proband and parents is smaller than the min_depth_ratio parameter, if; the allele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirva",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:58965,config,config,58965,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,"he collection.; collection (ArrayExpression or SetExpression) – Collection expression. Returns:; ArrayExpression or SetExpression. – Collection where each element has been transformed by f. hail.expr.functions.zip(*arrays, fill_missing=False)[source]; Zip together arrays into a single array.; Examples; >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the fill_missing parameter.; >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; The element type of the resulting array is a ttuple with a field; for each array. Parameters:. arrays (: variable-length args of ArrayExpression) – Array expressions.; fill_missing (bool) – If False, return an array with length equal to the shortest length; of the arrays. If True, return an array equal to the longest; length of the arrays, by extending the shorter arrays with missing; values. Returns:; ArrayExpression. hail.expr.functions.enumerate(a, start=0, *, index_first=True)[source]; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.enumerate(['A', 'B', 'C'])); [(0, 'A'), (1, 'B'), (2, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], start=3)); [(3, 'A'), (4, 'B'), (5, 'C')]. >>> hl.eval(hl.enumerate(['A', 'B', 'C'], index_first=False)); [('A', 0), ('B', 1), ('C', 2)]. Parameters:. a (ArrayExpression); start (Int32Expression) – The index value from which the counter is started, 0 by default.; index_first (bool) – If True, the index is the first value of the element tuples. If; False, the index is the second value. Returns:; ArrayExpression – Array of (index, element) or (element, index) tuples. hail.expr.functions.zip_with_index(a, index_first=True)[source]; Deprecated in favor of enumerate().; Returns an array of (index, element) tuples.; Examples; >>> hl.eval(hl.zip_with_index(['A',",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/collections.html:5597,extend,extending,5597,docs/0.2/functions/collections.html,https://hail.is,https://hail.is/docs/0.2/functions/collections.html,1,['extend'],['extending']
Modifiability,"he human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,; exac_amr_maf: Double,; exac_eas_allele: String,; exac_eas_maf: Double,; exac_fin_allele: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223964,plugin,plugin,223964,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['plugin'],['plugin']
Modifiability,"he probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; return _func(""ppois"", tfloat64, x, lamb, lower_tail, log_p). [docs]@typecheck(p=expr_float64, df=expr_float64, ncp=nullable(expr_float64), lower_tail=expr_bool, log_p=expr_bool); def qchisqtail(p, df, ncp=None, lower_tail=False, log_p=False) -> Float64Expression:; """"""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inverts :func:`~.pchisqtail`. Examples; --------. >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; -----; Returns right-quantile `x` for which `p` = Prob(:math:`Z^2` > x) with; :math:`Z^2` a chi-squared random variable with degrees of freedom specified; by `df`. The probability `p` must satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Corresponds to `ncp` parameter in :func:`.pchisqtail`.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pchisqtail`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""; if ncp is None:; return _func(""qchisqtail"", tfloat64, p, df, lower_tail, log_p); else:; return _func(""qnchisqtail"", tfloat64, p, df, ncp, lower_tail, log_p). [docs]@typecheck(p=expr_float64, mu=expr_float64, sigma=expr_float64, lower_tail=expr_bool, lo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:77762,variab,variable,77762,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"he variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; se",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74340,variab,variables,74340,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"heck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root and `y` a single expression, the following row-indexed; fields are added. - **<row key fields>** (Any) -- Row key fields.; - **<pass_through fields>** (Any) -- Row fields in `pass_through`.; - **n** (:py:data:`.tint32`) -- Number of columns used.; - **sum_x** (:py:data:`.tfloat64`) -- Sum of input values `x`.; - **y_transpose_x** (:py:data:`.tfloat64`) -- Dot product of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8881,variab,variable,8881,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],"['variable', 'variables']"
Modifiability,"hen(a.matches(_symbolic_regex), AlleleType.SYMBOLIC); .default(AlleleType.UNKNOWN),; AlleleType.UNKNOWN,; ),; ref,; alt,; ). @deprecated(version='0.2.129', reason=""Replaced by the public numeric_allele_type""); @typecheck(ref=expr_str, alt=expr_str); def _num_allele_type(ref, alt) -> Int32Expression:; """"""Provided for backwards compatibility, don't use it in new code, or; within the hail library itself; """"""; return numeric_allele_type(ref, alt). [docs]@typecheck(ref=expr_str, alt=expr_str); def is_snp(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a single nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.SNP. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_mnp(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a multiple nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.MNP. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_transition(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles constitute a transition. Examples; --------. >>> hl.eval(hl.is_transition('A', 'T')); False. >>> hl.eval(hl.is_transition('AAA', 'AGA')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return is_snp(ref, alt) & _is_snp_transition(ref, alt). [docs]@typecheck(ref=expr_str, alt=expr_str); def is_transversion(ref, alt) -> BooleanExpression:; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:97568,polymorphi,polymorphism,97568,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"hip_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_descent(dataset[, maf, bounded, ...]); Compute matrix of identity-by-descent estimates. king(call_expr, *[, block_size]); Compute relatedness estimates between individuals using a KING variant. pc_relate(call_expr, min_individual_maf, *); Compute relatedness estimates between individuals using a variant of the PC-Relate method. Miscellaneous. grep(regex, path[, max_count, show, force, ...]); Searches given paths for all lines con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:6684,config,config,6684,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['config'],['config']
Modifiability,"his method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified entry fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Select one existing field and compute a new one:; >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified global fields. select_rows(*exprs, **named_exprs)[source]; Select existing row fields or create new fields by name, dropping all; other non-key fields.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail.; Row keys are preserved. To drop or change a row key field, use; MatrixTable.key_rows_by(). Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over columns. For instance, the usage:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:56797,variab,variable-length,56797,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"hon3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """""". json_typ: hl.expr.HailType; data_bucket: str; data_mount: str; regions: List[str]; image: str; env: Dict[str, str]; data_bucket_is_requester_pays: bool; cloud: str; batch_run_command: List[str]; batch_run_csq_header_command: List[str]. @abc.abstractmethod; def command(; self, consequence: bool, tolerate_parse_error: bool, part_id: int, input_file: Optional[str], output_file: str; ) -> List[",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25772,config,config,25772,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"hysical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of Batch:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). The regions parameter of ServiceBackend:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). The HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. The batch/region configuration variable:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Warning; If none of the five options above are specified, your job may run in any region!. In Google Cloud Platform, the location of a multi-region bucket is considered different from any; region within that multi-region. For example, if a VM in the us-central1 region reads data from a; bucket in the us multi-region, this incurs network charges becuse us is not considered equal to; us-central1.; Container (aka Docker) images are a form of data. In Google Cloud Platform, we recommend storing; your images in a multi-regional artifact registry, which at time of writing, despite being; “multi-regional”, does not incur network charges in the manner described above. Using the UI; If you have submitted the batch above successfully, then it should open a page in your; browser with a UI page for the batch you submitted. T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:10789,variab,variable,10789,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['variab'],['variable']
Modifiability,"iately error when a job fails, rather now aligns with the ServiceBackend in running all jobs whose parents have succeeded.; (#12845) The LocalBackend now sets the working directory for dockerized jobs to the root directory instead of the temp directory. This behavior now matches ServiceBackend jobs. Version 0.2.111. (#12530) Added the ability to update an existing batch with additional jobs by calling Batch.run() more than once. The method Batch.from_batch_id(); can be used to construct a Batch from a previously submitted batch. Version 0.2.110. (#12734) PythonJob.call() now immediately errors when supplied arguments are incompatible with the called function instead of erroring only when the job is run.; (#12726) PythonJob now supports intermediate file resources the same as BashJob.; (#12684) PythonJob now correctly uses the default region when a specific region for the job is not given. Version 0.2.103. Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to; specify which cloud regions a job can run in. The default value is a job can run in any available region. Version 0.2.89. Support passing an authorization token to the ServiceBackend. Version 0.2.79. The bucket parameter in the ServiceBackend has been deprecated. Use remote_tmpdir instead. Version 0.2.75. Fixed a bug introduced in 0.2.74 where large commands were not interpolated correctly; Made resource files be represented as an explicit path in the command rather than using environment; variables; Fixed Backend.close to be idempotent; Fixed BatchPoolExecutor to always cancel all batches on errors. Version 0.2.74. Large job commands are now written to GCS to avoid Linux argument length and number limitations. Version 0.2.72. Made failed Python Jobs have non-zero exit codes. Version 0.2.71. Added the ability to set values for Job.cpu, Job.memory, Job.storage, and Job.timeout to None. Version 0.2.70. Made submitting PythonJob faster when using the ServiceBackend. Version 0",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:4082,config,configurable,4082,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['config'],['configurable']
Modifiability,"ice. Warning; To avoid substantial network costs, ensure your jobs and data reside in the same region. To execute a batch on the Batch service rather than locally, first; construct a ServiceBackend object with a billing project and; bucket for storing intermediate files. Your service account must have read; and write access to the bucket.; Next, pass the ServiceBackend object to the Batch constructor; with the parameter name backend.; An example of running “Hello World” on the Batch service rather than; locally is shown below. You can open iPython or a Jupyter notebook; and execute the following batch:; >>> import hailtop.batch as hb; >>> backend = hb.ServiceBackend('my-billing-project', remote_tmpdir='gs://my-bucket/batch/tmp/') ; >>> b = hb.Batch(backend=backend, name='test') ; >>> j = b.new_job(name='hello') ; >>> j.command('echo ""hello world""') ; >>> b.run(open=True) . You may elide the billing_project and remote_tmpdir parameters if you; have previously set them with hailctl:; hailctl config set batch/billing_project my-billing-project; hailctl config set batch/remote_tmpdir my-remote-tmpdir. Note; A trial billing project is automatically created for you with the name {USERNAME}-trial. Regions; Data and compute both reside in a physical location. In Google Cloud Platform, the location of data; is controlled by the location of the containing bucket. gcloud can determine the location of a; bucket:; gcloud storage buckets describe gs://my-bucket. If your compute resides in a different location from the data it reads or writes, then you will; accrue substantial network charges.; To avoid network charges ensure all your data is in one region and specify that region in one of the; following five ways. As a running example, we consider data stored in us-central1. The options are; listed from highest to lowest precedence. Job.regions():; >>> b = hb.Batch(backend=hb.ServiceBackend()); >>> j = b.new_job(); >>> j.regions(['us-central1']). The default_regions parameter of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:9558,config,config,9558,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,2,['config'],['config']
Modifiability,"idth); sp, sp_legend_items, sp_legend, sp_color_bar, sp_color_mappers, sp_scatter_renderers = _get_scatter_plot_elements(; sp, source_pd, _x[0], _y[0], label_cols, colors_by_col, size, hover_cols={'x', 'y'} | set(hover_fields); ). continuous_cols = [; col; for col in label_cols; if (str(source_pd.dtypes[col]).startswith('float') or str(source_pd.dtypes[col]).startswith('int')); ]; factor_cols = [col for col in label_cols if col not in continuous_cols]. # Density plots; def get_density_plot_items(; source_pd,; data_col,; p,; x_axis,; colors: Optional[Dict[str, ColorMapper]],; continuous_cols: List[str],; factor_cols: List[str],; ):; density_renderers = []; max_densities = {}; if not factor_cols or continuous_cols:; dens, edges = np.histogram(source_pd[data_col], density=True); edges = edges[:-1]; xy = (edges, dens) if x_axis else (dens, edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); line = p.line('x', 'y', source=cds); density_renderers.extend([(col, """", line) for col in continuous_cols]); max_densities = {col: np.max(dens) for col in continuous_cols}. for factor_col in factor_cols:; assert colors is not None, (colors, factor_cols); factor_colors = colors.get(factor_col, _get_categorical_palette(list(set(source_pd[factor_col])))); factor_colors = dict(zip(factor_colors.factors, factor_colors.palette)); density_data = (; source_pd[[factor_col, data_col]]; .groupby(factor_col); .apply(lambda df: np.histogram(df['x' if x_axis else 'y'], density=True)); ); for factor, (dens, edges) in density_data.iteritems():; _edges = edges[:-1]; xy = (_edges, dens) if x_axis else (dens, _edges); cds = ColumnDataSource({'x': xy[0], 'y': xy[1]}); density_renderers.append((; factor_col,; factor,; p.line('x', 'y', color=factor_colors.get(factor, 'gray'), source=cds),; )); max_densities[factor_col] = np.max([*list(dens), max_densities.get(factor_col, 0)]). p.grid.visible = False; p.outline_line_color = None; return p, density_renderers, max_densities. xp = figure(title=title, hei",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:41795,extend,extend,41795,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['extend'],['extend']
Modifiability,"ies; instead. Version 0.2.128; Released 2024-02-16; In GCP, the Hail Annotation DB and Datasets API have moved from; multi-regional US and EU buckets to regional US-CENTRAL1 and; EUROPE-WEST1 buckets. These buckets are requester pays which means; unless your cluster is in the US-CENTRAL1 or EUROPE-WEST1 region, you; will pay a per-gigabyte rate to read from the Annotation DB or Datasets; API. We must make this change because reading from a multi-regional; bucket into a regional VM is no longer; free.; Unfortunately, cost constraints require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:15926,config,config,15926,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['config']
Modifiability,"if isinstance(obj, StructExpression):; return 'StructExpression', StructExpression, struct_error(obj), True; elif isinstance(obj, ArrayStructExpression):; return 'ArrayStructExpression', ArrayStructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.form",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8318,inherit,inherited,8318,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['inherit'],['inherited']
Modifiability,"ifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4289,polymorphi,polymorphism,4289,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ig: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37954,config,config,37954,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"il selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name (str) – A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master (str, optional) – Spark Backend only. URL identifying the Spark leader (master) node or local[N] for local; clusters.; local (str) – Spark Backend only. Local-mode core limit indicator. Must either be local[N] where N is a; positive integer or local[*]. The latter indicates Spark should use all cores; available. local[*] does not respect most containerization CPU limits. This option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:3484,config,config,3484,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,2,['config'],['config']
Modifiability,"il/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large :math:`n`, we recommend using a high-memory configuration such as ``highmem`` workers. **Linear mixed model**. :py:meth:`.lmmreg` estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model, and then optionally tests each variant for association using the likelihood ratio test. Inference is exact. We first describe the sample-covariates-only model used to estimate heritability, which we simply refer to as the *global model*. With :math:`n` samples and :math:`c` sample covariates, we define:. - :math:`y = n \\times 1` vector of phenotypes; - :math:`X = n \\times c` matrix of sample covariates and intercept column of ones; - :math:`K = n \\times n` kinship matrix; - :math:`I = n \\times n` identity matrix; - :math:`\\beta = c \\times 1` vector of covariate coefficients; - :math:`\sigma_g^2 =` coefficient of genetic variance component :math:`K`; - :math:`\sigma_e^2 =` coefficient of environmental variance component :math:`I`; - :math:`\delta = \\frac{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:126441,config,configuration,126441,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['config'],['configuration']
Modifiability,"il’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS envir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1740,config,configuration,1740,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['config'],['configuration']
Modifiability,"imum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on \(\mathrm{ln}(\delta)\) of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid.; Note that \(h^2\) is related to \(\mathrm{ln}(\delta)\) through the sigmoid function. More precisely,. \[h^2 = 1 - \mathrm{sigmoid}(\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\mathrm{ln}(\delta))\]; Hence one can change variables to extract a high-resolution discretization of the likelihood function of \(h^2\) over \([0,1]\) at the corresponding REML estimators for \(\beta\) and \(\sigma_g^2\), as well as integrate over the normalized likelihood function using change of variables and the sigmoid differential equation.; For convenience, global.lmmreg.fit.normLkhdH2 records the the likelihood function of \(h^2\) normalized over the discrete grid 0.01, 0.02, ..., 0.98, 0.99. The length of the array is 101 so that index i contains the likelihood at percentage i. The values at indices 0 and 100 are left undefined.; By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of \(h^2\) as follows. Let \(x_2\) be the maximum likelihood estimate of \(h^2\) and let \(x_ 1\) and \(x_3\) be just to the left and right of \(x_2\). Let \(y_1\), \(y_2\), and \(y_3\) be the corresponding values of the (unnormalized) log likelihood function. Setting equal the le",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:102853,variab,variables,102853,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['variab'],['variables']
Modifiability,"ing to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting all alleles other than i to ref; Examples; Preserve the third allele and downcode all other alleles to reference.; >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters:. c (CallExpression) – A call.; i (Expression of type tint32) – The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns:; CallExpression. hail.expr.functions.triangle(n)[source]; Returns the triangle number of n.; Examples; >>> hl.eval(hl.triangle(3)); 6. Notes; The calculation is n * (n + 1) / 2. Parameters:; n (Expression of type tint32). Returns:; Expression of type tint32. hail.expr.functions.is_snp(ref, alt)[source]; Returns True if the alleles constitute a single nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_mnp(ref, alt)[source]; Returns True if the alleles constitute a multiple nucleotide polymorphism.; Examples; >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transition(ref, alt)[source]; Returns True if the alleles constitute a transition.; Examples; >>> hl.eval(hl.is_transition('A', 'T')); False. >>> hl.eval(hl.is_transition('AAA', 'AGA')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_transversion(ref, alt)[source]; Returns True if the alleles constitute a transversion.; Examples",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:12328,polymorphi,polymorphism,12328,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"ingular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:39272,config,configuration-dependent,39272,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['config'],['configuration-dependent']
Modifiability,"ion; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:1155,variab,variable,1155,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],['variable']
Modifiability,"ion=False,; local_tmpdir=None,; _optimizer_iterations=None,; *,; backend: Optional[BackendType] = None,; driver_cores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buck",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7089,variab,variable,7089,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['variab'],['variable']
Modifiability,"ion` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _bit_op(x, y, '|'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_oneof(expr_int32, expr_int64)); def bit_xor(x, y):; """"""Bitwise exclusive-or `x` and `y`. Examples; --------; >>> hl.eval(hl.bit_xor(5, 3)); 6. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _bit_op(x, y, '^'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32); def bit_lshift(x, y):; """"""Bitwise left-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""; return _shift_op(x, y, '<<'). [docs]@typecheck(x=expr_oneof(expr_int32, expr_int64), y=expr_int32, logical=builtins.bool); def bit_rshift(x, y, logical=False):; """"""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:180505,extend,extended,180505,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['extend'],['extended']
Modifiability,"ique genotypes possible for this variant. For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1. For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2. For a variant with N alleles, this value is:. .. math::. \\frac{N * (N + 1)}{2}. :rtype: int"""""". return self._jrep.nGenotypes(). [docs] def locus(self):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5259,polymorphi,polymorphism,5259,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112044,config,config,112044,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['config'],['config']
Modifiability,"is batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.cpu().; default_storage (Union[str, int, None]) – Storage setting to use b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:2037,config,configurations,2037,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['config'],['configurations']
Modifiability,"is function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local pat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7987,config,config,7987,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,4,['config'],['config']
Modifiability,"is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not avail",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:5690,plugin,plugin,5690,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['plugin'],['plugin']
Modifiability,"is tool has a submodule called dataproc for working with; Google Dataproc clusters configured for Hail.; This tool requires the Google Cloud SDK.; Until full documentation for the command-line interface is written, we encourage; you to run the following command to see the list of modules:; hailctl dataproc. It is possible to print help for a specific command using the help flag:; hailctl dataproc start --help. To start a cluster, use:; hailctl dataproc start CLUSTER_NAME [optional args...]. To submit a Python job to that cluster, use:; hailctl dataproc submit CLUSTER_NAME SCRIPT [optional args to your python script...]. To connect to a Jupyter notebook running on that cluster, use:; hailctl dataproc connect CLUSTER_NAME notebook [optional args...]. To list active clusters, use:; hailctl dataproc list. Importantly, to shut down a cluster when done with it, use:; hailctl dataproc stop CLUSTER_NAME. Reading from Google Cloud Storage; A dataproc cluster created through hailctl dataproc will automatically be configured to allow hail to read files from; Google Cloud Storage (GCS). To allow hail to read from GCS when running locally, you need to install the; Cloud Storage Connector. The easiest way to do that is to; run the following script from your command line:; curl -sSL https://broad.io/install-gcs-connector | python3. After this is installed, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:1999,config,configured,1999,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['config'],['configured']
Modifiability,"it multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:102008,config,configuration,102008,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; class_name, word, ', '.join(""'{}'"".format(m) for m in inherited_matches); ); ); elif has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def get_nice_field_error(obj, item):; class_name, _, handler, has_describe = get_obj_metadata(obj). field_names = obj._fields.keys(); dd = defaultdict(lambda: []); for f in field_names:; dd[f.lower()].append(f). item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, dd, n=5). s = [""{} instance has no field '{}'"".format(class_name, item)]; if field_matches:; s.append('\n Did you mean:'); for f in field_matches:; for orig_f in dd[f]:; s.append(""\n {}"".format(handler(orig_f))); if has_describe:; s.append(""\n Hint: use 'describe()' to show the names of all data fields.""); return ''.join(s). def check_collisions(caller, names, indices, override_protected_indices=None):; from hail.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:9413,inherit,inherited,9413,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['inherit'],['inherited']
Modifiability,"itten as “NA” in export modules. The empirical standard deviation is computed; with zero degrees of freedom. Parameters:root (str) – Variant annotation root for computed struct. Returns:Annotated variant dataset with new variant QC annotations. Return type:VariantDataset. variant_schema¶; Returns the signature of the variant annotations contained in this VDS.; Examples; >>> print(vds.variant_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174255,plugin,plugin,174255,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability,"ity as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:2158,variab,variables,2158,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['variab'],['variables']
Modifiability,"ixTable with specified global fields. select_rows(*exprs, **named_exprs)[source]; Select existing row fields or create new fields by name, dropping all; other non-key fields.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail.; Row keys are preserved. To drop or change a row key field, use; MatrixTable.key_rows_by(). Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over columns. For instance, the usage:; >>> dataset_result = dataset.select_rows(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per row. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified row fields. semi_join_cols(other)[source]; Filters the matrix table to columns whose key appears in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The column key type of the matrix table must match the key type of other.; This method does not change the schema of the matrix table; it is a; filtering the matrix table to column keys not present in another table.; To discard collumns whose key is present in other, use; anti_join_cols().; Examples; >>> ds_result = ds.semi_join_cols(cols_to_keep). It may be inconvenient to key the matrix table by the right-side key.; In this case, it is possible to implement a semi-join using a non-key; field as follows:; >>> ds_result = ds.filter_cols(hl.is_defined(cols_to_keep.index(ds['s']))). S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:57982,variab,variable-length,57982,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"kend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" back",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11652,config,config,11652,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['config']
Modifiability,"kend, s will always run before t.; >>> b = hb.Batch(name='hello-serial'); >>> s = b.new_job(name='j1'); >>> s.command(f'echo ""hello world"" > {s.ofile}'); >>> t = b.new_job(name='j2'); >>> t.command(f'cat {s.ofile}'); >>> b.run(). Scatter / Gather; Batch is implemented in Python making it easy to use for loops; to create more complicated dependency graphs between jobs. A scatter; is a set of jobs with the same command but varying input parameters. A gather; is a final job or “sink” that waits for all of the jobs in the scatter to be complete; before executing.; In the example below, we use a for loop to create a job for each one of; ‘Alice’, ‘Bob’, and ‘Dan’ that prints the name of the user programatically; thereby scattering the echo command over users.; >>> b = hb.Batch(name='scatter'); >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); >>> b.run(). In the previous example, we did not assign the jobs we created for each; user to a unique variable name and instead named it j each time in the; for loop. However, if we want to add a final gather job (sink) that depends on the; completion of all user jobs, then we need to keep track of all of the user; jobs so we can use the Job.depends_on() method to explicitly link; the sink job to be dependent on the user jobs, which are stored in the; jobs array. The single asterisk before jobs is used in Python to have; all elements in the array be treated as separate input arguments to the function,; in this case Job.depends_on(). >>> b = hb.Batch(name='scatter-gather-1'); >>> jobs = []; >>> for name in ['Alice', 'Bob', 'Dan']:; ... j = b.new_job(name=name); ... j.command(f'echo ""hello {name}""'); ... jobs.append(j); >>> sink = b.new_job(name='sink'); >>> sink.command(f'echo ""I wait for everyone""'); >>> sink.depends_on(*jobs); >>> b.run(). Now that we know how to create a sink job that depends on an arbitrary; number of jobs, we want to have the outputs of each of the per",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:6512,variab,variable,6512,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['variab'],['variable']
Modifiability,"keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; batch_run_command (list of str) – The command line to run for a VEP job for a partition.; batch_run_csq_header_command (list of str) – The command line to run when generating the consequence header.; env (dict of str to str) – A map of environment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3777,config,configuring,3777,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuring']
Modifiability,"l-python.zip:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"" \; ipython. Within the interactive shell, check that you can create a; HailContext by running the following commands. Note that you have to pass in; the existing SparkContext instance sc to the HailContext; constructor. >>> from hail import *; >>> hc = HailContext(). Files can be accessed from both Hadoop and Google Storage. If you’re running on Google’s Dataproc, you’ll want to store your files in Google Storage. In most on premises clusters, you’ll want to store your files in Hadoop.; To convert sample.vcf stored in Google Storage into Hail’s .vds format, run:. >>> hc.import_vcf('gs:///path/to/sample.vcf').write('gs:///output/path/sample.vds'). To convert sample.vcf stored in Hadoop into Hail’s .vds format, run:. >>> hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). It is also possible to run Hail non-interactively, by passing a Python script to; spark-submit. In this case, it is not necessary to set any environment; variables.; For example,. $ spark-submit --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; hailscript.py. runs the script hailscript.py (which reads and writes files from Hadoop):. import hail; hc = hail.HailContext(); hc.import_vcf('/path/to/sample.vcf').write('/output/path/sample.vds'). Running on a Cloudera Cluster¶; These instructions; explain how to install Spark 2 on a Cloudera cluster. You should work on a; gateway node on the cluster that has the Hadoop and Spark packages installed on; it.; Once Spark is installed, building and running Hail on a Cloudera cluster is exactly; the same as above, except:. On a Cloudera cluster, when building a Hail JAR, you must specify a Cloudera version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloude",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:4986,variab,variables,4986,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,1,['variab'],['variables']
Modifiability,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10003,config,configuration,10003,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"ld.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log stat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:84607,config,configuration,84607,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"le into a Table. import_gen(path[, sample_file, tolerance, ...]); Import GEN file(s) as a MatrixTable. import_locus_intervals(path[, ...]); Import a locus interval list as a Table. import_matrix_table(paths[, row_fields, ...]); Import tab-delimited file(s) as a MatrixTable. import_plink(bed, bim, fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call conc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:3440,variab,variable,3440,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['variab'],['variable']
Modifiability,"le, use an expression that defines the columns of the output file. Multiple columns are separated by commas. Export the variant name v, the PASS annotation va.pass, and the mean GQ annotation va.gqStats.mean to a TSV file. There will be one line per variant and the output for the variant column v will be of the form contig:start:ref:alt. No header line will be present!!. v, va.pass, va.gqStats.mean. Same as above but include a header with the column names “Variant”, “PASS”, and “MeanGQ”. Variant = v, PASS = va.pass, MeanGQ = va.gqStats.mean. Export the sample name s, a sample annotation for the number of het calls sa.nHet, and a sample annotation for case status sa.pheno.isCase. There will be one line per sample. The header line will be “Sample”, “nHet”, and “Phenotype”. Sample = s, nHet = sa.nHet, Phenotype = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:4449,variab,variables,4449,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,2,['variab'],"['variable', 'variables']"
Modifiability,"le_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. Entry Fields. GT (tcall) – The hard call corresponding to the genotype with; the highest probability.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the tolerance; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:17522,parameteriz,parameterized,17522,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability,"lele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched bases in this alternate allele. Fails if the ref and alt alleles are not the same length. :rtype: int; """""". return self._jrep.nMismatch(). [docs] def stripped_snp(self):; """"""Returns the one-character reduced SNP. Fails if called on an alternate allele that is not a SNP. :rtype: str, str; """""". r = self._jrep.strippedSNP(); return r._1(), r._2(). [docs] def is_SNP(self):; """"""True if this alternate allele is a single nucleotide polymorphism (SNP). :rtype: bool; """""". return self._jrep.isSNP(). [docs] def is_MNP(self):; """"""True if this alternate allele is a multiple nucleotide polymorphism (MNP). :rtype: bool; """""". return self._jrep.isMNP(). [docs] def is_insertion(self):; """"""True if this alternate allele is an insertion of one or more bases. :rtype: bool; """""". return self._jrep.isInsertion(). [docs] def is_deletion(self):; """"""True if this alternate allele is a deletion of one or more bases. :rtype: bool; """""". return self._jrep.isDeletion(). [docs] def is_indel(self):; """"""True if this alternate allele is either an insertion or deletion of one or more bases. :rtype: bool; """""". return self._jrep.isIndel(). [docs] def is_complex(self):; """"""True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. :rtype: bool; """""". return self._jrep.isComplex(). [docs] def is_transition(self):; """"""True if this alternate allele is a transition SNP. This is tr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:7369,polymorphi,polymorphism,7369,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"leotide variants for which both; individuals \(i\) and \(j\) have a non-missing genotype.; \(X_{i,s}\) be the genotype score matrix. Each entry corresponds to; the genotype of individual \(i\) at variant; \(s\). Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. \(X_{i,s}\) is calculated by invoking; n_alt_alleles() on the call_expr. The three counts above, \(N^{Aa}\), \(N^{Aa,Aa}\), and; \(N^{AA,aa}\), exclude variants where one or both individuals have; missing genotypes.; In terms of the symbols above, we can define \(d\), the genetic distance; between two samples. We can interpret \(d\) as an unnormalized; measurement of the genetic material not shared identically-by-descent:. \[d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2\]; In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. \((X_{i,s} - X_{j,s})^2\); homref; het; homalt. homref; 0; 1; 4. het; 1; 0; 1. homalt; 4; 1; 0. which leads to this expression for genetic distance:. \[d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}\]; The first term, \(4 N^{AA,aa}_{i,j}\), accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case of a pair of heteroyzgous genotypes. We offset this; with the fourth and final term.; The genetic distance, \(d_{i,j}\), ranges between zero and four times; the number of variants in the dataset. In the supplement to Manichaikul,; et. al, the authors demonstrate that the kinship coefficient,; \(\phi_{i,j}\), between two individuals from the same population is; rel",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:7191,config,configurations,7191,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['config'],['configurations']
Modifiability,"les so that no two have a PI_HAT value greater than or equal to 0.6.; >>> pruned_vds = vds.ibd_prune(0.6). Prune samples so that no two have a PI_HAT value greater than or equal to 0.5, with a tiebreaking expression that ; selects cases over controls:; >>> pruned_vds = vds.ibd_prune(; ... 0.5,; ... tiebreaking_expr=""if (sa1.isCase && !sa2.isCase) -1 else if (!sa1.isCase && sa2.isCase) 1 else 0""). Notes; The variant dataset returned may change in near future as a result of algorithmic improvements. The current algorithm is very efficient on datasets with many small; families, less so on datasets with large families. Currently, the algorithm works by deleting the person from each family who has the highest number of relatives,; and iterating until no two people have a PI_HAT value greater than that specified. If two people within a family have the same number of relatives, the tiebreaking_expr; given will be used to determine which sample gets deleted.; The tiebreaking_expr namespace has the following variables available:. s1: The first sample id.; sa1: The annotations associated with s1.; s2: The second sample id.; sa2: The annotations associated with s2. The tiebreaking_expr returns an integer expressing the preference for one sample over the other. Any negative integer expresses a preference for keeping s1. Any positive integer expresses a preference for keeping s2. A zero expresses no preference. This function must induce a preorder on the samples, in particular:. tiebreaking_expr(sample1, sample2) must equal -1 * tie breaking_expr(sample2, sample1), which evokes the common sense understanding that if x < y then y > x`.; tiebreaking_expr(sample1, sample1) must equal 0, i.e. x = x; if sample1 is preferred to sample2 and sample2 is preferred to sample3, then sample1 must also be preferred to sample3. The last requirement is only important if you have three related samples with the same number of relatives and all three are related to one another. In cases like this ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:69462,variab,variables,69462,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['variab'],['variables']
Modifiability,"les; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greeti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:1253,variab,variable,1253,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"lf):; """"""Returns the locus object for this polymorphism. :rtype: :class:`.Locus`; """"""; return Locus._from_java(self._jrep.locus()). [docs] def is_autosomal_or_pseudoautosomal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5583,polymorphi,polymorphism,5583,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"lf,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28446,plugin,plugin,28446,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['plugin'],['plugin']
Modifiability,"lf.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6938,config,configuration,6938,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"lf.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30633,plugin,plugin,30633,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,3,"['Plugin', 'plugin']","['Plugins', 'plugin']"
Modifiability,"licate field {k!r}""). def get_key_by_exprs(caller, exprs, named_exprs, indices, override_protected_indices=None):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}. bindings = []. def is_top_level_field(e):; return e in indices.source._fields_inverse. existing_key_fields = []; final_key = []; for e in exprs:; analyze(caller, e, indices, broadcast=False); if not e._ir.is_nested_field:; raise ExpressionException(; f""{caller!r} expects keyword arguments for complex expressions\n""; f"" Correct: ht = ht.key_by('x')\n""; f"" Correct: ht = ht.key_by(ht.x)\n""; f"" Correct: ht = ht.key_by(x = ht.x.replace(' ', '_'))\n""; f"" INCORRECT: ht = ht.key_by(ht.x.replace(' ', '_'))""; ). name = e._ir.name; final_key.append(name). if not is_top_level_field(e):; bindings.append((name, e)); else:; existing_key_fields.append(name). final_key.extend(named_exprs); bindings.extend(named_exprs.items()); check_collisions(caller, final_key, indices, override_protected_indices=override_protected_indices); return final_key, dict(bindings). def check_keys(caller, name, protected_key):; from hail.expr.expressions import ExpressionException. if name in protected_key:; msg = (; f""{caller!r}: cannot overwrite key field {name!r} with annotate, select or drop; ""; f""use key_by to modify keys.""; ); error('Analysis exception: {}'.format(msg)); raise ExpressionException(msg). def get_select_exprs(caller, exprs, named_exprs, indices, base_struct):; from hail.expr.expressions import ExpressionException, analyze, to_expr. exprs = [indices.source[e] if isinstance(e, str) else e for e in exprs]; named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; select_fields = indices.protected_key[:]; protected_key = set(select_fields); insertions = {}. final_fields = select_fields[:]. def is_top_level_field(e):; return e in indices.source._fields_inverse. for e i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:12114,extend,extend,12114,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['extend'],['extend']
Modifiability,"linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query records from a table corresponding to a given point or range of keys. hail.expr.functions.literal(x, dtype=None)[source]; Captures and broadcasts a Python variable or object as an expression.; Examples; >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:1328,variab,variable,1328,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38882,config,configuration,38882,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,"['config', 'plugin']","['configuration', 'plugin']"
Modifiability,"lled, you’ll be able to read from paths beginning with gs directly from you laptop. Requester Pays; Some google cloud buckets are Requester Pays, meaning; that accessing them will incur charges on the requester. Google breaks down the charges in the linked document,; but the most important class of charges to be aware of are Network Charges.; Specifically, the egress charges. You should always be careful reading data from a bucket in a different region; then your own project, as it is easy to rack up a large bill. For this reason, you must specifically enable; requester pays on your hailctl dataproc cluster if you’d like to use it.; To allow your cluster to read from any requester pays bucket, use:; hailctl dataproc start CLUSTER_NAME --requester-pays-allow-all. To make it easier to avoid accidentally reading from a requester pays bucket, we also have; --requester-pays-allow-buckets. If you’d like to enable only reading from buckets named; hail-bucket and big-data, you can specify the following:; hailctl dataproc start my-cluster --requester-pays-allow-buckets hail-bucket,big-data. Users of the Annotation Database will find that many of the files are stored in requester pays buckets.; In order to allow the dataproc cluster to read from them, you can either use --requester-pays-allow-all from above; or use the special --requester-pays-allow-annotation-db to enable the specific list of buckets that the annotation database; relies on. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl dataproc start NAME --vep GRCh37. Hail also supports VEP for GRCh38 variants, but you must start a cluster with; the argument --vep GRCh38. A cluster started without the --vep argument is; unable to run VEP and cannot be modified to run VEP. You must start a new; cluster using --vep. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/google_cloud.html:3850,config,configuration,3850,docs/0.2/cloud/google_cloud.html,https://hail.is,https://hail.is/docs/0.2/cloud/google_cloud.html,1,['config'],['configuration']
Modifiability,"llo”:; >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:; >>> p.run(). Require all jobs in this batch to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; The methods Batch.read_input() and Batch.read_input_group(); are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in.; Files generated by executing a job are temporary files and must be written; to a permanent location using the method Batch.write_output(). Parameters:. name (Optional[str]) – Name of the batch.; backend (Union[LocalBackend, ServiceBackend, None]) – Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either local or service, and will result in the use of a; LocalBackend and ServiceBackend respectively. If no; argument is given and no configurations are set, the default is; LocalBackend.; attributes (Optional[Dict[str, str]]) – Key-value pairs of additional attributes. ‘name’ is not a valid keyword.; Use the name argument instead.; requester_pays_project (Optional[str]) – The name of the Google project to be billed when accessing requester pays buckets.; default_image (Optional[str]) – Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is latest).; default_memory (Union[str, int, None]) – Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the LocalBackend; or the ServiceBackend. See Job.memory().; default_cpu (Union[str, int, float, None]) – CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the Local",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html:1869,config,configurations,1869,docs/batch/api/batch/hailtop.batch.batch.Batch.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.batch.Batch.html,2,['config'],['configurations']
Modifiability,"loid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:2525,polymorphi,polymorphism,2525,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"loid_gt_index_call(gt_index); Construct an unphased, diploid call from a genotype index. parse_call(s); Construct a call expression by parsing a string or string expression. downcode(c, i); Create a new call by setting all alleles other than i to ref. triangle(n); Returns the triangle number of n. is_snp(ref, alt); Returns True if the alleles constitute a single nucleotide polymorphism. is_mnp(ref, alt); Returns True if the alleles constitute a multiple nucleotide polymorphism. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/index.html:14270,polymorphi,polymorphism,14270,docs/0.2/functions/index.html,https://hail.is,https://hail.is/docs/0.2/functions/index.html,1,['polymorphi'],['polymorphism']
Modifiability,"lt PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: Stri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175825,config,configuration,175825,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['config'],['configuration']
Modifiability,"lt is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """""". _valid_key_properties: ClassVar = {'gene', 'unique'}; _valid_regions: ClassVar = {'us', 'us-central1', 'europe-west1'}; _valid_clouds: ClassVar = {'gcp', 'aws'}; _valid_combinations: ClassVar = {('us', 'aws'), ('us-central1', 'gcp'), ('europe-west1', 'gcp')}. def __init__(; self,; *,; region: str = 'us-central1',; cloud: str = 'gcp',; url: Optional[str] = None,; config: Optional[dict] = None,; ):; if region not in DB._valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid regions are {DB._valid_regions}.'; ); if cloud not in DB._valid_clouds:; raise ValueError(; f'Specify valid cloud parameter,'; f' received: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:10838,config,config,10838,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['config'],['config']
Modifiability,"mal(self):; """"""True if this polymorphism is found on an autosome, or the PAR on X or Y. :rtype: bool; """"""; return self._jrep.isAutosomalOrPseudoAutosomal(). [docs] def is_autosomal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5754,polymorphi,polymorphism,5754,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"max`, :func:`min`, :func:`nanmin`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """""". return _comparison_func('max', exprs, filter_missing, filter_nan=True). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def max(*exprs, filter_missing: builtins.bool = True) -> NumericExpression:; """"""Returns the maximum element of a collection or of given numeric expressions. Examples; --------. Take the maximum value of an array:. >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:. >>> hl.eval(hl.max(1, 50, 2)); 50. Notes; -----; Like the Python builtin ``max`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the maximum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. If any element or argument is `NaN`, then the result is `NaN`. See Also; --------; :func:`nanmax`, :func:`min`, :func:`nanmin`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """"""; return _comparison_func('max', exprs, filter_missing, filter_nan=False). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def nanmin(*",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:127073,variab,variable-length,127073,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:38244,config,configuration,38244,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"mentation of a structured matrix. hail.GroupedMatrixTable; Matrix table grouped by row or column that can be aggregated into a new matrix table. Modules. expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2269,config,configuration,2269,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability,"mericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return _func(""sqrt"", tfloat64, x). [docs]@typecheck(x=expr_array(expr_float64), y=expr_array(expr_float64)); def corr(x, y) -> Float64Expression:; """"""Compute the; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between `x` and `y`. Examples; --------; >>> hl.eval(hl.corr([1, 2, 4], [2, 3, 1])); -0.6546536707079772. Notes; -----; Only indices where both `x` and `y` are non-missing will be included in the; calculation. If `x` and `y` have length zero, then the result is missing. Parameters; ----------; x : :class:`.Expression` of type ``array<tfloat64>``; y : :class:`.Expression` of type ``array<tfloat64>``. Returns; -------; :class:`.Float64Expression`; """"""; return _func(""corr"", tfloat64, x, y). [docs]@typecheck(ref=expr_str, alt=expr_str); @ir.udf(tstr, tstr); def numeric_allele_type(ref, alt) -> Int32Expression:; """"""Returns the type of the polymorphism as an integer. The value returned; is the integer value of :class:`.AlleleType` representing that kind of; polymorphism. Examples; --------. >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; -----; The values of :class:`.AlleleType` are not stable and thus should not be; relied upon across hail versions.; """"""; _base_regex = ""^([ACGTNM])+$""; _symbolic_regex = r""(^\.)|(\.$)|(^<)|(>$)|(\[)|(\])""; return hl.bind(; lambda r, a: hl.if_else(; r.matches(_base_regex),; hl.case(); .when(; a.matches(_base_regex),; hl.case(); .when(; r.length() == a.length(),; hl.if_else(; r.length() == 1,; hl.if_else(r != a, AlleleType.SNP, AlleleType.UNKNOWN),; hl.if_else(hamming(r, a) == 1, AlleleType.SNP, AlleleType.MNP),; ),; ); .when((r.length() < a.length()) & (r[0] == a[0]) & a.endswith(r[1:]), AlleleType.INSERTION); .when((r[0] == a[0]) & r.endswith(a[1:]), AlleleType.DELETION); .default(AlleleType.COMPLEX),; ); .when(a == '*', A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:95512,polymorphi,polymorphism,95512,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"method; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,...ALTN. Below is an example of; each:. >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :rtype: :class:`.Variant`; """"""; jrep = scala_object(Env.hail().variant, 'Variant').parse(string); return Variant._from_java(jrep). @property; def contig(self):; """"""; Chromosome identifier. :rtype: str; """"""; return self._contig. @property; def start(self):; """"""; Chromosomal position (1-based). :rtype: int; """"""; return self._start. @property; def ref(self):; """"""; Reference allele at this locus. :rtype: str; """""". return self._ref. @property; def alt_alleles(self):; """"""; List of alternate allele objects in this polymorphism. :rtype: list of :class:`.AltAllele`; """"""; return self._alt_alleles. [docs] def num_alt_alleles(self):; """"""Returns the number of alternate alleles in this polymorphism. :rtype: int; """""". return self._jrep.nAltAlleles(). [docs] def is_biallelic(self):; """"""True if there is only one alternate allele in this polymorphism. :rtype: bool; """""". return self._jrep.isBiallelic(). [docs] def alt_allele(self):; """"""Returns the alternate allele object, assumes biallelic. Fails if called on a multiallelic variant. :rtype: :class:`.AltAllele`; """""". return AltAllele._from_java(self._jrep.altAllele()). [docs] def alt(self):; """"""Returns the alternate allele string, assumes biallelic. Fails if called on a multiallelic variant. :rtype: str; """""". return self._jrep.alt(). [docs] def num_alleles(self):; """"""Returns the number of total alleles in this polymorphism, including the reference. :rtype: int; """""". return self._jrep.nAlleles(). [docs] @handle_py4j; @typeche",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:2690,polymorphi,polymorphism,2690,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"miter='\\\\s+',; missing='NA',; quant_pheno=False,; a2_reference=True,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; n_partitions=None,; block_size=None,; ) -> MatrixTable:; """"""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9""",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:86597,parameteriz,parameterized,86597,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability,"mmunity Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl.allele_type('a', 'att')); True; """"""; return _ALLELE_S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:952,Polymorphi,Polymorphism,952,docs/0.2/_modules/hail/genetics/allele_type.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html,3,['Polymorphi'],['Polymorphism']
Modifiability,"mulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`.; a : :class:`.ArrayExpression`. Returns; -------; :class:`.ArrayExpression`.; """"""; return a.scan(lambda x, y: f(x, y), zero). @typecheck(streams=expr_stream(), fill_missing=bool); def _zip_streams(*streams, fill_missing: bool = False) -> StreamExpression:; n_streams = builtins.len(streams); uids = [Env.get_uid() for _ in builtins.range(n_streams)]; types = [stream._type.element_type for stream in streams]; body_ir = ir.MakeTuple([ir.Ref(uid, type) for uid, type in builtins.zip(uids, types)]); indices, aggregations = unify_all(*streams); behavior = 'ExtendNA' if fill_missing else 'TakeMinLength'; return construct_expr(; ir.StreamZip([s._ir for s in streams], uids, body_ir, behavior),; tstream(ttuple(*(s.dtype.element_type for s in streams))),; indices,; aggregations,; ). [docs]@typecheck(arrays=expr_array(), fill_missing=bool); def zip(*arrays, fill_missing: bool = False) -> ArrayExpression:; """"""Zip together arrays into a single array. Examples; --------. >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the `fill_missing` parameter. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:115833,Extend,ExtendNA,115833,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Extend'],['ExtendNA']
Modifiability,"my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2981,config,configuration,2981,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,4,['config'],"['config', 'configuration']"
Modifiability,"n evaluated before aborting.; n_iterations : Int32Expression. The number of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:21684,variab,variable,21684,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"n of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3831,polymorphi,polymorphism,3831,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"n released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memory and storage request strings may now be optionally terminated with a B for bytes.; (#13051) Azure Blob Storage https URLs are now supported. Version 0.2.115. (#12731) Introduced hailtop.fs that makes public a filesystem module that works for local fs, gs, s3 and abs. This can be used by import hailtop.fs as hfs.; (#12918) ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1539,config,configuration,1539,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['config'],['configuration']
Modifiability,"n_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the total number of unique genotypes possible for this variant.; For a biallelic variant, this value is 3: 0/0, 0/1, and 1/1.; For a triallelic variant, this value is 6: 0/0, 0/1, 1/1, 0/2, 1/2, 2/2.; For a variant with N alleles, this value is:. \[\frac{N * (N + 1)}{2}\]. Return type:int. static parse(string)[source]¶; Parses a variant object from a string.; There are two acceptable formats: CHR:POS:REF:ALT, and; CHR:POS:REF:ALT1,ALT2,…ALTN. Below is an example of; each:; >>> v_biallelic = Variant.parse('16:20012:A:TT'); >>> v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). Return type:Variant. ref¶; Reference allele at this locus. Return type:str. start¶; Chromosomal position (1-based). Return type:int. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:4522,polymorphi,polymorphism,4522,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"n`, :func:`max`, :func:`nanmax`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing minimum. Returns; -------; :class:`.NumericExpression`; """""". return _comparison_func('min', exprs, filter_missing, filter_nan=True). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def min(*exprs, filter_missing: builtins.bool = True) -> NumericExpression:; """"""Returns the minimum element of a collection or of given numeric expressions. Examples; --------. Take the minimum value of an array:. >>> hl.eval(hl.min([1, 3, 5, 6, 7, 9])); 1. Take the minimum value of arguments:. >>> hl.eval(hl.min(1, 50, 2)); 1. Notes; -----; Like the Python builtin ``min`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the minimum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. If any element or argument is `NaN`, then the result is `NaN`. See Also; --------; :func:`nanmin`, :func:`max`, :func:`nanmax`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing minimum. Returns; -------; :class:`.NumericExpression`; """"""; return _comparison_func('min', exprs, filter_missing, filter_nan=False). [docs]@typecheck(x=expr_oneof(expr_numeric, expr_array(expr_numeric), expr_ndarray(expr_numeric))); def abs(x):; """"""Take the absolute value of a n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:129999,variab,variable-length,129999,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"na(type_and_data['data']), typ, key=['id']). [docs]@typecheck(regex=str, path=oneof(str, sequenceof(str)), max_count=int, show=bool, force=bool, force_bgz=bool); def grep(regex, path, max_count=100, *, show: bool = True, force: bool = False, force_bgz: bool = False):; r""""""Searches given paths for all lines containing regex matches. Examples; --------. Print all lines containing the string ``hello`` in *file.txt*:. >>> hl.grep('hello','data/file.txt'). Print all lines containing digits in *file1.txt* and *file2.txt*:. >>> hl.grep('\\d', ['data/file1.txt','data/file2.txt']). Notes; -----; :func:`.grep` mimics the basic functionality of Unix ``grep`` in; parallel, printing results to the screen. This command is provided as a; convenience to those in the statistical genetics community who often; search enormous text files like VCFs. Hail uses `Java regular expression; patterns; <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html>`__.; The `RegExr sandbox <http://regexr.com/>`__ may be helpful. Parameters; ----------; regex : :class:`str`; The regular expression to match.; path : :class:`str` or :obj:`list` of :obj:`str`; The files to search.; max_count : :obj:`int`; The maximum number of matches to return; show : :obj:`bool`; When `True`, show the values on stdout. When `False`, return a; dictionary mapping file names to lines.; force_bgz : :obj:`bool`; If ``True``, read files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, read gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; ---; :obj:`dict` of :class:`str` to :obj:`list` of :obj:`str`; """"""; from hail.backend.spark_backend import S",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:35608,sandbox,sandbox,35608,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['sandbox'],['sandbox']
Modifiability,"nary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48652,variab,variable,48652,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"nce: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str) – Path to Nirvana configuration file.; block_size (int) – Number of rows to process per Nirvana invocation.; name (str) – Name for resulting row field. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing Nirvana annotations. hail.methods.sample_qc(mt, name='sample_qc')[source]; Compute per-sample metrics useful for quality control. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Compute sample QC metrics and remove low-quality samples:; >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; name parameter.; If mt contains an entry field DP of type tint32, then the; field dp_stats is computed. If mt contai",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:65023,config,config,65023,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['config'],"['config', 'configuration']"
Modifiability,"nced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; ServiceBackend. View page source. ServiceBackend. class hailtop.batch.backend.ServiceBackend(*args, billing_project=None, bucket=None, remote_tmpdir=None, google_project=None, token=None, regions=None, gcs_requester_pays_configuration=None, gcs_bucket_allow_list=None); Bases: Backend[Batch]; Backend that executes batches on Hail’s Batch Service on Google Cloud.; Examples; Create and use a backend that bills to the Hail Batch billing project named “my-billing-account”; and stores temporary intermediate files in “gs://my-bucket/temporary-files”.; >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) ; >>> b = hb.Batch(backend=service_backend) ; >>> j = b.new_job() ; >>> j.command('echo hello world!') ; >>> b.run() . Same as above, but set the billing project and temporary intermediate folders via a; configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-acc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:1330,config,configuration,1330,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['config'],['configuration']
Modifiability,"ncluded in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[sourc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3452,polymorphi,polymorphism,3452,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"ncoding(byte_writer, list(value)). def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tset) and self.element_type.unify(t.element_type). def subst(self):; return tset(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). class _freeze_this_type(HailType):; def __init__(self, t):; self.t = t. def _convert_from_json_na(self, x, _should_freeze: bool = False):; return self.t._convert_from_json_na(x, _should_freeze=True). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; return self.t._convert_from_encoding(byte_reader, _should_freeze=True). def _convert_to_encoding(self, byte_writer, x):; return self.t._convert_to_encoding(byte_writer, x). [docs]class tdict(HailType):; """"""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """""". @typecheck_method(key_type=hail_type, value_type=hail_type); def __init__(self, key_type, value_type):; self._key_type = key_type; self._value_type = value_type; self._array_repr = tarray(tstruct(key=_freeze_this_type(key_type), value=value_type)); super(tdict, self).__init__(). @property; def key_type(self):; """"""Dict key type. Returns; -------; :class:`.HailType`; Key type.; """"""; return self._key_type. @property; def value_type(self):; """"""Dict value type. Returns; -------; :class:`.HailType`; Value type.; """"""; return self._value_type. @property; def element_type(self):; return tstruct(key=self._key_type, value=self._value_type). def _traverse(self, obj, f):; if f(self, obj):; for k, v in obj.items():; self.key_type._traverse(k,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:26159,parameteriz,parameterize,26159,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['parameteriz'],['parameterize']
Modifiability,"nd compute a new one:; >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:60346,variab,variable-length,60346,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"nd(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A specia",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:3864,config,configure,3864,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['config'],['configure']
Modifiability,"nd=tint32,; swissprot=tstr,; transcript_id=tstr,; trembl=tstr,; uniparc=tstr,; variant_allele=tstr,; ); ),; variant_class=tstr,; ). [docs]class VEPConfig(abc.ABC):; """"""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:23833,variab,variables,23833,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['variab'],['variables']
Modifiability,"nder the null hypothesis, a particular value, :math:`Q`, is distributed; according to a generalized chi-squared distribution with parameters determined by the genotypes,; weights, and residual phenotypes. The SKAT p-value is the probability of drawing even larger; values of :math:`Q`. :math:`Q` is defined by Wu et al. as:. .. math::. \begin{align*}; r &= y - \widehat{\beta_\textrm{null}} X \\; W_{ii} &= w_i \\; \\; Q &= r^T G W G^T r; \end{align*}. :math:`\widehat{\beta_\textrm{null}}` is the best-fit beta under the null model:. .. math::. y = \beta_\textrm{null} X + \varepsilon \quad\quad \varepsilon \sim N(0, \sigma^2). Therefore :math:`r`, the residual phenotype, is the portion of the phenotype unexplained by the; covariates alone. Also notice:. 1. The residual phenotypes are normally distributed with mean zero and variance; :math:`\sigma^2`. 2. :math:`G W G^T`, is a symmetric positive-definite matrix when the weights are non-negative. We can transform the residuals into standard normal variables by normalizing by their; variance. Note that the variance is corrected for the degrees of freedom in the null model:. .. math::. \begin{align*}; \widehat{\sigma} &= \frac{1}{N - K} r^T r \\; h &= \frac{1}{\widehat{\sigma}} r \\; h &\sim N(0, 1) \\; r &= h \widehat{\sigma}; \end{align*}. We can rewrite :math:`Q` in terms of a Grammian matrix and these new standard normal random variables:. .. math::. \begin{align*}; Q &= h^T \widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:73297,variab,variables,73297,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"ndim)]; total_num_elements = np.product(shape, dtype=np.int64). if self.element_type in _numeric_types:; element_byte_size = self.element_type._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shape=shape, buffer=np.array(elements, dtype=np_type), dtype=np_type, order=""F""). def _convert_to_encoding(self, byte_writer, value: np.ndarray):; for dim in value.shape:; byte_writer.write_int64(dim). if value.size > 0:; if self.element_type in _numeric_types:; byte_writer.write_bytes(value.data); else:; for elem in np.nditer(value, order='F'):; self.element_type._convert_to_encoding(byte_writer, elem). [docs]class tarray(HailType):; """"""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; super(tarray, self).__init__(). @property; def element_type(self):; """"""Array element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not isinstance(annotation, Sequence):; raise TypeError(""type 'array' expected Python 'list', but found typ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:18290,variab,variable-length,18290,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['variab'],['variable-length']
Modifiability,"ng the array of entry structs; for the given row.; columns_array_field_name (str) – The name of the global field containing the array of column; structs. Returns:; Table – A table whose fields are the row fields of this matrix table plus; one field named entries_array_field_name. The global fields of; this table are the global fields of this matrix table plus one field; named columns_array_field_name. make_table(separator='.')[source]; Make a table from a matrix table with one field per sample. Deprecated since version 0.2.129: use localize_entries() instead because it supports more; columns. Parameters:; separator (str) – Separator between sample IDs and entry field names. Returns:; Table. See also; localize_entries(). Notes; The table has one row for each row of the input matrix. The; per sample and entry fields are formed by concatenating the; sample ID with the entry field name using separator. If the; entry field name is empty, the separator is omitted.; The table inherits the globals from the matrix table.; Examples; Consider a matrix table with the following schema:; Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: A, B and C. Then the result of; make_table():; >>> ht = mt.make_table() . has the original row fields along with 6 additional fields,; one for each sample and entry field:; Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>. n_partitions()[source]; Number of partitions.; Notes; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:47609,inherit,inherits,47609,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['inherit'],['inherits']
Modifiability,"ng) – Family ID.; nErrors (Int) – Number of Mendel errors found involving this individual.; nSNP (Int) – Number of Mendel errors found involving this individual at SNPs.; error (String) – Readable representation of Mendel error. Fourth table: errors per variant. This table contains one row per variant in the dataset.; Columns:. v (Variant) – Variant (key column).; nErrors (Int) – Number of Mendel errors in this variant. PLINK Mendel error formats:. *.mendel – all mendel errors: FID KID CHR SNP CODE ERROR; *.fmendel – error count per nuclear family: FID PAT MAT CHLD N; *.imendel – error count per individual: FID IID N; *.lmendel – error count per variant: CHR SNP N. In the PLINK formats, FID, KID, PAT, MAT, and IID refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to 0. SNP denotes ; the variant identifier chr:pos:ref:alt. N is the error count. CHLD is the number of ; children in a nuclear family.; The CODE of each Mendel error is determined by the table below,; extending the Plink; classification.; Those individuals implicated by each code are in bold.; The copy state of a locus with respect to a trio is defined as follows,; where PAR is the pseudoautosomal region (PAR). HemiX – in non-PAR of X, male child; HemiY – in non-PAR of Y, male child; Auto – otherwise (in autosome or PAR, or female child). Any refers to \(\{ HomRef, Het, HomVar, NoCall \}\) and ! denotes complement in this set. Code; Dad; Mom; Kid; Copy State. 1; HomVar; HomVar; Het; Auto. 2; HomRef; HomRef; Het; Auto. 3; HomRef; ! HomRef; HomVar; Auto. 4; ! HomRef; HomRef; HomVar; Auto. 5; HomRef; HomRef; HomVar; Auto. 6; HomVar; ! HomVar; HomRef; Auto. 7; ! HomVar; HomVar; HomRef; Auto. 8; HomVar; HomVar; HomRef; Auto. 9; Any; HomVar; HomRef; HemiX. 10; Any; HomRef; HomVar; HemiX. 11; HomVar; Any; HomRef; HemiY. 12; HomRef; Any; HomVar; HemiY. This method only considers children with two parents and a defined sex.; PAR is currently defined with respect to reference; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:126495,extend,extending,126495,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['extend'],['extending']
Modifiability,"nment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:3736,config,configure,3736,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['config'],['configure']
Modifiability,"nning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_method>`__ to find the precise location of the maximum over the same range, with initial guess given by the optimal grid point and a tolerance on :math:`\\mathrm{ln}(\delta)` of 1e-6. If this location differs from the optimal grid point by more than 0.01, a warning will be displayed and logged, and one would be wise to investigate by plotting the values over the grid. Note that :math:`h^2` is related to :math:`\\mathrm{ln}(\delta)` through the `sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>`_. More precisely,. .. math::. h^2 = 1 - \mathrm{sigmoid}(\\mathrm{ln}(\delta)) = \mathrm{sigmoid}(-\\mathrm{ln}(\delta)). Hence one can change variables to extract a high-resolution discretization of the likelihood function of :math:`h^2` over :math:`[0,1]` at the corresponding REML estimators for :math:`\\beta` and :math:`\sigma_g^2`, as well as integrate over the normalized likelihood function using `change of variables <https://en.wikipedia.org/wiki/Integration_by_substitution>`_ and the `sigmoid differential equation <https://en.wikipedia.org/wiki/Sigmoid_function#Properties>`_. For convenience, ``global.lmmreg.fit.normLkhdH2`` records the the likelihood function of :math:`h^2` normalized over the discrete grid ``0.01, 0.02, ..., 0.98, 0.99``. The length of the array is 101 so that index ``i`` contains the likelihood at percentage ``i``. The values at indices 0 and 100 are left undefined. By the theory of maximum likelihood estimation, this normalized likelihood function is approximately normally distributed near the maximum likelihood estimate. So we estimate the standard error of the estimator of :math:`h^2` as follows. Let :math:`x_2` be the maximum likelihood estimate of :math:`h^2` and let :math:`x_ 1` and :math:`x_3` be just to the left and right of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:132028,variab,variables,132028,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['variab'],['variables']
Modifiability,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7975,variab,variable,7975,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],['variable']
Modifiability,"non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; heterozygous genotypes are represented as 1, and homozygous-alternate; genotypes are represented as 2. :math:`X_{i,s}` is calculated by invoking; :meth:`~.CallExpression.n_alt_alleles` on the `call_expr`. The three counts above, :math:`N^{Aa}`, :math:`N^{Aa,Aa}`, and; :math:`N^{AA,aa}`, exclude variants where one or both individuals have; missing genotypes. In terms of the symbols above, we can define :math:`d`, the genetic distance; between two samples. We can interpret :math:`d` as an unnormalized; measurement of the genetic material not shared identically-by-descent:. .. math::. d_{i,j} = \sum_{s \in S_{i,j}}\left(X_{i,s} - X_{j,s}\right)^2. In the supplement to Manichaikul, et. al, the authors show how to re-express; the genetic distance above in terms of the three counts of hetero- and; homozygosity by considering the nine possible configurations of a pair of; genotypes:. +-------------------------------+----------+----------+----------+; |:math:`(X_{i,s} - X_{j,s})^2` |homref |het |homalt |; +-------------------------------+----------+----------+----------+; |homref |0 |1 |4 |; +-------------------------------+----------+----------+----------+; |het |1 |0 |1 |; +-------------------------------+----------+----------+----------+; |homalt |4 |1 |0 |; +-------------------------------+----------+----------+----------+. which leads to this expression for genetic distance:. .. math::. d_{i,j} = 4 N^{AA,aa}_{i,j}; + N^{Aa}_{i}; + N^{Aa}_{j}; - 2 N^{Aa,Aa}_{i,j}. The first term, :math:`4 N^{AA,aa}_{i,j}`, accounts for all pairs of; genotypes with opposing homozygous genotypes. The second and third terms; account for the four cases of one heteroyzgous genotype and one; non-heterozygous genotype. Unfortunately, the second and third term also; contribute to the case",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:2844,config,configurations,2844,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,2,['config'],['configurations']
Modifiability,"notation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logistic': use_logistic,; 'maxSize': max_size,; 'accuracy': accuracy,; 'iterations': iterations,; 'logistic_max_iterations': max_iterations,; 'logistic_tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). [docs]@typecheck(p_value=expr_numeric, approximate=bool); def lambda_gc(p_value, approximate=True):; """"""; Compute genomic inflation factor (lambda GC) from an Expression of p-values. .. include:: ../_templates/experimental.rst. Parameters; ----------; p_value : :class:`.NumericExpression`; Row-indexed numeric expression of p-values.; approximate : :obj:`bool`; If False, computes exact lambda GC (slower and uses more memory). Returns; -------; :obj:`float`; Genomic inflation factor (lambda genomic control).; """"""; raise_unless_row_indexed('lambda_gc', p_value); t = table_source('lambda_gc', p_value); med_chisq = _lambda_gc_agg(p_value, approximate); return t.aggregate(med_chisq). @typecheck(p_value=expr_numeric, approximate=bool); def _lambda_gc_agg(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:109291,config,config,109291,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"ns. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the minimum value of NaN and; any non-NaN value x is x. See also; min(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.max(*exprs, filter_missing=True)[source]; Returns the maximum element of a collection or of given numeric expressions.; Examples; Take the maximum value of an array:; >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:; >>> hl.eval(hl.max(1, 50, 2)); 50. Notes; Like the Python builtin max function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the maximum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmax(), min(), nanmin(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing maximum. Returns:; NumericExpression. hail.expr.functions.nanmax(*exprs, filter_missing=True)[source]; Returns the maximum value of a collection or of given arguments, excluding NaN.; Examples; Compute the maximum value of an array:; >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:; >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 5",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:11086,variab,variable-length,11086,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['variab'],['variable-length']
Modifiability,"nservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,; exac_amr_maf: Double,; exac_eas_allele: String,; exac_eas_maf: Double,; exac_fin_allele: String,; exac_fin_maf: Double,; exac_maf: Double,; exac_nfe_a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:176472,plugin,plugin,176472,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['plugin'],['plugin']
Modifiability,"nt object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3327,polymorphi,polymorphism,3327,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"nt to append, same type as the array element type. Returns; -------; :class:`.ArrayExpression`; """"""; if item._type != self._type.element_type:; raise TypeError(; ""'ArrayExpression.append' expects 'item' to be the same type as its elements\n""; "" array element type: '{}'\n""; "" type of arg 'item': '{}'"".format(self._type._element_type, item._type); ); return self._method(""append"", self._type, item). [docs] @typecheck_method(a=expr_array()); def extend(self, a):; """"""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""; if not a._type == self._type:; raise TypeError(; ""'ArrayExpression.extend' expects 'a' to be the same type as the caller\n""; "" caller type: '{}'\n""; "" type of 'a': '{}'"".format(self._type, a._type); ); return self._method(""extend"", self._type, a). [docs] @typecheck_method(f=func_spec(2, expr_any), zero=expr_any); def scan(self, f, zero):; """"""Map each element of the array to cumulative value of function `f`, with initial value `zero`. Examples; --------; >>> a = [0, 1, 2]. >>> hl.eval(hl.array_scan(lambda i, j: i + j, 0, a)); [0, 0, 1, 3]. Parameters; ----------; f : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); Function which takes the cumulative value and the next element, and; returns a new value.; zero : :class:`.Expression`; Initial value to pass in as left argument of `f`. Returns; -------; :class:`.ArrayExpression`.; """"""; return self._to_stream().scan(lambda x, y: f(x, y), zero).to_array(). [docs] @typecheck_method(group_size=expr_int32); def grouped(self, group_size):; """"""Partition an array into fixed size subarrays. Examples; --------; >>> a = hl.array([0, 1, 2, 3, 4]). >>> hl.eval(a.grouped(2)); [[0, 1], [2, 3], [4]]. Parameters; ----------; group_size : :class:`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:17754,extend,extend,17754,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['extend'],['extend']
Modifiability,"nt(x); Parse a string as a 32-bit integer. parse_int32(x); Parse a string as a 32-bit integer. parse_int64(x); Parse a string as a 64-bit integer. parse_float(x); Parse a string as a 64-bit floating point number. parse_float32(x); Parse a string as a 32-bit floating point number. parse_float64(x); Parse a string as a 64-bit floating point number. hail.expr.functions.format(f, *args)[source]; Returns a formatted string using a specified format string and arguments.; Examples; >>> hl.eval(hl.format('%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; See the Java documentation; for valid format specifiers and arguments.; Missing values are printed as 'null' except when using the; format flags ‘b’ and ‘B’ (printed as 'false' instead). Parameters:. f (StringExpression) – Java format string.; args (variable-length arguments of Expression) – Arguments to format. Returns:; StringExpression. hail.expr.functions.json(x)[source]; Convert an expression to a JSON string expression.; Examples; >>> hl.eval(hl.json([1,2,3,4,5])); '[1,2,3,4,5]'. >>> hl.eval(hl.json(hl.struct(a='Hello', b=0.12345, c=[1,2], d={'hi', 'bye'}))); '{""a"":""Hello"",""b"":0.12345,""c"":[1,2],""d"":[""bye"",""hi""]}'. Parameters:; x – Expression to convert. Returns:; StringExpression – String expression with JSON representation of x. hail.expr.functions.parse_json(x, dtype)[source]; Convert a JSON string to a structured expression.; Examples; >>> json_str = '{""a"": 5, ""b"": 1.1, ""c"": ""foo""}'; >>> parsed = hl.parse_json(json_str, dtype='struct{a: int32, b: float64, c: str}'); >>> hl.eval(parsed.a); 5. Parameters:. x (StringExpression) – JSON string.; dtype – Type of value to parse. Returns:; Expression. hail.expr.functions.hamming(s1, s2)[source]; Returns the Hamming distance between the two strings.; Examples; >>> hl.eval(hl.hamming('ATATA', 'ATGCA')); 2.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/string.html:2188,variab,variable-length,2188,docs/0.2/functions/string.html,https://hail.is,https://hail.is/docs/0.2/functions/string.html,1,['variab'],['variable-length']
Modifiability,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:42220,config,config,42220,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,6,"['config', 'variab']","['config', 'variable']"
Modifiability,"nt_type. var = Env.get_uid(base='agg'); ref = construct_expr(ir.Ref(var, elt), elt, array._indices); aggregated = f(ref). if not aggregated._aggregations:; raise ExpressionException(; ""'hl.aggregate_local_array' "" ""must take a mapping that contains aggregation expression.""; ). indices, _ = unify_all(array, aggregated); if isinstance(array.dtype, tarray):; stream = ir.toStream(array._ir); else:; stream = array._ir; return construct_expr(; ir.StreamAgg(stream, var, aggregated._ir),; aggregated.dtype,; Indices(indices.source, indices.axes),; array._aggregations,; ). _agg_func = AggFunc(). def _check_agg_bindings(expr, bindings):; bound_references = {; ref.name; for ref in expr._ir.search(; lambda x: (; isinstance(x, ir.Ref); and not isinstance(x, ir.TopLevelReference); and not x.name.startswith('__uid_scan'); and not x.name.startswith('__uid_agg'); and not x.name == '__rng_state'; ); ); }; free_variables = bound_references - expr._ir.bound_variables - bindings; if free_variables:; raise ExpressionException(; ""dynamic variables created by 'hl.bind' or lambda methods like 'hl.map' may not be aggregated""; ). [docs]@typecheck(expr=expr_numeric, k=int, _raw=bool); def approx_cdf(expr, k=100, *, _raw=False):; """"""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:11544,variab,variables,11544,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['variab'],['variables']
Modifiability,"ntain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""; left_key = self.key.dtype; for (; i,; ht,; ) in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(; f""'union': table {i} has a different key.""; f"" Expected: {left_key}\n""; f"" Table {i}: {ht.key.dtype}""; ). if not (unify or ht.row.dtype == self.row.dtype):; raise ValueError(; f""'union': table {i} has a different row type.\n""; f"" Expected: {self.row.dtype}\n""; f"" Table {i}: {ht.row.dtype}\n""; f"" If the tables have the same fields in different orders, or some\n""; f"" common and some unique fields, then the 'unify' parameter may be\n""; f"" able to coerce the tables to a common type.""; ); all_tables = [self]; all_tables.extend(tables). if unify and not len(set(ht.row_value.dtype for ht in all_tables)) == 1:; discovered = collections.defaultdict(dict); for i, ht in enumerate(all_tables):; for field_name in ht.row_value:; discovered[field_name][i] = ht[field_name]; all_fields = [{} for _ in all_tables]; for field_name, expr_dict in discovered.items():; *unified, can_unify = hl.expr.expressions.unify_exprs(*expr_dict.values()); if not can_unify:; raise ValueError(; f""cannot unify field {field_name!r}: found fields of types ""; f""{[str(t) for t in {e.dtype for e in expr_dict.values()}]}""; ); unified_map = dict(zip(expr_dict.keys(), unified)); default = hl.missing(unified[0].dtype); for i in range(len(all_tables)):; all_fields[i][field_name] = unified_map.get(i, default). for i, t in enumerate(all_tables):; all_tables[i] = t.select(**all_fields[i]). return Table(ir.TableUnion([table._tir for table in all_tables])). [docs] @typecheck_method(n=int, _localize=bool); def take(self, n, _localize=True):; """"""Collect the f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:86532,extend,extend,86532,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['extend'],['extend']
Modifiability,"ntaining VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44638,config,config,44638,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"ny) – Row fields in pass_through.; n (tint32) – Number of columns used.; sum_x (tfloat64) – Sum of input values x.; y_transpose_x (tfloat64) – Dot product of response; vector y with the input vector x.; beta (tfloat64) –; Fit effect coefficient of x, \(\hat\beta_1\) below.; standard_error (tfloat64) –; Estimated standard error, \(\widehat{\mathrm{se}}_1\).; t_stat (tfloat64) – \(t\)-statistic, equal to; \(\hat\beta_1 / \widehat{\mathrm{se}}_1\).; p_value (tfloat64) – \(p\)-value. If y is a list of expressions, then the last five fields instead have type; tarray of tfloat64, with corresponding indexing of; the list and each array.; If y is a list of lists of expressions, then n and sum_x are of type; array<float64>, and the last five fields are of type; array<array<float64>>. Index into these arrays with; a[index_in_outer_list, index_in_inner_list]. For example, if; y=[[a], [b, c]] then the p-value for b is p_value[1][0].; In the statistical genetics example above, the input variable x encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. \[\mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:4240,variab,variable,4240,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"o compute them. Returns; -------; :class:`.MatrixTable`; Matrix table with new row-and-column-indexed field(s).; """"""; caller = ""MatrixTable.annotate_entries""; check_annotate_exprs(caller, named_exprs, self._entry_indices, set()); return self._select_entries(caller, s=self.entry.annotate(**named_exprs)). [docs] def select_globals(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing global fields or create new fields by name, dropping the rest. Examples; --------; Select one existing field and compute a new one:. >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; -----; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified global fields.; """""". caller = 'MatrixTable.select_globals'; new_global = get_select_exprs(caller, exprs, named_exprs, self._global_indices, self._globals); return self._select_globals(caller, new_global). [docs] def select_rows(self, *exprs, **named_exprs) -> 'MatrixTable':; """"""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:37681,variab,variable-length,37681,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability,"obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11089,config,configure,11089,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configure']
Modifiability,"od uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""; return GeomDensity(mapping, k, smoothing, fill, color, alpha, smoothed). class GeomHLine(Geom):; def __init__(self, yintercept, linetype=""solid"", color=None):; self.yintercept = yintercept; self.aes = aes(); self.linetype = linetype; self.color = color. def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; line_attributes = {""y"": self.yintercept, ""line_dash"": linetype_plotly_to_gg(self.linetype)}; if self.color is not None:; line_attributes[""line_color""] = self.color. fig_so_far.add_hline(**line_attributes). def get_stat(self):; return StatNone(). [docs]def geom_hline(yintercept, *, linetype=""solid"", color=None):; """"""Plots a horizontal line at ``yintercept``. Parameters; ----------; yintercept : :class:`float`; Location to draw line.; linetype : :class:`str`; Type of line to draw. C",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:21095,variab,variable,21095,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,2,['variab'],['variable']
Modifiability,"odegen; infrastructure improvement resulting in ~3% overall speedup. hailctl dataproc. (#8399) Enable spark; speculation by default.; (#8340) Add new; Australia region to --vep.; (#8347) Support all; GCP machine types as potential master machines. Version 0.2.34; Released 2020-03-12. New features. (#8233); StringExpression.matches can now take a hail; StringExpression, as opposed to only regular python strings.; (#8198) Improved; matrix multiplication interoperation between hail; NDArrayExpression and numpy. Bug fixes. (#8279) Fix a bug; where hl.agg.approx_cdf failed inside of a group_cols_by.; (#8275) Fix bad error; message coming from mt.make_table() when keys are missing.; (#8274) Fix memory; leak in hl.export_bgen.; (#8273) Fix segfault; caused by hl.agg.downsample inside of an array_agg or; group_by. hailctl dataproc. (#8253); hailctl dataproc now supports new flags; --requester-pays-allow-all and; --requester-pays-allow-buckets. This will configure your hail; installation to be able to read from requester pays buckets. The; charges for reading from these buckets will be billed to the project; that the cluster is created in.; (#8268) The data; sources for VEP have been moved to gs://hail-us-vep,; gs://hail-eu-vep, and gs://hail-uk-vep, which are; requester-pays buckets in Google Cloud. hailctl dataproc will; automatically infer which of these buckets you should pull data from; based on the region your cluster is spun up in. If you are in none of; those regions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:75879,config,configure,75879,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configure']
Modifiability,"of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""; args = []; uids = []; irs = []. for expr in exprs:; uid = Env.get_uid(base=_ctx); args.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uids.append(uid); irs.append(expr._ir). lambda_result = to_expr(f(*args)); if _ctx:; indices, aggregations = unify_all(lambda_result) # FIXME: hacky. May drop field refs from errors?; else:; indices, aggregations = unify_all(*exprs, lambda_result). res_ir = lambda_result._ir; for uid, value_ir in builtins.zip(uids, irs):; if _ctx == 'agg':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=False); elif _ctx == 'scan':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=True); else:; res_ir = ir.Let(uid, value_ir, res_ir). return construct_expr(res_ir, lambda_result.dtype, indices, aggregations). [docs]def rbind(*exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """""". *args, f = exprs; args = [expr_any.check(arg, 'rbind', f'argument {index}') for index, arg in builtins.enumerate(args)]. return hl.bind(f, *args, _ctx=_ctx). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def chi_squared_test(c1, c2, c3, c4) -> StructExpression:; """"""Performs chi-squared test of independence on a 2x2 contingency table. Examples; --------. >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odd",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:18357,variab,variable,18357,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable']
Modifiability,"of iterations before stopping.; converged : BooleanExpression. True if the min_accuracy was achieved and round; off error is not likely significant.; fault : Int32Expression. If converged is true, fault is zero. If converged is; false, fault is either one or two. One indicates that the requried accuracy was not; achieved. Two indicates the round-off error is possibly significant. hail.expr.functions.pnorm(x, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The cumulative probability function of a normal distribution with mean; mu and standard deviation sigma. Returns cumulative probability of; standard normal distribution by default.; Examples; >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; Returns the left-tail probability p = Prob(\(Z < x\)) with \(Z\); a normal random variable. Defaults to a standard normal random variable. Parameters:. x (float or Expression of type tfloat64); mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pT(x, n, lower_tail=True, log_p=False)[source]; The cumulative probability function of a t-distribution with; n degrees of freedom.; Examples; >>> hl.eval(hl.pT(0, 10)); 0.5. >>> hl.eval(hl.pT(1, 10)); 0.82955343384897. >>> hl.eval(hl.pT(1, 10, lower_tail=False)); 0.17044656615103004. >>> hl.eval(hl.pT(1, 10, log_p=True)); -0.186867754489647. Notes; If lower_tail is true, returns Prob(\(X \leq\) x) where \(X\) is; a t-distributed random variable with n degrees of freedom. If lower_tail; is fal",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:21731,variab,variable,21731,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"of response; vector `y` with the input vector `x`.; - **beta** (:py:data:`.tfloat64`) --; Fit effect coefficient of `x`, :math:`\hat\beta_1` below.; - **standard_error** (:py:data:`.tfloat64`) --; Estimated standard error, :math:`\widehat{\mathrm{se}}_1`.; - **t_stat** (:py:data:`.tfloat64`) -- :math:`t`-statistic, equal to; :math:`\hat\beta_1 / \widehat{\mathrm{se}}_1`.; - **p_value** (:py:data:`.tfloat64`) -- :math:`p`-value. If `y` is a list of expressions, then the last five fields instead have type; :class:`.tarray` of :py:data:`.tfloat64`, with corresponding indexing of; the list and each array. If `y` is a list of lists of expressions, then `n` and `sum_x` are of type; ``array<float64>``, and the last five fields are of type; ``array<array<float64>>``. Index into these arrays with; ``a[index_in_outer_list, index_in_inner_list]``. For example, if; ``y=[[a], [b, c]]`` then the p-value for ``b`` is ``p_value[1][0]``. In the statistical genetics example above, the input variable `x` encodes; genotype as the number of alternate alleles (0, 1, or 2). For each variant; (row), genotype is tested for association with height controlling for age; and sex, by fitting the linear regression model:. .. math::. \mathrm{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). Boolean covariates like :math:`\mathrm{is\_female}` are encoded as 1 for; ``True`` and 0 for ``False``. The null model sets :math:`\beta_1 = 0`. The standard least-squares linear regression model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; -",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:10512,variab,variable,10512,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"of(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; ); def from_fasta_file(cls, name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X chromosomes.; y_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as Y chromosomes.; mt_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as mitochondrial DNA.; par : :obj:`list` of :obj:`tuple` of (str, int, int); List of tuples with (contig, start, end). Returns; -------; :class:`.ReferenceGenome`; """"""; par_strings = [""{}:{}-{}"".format(contig, start, end) for (contig, start, end) in par]; config = Env.backend().from_fasta_file(; name, fasta_file, index_file, x_contigs, y_contigs, mt_contigs, par_strings; ). rg = ReferenceGenome._from_config(config); rg.add_sequence(fasta_file, index_file); return rg. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def has_liftover(self, dest_reference_genome):; """"""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""; return dest_reference_genome.name in self._liftovers. [docs] @typecheck_method(dest_reference_genome=reference_genome_type); def remove_liftover(self, dest_reference_genome):; """"""Remove liftover to `dest_reference_genome`. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; """"""; if dest_reference_genome.name in self._liftovers:; del self._liftovers[dest_reference_g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:12383,config,config,12383,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['config'],['config']
Modifiability,"og10 of the bin counts. Returns:; bokeh.plotting.figure. hail.plot.cumulative_histogram(data, range=None, bins=50, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative histogram. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; range (Tuple[float]) – Range of x values in the histogram.; bins (int) – Number of bins in the histogram.; legend (str) – Label of data on the x-axis.; title (str) – Title of the histogram.; normalize (bool) – Whether or not the cumulative data should be normalized.; log (bool) – Whether or not the y-axis should be of type log. Returns:; bokeh.plotting.figure. hail.plot.histogram2d(x, y, bins=40, range=None, title=None, width=600, height=600, colors=('#eff3ff', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'), log=False)[source]; Plot a two-dimensional histogram.; x and y must both be a NumericExpression from the same Table.; If x_range or y_range are not provided, the function will do a pass through the data to determine; min and max of each variable.; Examples; >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters:. x (NumericExpression) – Expression for x-axis (from a Hail table).; y (NumericExpression) – Expression for y-axis (from the same Hail table as x).; bins (int or [int, int]) – The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range (None or ((float, float), (float, float))) – The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:4587,variab,variable,4587,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['variab'],['variable']
Modifiability,"og; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93537,rewrite,rewrite,93537,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['rewrite'],['rewrite']
Modifiability,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:10105,config,configuration,10105,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"oins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fields. You can think of; globals as extra fields in the table whose values are identical for every row.; For example, the same table above with the global field G = 5 can be thought; of as; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | G |; +-------+-------+-----+-------+----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:1340,variab,variable,1340,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['variab'],['variable']
Modifiability,"ol. is_hom_ref()[source]¶; True if the genotype call is 0/0. Return type:bool. is_hom_var()[source]¶; True if the genotype call contains two identical alternate alleles. Return type:bool. is_not_called()[source]¶; True if the genotype call is missing. Return type:bool. num_alt_alleles()[source]¶; Returns the count of non-reference alleles.; This function returns None if the genotype call is missing. Return type:int or None. od()[source]¶; Returns the difference between the total depth and the allelic depth sum.; Equivalent to:; g.dp - sum(g.ad). Return type:int or None. one_hot_alleles(num_alleles)[source]¶; Returns a list containing the one-hot encoded representation of the called alleles.; This one-hot representation is the positional sum of the one-hot; encoding for each called allele. For a biallelic variant, the; one-hot encoding for a reference allele is [1, 0] and the one-hot; encoding for an alternate allele is [0, 1]. Thus, with the; following variables:; num_alleles = 2; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_alleles(num_alleles) == [2, 0]; het.one_hot_alleles(num_alleles) == [1, 1]; hom_var.one_hot_alleles(num_alleles) == [0, 2]. This function returns None if the genotype call is missing. Parameters:num_alleles (int) – number of possible alternate alleles. Return type:list of int or None. one_hot_genotype(num_genotypes)[source]¶; Returns a list containing the one-hot encoded representation of the genotype call.; A one-hot encoding is a vector with one ‘1’ and many ‘0’ values, like; [0, 0, 1, 0] or [1, 0, 0, 0]. This function is useful for transforming; the genotype call (gt) into a one-hot encoded array. With the following; variables:; num_genotypes = 3; hom_ref = Genotype(0); het = Genotype(1); hom_var = Genotype(2). All the below statements are true:; hom_ref.one_hot_genotype(num_genotypes) == [1, 0, 0]; het.one_hot_genotype(num_genotypes) == [0, 1, 0]; hom_var.one_hot_genotyp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html:4655,variab,variables,4655,docs/0.1/representation/hail.representation.Genotype.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Genotype.html,1,['variab'],['variables']
Modifiability,"ommit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25824,config,config,25824,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['config']
Modifiability,on method). (hail.expr.Expression method). (hail.expr.Float32Expression method). (hail.expr.Float64Expression method). (hail.expr.Int32Expression method). (hail.expr.Int64Expression method). (hail.expr.IntervalExpression method). (hail.expr.LocusExpression method). (hail.expr.NDArrayExpression method). (hail.expr.NDArrayNumericExpression method). (hail.expr.NumericExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). (hail.expr.StructExpression method). (hail.expr.TupleExpression method). (hail.linalg.BlockMatrix static method). (hail.Table method). export_bgen() (in module hail.methods). export_blocks() (hail.linalg.BlockMatrix method). export_elasticsearch() (in module hail.methods). export_entries_by_col() (in module hail.experimental). export_gen() (in module hail.methods). export_plink() (in module hail.methods). export_rectangles() (hail.linalg.BlockMatrix method). export_vcf() (in module hail.methods). Expression (class in hail.expr). extend() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). eye() (in module hail.nd). F. facet_wrap() (in module hail.ggplot). fam_id (hail.genetics.Trio property). FigureAttribute (class in hail.ggplot). fill() (hail.linalg.BlockMatrix class method). filter() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.linalg.BlockMatrix method). (hail.Table method). (in module hail.expr.aggregators). (in module hail.expr.functions). filter_alleles() (in module hail.methods). filter_alleles_hts() (in module hail.methods). filter_chromosomes() (in module hail.vds). filter_cols() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_entries() (hail.MatrixTable method). filter_intervals() (in module hail.methods). (in module hail.vds). filter_rows() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). filter_samples() (in module hail.vds). filter_,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:21288,extend,extend,21288,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['extend'],['extend']
Modifiability,"on. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:3067,polymorphi,polymorphism,3067,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"on_name: String,; start: Int,; strand: Int,; transcript_consequences: Array[Struct{; allele_num: Int,; amino_acids: String,; biotype: String,; canonical: Int,; ccds: String,; cdna_start: Int,; cdna_end: Int,; cds_end: Int,; cds_start: Int,; codons: String,; consequence_terms: Array[String],; distance: Int,; domains: Array[Struct{; db: String; name: String; }],; exon: String,; gene_id: String,; gene_pheno: Int,; gene_symbol: String,; gene_symbol_source: String,; hgnc_id: String,; hgvsc: String,; hgvsp: String,; hgvs_offset: Int,; impact: String,; intron: String,; lof: String,; lof_flags: String,; lof_filter: String,; lof_info: String,; minimised: Int,; polyphen_prediction: String,; polyphen_score: Double,; protein_end: Int,; protein_start: Int,; protein_id: String,; sift_prediction: String,; sift_score: Double,; strand: Int,; swissprot: String,; transcript_id: String,; trembl: String,; uniparc: String,; variant_allele: String; }],; variant_class: String; }. Parameters:; config (str) – Path to VEP configuration file.; block_size (int) – Number of variants to annotate per VEP invocation.; root (str) – Variant annotation path to store VEP output.; csq (bool) – If True, annotates VCF CSQ field as a String.; If False, annotates with the full nested struct schema. Returns:An annotated with variant annotations from VEP. Return type:VariantDataset. was_split()[source]¶; True if multiallelic variants have been split into multiple biallelic variants.; Result is True if split_multi() or filter_multi() has been called on this variant dataset,; or if the variant dataset was imported with import_plink(), import_gen(),; or import_bgen(), or if the variant dataset was simulated with balding_nichols_model(). Return type:bool. write(output, overwrite=False, parquet_genotypes=False)[source]¶; Write variant dataset as VDS file.; Examples; Import data from a VCF file and then write the data to a VDS file:; >>> vds.write(""output/sample.vds""). Parameters:; output (str) – Path of VDS file t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:179545,config,config,179545,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['config'],"['config', 'configuration']"
Modifiability,"on_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'logistic_regression_rows': found no values for 'y'""); y = [raise_unless_column_indexed('logistic_regression_rows/y', y) or y for y in wrap_to_list(y)]. for e in covariates:; analyze('logistic_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('logistic_regression_rows', covariates). x_field_name = Env.get_uid(); y_field = [f'__y_{i}' for i in range(len(y))]. y_dict = dict(zip(y_field, y)). cov_field_names = [f'__cov{i}' for i in range(len(covariates))]; row_fields = _get_regression_row_fields(mt, pass_through, 'logistic_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**y_dict, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'LogisticRegression',; 'test': test,; 'yFields': y_field,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. result = Table(ir.MatrixToTableApply(mt._mir, config)). if not y_is_list:; result = result.transmute(**result.logistic_regression[0]). return result.persist(). # Helpers for logreg:; def mean_impute(hl_array):; non_missing_mean = hl.mean(hl_array, filter_missing=True); return hl_array.map(lambda entry: hl.coalesce(entry, non_missing_mean)). sigmoid = expit. def nd_max(hl_nd):; return hl.max(hl.array(hl_nd.reshape(-1))). def logreg_fit(; X: NDArrayNumericExpression, # (K,); y: NDArrayNumericExpression, # (N, K); null_fit: Optional[StructExpression],; max_iterations: int,; tolerance: float,; ) -> StructExpression:; """"""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""; assert max_iterations >= 0; assert X.ndim =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:38358,config,config,38358,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"onal) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions (list of str, optional) – List of regions to run jobs in when using the Batch backend. Use ANY_REGION to specify any region is allowed; or use None to use the underlying default regions from the hailctl environment configuration. For example, use; hailctl config set batch/regions region1,region2 to set the default regions to use.; gcs_bucket_allow_list – A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use “cold” storage. Should look like [""bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:6435,config,configure,6435,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configure']
Modifiability,"onal[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token (Optional[str]) – The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions (Optional[List[str]]) – Cloud regions in which jobs may run. ServiceBackend.ANY_REGION indicates jobs may; run in any region. If unspecified or None, the batch/regions Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. ServiceBackend.supported_regions() lists the available regions.; gcs_bucket_allow_list (Optional[List[str]]) – A list of buckets that the ServiceBackend should be permitted to read from or write to, even if their; default policy is to use “cold” storage. Attributes. ANY_REGION; A special value that indicates a job may run in any region. Methods. _async_run; Execute a batch. supported_regions; Get the supported cloud regions. ANY_REGION: ClassVar[List[str]] = ['any_region']; A special value that indicates a job may run in any region. async _async_run(batch, dry_run, verbose, delete_scratch_on_exit, wait=True, open=False, disable_progress_bar=False, callback=None, token=None, **backend_kwargs); Execute a batch. Warning; This method should not be called directly. Instead, use batch.Batch.run(); and pass ServiceBackend specific arguments as key-word argumen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:4297,config,configuration,4297,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,4,"['config', 'variab']","['configuration', 'variable']"
Modifiability,"one, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2748,config,configuration,2748,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability,"one, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/buckets to achieve the same effect. See also; stop(). Parameters:. sc (pyspark.SparkContext, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2684,variab,variable,2684,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['variab'],['variable']
Modifiability,"onfiguration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration instead.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, config",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2735,variab,variable,2735,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,2,['variab'],['variable']
Modifiability,"onment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223269,config,configuration,223269,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['config'],['configuration']
Modifiability,"oolean lambda expression as input (g => Boolean Expression). Transform the genotype aggregable to an aggregable of GQ scores using the map function and then calculate summary statistics on the GQ scores with the stats function. va.gqStats = gs.map(g => g.gq).stats(). Filter the genotype aggregable based on case status (sa.pheno.isCase) and genotype call (g.isHet and g.isHomVar) and then count the number of elements remaining. va.caseMAC = gs.filter(g => sa.pheno.isCase && g.isHet).count() +; 2 * gs.filter(g => sa.pheno.isCase && g.isHomVar).count(). Define a filtered genotype aggregable from cases (sa.pheno.isCase) using the let..in syntax and then use the case-only genotype aggregable to calculate the fraction of genotypes called. va.caseCallRate = let caseGS = gs.filter(g => sa.pheno.isCase) in caseGS.fraction(g => g.isCalled). Count the number of genotypes remaining after filtering the genotype aggregable to genotypes with a variant allele (g.isCalledNonRef) and then create a boolean variable by comparing the result to 1. va.isSingleton = gs.filter(g => g.isCalledNonRef).count() == 1. Sample Annotation Computed from a Genotype Aggregable (gs)¶; In the context of creating new sample annotations, a genotype aggregable (gs) represents a column of genotypes in the variant-sample matrix.; The result of evaluating the genotype aggregable expression per column is added to the corresponding sample annotation.; The map function takes a lambda expression as input (g => ...). The filter function takes a boolean lambda expression as input (g => Boolean Expression). Filter the genotype aggregable to only genotypes that have a heterozygote call (g.isHet) and count the number of elements remaining. sa.numHet = gs.filter(g => g.isHet).count(). Count the number of elements remaining after filtering the genotype aggregable to only genotypes where the corresponding variant annotation is True for isSingleton and the genotype call has a variant allele (g.isCalledNonRef). sa.nSingleto",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:8176,variab,variable,8176,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"op Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:40823,parameteriz,parameterized,40823,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability,"option is only; used if master is unset and spark.master is not set in the Spark configuration.; log (str) – Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet (bool) – Print fewer log messages.; append (bool) – Append to the end of the log file.; min_block_size (int) – Minimum file block size in MB.; branching_factor (int) – Branching factor for tree aggregation.; tmp_dir (str, optional) – Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference (str) – Deprecated. Please use default_reference() to set the default reference genome; Default reference genome. Either 'GRCh37', 'GRCh38',; 'GRCm38', or 'CanFam3'. idempotent (bool) – If True, calling this function is a no-op if Hail has already been initialized.; global_seed (int, optional) – Global random seed.; spark_conf (dict of str to :class`str`, optional) – Spark backend only. Spark configuration parameters.; skip_logging_configuration (bool) – Spark Backend only. Skip logging configuration in java and python.; local_tmpdir (str, optional) – Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores (str or int, optional) – Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory (str, optional) – Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores (str or int, optional) – Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory (str, optional) – Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration (either str or tuple of str and list of str, optional) – If a string is provided, configure the Google Cloud Stor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:5329,config,configuration,5329,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability,"or association with a; binary response variable using logistic regression. Examples; --------; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48243,variab,variables,48243,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"or every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as a linear function of their first k principal; co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:131001,inherit,inherited,131001,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['inherit'],['inherited']
Modifiability,"ore/hour; for highmem spot workers, and $0.02429905 per core/hour for highcpu spot workers. There is also an additional; cost of $0.00023 per GB per hour of extra storage requested.; At any given moment as many as four cores of the cluster may come from a 4 core machine if the worker type; is standard. If a job is scheduled on this machine, then the cost per core hour is $0.02774 plus; $0.00023 per GB per hour storage of extra storage requested.; For jobs that run on non-preemptible machines, the costs are $0.06449725 per core/hour for standard workers, $0.076149 per core/hour; for highmem workers, and $0.0524218 per core/hour for highcpu workers. Note; If the memory is specified as either ‘lowmem’, ‘standard’, or ‘highmem’, then the corresponding worker types; used are ‘highcpu’, ‘standard’, and ‘highmem’. Otherwise, we will choose the cheapest worker type for you based; on the cpu and memory requests. In this case, it is possible a cheaper configuration will round up the cpu requested; to the next power of two in order to obtain more memory on a cheaper worker type. Note; The storage for the root file system (/) is 5 Gi per job for jobs with at least 1 core. If a job requests less; than 1 core, then it receives that fraction of 5 Gi. If you need more storage than this,; you can request more storage explicitly with the Job.storage() method. The minimum storage request is 10 GB; which can be incremented in units of 1 GB maxing out at 64 TB. The additional storage is mounted at /io. Note; If a worker is preempted by google in the middle of running a job, you will be billed for; the time the job was running up until the preemption time. The job will be rescheduled on; a different worker and run again. Therefore, if a job takes 5 minutes to run, but was preempted; after running for 2 minutes and then runs successfully the next time it is scheduled, the; total cost for that job will be 7 minutes. Setup; We assume you’ve already installed Batch and the Google Cloud SDK ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/service.html:6890,config,configuration,6890,docs/batch/service.html,https://hail.is,https://hail.is/docs/batch/service.html,1,['config'],['configuration']
Modifiability,"ores=None,; driver_memory=None,; worker_cores=None,; worker_memory=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkCo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7160,config,configuration,7160,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"ort pprint; >>> pprint(vds.variant_schema). Return type:Type. variants_table()[source]¶; Convert variants and variant annotations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Requir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174786,variab,variable,174786,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['variab'],['variable']
Modifiability,"orted Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow_list. Environment Variable Name; HAIL_GCS_BUCKET_ALLOW_LIST. hailctl and Environment Variable Format; bucket1,bucket2. Effect; Prevents Hail Query from erroring if the default storage policy for any of the given buckets is to use cold storage. Note: Only t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:1244,variab,variables,1244,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,1,['variab'],['variables']
Modifiability,"ot': min must be less than max in call to uniroot, got: min %.1e, max %.1e"", min, max)); ). [docs]@typecheck(f=expr_str, args=expr_any); def format(f, *args):; """"""Returns a formatted string using a specified format string and arguments. Examples; --------. >>> hl.eval(hl.format('%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; -----; See the `Java documentation <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__; for valid format specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """""". return _func(""format"", hl.tstr, f, hl.tuple(args)). [docs]@typecheck(x=expr_float64, y=expr_float64, tolerance=expr_float64, absolute=expr_bool, nan_same=expr_bool); def approx_equal(x, y, tolerance=1e-6, absolute=False, nan_same=False):; """"""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; ----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:176515,variab,variable-length,176515,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"otations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6243,config,config,6243,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,"ote; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; If any element or argument is NaN, then the result is NaN. See also; nanmin(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.nanmin(*exprs, filter_missing=True)[source]; Returns the minimum value of a collection or of given arguments, excluding NaN.; Examples; Compute the minimum value of an array:; >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:; >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.1. Notes; Like the Python builtin min function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; If filter_missing is True, then the result is the minimum of; non-missing arguments or elements. If filter_missing is False, then; any missing argument or element causes the result to be missing.; NaN arguments / array elements are ignored; the minimum value of NaN and; any non-NaN value x is x. See also; min(), max(), nanmax(). Parameters:. exprs (ArrayExpression or SetExpression or varargs of NumericExpression) – Single numeric array or set, or multiple numeric values.; filter_missing (bool) – Remove missing arguments/elements before computing minimum. Returns:; NumericExpression. hail.expr.functions.max(*exprs, filter_missing=True)[source]; Returns the maximum element of a collection or of given numeric expressions.; Examples; Take the maximum value of an array:; >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:; >>> hl.eval(hl.max(1, 50, 2)); 50. No",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:9990,variab,variable-length,9990,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['variab'],['variable-length']
Modifiability,"ouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; raise ValueError('annotation database can only annotate Hail' ' MatrixTable or Table'). def _dataset_by_name(self, name: str) -> Dataset:; """"""Retrieve :class:`Dataset` object by name. Parameters; ----------; name : :obj:`str`; Name of dataset. Returns; -------; :class:`Dataset`; """"""; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:12235,config,config,12235,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,2,['config'],['config']
Modifiability,"parallel); Env.backend().execute(; ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)); ). [docs] def group_by(self, *exprs, **named_exprs) -> 'GroupedTable':; """"""Group by a new key for use with :meth:`.GroupedTable.aggregate`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:55265,variab,variable-length,55265,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"pe = sa.pheno.isCase. Export all annotations generated by variant_qc(). Variant = v, va.qc.*. Input Variables to Methods¶; The linear and logistic regression commands utilize expressions containing sample annotation variables to define the response variable and covariates. Linear regression command defining the response variable and covariates from sample annotations. >>> vds.linreg('sa.isCase', covariates='sa.PC1, sa.PC2, sa.PC3, sa.AGE'). Filtering¶; Filter commands take a boolean expression. Here are some examples of boolean expressions using VDS elements:. Variant chromosome name v.contig does not equal “X” or “Y”. v.contig != “X” && v.contig != “Y”. Sample id s does not match the substring “NA12”. !(""NA12"" ~ s). Sample annotation for whether a sample is female sa.isFemale, which is a boolean variable. sa.isFemale. Variant annotation for whether a variant has a pass flag va.pass, which is a boolean variable. va.pass. Variant annotation for the quality score va.qual (numeric variable) is greater than 20. va.qual > 20. Expression that combines attributes of both v and va. (va.qual > 20 && va.pass) || v.nAlleles == 2. Expression that combine attributes of both s and sa. ""CONTROL"" ~ s || !sa.pheno.isCase. Add New Annotations¶; To add new annotations, define an equation where the left-hand side is the name (path) of the new sample annotation and the right-hand side is the result of evaluating an expression with VDS elements. Computed From Existing Annotations¶. Add a new variant annotation called passAll which is the result of a boolean expression evaluating other variant annotation variables. va.passAll = va.pass && va.meanGQ > 20 && va.meanDP > 20. Add a new sample annotation called batch1 which is the result of a boolean expression comparing an existing boolean sample annotation variable to the string “Batch1”. sa.batch1 = sa.cohort == ""Batch1"". Add a new boolean sample annotation based on the length of the sample ID. sa.idTooLong = s.length > 10. Add a new variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:5226,variab,variable,5226,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"pe._byte_size; bytes_to_read = element_byte_size * total_num_elements; buffer = byte_reader.read_bytes_view(bytes_to_read); return np.frombuffer(buffer, self.element_type.to_numpy, count=total_num_elements).reshape(shape); else:; elements = [; self.element_type._convert_from_encoding(byte_reader, _should_freeze) for i in range(total_num_elements); ]; np_type = self.element_type.to_numpy(); return np.ndarray(shape=shape, buffer=np.array(elements, dtype=np_type), dtype=np_type, order=""F""). def _convert_to_encoding(self, byte_writer, value: np.ndarray):; for dim in value.shape:; byte_writer.write_int64(dim). if value.size > 0:; if self.element_type in _numeric_types:; byte_writer.write_bytes(value.data); else:; for elem in np.nditer(value, order='F'):; self.element_type._convert_to_encoding(byte_writer, elem). [docs]class tarray(HailType):; """"""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; super(tarray, self).__init__(). @property; def element_type(self):; """"""Array element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not isinstance(annotation, Sequence):; raise TypeError(""type 'array' expected Python 'list', but found type '%s'"" % type(annotation)). def __str__(self):; return ""array<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:18440,parameteriz,parameterized,18440,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['parameteriz'],['parameterized']
Modifiability,"ped('isDefined(1)'). Out[15]:. (True, Boolean). In [16]:. hc.eval_expr_typed('isDefined(NA: Int)'). Out[16]:. (False, Boolean). In [17]:. hc.eval_expr_typed('isMissing(NA: Double)'). Out[17]:. (True, Boolean). orElse lets you convert missing to a default value and orMissing; lets you turn a value into missing based on a condtion. In [18]:. hc.eval_expr_typed('orElse(5, 2)'). Out[18]:. (5, Int). In [19]:. hc.eval_expr_typed('orElse(NA: Int, 2)'). Out[19]:. (2, Int). In [20]:. hc.eval_expr_typed('orMissing(true, 5)'). Out[20]:. (5, Int). In [21]:. hc.eval_expr_typed('orMissing(false, 5)'). Out[21]:. (None, Int). Let¶; You can assign a value to a variable with a let expression. Here is; an example. In [22]:. hc.eval_expr_typed('let a = 5 in a + 1'). Out[22]:. (6, Int). The variable, here a is only visible in the body of the let, the; expression following in. You can assign multiple variables. Variable; assignments are separated by and. Each variable is visible in the; right hand side of the following variables as well as the body of the; let. For example:. In [23]:. hc.eval_expr_typed('''; let a = 5; and b = a + 1; in a * b; '''). Out[23]:. (30, Int). Conditionals¶; Unlike other languages, conditionals in Hail return a value. The arms of; the conditional must have the same type. The predicate must be of type; Boolean. If the predicate is missing, the value of the entire; conditional is missing. Here are some simple examples. In [24]:. hc.eval_expr_typed('if (true) 1 else 2'). Out[24]:. (1, Int). In [25]:. hc.eval_expr_typed('if (false) 1 else 2'). Out[25]:. (2, Int). In [26]:. hc.eval_expr_typed('if (NA: Boolean) 1 else 2'). Out[26]:. (None, Int). The if and else branches need to return the same type. The below; expression is invalid. In [27]:. # Uncomment and run the below code to see the error message. # hc.eval_expr_typed('if (true) 1 else ""two""'). Compound Types¶; Hail has several compound types: -; Array[T] -; Set[T] - Dict[K,; V] -; Aggregable[T] -; Struct; T, K a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:6295,variab,variable,6295,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,2,['variab'],"['variable', 'variables']"
Modifiability,"perimental. Top-Level Functions. hail.init(sc=None, app_name=None, master=None, local='local[*]', log=None, quiet=False, append=False, min_block_size=0, branching_factor=50, tmp_dir=None, default_reference=None, idempotent=False, global_seed=None, spark_conf=None, skip_logging_configuration=False, local_tmpdir=None, _optimizer_iterations=None, *, backend=None, driver_cores=None, driver_memory=None, worker_cores=None, worker_memory=None, gcs_requester_pays_configuration=None, regions=None, gcs_bucket_allow_list=None, copy_spark_log_on_error=False)[source]; Initialize and configure Hail.; This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; init():; >>> import hail as hl; >>> hl.init(global_seed=0) . Hail has two backends, spark and batch. Hail selects a backend by consulting, in order,; these configuration locations:. The backend parameter of this function.; The HAIL_QUERY_BACKEND environment variable.; The value of hailctl config get query/backend. If no configuration is found, Hail will select the Spark backend.; Examples; Configure Hail to use the Batch backend:; >>> import hail as hl; >>> hl.init(backend='batch') . If a pyspark.SparkContext is already running, then Hail must be; initialized with it as an argument:; >>> hl.init(sc=sc) . Configure Hail to bill to my_project when accessing any Google Cloud Storage bucket that has; requester pays enabled:; >>> hl.init(gcs_requester_pays_configuration='my-project') . Configure Hail to bill to my_project when accessing the Google Cloud Storage buckets named; bucket_of_fish and bucket_of_eels:; >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) . You may also use hailctl config set gcs_requester_pays/project and hailctl config set; gcs_requester_pays/bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:2582,config,configuration,2582,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['config'],['configuration']
Modifiability,"port_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the prob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:48549,parameteriz,parameterized,48549,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability,"puted principal component scores.; To produce the same results as in the previous example:. >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) # doctest: +SKIP. Notes; -----; The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with estimated allele; frequencies :math:`\widehat{p}_{s}` at SNP :math:`s`, is given by:. .. math::. \widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of a sample's first ``k``; principal component coordinates. As such, the efficacy of this method; rests o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:3826,inherit,inherited,3826,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['inherit'],['inherited']
Modifiability,"qtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters:. x (float or Expression of type tfloat64) – The value at which to evaluate the CDF.; df (float or Expression of type tfloat64) – Degrees of freedom.; ncp (float or Expression of type tfloat64) – Noncentrality parameter, defaults to 0 if unspecified.; lower_tail (bool or BooleanExpression) – If True, compute the probability of an outcome at or below x,; otherwise greater than x.; log_p (bool or BooleanExpression) – Return the natural logarithm of the probability. Returns:; Expression of type tfloat64. hail.expr.functions.pgenchisq(x, w, k, lam, mu, sigma, *, max_iterations=None, min_accuracy=None)[source]; The cumulative probability function of a generalized chi-squared distribution.; The generalized chi-squared distribution has many interpretations. We share here four; interpretations of the values of this distribution:. A linear combination of normal variables and squares of normal variables.; A weighted sum of sums of squares of normally distributed values plus a normally distributed; value.; A weighted sum of chi-squared distributed values plus a normally distributed value.; A “quadratic form” in a vector; of uncorrelated standard normal values. The parameters of this function correspond to the parameters of the third interpretation. \[\begin{aligned}; w &: R^n \quad k : Z^n \quad lam : R^n \quad mu : R \quad sigma : R \\; \\; x &\sim N(mu, sigma^2) \\; y_i &\sim \mathrm{NonCentralChiSquared}(k_i, lam_i) \\; \\; Z &= x + w y^T \\; &= x + \sum_i w_i y_i \\; Z &\sim \mathrm{GeneralizedNonCentralChiSquared}(w, k, lam, mu, sigma); \end{aligned}\]; The generalized chi-squared distribution often arises when working on linear models with standard; normal noise because the sum of the squares of the residuals should follow a generalized; chi-squared distribution.; Examples; The following plot shows three examples of the generalized chi-squared",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:15610,variab,variables,15610,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,2,['variab'],['variables']
Modifiability,"quency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent. When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals. PC-Relate slightly modifies the usual estimator for relatedness:; occurences of population allele frequency are replaced with an; ""individual-specific allele frequency"". This modification allows the; method to correctly weight an allele according to an individual's unique; ancestry profile. The ""individual-specific allele frequency"" at a given genetic locus is; modeled by PC-Relate as a linear function of their first ``k`` principal; component coordinates. As such, the efficacy of this method rests on two; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:170285,inherit,inherited,170285,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['inherit'],['inherited']
Modifiability,"r association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=())[source]; For each row, test an input variable for association using a linear; mixed model. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None)[source]; For each row, test an input variable for association with; response variables using linear regression.; Examples; >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; As in the example, the intercept covariate 1 must be; included explicitly if desired. Warning; If y is a single value or a list, linear_regression_rows(); considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which all response variables; and covariates are defined.; If y is a list of lists, then each inner list is treated as ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:2074,variab,variable,2074,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"r gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the ServiceBackend via configuration file:; cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; “https://my-account.blob.core.windows.net/my-container/tempdir”.; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) . Require all jobs in all batches in this backend to execute in us-central1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the HAIL_BATCH_REGIONS environment variable:; export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in either us-central1 or us-east1:; >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file:; hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are “cold” storage:; >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters:. billing_project (Optional[str]) – Name of billing project to use.; bucket (Optional[str]) – This argument is deprecated. Use remote_tmpdir instead.; remote_tmpdir (Optional[str]) – Temporary data will be stored in this cloud storage folder.; google_project (Optional[str]) – This argument is deprecated. Use gcs_requester_pays_configuration i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html:2580,config,configuration,2580,docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,https://hail.is,https://hail.is/docs/batch/api/backend/hailtop.batch.backend.ServiceBackend.html,4,['config'],"['config', 'configuration']"
Modifiability,"r missing) key per variant,; rather than as a collection of keys. :param str agg_expr: Sample aggregation expression (per key). :param str test: Statistical test, one of: 'wald', 'lrt', 'score', or 'firth'. :param str y: Response expression. :param covariates: list of covariate expressions.; :type covariates: list of str. :return: Tuple of logistic regression key table and sample aggregation key table.; :rtype: (:py:class:`.KeyTable`, :py:class:`.KeyTable`); """""". r = self._jvdf.logregBurden(key_name, variant_keys, single_key, agg_expr, test, y, jarray(Env.jvm().java.lang.String, covariates)); logreg_kt = KeyTable(self.hc, r._1()); sample_kt = KeyTable(self.hc, r._2()). return logreg_kt, sample_kt. [docs] @handle_py4j; @requireTGenotype; @typecheck_method(pedigree=Pedigree); def mendel_errors(self, pedigree):; """"""Find Mendel errors; count per variant, individual and nuclear; family. .. include:: requireTGenotype.rst. **Examples**. Find all violations of Mendelian inheritance in each (dad,; mom, kid) trio in a pedigree and return four tables:. >>> ped = Pedigree.read('data/trios.fam'); >>> all, per_fam, per_sample, per_variant = vds.mendel_errors(ped); ; Export all mendel errors to a text file:; ; >>> all.export('output/all_mendel_errors.tsv'). Annotate samples with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_samples_table(per_sample, root=""sa.mendel""); ; Annotate variants with the number of Mendel errors:; ; >>> annotated_vds = vds.annotate_variants_table(per_variant, root=""va.mendel""); ; **Notes**; ; This method assumes all contigs apart from X and Y are fully autosomal;; mitochondria, decoys, etc. are not given special treatment. The example above returns four tables, which contain Mendelian violations grouped in; various ways. These tables are modeled after the ; `PLINK mendel formats <https://www.cog-genomics.org/plink2/formats#mendel>`_. The four; tables contain the following columns:; ; **First table:** all Mendel errors. This table contain",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:154588,inherit,inheritance,154588,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['inherit'],['inheritance']
Modifiability,"r of Mendel errors found involving this individual at SNPs.; - **error** (*String*) -- Readable representation of Mendel error.; ; **Fourth table:** errors per variant. This table contains one row per variant in the dataset.; ; Columns:; ; - **v** (*Variant*) -- Variant (key column).; - **nErrors** (*Int*) -- Number of Mendel errors in this variant.; ; **PLINK Mendel error formats:**. - ``*.mendel`` -- all mendel errors: FID KID CHR SNP CODE ERROR; - ``*.fmendel`` -- error count per nuclear family: FID PAT MAT CHLD N; - ``*.imendel`` -- error count per individual: FID IID N; - ``*.lmendel`` -- error count per variant: CHR SNP N; ; In the PLINK formats, **FID**, **KID**, **PAT**, **MAT**, and **IID** refer to family, kid,; dad, mom, and individual ID, respectively, with missing values set to ``0``. SNP denotes ; the variant identifier ``chr:pos:ref:alt``. N is the error count. CHLD is the number of ; children in a nuclear family. The CODE of each Mendel error is determined by the table below,; extending the `Plink; classification <https://www.cog-genomics.org/plink2/basic_stats#mendel>`__. Those individuals implicated by each code are in bold. The copy state of a locus with respect to a trio is defined as follows,; where PAR is the `pseudoautosomal region <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__ (PAR). - HemiX -- in non-PAR of X, male child; - HemiY -- in non-PAR of Y, male child; - Auto -- otherwise (in autosome or PAR, or female child). Any refers to :math:`\{ HomRef, Het, HomVar, NoCall \}` and ! denotes complement in this set. +--------+------------+------------+----------+------------------+; |Code | Dad | Mom | Kid | Copy State |; +========+============+============+==========+==================+; | 1 | HomVar | HomVar | Het | Auto |; +--------+------------+------------+----------+------------------+; | 2 | HomRef | HomRef | Het | Auto |; +--------+------------+------------+----------+------------------+; | 3 | HomRef | ! HomRef | HomVar | Auto",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:158077,extend,extending,158077,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['extend'],['extending']
Modifiability,"rElse(a: T, b: T): T. If a is not missing, returns a. Otherwise, returns b.; Examples; Replace missing phenotype values with the mean value:; >>> [mean_height] = vds.query_samples(['samples.map(s => sa.pheno.height).stats()'])['mean']; >>> vds.annotate_samples_expr('sa.pheno.heightImputed = orElse(sa.pheno.height, %d)' % mean_height). orMissing(a: Boolean, b: T): T – If predicate evaluates to true, returns value. Otherwise, returns NA. pchisqtail(x: Double, df: Double): Double. Returns right-tail probability p for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. x must be positive.; Arguments. x (Double) – Number at which to compute the probability.; df (Double) – Degrees of freedom. pcoin(p: Double): Boolean. Returns true with probability p. This function is non-deterministic.; Arguments. p (Double) – Probability. Should be between 0.0 and 1.0. pnorm(x: Double): Double. Returns left-tail probability p for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable.; Arguments. x (Double) – Number at which to compute the probability. pow(b: Double, x: Double): Double. Returns b raised to the power of x.; Arguments. b (Double) – the base.; x (Double) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – No",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:13658,variab,variable,13658,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability,"rack name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field `cnv_region` indicating inclusion in; at least one interval of the three-column BED file:. >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field `cnv_id` with the value given by the; fourth column of a BED file:. >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; -----. The table produced by this method has one of two possible structures. If; the .bed file has only three fields (`chrom`, `chromStart`, and; `chromEnd`), then the produced table has only one column:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :py:data:`.tstr` and `position` with type :py:data:`.tint32`. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. - *interval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:29306,parameteriz,parameterized,29306,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability,"re symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84631,variab,variables,84631,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"regression_rows(y=[mt.pheno.height, mt.pheno.blood_pressure],; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #2: Use the linear_regression_rows() method for each phenotype sequentially; >>> ht1 = hl.linear_regression_rows(y=mt.pheno.height,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). >>> ht2 = hl.linear_regression_rows(y=mt.pheno.blood_pressure,; ... x=mt.GT.n_alt_alleles(),; ... covariates=[1]). Approach #3: Use the aggregators.linreg() aggregator; >>> mt_linreg = mt.annotate_rows(; ... linreg_height=hl.agg.linreg(y=mt.pheno.height,; ... x=[1, mt.GT.n_alt_alleles()]),; ... linreg_bp=hl.agg.linreg(y=mt.pheno.blood_pressure,; ... x=[1, mt.GT.n_alt_alleles()])). dependencies:; linear_regression_rows(), aggregators.linreg(). understanding:. The linear_regression_rows() method is more efficient than using the aggregators.linreg(); aggregator, especially when analyzing many phenotypes. However, the aggregators.linreg(); aggregator is more flexible (multiple covariates can vary by entry) and returns a richer set of; statistics. The linear_regression_rows() method drops samples that have a missing value for; any of the phenotypes. Therefore, Approach #1 may not be suitable for phenotypes with differential; patterns of missingness. Approach #2 will do two passes over the data while Approaches #1 and #3 will; do one pass over the data and compute the regression statistics for each phenotype simultaneously. Using Variants (SNPs) as Covariates. tags:; sample genotypes covariate. description:; Use sample genotype dosage at specific variant(s) as covariates in regression routines. code:; Create a sample annotation from the genotype dosage for each variant of; interest by combining the filter and collect aggregators:; >>> mt_annot = mt.annotate_cols(; ... snp1 = hl.agg.filter(hl.parse_variant('20:13714384:A:C') == mt.row_key,; ... hl.agg.collect(mt.GT.n_alt_alleles()))[0],; ... snp2 = hl.agg.filter(hl.parse_variant('20:17479730:T:C') == mt.row_key,; ... hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:9705,flexible,flexible,9705,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['flexible'],['flexible']
Modifiability,"req_hwe (float64) – Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; functions.hardy_weinberg_test() for details.; p_value_hwe (float64) – p-value from two-sided test of Hardy-Weinberg; equilibrium. See functions.hardy_weinberg_test() for details.; p_value_excess_het (float64) – p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See functions.hardy_weinberg_test() for details. Warning; het_freq_hwe and p_value_hwe are calculated as in; functions.hardy_weinberg_test(), with non-diploid calls; (ploidy != 2) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, variant_qc(); sets both fields to missing for multiallelic variants. Consider using; split_multi() to split multi-allelic variants beforehand. Parameters:. mt (MatrixTable) – Dataset.; name (str) – Name for resulting field. Returns:; MatrixTable. hail.methods.vep(dataset, config=None, block_size=1000, name='vep', csq=False, tolerate_parse_error=False)[source]; Annotate variants with VEP. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). vep() runs Variant Effect Predictor on the; current dataset and adds the result as a row field.; Examples; Add VEP annotations to the dataset:; >>> result = hl.vep(dataset, ""data/vep-configuration.json"") . Notes; Installation; This VEP command only works if you have already installed VEP on your; computing environment. If you use hailctl dataproc to start Hail clusters,; installing VEP is achieved by specifying the –vep flag. For more detailed instructions,; see Variant Effect Predictor (VEP). If you use hailctl hdinsight, see Variant Effect Predictor (VEP).; Spark Configuration; vep() needs a configuration file to tell it how to run VEP. This is the config argument; to the VEP function. If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:101180,config,config,101180,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['config']
Modifiability,"ression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See Also; --------; :class:`.SwitchBuilder`, :func:`.case`, :func:`.cond`. Parameters; ----------; expr : :class:`.Expression`; Value to match against. Returns; -------; :class:`.SwitchBuilder`; """"""; from .builders import SwitchBuilder. return SwitchBuilder(expr). [docs]@typecheck(f=anytype, exprs=expr_any, _ctx=nullable(str)); def bind(f: Callable, *exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. Examples; --------. >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. :func:`.bind` also can take multiple arguments:. >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters; ----------; f : function ( (args) -> :class:`.Expression`); Function of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""; args = []; uids = []; irs = []. for expr in exprs:; uid = Env.get_uid(base=_ctx); args.append(construct_variable(uid, expr._type, expr._indices, expr._aggregations)); uids.append(uid); irs.append(expr._ir). lambda_result = to_expr(f(*args)); if _ctx:; indices, aggregations = unify_all(lambda_result) # FIXME: hacky. May drop field refs from errors?; else:; indices, aggregations = unify_all(*exprs, lambda_result). res_ir = lambda_result._ir; for uid, value_ir in builtins.zip(uids, irs):; if _ctx == 'agg':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=False); elif _ctx == 'scan':; res_ir = ir.AggLet(uid, value_ir, res_ir, is_scan=True); else:; res_ir = ir.Let(uid, value_ir, res_ir). return construct_expr(res_ir, lambda_result.dtype, indices, aggregations). [docs]def rbind(*exprs, _ctx=None):; """"""Bind a temporary variable and use it i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:17366,variab,variable-length,17366,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:41977,config,configuration,41977,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEP",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106024,config,configuration,106024,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['config'],['configuration']
Modifiability,"rm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.balding_nichols_model(1, n_samples=100, n_variants=20); >>> mt = mt.annotate_rows(gene = mt.locus.position // 12); >>> mt = mt.annotate_rows(weight = 1); >>> mt = mt.annotate_cols(phenotype = hl.agg.sum(mt.GT.n_alt_alleles()) - 20 + hl.rand_norm(0, 1)). Test if the phenotype is significantly associated with the genotype:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0]); >>> ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:75170,variab,variables,75170,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"rn(message, UserWarning, stacklevel=1); return valid_region. def maybe_index(self, indexer_key_expr: StructExpression, all_matches: bool) -> Optional[StructExpression]:; """"""Find the prefix of the given indexer expression that can index the; :class:`.DatasetVersion`, if it exists. Parameters; ----------; indexer_key_expr : :class:`StructExpression`; Row key struct from relational object to be annotated.; all_matches : :obj:`bool`; ``True`` if `indexer_key_expr` key is not unique, indicated in; :attr:`.Dataset.key_properties` for each dataset. If ``True``, value; of `indexer_key_expr` is array of all matches. If ``False``, there; will only be single value of expression. Returns; -------; :class:`StructExpression`, optional; Struct of compatible indexed values, if they exist.; """"""; return hl.read_table(self.url)._maybe_flexindex_table_by_expr(indexer_key_expr, all_matches=all_matches). class Dataset:; """"""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """""". @staticmethod; def from_name_and_json(name: str, doc: dict, region: str, cloud: str) -> Optional['Dataset']:; """"""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:5379,config,configuration,5379,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['config'],"['configuration', 'configurations']"
Modifiability,"rns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this poly",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3199,polymorphi,polymorphism,3199,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"rns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiall",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:2117,polymorphi,polymorphism,2117,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"rocess. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; defa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:11610,config,configuration,11610,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"ror (tfloat64):; Estimated standard deviation of the random error.; multiple_r_squared (tfloat64):; Coefficient of determination for nested models.; adjusted_r_squared (tfloat64):; Adjusted multiple_r_squared taking into account degrees of; freedom.; f_stat (tfloat64):; F-statistic for nested models.; multiple_p_value (tfloat64):; p-value for the; F-test of; nested models.; n (tint64):; Number of samples included in the regression. A sample is included if and; only if y, all elements of x, and weight (if set) are non-missing. All but the last field are missing if n is less than or equal to the; number of covariates or if the covariates are linearly dependent.; If set, the weight parameter generalizes the model to weighted least; squares, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; If any weight is negative, the resulting statistics will be nan. Parameters:. y (Float64Expression) – Response (dependent variable).; x (Float64Expression or list of Float64Expression) – Covariates (independent variables).; nested_dim (int) – The null model includes the first nested_dim covariates.; Must be between 0 and k (the length of x).; weight (Float64Expression, optional) – Non-negative weight for weighted least squares. Returns:; StructExpression – Struct of regression results. hail.expr.aggregators.corr(x, y)[source]; Computes the; Pearson correlation coefficient; between x and y.; Examples; >>> ds.aggregate_cols(hl.agg.corr(ds.pheno.age, ds.pheno.blood_pressure)) ; 0.16592876044845484. Notes; Only records where both x and y are non-missing will be included in the; calculation.; In the case that there are no non-missing pairs, the result will be missing. See also; linreg(). Parameters:. x (Expression of type tfloat64); y (Expression of type tfloat64). Returns:; Float64Expression. hail.expr.aggregators.group_by(group, agg_expr)[source]; Compute aggregation statistics stratified by one or more groups.; Examples; Compute linear regression statistics s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:29765,variab,variables,29765,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['variab'],['variables']
Modifiability,"rrayNumericExpression`; ndarray of the specified size full of ones.; """"""; return full(shape, 1, dtype). [docs]@typecheck(nd=expr_ndarray()); def diagonal(nd):; """"""Gets the diagonal of a 2 dimensional NDArray. Examples; --------. >>> hl.eval(hl.nd.diagonal(hl.nd.array([[1, 2], [3, 4]]))); array([1, 4], dtype=int32). Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional NDArray, shape(M, N). Returns; -------; :class:`.NDArrayExpression`; A 1 dimension NDArray of length min(M, N), containing the diagonal of `nd`.; """"""; assert nd.ndim == 2, ""diagonal requires 2 dimensional ndarray""; shape_min = hl.min(nd.shape[0], nd.shape[1]); return hl.nd.array(hl.range(hl.int32(shape_min)).map(lambda i: nd[i, i])). [docs]@typecheck(a=expr_ndarray(), b=expr_ndarray(), no_crash=bool); def solve(a, b, no_crash=False):; """"""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""; b_ndim_orig = b.ndim; a, b = solve_helper(a, b, b_ndim_orig); if no_crash:; name = ""linear_solve_no_crash""; return_type = hl.tstruct(solution=hl.tndarray(hl.tfloat64, 2), failed=hl.tbool); else:; name = ""linear_solve""; return_type = hl.tndarray(hl.tfloat64, 2). indices, aggregations = unify_all(a, b); ir = Apply(name, return_type, a._ir, b._ir); result = construct_expr(ir, return_type, indices, aggregations). if b_ndim_orig == 1:; if no_crash:; result = hl.struct(solution=result.solution.reshape((-1)), failed=result.failed); else:; result = result.reshape((-1)); return result. [docs]@typecheck(A=expr_ndarray(), b=expr_ndarray(), lower=expr_bool, no_crash=bool); def solve_triangular(A, b, lower=False, no_crash=False):; """"""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:6713,variab,variables,6713,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,2,['variab'],['variables']
Modifiability,"rt modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:221769,config,configuration,221769,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['config'],['configuration']
Modifiability,"ructExpression, struct_error(obj), True; elif isinstance(obj, SetStructExpression):; return 'SetStructExpression', SetStructExpression, struct_error(obj), True; else:; raise NotImplementedError(obj). def get_nice_attr_error(obj, item):; class_name, cls, handler, has_describe = get_obj_metadata(obj). if item.startswith('_'):; # don't handle 'private' attribute access; return ""{} instance has no attribute '{}'"".format(class_name, item); else:; field_names = obj._fields.keys(); field_dict = defaultdict(lambda: []); for f in field_names:; field_dict[f.lower()].append(f). obj_namespace = {x for x in dir(cls) if not x.startswith('_')}; inherited = {x for x in obj_namespace if x not in cls.__dict__}; methods = {x for x in obj_namespace if x in cls.__dict__ and callable(cls.__dict__[x])}; props = obj_namespace - methods - inherited. item_lower = item.lower(). field_matches = difflib.get_close_matches(item_lower, field_dict, n=5); inherited_matches = difflib.get_close_matches(item_lower, inherited, n=5); method_matches = difflib.get_close_matches(item_lower, methods, n=5); prop_matches = difflib.get_close_matches(item_lower, props, n=5). s = [""{} instance has no field, method, or property '{}'"".format(class_name, item)]; if any([field_matches, method_matches, prop_matches, inherited_matches]):; s.append('\n Did you mean:'); if field_matches:; fs = []; for f in field_matches:; fs.extend(field_dict[f]); word = plural('field', len(fs)); s.append('\n Data {}: {}'.format(word, ', '.join(handler(f) for f in fs))); if method_matches:; word = plural('method', len(method_matches)); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(m) for m in method_matches)); ); if prop_matches:; word = plural('property', len(prop_matches), 'properties'); s.append(; '\n {} {}: {}'.format(class_name, word, ', '.join(""'{}'"".format(p) for p in prop_matches)); ); if inherited_matches:; word = plural('inherited method', len(inherited_matches)); s.append(; '\n {} {}: {}'.format(; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:8486,inherit,inherited,8486,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,2,['inherit'],['inherited']
Modifiability,"rue, skip loci that are not consistent with reference_genome. Returns:; MatrixTable. hail.methods.import_locus_intervals(path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs)[source]; Import a locus interval list as a Table.; Examples; Add the row field capture_region indicating inclusion in; at least one locus interval from capture_intervals.txt:; >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. contig:start-end; contig  start  end (tab-separated); contig  start  end  direction  target (tab-separated). A file in either of the first two formats produces a table with one; field:. interval (tinterval) - Row key. Genomic interval. If; reference_genome is defined, the point type of the interval will be; tlocus parameterized by the reference_genome. Otherwise,; the point type is a tstruct with two fields: contig with; type tstr and position with type tint32. A file in the third format (with a “target” column) produces a table with two; fields:. interval (tinterval) - Row key. Same schema as above.; target (tstr). If reference_genome is defined AND the file has one field, intervals; are parsed with parse_locus_interval(). See the documentation for; valid inputs.; If reference_genome is NOT defined and the file has one field,; intervals are parsed with the regex `""([^:]*):(\d+)\-(\d+)""; where contig, start, and end match each of the three capture groups.; start and end match positions inclusively, e.g.; start <= position <= end.; For files with three or five fields, start and end match positions; inclusively, e.g. start <= position <= end. Parameters:. path (str) – Path to file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:20289,parameteriz,parameterized,20289,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability,"ry=None,; gcs_requester_pays_configuration: Optional[GCSRequesterPaysConfiguration] = None,; regions: Optional[List[str]] = None,; gcs_bucket_allow_list: Optional[Dict[str, List[str]]] = None,; copy_spark_log_on_error: bool = False,; ):; """"""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not speci",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:7240,Config,Configure,7240,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['Config'],['Configure']
Modifiability,"s locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multial",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1960,polymorphi,polymorphism,1960,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"s of the sampled object. Parameters:. p (float) – Probability of keeping each row.; seed (int) – Random seed. Returns:; MatrixTable – Matrix table with approximately p * n_rows rows. select_cols(*exprs, **named_exprs)[source]; Select existing column fields or create new fields by name, dropping the rest.; Examples; Select existing fields and compute a new one:; >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method supports aggregation over rows. For instance, the usage:; >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified column fields. select_entries(*exprs, **named_exprs)[source]; Select existing entry fields or create new fields by name, dropping the rest.; Examples; Drop all entry fields aside from GT:; >>> dataset_result = dataset.select_entries(dataset.GT). Notes; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:55111,variab,variable-length,55111,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"s.major_label_text_font_size = font_size; if hasattr(p.title, 'text_font_size'):; p.title.text_font_size = font_size; if hasattr(p.xaxis, 'group_text_font_size'):; p.xaxis.group_text_font_size = font_size; return p. [docs]@typecheck(; x=expr_numeric,; y=expr_numeric,; bins=oneof(int, sequenceof(int)),; range=nullable(sized_tupleof(nullable(sized_tupleof(numeric, numeric)), nullable(sized_tupleof(numeric, numeric)))),; title=nullable(str),; width=int,; height=int,; colors=sequenceof(str),; log=bool,; ); def histogram2d(; x: NumericExpression,; y: NumericExpression,; bins: int = 40,; range: Optional[Tuple[int, int]] = None,; title: Optional[str] = None,; width: int = 600,; height: int = 600,; colors: Sequence[str] = bokeh.palettes.all_palettes['Blues'][7][::-1],; log: bool = False,; ) -> figure:; """"""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be consid",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:17232,variab,variable,17232,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,2,['variab'],['variable']
Modifiability,"s:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolera",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106247,config,config,106247,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,"['config', 'variab']","['config', 'variable']"
Modifiability,"s; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """"""; return _comparison_func('max', exprs, filter_missing, filter_nan=False). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filter_missing=builtins.bool; ); def nanmin(*exprs, filter_missing: builtins.bool = True) -> NumericExpression:; """"""Returns the minimum value of a collection or of given arguments, excluding NaN. Examples; --------. Compute the minimum value of an array:. >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:. >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.1. Notes; -----; Like the Python builtin ``min`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the minimum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. NaN arguments / array elements are ignored; the minimum value of `NaN` and; any non-`NaN` value `x` is `x`. See Also; --------; :func:`min`, :func:`max`, :func:`nanmax`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing minimum. Returns; -------; :class:`.NumericExpression`; """""". return _comparison_func('min', exprs, filter_missing, filter_nan=True). [docs]@typecheck(; exprs=expr_oneof(expr_numeric, expr_set(expr_numeric), expr_array(expr_numeric)), filt",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:128536,variab,variable-length,128536,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"s; else:; raise ValueError(f""'{method}/pass_through': found duplicated field {f!r}""); row_fields[f] = mt[f]; else:; assert isinstance(f, Expression); if not f._ir.is_nested_field:; raise ValueError(f""'{method}/pass_through': expect fields or nested fields, not complex expressions""); if not f._indices == mt._row_indices:; raise ExpressionException(; f""'{method}/pass_through': require row-indexed fields, found indices {f._indices.axes}""; ); name = f._ir.name; if name in row_fields:; # allow silent pass through of key fields; if not (name in mt.row_key and f._ir == mt[name]._ir):; raise ValueError(f""'{method}/pass_through': found duplicated field {name!r}""); row_fields[name] = f; for k in mt.row_key:; del row_fields[k]; return row_fields. [docs]@typecheck(; y=oneof(expr_float64, sequenceof(expr_float64), sequenceof(sequenceof(expr_float64))),; x=expr_float64,; covariates=sequenceof(expr_float64),; block_size=int,; pass_through=sequenceof(oneof(str, Expression)),; weights=nullable(oneof(expr_float64, sequenceof(expr_float64))),; ); def linear_regression_rows(y, x, covariates, block_size=16, pass_through=(), *, weights=None) -> Table:; r""""""For each row, test an input variable for association with; response variables using linear regression. Examples; --------. >>> result_ht = hl.linear_regression_rows(; ... y=dataset.pheno.height,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; As in the example, the intercept covariate ``1`` must be; included **explicitly** if desired. Warning; -------; If `y` is a single value or a list, :func:`.linear_regression_rows`; considers the same set of columns (i.e., samples, points) for every response; variable and row, namely those columns for which **all** response variables; and covariates are defined. If `y` is a list of lists, then each inner list is treated as an; independent group, subsetting columns for missingness separately. Notes; -----; With the default root a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:8336,variab,variable,8336,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,4,['variab'],"['variable', 'variables']"
Modifiability,"se ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regression_rows/y', y, mt._col_indices). all_exprs = [y]; for e in covariates:; all_exprs.append(e); analyze('poisson_regression_rows/covariates', e, mt._col_indices). _warn_if_no_intercept('poisson_regression_rows', covariates). x_field_name = Env.get_uid(); y_field_name = '__y'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))); row_fields = _get_regression_row_fields(mt, pass_through, 'poisson_regression_rows'). # FIXME: selecting an existing entry field should be emitted as a SelectFields; mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs=row_fields,; col_key=[],; entry_exprs={x_field_name: x},; ). config = {; 'name': 'PoissonRegression',; 'test': test,; 'yField': y_field_name,; 'xField': x_field_name,; 'covFields': cov_field_names,; 'passThrough': [x for x in row_fields if x not in mt.row_key],; 'maxIterations': max_iterations,; 'tolerance': tolerance,; }. return Table(ir.MatrixToTableApply(mt._mir, config)).persist(). @typecheck(; test=enumeration('wald', 'lrt', 'score'),; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ):; assert max_iterations > 0. if tolerance is None:; tolerance = 1e-8; assert tolerance > 0.0. k = len(covariates); if k == 0:; raise ValueError('_lowered_poisson_regression_rows: at least one covariate is required.'); _warn_if_no_intercept('_lowered_poisson_regression_rows', covariates). mt = matrix_table_source('_lowered_poisson_regression_rows/x', x); raise_unless_entry_indexed('_lowere",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:63611,config,config,63611,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['config'],['config']
Modifiability,"se where the position is ``0`` **AND** the interval is; **left-exclusive** which is normalized to be ``1`` and left-inclusive.; Likewise, in the case where the position is ``END + 1`` **AND**; the interval is **right-exclusive** which is normalized to be ``END``; and right-inclusive. Parameters; ----------; s : str or :class:`.StringExpression`; String to parse.; reference_genome : :class:`str` or :class:`.hail.genetics.ReferenceGenome`; Reference genome to use.; invalid_missing : :class:`.BooleanExpression`; If ``True``, invalid intervals are set to NA rather than causing an exception. Returns; -------; :class:`.IntervalExpression`; """"""; return _func('LocusInterval', tinterval(tlocus(reference_genome)), s, invalid_missing). [docs]@typecheck(alleles=expr_int32, phased=expr_bool); def call(*alleles, phased=False) -> CallExpression:; """"""Construct a call expression. Examples; --------. >>> hl.eval(hl.call(1, 0)); Call(alleles=[0, 1], phased=False). Parameters; ----------; alleles : variable-length args of :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; List of allele indices.; phased : :obj:`bool`; If ``True``, preserve the order of `alleles`. Returns; -------; :class:`.CallExpression`; """"""; if builtins.len(alleles) > 2:; raise NotImplementedError(""'call' supports a maximum of 2 alleles.""); return _func('Call', tcall, *alleles, phased). [docs]@typecheck(gt_index=expr_int32); def unphased_diploid_gt_index_call(gt_index) -> CallExpression:; """"""Construct an unphased, diploid call from a genotype index. Examples; --------. >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters; ----------; gt_index : :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; Unphased, diploid genotype index. Returns; -------; :class:`.CallExpression`; """"""; return _func('UnphasedDiploidGtIndexCall', tcall, to_expr(gt_index)). [docs]@typecheck(s=expr_str); def parse_call(s) -> CallExpression:; """"""Construct a call expression by par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:49266,variab,variable-length,49266,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"sents a genetic data set as a matrix where the rows are keyed by; Variant objects, the columns are keyed by samples, and each cell is a; Genotype object. Variant objects and Genotype objects each; have methods to access attributes such as chromosome name and genotype call.; Although this representation is similar to the VCF format, Hail uses a fast and; storage-efficient internal representation called a Variant Dataset (VDS).; In addition to information about Samples, Variants, and Genotypes, Hail stores meta-data as annotations that can be attached to each variant (variant annotations),; each sample (sample annotations), and global to the dataset (global annotations).; Annotations in Hail can be thought of as a hierarchical data structure with a specific schema that is typed (similar to the JSON format).; For example, given this schema:; va: Struct {; qc: Struct {; callRate: Double,; AC: Int,; hwe: Struct {; rExpectedHetFrequency: Double,; pHWE: Double; }; }; }. The callRate variable can be accessed with va.qc.callRate and has a Double type and the AC variable can be accessed with va.qc.AC and has an Int type.; To access the pHWE and the rExpectedHetFrequency variables which are nested inside an extra struct referenced as va.hwe, use va.qc.hwe.pHWE and va.qc.hwe.rExpectedHetFrequency. Expressions¶; Expressions are snippets of code written in Hail’s expression language referencing elements of a VDS that are used for the following operations:. Define Variables to Export; Input Variables to Methods; Filter Data; Add New Annotations. The abbreviations for the VDS elements in expressions are as follows:. Symbol; Description. v; Variant. s; sample. va; Variant Annotations. sa; Sample Annotations. global; Global Annotations. gs; Row or Column of Genotypes (Genotype Aggregable). variants; Variant Aggregable. samples; Sample Aggregable. Which VDS elements are accessible in an expression is dependent on the command being used. Define Variables to Export¶; To define how to exp",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/overview.html:2378,variab,variable,2378,docs/0.1/overview.html,https://hail.is,https://hail.is/docs/0.1/overview.html,1,['variab'],['variable']
Modifiability,"set.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{is\_female}\) is coded as; for True (female) and 0 for False (male). The null model sets; \(\beta_1 = 0\).; The structure of the emitted row field depends on the test statistic as; shown in the tables below. Test; Field; Type; Value. Wald; beta; float64; fit effect coefficient,; \(\hat\beta_1\). Wald; standard_error; float64; estimated standard error,; \(\widehat{\mathrm{se}}\). Wald; z_stat; float64; Wald \(z\)-statist",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:8417,variab,variable,8417,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphism (MNP). Return type:bool. is_SNP()[source]¶; True if this alternate allele is a single nucleotide polymorphism (SNP). Return type:bool. is_complex()[source]¶; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. Return type:bool. is_deletion()[source]¶; True if this alternate allele is a deletion of one or more bases. Return type:bool. is_indel()[source]¶; True if this alternate allele is either an insertion or deletion of one or more bases. Return type:bool. is_insertion()[source]¶; True if this alternate allele is an insertion of one or more bases. Return type:bool. is_transition()[source]¶; True if this alternate allele is a transition SNP.; This is true if the reference and alternate bases are; both purine (A/G) or both pyrimidine (C/T). This method; raises an exception if the polymorphism is not a SNP. Return type:bool. is_transversion()[source]¶; True if this alternate allele is a transversion SNP.; This is true if the reference and alternate bases contain; one purine (A/G) and one pyrimidine (C/T). Thi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:2100,polymorphi,polymorphism,2100,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,1,['polymorphi'],['polymorphism']
Modifiability,"sm. is_transition(ref, alt); Returns True if the alleles constitute a transition. is_transversion(ref, alt); Returns True if the alleles constitute a transversion. is_insertion(ref, alt); Returns True if the alleles constitute an insertion. is_deletion(ref, alt); Returns True if the alleles constitute a deletion. is_indel(ref, alt); Returns True if the alleles constitute an insertion or deletion. is_star(ref, alt); Returns True if the alleles constitute an upstream deletion. is_complex(ref, alt); Returns True if the alleles constitute a complex polymorphism. is_strand_ambiguous(ref, alt); Returns True if the alleles are strand ambiguous. is_valid_contig(contig[, reference_genome]); Returns True if contig is a valid contig name in reference_genome. is_valid_locus(contig, position[, ...]); Returns True if contig and position is a valid site in reference_genome. contig_length(contig[, reference_genome]); Returns the length of contig in reference_genome. allele_type(ref, alt); Returns the type of the polymorphism as a string. numeric_allele_type(ref, alt); Returns the type of the polymorphism as an integer. pl_dosage(pl); Return expected genotype dosage from array of Phred-scaled genotype likelihoods with uniform prior. gp_dosage(gp); Return expected genotype dosage from array of genotype probabilities. get_sequence(contig, position[, before, ...]); Return the reference sequence at a given locus. mendel_error_code(locus, is_female, father, ...); Compute a Mendelian violation code for genotypes. liftover(x, dest_reference_genome[, ...]); Lift over coordinates to a different reference genome. min_rep(locus, alleles); Computes the minimal representation of a (locus, alleles) polymorphism. reverse_complement(s[, rna]); Reverses the string and translates base pairs into their complements . hail.expr.functions.locus(contig, pos, reference_genome='default')[source]; Construct a locus expression from a chromosome and position.; Examples; >>> hl.eval(hl.locus(""1"", 10000, referen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:2986,polymorphi,polymorphism,2986,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"solr(self, zk_host, collection, block_size=100):; """"""Export to Solr.; ; .. warning::. :py:meth:`~.export_solr` is EXPERIMENTAL. """""". self._jkt.exportSolr(zk_host, collection, block_size). [docs] @handle_py4j; @typecheck_method(address=strlike,; keyspace=strlike,; table=strlike,; block_size=integral,; rate=integral); def export_cassandra(self, address, keyspace, table, block_size=100, rate=1000):; """"""Export to Cassandra. .. warning::. :py:meth:`~.export_cassandra` is EXPERIMENTAL. """""". self._jkt.exportCassandra(address, keyspace, table, block_size, rate). [docs] @handle_py4j; @typecheck_method(host=strlike,; port=integral,; index=strlike,; index_type=strlike,; block_size=integral,; config=nullable(dictof(strlike, strlike)),; verbose=bool); def export_elasticsearch(self, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export to Elasticsearch. .. warning::. :py:meth:`~.export_elasticsearch` is EXPERIMENTAL. """""". self._jkt.exportElasticsearch(host, port, index, index_type, block_size, config, verbose). [docs] @handle_py4j; @typecheck_method(column_names=oneof(strlike, listof(strlike))); def explode(self, column_names):; """"""Explode columns of this key table. The explode operation unpacks the elements in a column of type ``Array`` or ``Set`` into its own row.; If an empty ``Array`` or ``Set`` is exploded, the entire row is removed from the :py:class:`.KeyTable`. **Examples**. Assume ``kt3`` is a :py:class:`.KeyTable` with three columns: c1, c2 and; c3. >>> kt3 = hc.import_table('data/kt_example3.tsv', impute=True,; ... types={'c1': TString(), 'c2': TArray(TInt()), 'c3': TArray(TArray(TInt()))}). The types of each column are ``String``, ``Array[Int]``, and ``Array[Array[Int]]`` respectively.; c1 cannot be exploded because its type is not an ``Array`` or ``Set``.; c2 can only be exploded once because the type of c2 after the first explode operation is ``Int``. +----+----------+----------------+; | c1 | c2 | c3 |; +====+==========+================+",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:17477,config,config,17477,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['config'],['config']
Modifiability,"somal(self):; """"""True if this polymorphism is located on an autosome. :rtype: bool; """"""; return self._jrep.isAutosomal(). [docs] def is_mitochondrial(self):; """"""True if this polymorphism is mapped to mitochondrial DNA. :rtype: bool; """""". return self._jrep.isMitochondrial(). [docs] def in_X_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXPar(). [docs] def in_Y_PAR(self):; """"""True of this polymorphism is found on the pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYPar(). [docs] def in_X_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. :rtype: bool; """""". return self._jrep.inXNonPar(). [docs] def in_Y_non_PAR(self):; """"""True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. :rtype: bool; """""". return self._jrep.inYNonPar(). [docs]class AltAllele(object):; """"""; An object that represents an allele in a polymorphism deviating from the reference allele. :param str ref: reference allele; :param str alt: alternate allele; """""". @handle_py4j; def __init__(self, ref, alt):; jaa = scala_object(Env.hail().variant, 'AltAllele').apply(ref, alt); self._init_from_java(jaa); self._ref = ref; self._alt = alt. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'AltAllele(ref=%s, alt=%s)' % (self.ref, self.alt). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep. @classmethod; def _from_java(cls, jaa):; aa = AltAllele.__new__(cls); aa._init_from_java(jaa); aa._ref = jaa.ref(); aa._alt = jaa.alt(); return aa. @property; def ref(self):; """"""; Reference allele. :rtype: str; """"""; return self._ref. @property; def alt(self):; """"""; Alternate allele. :rtype: str; """"""; return self._alt. [docs] def num_mismatch(self):; """"""Returns the number of mismatched ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:5955,polymorphi,polymorphism,5955,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"sp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""; if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'nirvana'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'nirvana'); ht = dataset.select(). annotations = Table(; TableToTableApply(ht._tir, {'name': 'Nirvana', 'config': config, 'blockSize': block_size}); ).persist(). if isinstance(dataset, MatrixTable):; return dataset.annotate_rows(**{name: annotations[dataset.row_key].nirvana}); else:; return dataset.annotate(**{name: annotations[dataset.key].nirvana}). class _VariantSummary(object):; def __init__(self, rg, n_variants, alleles_per_variant, variants_per_contig, allele_types, nti, ntv):; self.rg = rg; self.n_variants = n_variants; self.all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:51693,config,config,51693,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['config'],"['config', 'configuration']"
Modifiability,"ssion) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_deletion(ref, alt)[source]; Returns True if the alleles constitute a deletion.; Examples; >>> hl.eval(hl.is_deletion('ATT', 'A')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_indel(ref, alt)[source]; Returns True if the alleles constitute an insertion or deletion.; Examples; >>> hl.eval(hl.is_indel('ATT', 'A')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_star(ref, alt)[source]; Returns True if the alleles constitute an upstream deletion.; Examples; >>> hl.eval(hl.is_star('A', '*')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_complex(ref, alt)[source]; Returns True if the alleles constitute a complex polymorphism.; Examples; >>> hl.eval(hl.is_complex('ATT', 'GCAC')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_strand_ambiguous(ref, alt)[source]; Returns True if the alleles are strand ambiguous.; Strand ambiguous allele pairs are A/T, T/A,; C/G, and G/C where the first allele is ref; and the second allele is alt.; Examples; >>> hl.eval(hl.is_strand_ambiguous('A', 'T')); True. Parameters:. ref (StringExpression) – Reference allele.; alt (StringExpression) – Alternate allele. Returns:; BooleanExpression. hail.expr.functions.is_valid_contig(contig, reference_genome='default')[source]; Returns True if contig is a valid contig name in reference_genome.; Examples; >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters:. contig (Expression of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:14798,polymorphi,polymorphism,14798,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['polymorphi'],['polymorphism']
Modifiability,"ssions:; >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:36316,variab,variable-length,36316,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"st. This method is much less computationally expensive than; :py:meth:`.split_multi`, and can also be used to produce; a variant dataset that can be used with methods that do not; support multiallelic variants. :return: Dataset with no multiallelic sites, which can; be used for biallelic-only methods.; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.filterMulti()). [docs] @handle_py4j; def drop_samples(self):; """"""Removes all samples from variant dataset. The variants, variant annotations, and global annnotations will remain,; producing a sites-only variant dataset. :return: Sites-only variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.dropSamples()). [docs] @handle_py4j; @typecheck_method(expr=strlike,; keep=bool); def filter_samples_expr(self, expr, keep=True):; """"""Filter samples with the expression language. **Examples**. Filter samples by phenotype (assumes sample annotation *sa.isCase* exists and is a Boolean variable):. >>> vds_result = vds.filter_samples_expr(""sa.isCase""). Remove samples with an ID that matches a regular expression:. >>> vds_result = vds.filter_samples_expr('""^NA"" ~ s' , keep=False). Filter samples from sample QC metrics and write output to a new variant dataset:. >>> (vds.sample_qc(); ... .filter_samples_expr('sa.qc.callRate >= 0.99 && sa.qc.dpMean >= 10'); ... .write(""output/filter_samples.vds"")). **Notes**. ``expr`` is in sample context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. For more information, see the documentation on `data representation, annotations <overview.html#>`__, and; the `expression language <exprlang.html>`__. .. caution::; When ``expr`` evaluates to missing, the sample will be removed regardless of whether ``keep=True`` or ``keep=False``. :param str expr: Boolean filter e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:68314,variab,variable,68314,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['variab'],['variable']
Modifiability,"standing:. We have presented two ways to compute linear regression statistics for each value of a grouping; variable. The first approach utilizes the linear_regression_rows() method and must be called; separately for each group even though it can compute statistics for multiple phenotypes; simultaneously. This is because the linear_regression_rows() method drops samples that have a; missing value for any of the phenotypes. When the groups are mutually exclusive,; such as ‘Male’ and ‘Female’, no samples remain! Note that we cannot define male_pheno = ~female_pheno; because we subsequently need male_pheno to be an expression on the mt_linreg matrix table; rather than mt. Lastly, the argument to root must be specified for both cases – otherwise; the ‘Male’ output will overwrite the ‘Female’ output.; The second approach uses the aggregators.group_by() and aggregators.linreg(); aggregators. The aggregation expression generates a dictionary where a key is a group; (value of the grouping variable) and the corresponding value is the linear regression statistics; for those samples in the group. The result of the aggregation expression is then used to annotate; the matrix table.; The linear_regression_rows() method is more efficient than the aggregators.linreg(); aggregator and can be extended to multiple phenotypes, but the aggregators.linreg(); aggregator is more flexible (multiple covariates can be vary by entry) and returns a richer; set of statistics. PLINK Conversions. Polygenic Score Calculation. plink:; >>> plink --bfile data --score scores.txt sum . tags:; PRS. description:; This command is analogous to plink’s –score command with the; sum option. Biallelic variants are required. code:; >>> mt = hl.import_plink(; ... bed=""data/ldsc.bed"", bim=""data/ldsc.bim"", fam=""data/ldsc.fam"",; ... quant_pheno=True, missing='-9'); >>> mt = hl.variant_qc(mt); >>> scores = hl.import_table('data/scores.txt', delimiter=' ', key='rsid',; ... types={'score': hl.tfloat32}); >>> mt = mt.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/guides/genetics.html:13106,variab,variable,13106,docs/0.2/guides/genetics.html,https://hail.is,https://hail.is/docs/0.2/guides/genetics.html,1,['variab'],['variable']
Modifiability,"stically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Releas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78542,config,configuration,78542,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28162,variab,variable,28162,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"submitted script. Bug Fixes. (#14673) Fix typo in; Interpret rule for TableAggregate.; (#14697) Set; QUAL=""."" to missing rather than htsjdk’s sentinel value.; (#14292) Prevent GCS; cold storage check from throwing an error when reading from a public; access bucket.; (#14651) Remove; jackson string length restriction for all backends.; (#14653) Add; --public-ip-address argument to gcloud dataproc start command; built by hailctl dataproc start, fixing creation of dataproc 2.2; clusters. Version 0.2.132; Released 2024-07-08. New Features. (#14572) Added; StringExpression.find for finding substrings in a Hail str. Bug Fixes. (#14574) Fixed; TypeError bug when initializing Hail Query with; backend='batch'.; (#14571) Fixed a; deficiency that caused certain pipelines that construct Hail; NDArrays from streams to run out of memory.; (#14579) Fix; serialization bug that broke some Query-on-Batch pipelines with many; complex expressions.; (#14567) Fix Jackson; configuration that broke some Query-on-Batch pipelines with many; complex expressions. Version 0.2.131; Released 2024-05-30. New Features. (#14560) The gvcf; import stage of the VDS combiner now preserves the GT of reference; blocks. Some datasets have haploid calls on sex chromosomes, and the; fact that the reference was haploid should be preserved. Bug Fixes. (#14563) The version; of notebook installed in Hail Dataproc clusters has been upgraded; from 6.5.4 to 6.5.6 in order to fix a bug where Jupyter Notebooks; wouldn’t start on clusters. The workaround involving creating a; cluster with --packages='ipython<8.22' is no longer necessary. Deprecations. (#14158) Hail now; supports and primarily tests against Dataproc 2.2.5, Spark 3.5.0, and; Java 11. We strongly recommend updating to Spark 3.5.0 and Java 11.; You should also update your GCS connector after installing Hail:; curl https://broad.io/install-gcs-connector | python3. Do not try; to update before installing Hail 0.2.131. Version 0.2.130; Released 2024",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:11299,config,configuration,11299,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"t than producing the full table and; then filtering using Table.filter().; >>> rel = hl.pc_relate(dataset.GT, 0.01, k=10, min_kinship=0.1) . One can also pass in pre-computed principal component scores.; To produce the same results as in the previous example:; >>> _, scores_table, _ = hl.hwe_normalized_pca(dataset.GT,; ... k=10,; ... compute_loadings=False); >>> rel = hl.pc_relate(dataset.GT,; ... 0.01,; ... scores_expr=scores_table[dataset.col_key].scores,; ... min_kinship=0.1) . Notes; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with estimated allele; frequencies \(\widehat{p}_{s}\) at SNP \(s\), is given by:. \[\widehat{\psi}_{ij} \coloneqq; \frac{1}{\left|\mathcal{S}_{ij}\right|}; \sum_{s \in \mathcal{S}_{ij}}; \frac{\left(g_{is} - 2\hat{p}_{s}\right)\left(g_{js} - 2\widehat{p}_{s}\right)}; {4 \widehat{p}_{s}\left(1-\widehat{p}_{s}\right)}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry groups are mixed in a sample, this model breaks; down. Alleles that are rare in all but one ancestry group are treated as; very informative to relatedness. However, these alleles are simply; markers of the ancestry group. The PC-Relate method corrects for this; situation and the related situation of admixed individuals.; PC-Relate slightly modifies the usual estimator for relatedness:; occurrences of population allele frequency are replaced with an; “individual-specific allele frequency”. This modification allows the; method to correctly weight an allele according to an individual’s unique; ancestry profile.; The “individual-specific allele frequency” at a given genetic locus is; modeled by PC-Relate as ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:13919,inherit,inherited,13919,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['inherit'],['inherited']
Modifiability,"t the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to control for confounding caused by population stratification. Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language¶; This notebook starts with the basics of the Hail expression language, and builds up practical experience; with the type system, syntax, and functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice, dice, filter, and query genetic data. Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate¶; This notebook uses the Hail expression language to query, filter, and annotate the same thousand genomes; dataset from the overview. We also cover how to compute aggregate statistics from a dataset using the; expression language. Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:2897,variab,variables,2897,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,1,['variab'],['variables']
Modifiability,"t will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W. weights_arr = hl.array(ht.weight); A = (; hl.case(); .when(; hl.all(weights_arr.map(lambda x: x >= 0)),; (ht.G - ht.covmat_Q @ (ht.covmat_Q.T @ ht.G)) * hl.sqrt(ht.weight),; ); .or_error(; hl.format(; 'hl._linear_skat: every weight must be positive, in group %s, the weights were: %s',; ht.group,; weights_arr,; ); ); ); singular_values = hl.nd.svd(A, compute_uv=False). # SVD(M) = U S V. U and V are unitary, therefore SVD(k M) = U (k S) V.; eigenvalues = ht.s2 * singular_values.map(lambda x: x**2). # The R implementation of SKAT, Function.R, Get_Lambda_Approx filters the eigenvalues,; # presumably because a good estimate of the Generalized Chi-Sqaured CDF is not significantly; # affected by chi-squared components with very tiny weight",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:84759,variab,variable,84759,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"t"":[1,1,1],""fst"":[0.1,0.1,0.1],""mixture"":false}. Notes; For entry-indexed expressions, if there is one column key field, the; result of calling str() on that field is used as; the column header. Otherwise, each compound column key is converted to; JSON and used as a column header. For example:; >>> small_mt = small_mt.key_cols_by(s=small_mt.sample_idx, family='fam1'); >>> small_mt.GT.export('output/gt-no-header.tsv'); >>> with open('output/gt-no-header.tsv', 'r') as f:; ... for line in f:; ... print(line, end=''); locus alleles {""s"":0,""family"":""fam1""} {""s"":1,""family"":""fam1""} {""s"":2,""family"":""fam1""} {""s"":3,""family"":""fam1""}; 1:1 [""A"",""C""] 0/1 0/0 0/1 0/0; 1:2 [""A"",""C""] 1/1 0/1 0/1 0/1; 1:3 [""A"",""C""] 0/0 0/1 0/0 0/0; 1:4 [""A"",""C""] 0/1 1/1 0/1 0/1. Parameters:. path (str) – The path to which to export.; delimiter (str) – The string for delimiting columns.; missing (str) – The string to output for missing values.; header (bool) – When True include a header line. extend(a); Concatenate two arrays and return the result.; Examples; >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters:; a (ArrayExpression) – Array to concatenate, same type as the callee. Returns:; ArrayExpression. filter(f); Returns a new collection containing elements where f returns True.; Examples; >>> hl.eval(a.filter(lambda x: x % 2 == 0)); [2, 4]. >>> hl.eval(s3.filter(lambda x: ~(x[-1] == 'e'))) ; {'Bob'}. Notes; Returns a same-type expression; evaluated on a SetExpression, returns a; SetExpression. Evaluated on an ArrayExpression,; returns an ArrayExpression. Parameters:; f (function ( (arg) -> BooleanExpression)) – Function to evaluate for each element of the collection. Must return a; BooleanExpression. Returns:; CollectionExpression – Expression of the same type as the callee. find(f); Returns the first element where f returns True.; Examples; >>> hl.eval(a.find(lambda x: x ** 2 > 20)); 5. >>> hl.eval(s3.find(lambda x: x[0] == 'D')); None. Notes; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html:9958,extend,extend,9958,docs/0.2/hail.expr.ArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.ArrayNumericExpression.html,1,['extend'],['extend']
Modifiability,"t.id][k],; proband=mt[cols_sym][t.id],; father=mt[cols_sym][t.pat_id],; mother=mt[cols_sym][t.mat_id],; is_female=t.is_female,; fam_id=t.fam_id,; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.annotate(**{; entries_sym: hl.map(; lambda i: hl.bind(; lambda t: hl.struct(; proband_entry=mt[entries_sym][t.id],; father_entry=mt[entries_sym][t.pat_id],; mother_entry=mt[entries_sym][t.mat_id],; ),; mt[trios_sym][i],; ),; hl.range(0, n_trios),; ); }); mt = mt.drop(trios_sym). return mt._unlocalize_entries(entries_sym, cols_sym, ['id']). [docs]@typecheck(call=expr_call, pedigree=Pedigree); def mendel_errors(call, pedigree) -> Tuple[Table, Table, Table, Table]:; r""""""Find Mendel errors; count per variant, individual and nuclear family. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Find all violations of Mendelian inheritance in each (dad, mom, kid) trio in; a pedigree and return four tables (all errors, errors by family, errors by; individual, errors by variant):. >>> ped = hl.Pedigree.read('data/trios.fam'); >>> all_errors, per_fam, per_sample, per_variant = hl.mendel_errors(dataset['GT'], ped). Export all mendel errors to a text file:. >>> all_errors.export('output/all_mendel_errors.tsv'). Annotate columns with the number of Mendel errors:. >>> annotated_samples = dataset.annotate_cols(mendel=per_sample[dataset.s]). Annotate rows with the number of Mendel errors:. >>> annotated_variants = dataset.annotate_rows(mendel=per_variant[dataset.locus, dataset.alleles]). Notes; -----. The example above returns four tables, which contain Mendelian violations; grouped in various ways. These tables are modeled after the `PLINK mendel; formats <https://www.cog-genomics.org/plink2/formats#mendel>`_, resembling; the ``.mendel``, ``.fmendel``, ``.imendel``, and ``.lmendel`` formats,; respectively. **First table:** all Mendel errors. This table contains one row per Me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:4482,inherit,inheritance,4482,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['inherit'],['inheritance']
Modifiability,"t64,; covariates=sequenceof(expr_float64),; pass_through=sequenceof(oneof(str, Expression)),; max_iterations=int,; tolerance=nullable(float),; ); def poisson_regression_rows(; test, y, x, covariates, pass_through=(), *, max_iterations: int = 25, tolerance: Optional[float] = None; ) -> Table:; r""""""For each row, test an input variable for association with a; count response variable using `Poisson regression <https://en.wikipedia.org/wiki/Poisson_regression>`__. Notes; -----; See :func:`.logistic_regression_rows` for more info on statistical tests; of general linear models. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression`; Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; Non-empty list of column-indexed covariate expressions.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; tolerance : :obj:`float`, optional; The iterative fit of this model is considered ""converged"" if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns; -------; :class:`.Table`. """"""; if hl.current_backend().requires_lowering:; return _lowered_poisson_regression_rows(; test, y, x, covariates, pass_through, max_iterations=max_iterations, tolerance=tolerance; ). if tolerance is None:; tolerance = 1e-6; assert tolerance > 0.0. if len(covariates) == 0:; raise ValueError('Poisson regression requires at least one covariate expression'). mt = matrix_table_source('poisson_regression_rows/x', x); raise_unless_entry_indexed('poisson_regression_rows/x', x). analyze('poisson_regres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:61950,variab,variable,61950,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"t=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length. original_determine_file_length = DataFileReader.determine_file_length. try:; DataFileReader.determine_file_length = patched_determine_file_length. with DataFileReader(avro_file, DatumReader()) as data_file_reader:; tr = ir.AvroTableReader(avro.s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112391,config,config,112391,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['config'],['config']
Modifiability,"ta/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(; data_bucket='hail-qob-vep-grch38-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH38_95_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; }. def _supported_vep_config(cloud: str, reference_genome: str, *, regions: List[str]) -> VEPConfig:; domain = get_deploy_config()._domain. for region in regions:; config_params = (reference_genome, cloud, region, domain); if config_params in supported_vep_configs:; return supported_vep_configs[config_params]. raise ValueError(; f'could not find a supported vep configuration for reference genome {reference_genome}, '; f'cloud {cloud}, regions {regions}, and domain {domain}'; ). def _service_vep(; backend: ServiceBackend,; ht: Table,; config: Optional[VEPConfig],; block_size: int,; csq: bool,; tolerate_parse_error: bool,; temp_input_directory: str,; temp_output_directory: str,; ) -> Table:; reference_genome = ht.locus.dtype.reference_genome.name; cloud = async_to_blocking(backend._batch_client.cloud()); regions = backend.regions. if config is not None:; vep_config = config; else:; vep_config = _supported_vep_config(cloud, reference_genome, regions=regions). requester_pays_project = backend.flags.get('gcs_requester_pays_project'); if requester_pays_project is None and vep_config.data_bucket_is_requester_pays and vep_config.cloud == 'gcp':; raise ValueError(; ""No requester pays project has been set. ""; ""Use hl.init(gcs_requester_pays_configuration='MY_PROJECT') ""; ""to set the requester pays project to use.""; ). if csq:; vep_typ = hl.tarray(hl.tstr); else:; vep_typ = vep_config.json_typ. def build_vep_batch(b: bc.aioclient.Batch, vep_input_path: str, vep_output_path: str):; if csq:; local_output_file = '/io/output'; vep_command = vep_config.command(; consequence=csq,; part_id=-1,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:32066,config,config,32066,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Not",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:49231,variab,variable-length,49231,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""; try:; return self._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index table with given expressions:\n""; f"" Table key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index Expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). @staticmethod; def _maybe_truncate_for_flexindex(indexer, indexee_dtype):; if not len(indexee_dtype) > 0:; raise ValueError('Must have non-empty key to index'). if not isinstance(indexer.dtype, (hl.tstruct, hl.ttuple)):; indexer = hl.tuple([indexer]). matching_prefix = 0; for x, y in zip(indexer.dtype.types, indexee_dtype.types):; if x != y:; break; matching_prefix += 1; prefix_match = matching_prefix == len(indexee_dtype); direc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:71872,variab,variable-length,71872,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['variab'],['variable-length']
Modifiability,"tations to a KeyTable.; The resulting KeyTable has schema:; Struct {; v: Variant; va: variant annotations; }. with a single key v. Returns:Key table with variants and variant annotations. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:174903,variab,variable,174903,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['variab'],['variable']
Modifiability,"tch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields parameter.; (#13224); hailctl config get, set, and unset now support shell; auto-completion. Run hailctl --install-completion zsh to install; the auto-completion for zsh. You must already have completion; enabled for zsh.; (#13279) Add; hailctl batch init which helps new users interactively set up; hailctl for Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:25631,config,configuration,25631,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"tdt_table = tdt_table.filter(tdt_table.p_value < 0.001); >>> tdt_table.export(f""output/tdt_results.tsv""). Notes; -----; The; `transmission disequilibrium test <https://en.wikipedia.org/wiki/Transmission_disequilibrium_test#The_case_of_trios:_one_affected_child_per_family>`__; compares the number of times the alternate allele is transmitted (t) versus; not transmitted (u) from a heterozgyous parent to an affected child. The null; hypothesis holds that each case is equally likely. The TDT statistic is given by. .. math::. (t - u)^2 \over (t + u). and asymptotically follows a chi-squared distribution with one degree of; freedom under the null hypothesis. :func:`transmission_disequilibrium_test` only considers complete trios (two; parents and a proband with defined sex) and only returns results for the; autosome, as defined by :meth:`~.LocusExpression.in_autosome`, and; chromosome X. Transmissions and non-transmissions are counted only for the; configurations of genotypes and copy state in the table below, in order to; filter out Mendel errors and configurations where transmission is; guaranteed. The copy state of a locus with respect to a trio is defined as; follows:. - Auto -- in autosome or in PAR of X or female child; - HemiX -- in non-PAR of X and male child. Here PAR is the `pseudoautosomal region; <https://en.wikipedia.org/wiki/Pseudoautosomal_region>`__; of X and Y defined by :class:`.ReferenceGenome`, which many variant callers; map to chromosome X. +--------+--------+--------+------------+---+---+; | Kid | Dad | Mom | Copy State | t | u |; +========+========+========+============+===+===+; | HomRef | Het | Het | Auto | 0 | 2 |; +--------+--------+--------+------------+---+---+; | HomRef | HomRef | Het | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | HomRef | Het | HomRef | Auto | 0 | 1 |; +--------+--------+--------+------------+---+---+; | Het | Het | Het | Auto | 1 | 1 |; +--------+--------+--------+------------+---+---+; | Het | HomRef ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:14866,config,configurations,14866,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,4,['config'],['configurations']
Modifiability,"te to a version without this bug. The buggy; version of GCS SDK was introduced in 0.2.123.; (#13759) Since Hail; 0.2.123, Hail would hang in Dataproc Notebooks due to; (#13690).; (#13755) Ndarray; concatenation now works with arrays with size zero dimensions.; (#13817) Mitigate; new transient error from Google Cloud Storage which manifests as; aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:2548).; (#13715) Fix; (#13697), a long; standing issue with QoB. When a QoB driver or worker fails, the; corresponding Batch Job will also appear as failed.; (#13829) Fix; (#13828). The Hail; combiner now properly imports PGT fields from GVCFs.; (#13805) Fix; (#13767).; hailctl dataproc submit now expands ~ in the --files and; --pyfiles arguments.; (#13797) Fix; (#13756). Operations; that collect large results such as to_pandas may require up to 3x; less memory.; (#13826) Fix; (#13793). Ensure; hailctl describe -u overrides the gcs_requester_pays/project; config variable.; (#13814) Fix; (#13757). Pipelines; that are memory-bound by copious use of hl.literal, such as; hl.vds.filter_intervals, require substantially less memory.; (#13894) Fix; (#13837) in which; Hail could break a Spark installation if the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.rende",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:23551,config,config,23551,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,"['config', 'variab']","['config', 'variable']"
Modifiability,"tem, the following should suffice:; apt-get update; apt-get install \; openjdk-11-jdk-headless \; g++ \; python3 python3-pip \; libopenblas-dev liblapack-dev \; liblz4-dev. The next block of commands downloads, builds, and installs Hail from source.; git clone https://github.com/hail-is/hail.git; cd hail/hail; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.18 SPARK_VERSION=3.5.0. If you forget to install any of the requirements before running make install-on-cluster, it’s possible; to get into a bad state where make insists you don’t have a requirement that you have in fact installed.; Try doing make clean and then a fresh invocation of the make install-on-cluster line if this happens.; On every worker node of the cluster, you must install a BLAS and LAPACK library; such as the Intel MKL or OpenBLAS. On a Debian-like system you might try the; following on every worker node.; apt-get install libopenblas liblapack3. Hail is now installed! You can use ipython, python, and jupyter; notebook without any further configuration. We recommend against using the; pyspark command.; Let’s take Hail for a spin! Create a file called “hail-script.py” and place the; following analysis of a randomly generated dataset with five-hundred samples and; half-a-million variants.; import hail as hl; mt = hl.balding_nichols_model(n_populations=3,; n_samples=500,; n_variants=500_000,; n_partitions=32); mt = mt.annotate_cols(drinks_coffee = hl.rand_bool(0.33)); gwas = hl.linear_regression_rows(y=mt.drinks_coffee,; x=mt.GT.n_alt_alleles(),; covariates=[1.0]); gwas.order_by(gwas.p_value).show(25). Run the script and wait for the results. You should not have to wait more than a; minute.; python3 hail-script.py. Slightly more configuration is necessary to spark-submit a Hail script:; HAIL_HOME=$(pip3 show hail | grep Location | awk -F' ' '{print $2 ""/hail""}'); spark-submit \; --jars $HAIL_HOME/hail-all-spark.jar \; --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/other-cluster.html:2192,config,configuration,2192,docs/0.2/install/other-cluster.html,https://hail.is,https://hail.is/docs/0.2/install/other-cluster.html,1,['config'],['configuration']
Modifiability,"ternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix between variants. ld_prune(call_expr[, r2, bp_window_size, ...]); Returns a maximal subset of variants that are nearly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:5610,config,config,5610,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['config'],['config']
Modifiability,"ternate type of job called a PythonJob. Unlike BashJob, PythonJob; does not have a BashJob.command() method and instead have a PythonJob.call() method; that takes a Python function to call and the positional arguments and key-word arguments to provide; to the function. The result of PythonJob.call() is a PythonResult which can be; used as either arguments to another PythonJob or to other BashJob by using one; of the methods to convert a PythonResult to a file: PythonResult.as_str(),; PythonResult.as_repr(), and PythonResult.as_json().; In the example below, we first define two Python functions: hello_world() and upper().; Next, we create a batch and then create a new PythonJob with Batch.new_python_job().; Then we use PythonJob.call() and pass the hello_world function that we want to call.; Notice we just passed the reference to the function and not hello_world(). We also add; a Python string alice as an argument to the function. The result of the j.call() is; a PythonResult which we’ve assigned to the variable hello_str.; We want to use the hello_str result and make all the letters in upper case. We call; PythonJob.call() and pass a reference to the upper function.; But now the argument is hello_str which holds the result from calling hello_world; above. We assign the new output to the variable result.; At this point, we want to write out the transformed hello world result to a text file.; However, result is a PythonResult. Therefore, we need to use the PythonResult.as_str(); to convert result to a JobResourceFile with the string output HELLO WORLD ALICE. Now; we can write the result to a file.; def hello_world(name):; return f'hello {name}'. def upper(s):; return s.upper(). b = hb.Batch(name='hello'); j = b.new_python_job(); hello_str = j.call(hello_world, 'alice'); result = j.call(upper, hello_str); b.write_output(result.as_str(), 'output/hello-alice.txt'); b.run(). Backends; There are two backends that execute batches: the LocalBackend and the; ServiceBackend. T",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:17834,variab,variable,17834,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['variab'],['variable']
Modifiability,"tes with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44301,config,config,44301,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"tes; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; Using select; Select and its sibling methods (Table.select_globals(),; MatrixTable.select_globals(), MatrixTable.select_rows(),; MatrixTable.select_cols(), and MatrixTable.select_entries()) accept; both variable-length (f(x, y, z)) and keyword (f(a=x, b=y, c=z)); arguments.; Select methods will always preserve the key along that axis; e.g. for; Table.select(), the table key will aways be kept. To modify the; key, use key_by().; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a new table with; fields C1 and C2 of table1, and the table key ID.; First, variable-length string arguments:; >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:; >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:; >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, x.:; >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:; >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions.; >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:60451,variab,variable-length,60451,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"tes=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate :math:`\mathrm{is\_female}` is coded as; for ``True`` (female) and 0 for ``False`` (male). The null model sets; :math:`\beta_1 = 0`. .. _sigmoid function: https://en.wikipedia.org/wiki/Sigmoid_function. The structure of the emitted row field depends on the test statistic as; shown in the tables below. ========== ================== ======= ============================================; Test Field Type Value; ======",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:28484,variab,variable,28484,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"th one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list running clusters:; hailctl hdinsight list. Importantly, to shut down a cluster when done with it, use:; hailctl hdinsight stop CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. Variant Effect Predictor (VEP); The following cluster configuration enables Hail to run VEP in parallel on every; variant in a dataset containing GRCh37 variants:; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP \; --vep GRCh37 \; --vep-loftee-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/loftee-GRCh37 \; --vep-homo-sapiens-uri https://STORAGE_ACCOUNT.blob.core.windows.net/CONTAINER/homo-sapiens-GRCh37. Those two URIs must point at directories containing the VEP data files. You can populate them by; downloading the two tar files using gcloud storage cp,; gs://hail-us-central1-vep/loftee-beta/GRCh37.tar and gs://hail-us-central1-vep/homo-sapiens/85_GRCh37.tar,; extracting them into a local folder, and uploading that folder to your storage account using az; storage copy. The hail-us-central1-vep Google Cloud Storage bucket is a requester pays bucket which means; you must pay the cost of transferring them out of Google Cloud. We do not provide these files in; Azure because Azure Blob Storage lacks an equivalent cost control mechanism.; Hail also supports VEP for GRCh38 variants. The required tar f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:2225,config,configuration,2225,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['config'],['configuration']
Modifiability,"the tests; Contributing. Other Resources; Change Log And Version Policy. menu; Hail. For Software Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Ch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1441,variab,variable,1441,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['variab'],['variable']
Modifiability,"the usage:; >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified column fields. select_entries(*exprs, **named_exprs)[source]; Select existing entry fields or create new fields by name, dropping the rest.; Examples; Drop all entry fields aside from GT:; >>> dataset_result = dataset.select_entries(dataset.GT). Notes; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field names and the expressions to compute them. Returns:; MatrixTable – MatrixTable with specified entry fields. select_globals(*exprs, **named_exprs)[source]; Select existing global fields or create new fields by name, dropping the rest.; Examples; Select one existing field and compute a new one:; >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; See Table.select() for more information about using select methods. Note; This method does not support aggregation. Parameters:. exprs (variable-length args of str or Expression) – Arguments that specify field names or nested field reference expressions.; named_exprs (keyword args of Expression) – Field na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:55913,variab,variable-length,55913,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1915,polymorphi,polymorphism,1915,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"tig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial DNA. locus; Returns the locus object for this polymorphism. num_alleles; Returns the number of total alleles in this polymorphism, including the reference. num_alt_alleles; Returns the number of alternate alleles in this polymorphism. num_genotypes; Returns the total number of unique genotypes possible for this variant. parse; Parses a variant object from a string. allele(i)[source]¶; Returns the string allele representation for the ith allele.; The reference is included in the allele index. The index of; the first alternate allele is 1. The following is true for all; variants:; >>> v_multiallelic.ref == v_multiallelic",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:1604,polymorphi,polymorphism,1604,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"tint64 = dtype('int64'); Hail type for signed 64-bit integers.; Their values can range from \(-2^{63}\) to \(2^{63} - 1\).; In Python, these are represented as int. See also; Int64Expression, int64(). hail.expr.types.tfloat = dtype('float64'); Alias for tfloat64. hail.expr.types.tfloat32 = dtype('float32'); Hail type for 32-bit floating point numbers.; In Python, these are represented as float. See also; Float32Expression, float64(). hail.expr.types.tfloat64 = dtype('float64'); Hail type for 64-bit floating point numbers.; In Python, these are represented as float. See also; Float64Expression, float(), float64(). hail.expr.types.tstr = dtype('str'); Hail type for text strings.; In Python, these are represented as strings. See also; StringExpression, str(). hail.expr.types.tbool = dtype('bool'); Hail type for Boolean (True or False) values.; In Python, these are represented as bool. See also; BooleanExpression, bool(). class hail.expr.types.tarray(element_type)[source]; Hail type for variable-length arrays of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets conta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:8137,variab,variable-length,8137,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['variab'],['variable-length']
Modifiability,"tions.variant, reference_genome=reference_genome)); annotations = annotations.drop('variant'). if csq:; with hl.hadoop_open(f'{temp_output_directory}/csq-header') as f:; vep_csq_header = f.read().rstrip(); annotations = annotations.annotate_globals(vep_csq_header=vep_csq_header). return annotations. [docs]@typecheck(; dataset=oneof(Table, MatrixTable),; config=nullable(oneof(str, VEPConfig)),; block_size=int,; name=str,; csq=bool,; tolerate_parse_error=bool,; ); def vep(; dataset: Union[Table, MatrixTable],; config: Optional[Union[str, VEPConfig]] = None,; block_size: int = 1000,; name: str = 'vep',; csq: bool = False,; tolerate_parse_error: bool = False,; ):; """"""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:37453,config,configuration,37453,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"titions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), intervals=nullable(sequenceof(anytype))); def import_avro(paths, *, key=None, intervals=None):; if not paths:; raise ValueError('import_avro requires at least one path'); if (key is None) != (intervals is None):; raise ValueError('key and intervals must either be both defined or both undefined'). with hl.current_backend().fs.open(paths[0], 'rb') as avro_file:; # monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS. def patched_determine_file_length(self) -> int:; remember_pos = self.reader.tell(); self.reader.seek(-1, 2); file_length = self.reader.tell() + 1; self.reader.seek(remember_pos); return file_length.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:112139,config,configuration,112139,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['config'],['configuration']
Modifiability,"tor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - String specifying the local path where the output TSV file with the VEP result should be located.; VEP_INPUT_FILE - String specifying the local path where the input VCF shard is located for all jobs. The VEP_INPUT_FILE environment variable is not available for the single job that computes the consequence header when; csq=True. class hail.methods.VEPConfigGRCh37Version85(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh37 for VEP version 85.; This class takes the following constructor arguments:. data_bucket (str) – The location where the VEP data is stored.; data_mount (str) – The location in the container where the data should be mounted.; image (str) – The docker image to run VEP.; cloud (str) – The cloud where the Batch Service is located.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. class hail.methods.VEPConfigGRCh38Version95(*, data_bucket, data_mount, image, regions, cloud, data_bucket_is_requester_pays)[source]; The Hail-maintained VEP configuration for GRCh38 for VEP version 95.; This class takes the following constructor arguments:. data_bucke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:6680,variab,variable,6680,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['variab'],['variable']
Modifiability,"tr or StringExpression) – String to parse.; reference_genome (str or hail.genetics.ReferenceGenome) – Reference genome to use.; invalid_missing (BooleanExpression) – If True, invalid intervals are set to NA rather than causing an exception. Returns:; IntervalExpression. hail.expr.functions.variant_str(*args)[source]; Create a variant colon-delimited string. Parameters:; args – Arguments (see notes). Returns:; StringExpression. Notes; Expects either one argument of type; struct{locus: locus<RG>, alleles: array<str>, or two arguments of type; locus<RG> and array<str>. The function returns a string of the form; CHR:POS:REF:ALT1,ALT2,...ALTN; e.g.; 1:1:A:T; 16:250125:AAA:A,CAA. Examples; >>> hl.eval(hl.variant_str(hl.locus('1', 10000), ['A', 'T', 'C'])); '1:10000:A:T,C'. hail.expr.functions.call(*alleles, phased=False)[source]; Construct a call expression.; Examples; >>> hl.eval(hl.call(1, 0)); Call(alleles=[0, 1], phased=False). Parameters:. alleles (variable-length args of int or Expression of type tint32) – List of allele indices.; phased (bool) – If True, preserve the order of alleles. Returns:; CallExpression. hail.expr.functions.unphased_diploid_gt_index_call(gt_index)[source]; Construct an unphased, diploid call from a genotype index.; Examples; >>> hl.eval(hl.unphased_diploid_gt_index_call(4)); Call(alleles=[1, 2], phased=False). Parameters:; gt_index (int or Expression of type tint32) – Unphased, diploid genotype index. Returns:; CallExpression. hail.expr.functions.parse_call(s)[source]; Construct a call expression by parsing a string or string expression.; Examples; >>> hl.eval(hl.parse_call('0|2')); Call(alleles=[0, 2], phased=True). Notes; This method expects strings in the following format:. ploidy; Phased; Unphased. 0; |-; -. 1; |i; i. 2; i|j; i/j. 3; i|j|k; i/j/k. N; i|j|k|...|N; i/j/k/.../N. Parameters:; s (str or StringExpression) – String to parse. Returns:; CallExpression. hail.expr.functions.downcode(c, i)[source]; Create a new call by setting al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:10370,variab,variable-length,10370,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['variab'],['variable-length']
Modifiability,"tring,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with new row-indexed field name containing VEP annotations. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106989,config,config,106989,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['config'],"['config', 'configuration']"
Modifiability,"truct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. GRCh37: gs://hail-us-central1-vep/vep85-loftee-gcloud.json; GRCh38: gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json. If no config file is specified, this function will check to see if environment variable VEP_CONFIG_URI is set with a path to a config file.; Batch Service Configuration; If no config is specified, Hail will use the user’s Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for VEPConfig.; Annotations; A new row field is added in the location specified by name with type given; by the type given by the json_vep_schema (if csq is False) or; tarray of tstr (if csq is True).; If csq is True, then the CSQ header string is also added as a global; field with name name + '_csq_header'. Parameters:. dataset (MatrixTable or Table) – Dataset.; config (str or VEPConfig, optional) – Path to VEP configuration file or a VEPConfig object.; block_size (int) – Number of rows to process per VEP invocation.; name (str) – Name for resulting row field.; csq (bool) – If True, annotates with the VCF CSQ field as a tstr.; If False, annotates as the vep_json_schema.; tolerate_parse_error (bool) – If True, ignore invalid JSON produced by VEP and return a missing annotation. Returns:; MatrixTable or Table – Dataset with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:106417,config,config,106417,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,['config'],"['config', 'configuration']"
Modifiability,"type tfloat64) – Corresponds to ncp parameter in pchisqtail().; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pchisqtail().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pchisqtail(). Returns:; Expression of type tfloat64. hail.expr.functions.qnorm(p, mu=0, sigma=1, lower_tail=True, log_p=False)[source]; The quantile function of a normal distribution with mean mu and; standard deviation sigma, inverts pnorm(). Returns quantile of; standard normal distribution by default.; Examples; >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\); a normal random variable with mean mu and standard deviation sigma.; Defaults to a standard normal random variable, and the probability p must; satisfy 0 < p < 1. Parameters:. p (float or Expression of type tfloat64) – Probability.; mu (float or Expression of type tfloat64) – Mean (default = 0).; sigma (float or Expression of type tfloat64) – Standard deviation (default = 1).; lower_tail (bool or BooleanExpression) – Corresponds to lower_tail parameter in pnorm().; log_p (bool or BooleanExpression) – Exponentiate p, corresponds to log_p parameter in pnorm(). Returns:; Expression of type tfloat64. hail.expr.functions.qpois(p, lamb, lower_tail=True, log_p=False)[source]; The quantile function of a Poisson distribution with rate parameter; lamb, inverts ppois().; Examples; >>> hl.eval(hl.qpois(0.99, 1)); 4. Notes; Returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\); is a Poisson random variable with rate parameter lambda. Parameters:. p (float or Expression of type tfloat64); lamb (float or Expression of type tfloat64) – Rate parameter of Poisson distribution.; lowe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:26765,variab,variable,26765,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['variab'],['variable']
Modifiability,"u('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters:; cores (Union[str, int, float, None]) – Units are in cpu if cores is numeric. If None,; use the default value for the ServiceBackend; (1 cpu). Return type:; Self. Returns:; Same job object with CPU requirements set. depends_on(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:4855,variab,variable,4855,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['variab'],['variable']
Modifiability,"uble) – the exponent. ppois(x: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Double. If lowerTail equals true, returns Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda. If lowerTail equals false, returns Prob(\(X\) > x).; Arguments. x (Double) – Non-negative number at which to compute the probability density.; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the exclusive right-tail probability \(P(X > x)\).; logP (Boolean) – If true, probabilities are returned as log(p). ppois(x: Double, lambda: Double): Double. Returns the left-tail Prob(\(X \leq\) x) where \(X\) is a Poisson random variable with rate parameter lambda.; Arguments. x (Double) – Non-negative bound for the left-tail cumulative probability.; lambda (Double) – Poisson rate parameter. Must be non-negative. qchisqtail(p: Double, df: Double): Double. Returns right-quantile x for which p = Prob(\(Z^2\) > x) with \(Z^2\) a chi-squared random variable with degrees of freedom specified by df. p must satisfy 0 < p <= 1. Inverse of pchisq1tail.; Arguments. p (Double) – Probability; df (Double) – Degrees of freedom. qnorm(p: Double): Double. Returns left-quantile x for which p = Prob(\(Z\) < x) with \(Z\) a standard normal random variable. p must satisfy 0 < p < 1. Inverse of pnorm.; Arguments. p (Double) – Probability. qpois(p: Double, lambda: Double, lowerTail: Boolean, logP: Boolean): Int. If lowerTail equals true, returns the smallest integer \(x\) such that Prob(\(X \leq x\)) \(\geq\) p where \(X\) is a Poisson random variable with rate parameter lambda.; If lowerTail equals false, returns the largest integer \(x\) such that Prob(\(X > x\)) \(\geq\) p. Inverts ppois.; Arguments. p (Double) – Quantile to compute. Must satisfy \(0 \leq p \leq 1\).; lambda (Double) – Poisson rate parameter. Must be non-negative.; lowerTail (Boolean) – If false, returns the right-tail inverse cumulative density function.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/functions.html:14871,variab,variable,14871,docs/0.1/functions.html,https://hail.is,https://hail.is/docs/0.1/functions.html,1,['variab'],['variable']
Modifiability,"ud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:43123,config,configuration,43123,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['configuration']
Modifiability,"ue,; _create_row_uids=False,; ) -> Table:; """"""Read in a :class:`.Table` written with :meth:`.Table.write`. Parameters; ----------; path : :class:`str`; File to read. Returns; -------; :class:`.Table`; """"""; if _load_refs:; for rg_config in Env.backend().load_references_from_dataset(path):; hl.ReferenceGenome._from_config(rg_config). if _intervals is not None and _n_partitions is not None:; raise ValueError(""'read_table' does not support both _intervals and _n_partitions""); tr = ir.TableNativeReader(path, _intervals, _filter_intervals); ht = Table(ir.TableRead(tr, False, drop_row_uids=not _create_row_uids, _assert_type=_assert_type)). if _n_partitions:; intervals = ht._calculate_new_partitions(_n_partitions); return read_table(; path,; _intervals=intervals,; _assert_type=ht._type,; _load_refs=_load_refs,; _create_row_uids=_create_row_uids,; ); return ht. [docs]@typecheck(; t=Table,; host=str,; port=int,; index=str,; index_type=str,; block_size=int,; config=nullable(dictof(str, str)),; verbose=bool,; ); def export_elasticsearch(t, host, port, index, index_type, block_size, config=None, verbose=True):; """"""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """""". jdf = t.expand_types().to_spark(flatten=False)._jdf; Env.hail().io.ElasticsearchConnector.export(jdf, host, port, index, index_type, block_size, config, verbose). @typecheck(paths=sequenceof(str), key=nullable(sequenceof(str)), in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:111437,config,config,111437,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,4,['config'],['config']
Modifiability,"ug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#13732) Fix; (#13721) which; manifested with the message “Missing Range header in response”. The; root cause was a bug in the Google Cloud",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21509,variab,variable,21509,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['variab'],['variable']
Modifiability,"ugh=[mt.rsid]. Parameters:. test ({‘wald’, ‘lrt’, ‘score’, ‘firth’}) – Statistical test.; y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; All non-missing values must evaluate to 0 or 1.; Note that a BooleanExpression will be implicitly converted to; a Float64Expression with this property.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; max_iterations (int) – The maximum number of iterations.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1e-6. Returns:; Table. hail.methods.poisson_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=25, tolerance=None)[source]; For each row, test an input variable for association with a; count response variable using Poisson regression.; Notes; See logistic_regression_rows() for more info on statistical tests; of general linear models. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression) – Column-indexed response expression.; All non-missing values must evaluate to a non-negative integer.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – Non-empty list of column-indexed covariate expressions.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; tolerance (float, optional) – The iterative fit of this model is considered “converged” if the change in the estimated; beta is smaller than tolerance. By default the tolerance is 1",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:15563,variab,variable,15563,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['variab'],['variable']
Modifiability,"ultiallelic.ref == v_multiallelic.allele(0). Additionally, the following is true for all biallelic variants:; >>> v_biallelic.alt == v_biallelic.allele(1). Parameters:i (int) – integer index of desired allele. Returns:string representation of ith allele. Return type:str. alt()[source]¶; Returns the alternate allele string, assumes biallelic.; Fails if called on a multiallelic variant. Return type:str. alt_allele()[source]¶; Returns the alternate allele object, assumes biallelic.; Fails if called on a multiallelic variant. Return type:AltAllele. alt_alleles¶; List of alternate allele objects in this polymorphism. Return type:list of AltAllele. contig¶; Chromosome identifier. Return type:str. in_X_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome X. Return type:bool. in_X_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. Return type:bool. in_Y_PAR()[source]¶; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. Return type:bool. in_Y_non_PAR()[source]¶; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. Return type:bool. is_autosomal()[source]¶; True if this polymorphism is located on an autosome. Return type:bool. is_autosomal_or_pseudoautosomal()[source]¶; True if this polymorphism is found on an autosome, or the PAR on X or Y. Return type:bool. is_biallelic()[source]¶; True if there is only one alternate allele in this polymorphism. Return type:bool. is_mitochondrial()[source]¶; True if this polymorphism is mapped to mitochondrial DNA. Return type:bool. locus()[source]¶; Returns the locus object for this polymorphism. Return type:Locus. num_alleles()[source]¶; Returns the number of total alleles in this polymorphism, including the reference. Return type:int. num_alt_alleles()[source]¶; Returns the number of alternate alleles in this polymorphism. Return type:int. num_genotypes()[source]¶; Returns the tot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:3577,polymorphi,polymorphism,3577,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"um(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; This function is always followed by GroupedTable.aggregate(). Follow the; link for documentation on the aggregation step. Note; Using group_by; group_by and its sibling methods (MatrixTable.group_rows_by() and; MatrixTable.group_cols_by()) accept both variable-length (f(x, y, z)); and keyword (f(a=x, b=y, c=z)) arguments.; Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions.; The following three usages are all equivalent, producing a; GroupedTable grouped by fields C1 and C2 of table1.; First, variable-length string arguments:; >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:; >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:; >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field s:; >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, x:; >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:; >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:32173,variab,variable-length,32173,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variable-length']
Modifiability,"unctions.switch(expr)[source]; Build a conditional tree on the value of an expression.; Examples; >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. See also; SwitchBuilder, case(), cond(). Parameters:; expr (Expression) – Value to match against. Returns:; SwitchBuilder. hail.expr.functions.case(missing_false=False)[source]; Chain multiple if-else statements with a CaseBuilder.; Examples; >>> x = hl.literal('foo bar baz'); >>> expr = (hl.case(); ... .when(x[:3] == 'FOO', 1); ... .when(hl.len(x) == 11, 2); ... .when(x == 'secret phrase', 3); ... .default(0)); >>> hl.eval(expr); 2. Parameters:; missing_false (bool) – Treat missing predicates as False. See also; CaseBuilder, switch(), cond(). Returns:; CaseBuilder. hail.expr.functions.bind(f, *exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; Examples; >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. bind() also can take multiple arguments:; >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters:. f (function ( (args) -> Expression)) – Function of exprs.; exprs (variable-length args of Expression) – Expressions to bind. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.rbind(*exprs, _ctx=None)[source]; Bind a temporary variable and use it in a function.; This is bind() with flipped argument order.; Examples; >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. rbind() also can take multiple arguments:; >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters:. exprs (variable-length args of Expression) – Expressions to bind.; f (function ( (args) -> Expression)) – Function of exprs. Returns:; Expression – Result of evaluating f with exprs as arguments. hail.expr.functions.missing(t)[source]; Creates an expression representing ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:5998,variab,variable,5998,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"urn ""Stream["" + self.element_type._parsable_string() + ""]"". def _convert_from_json(self, x, _should_freeze: bool = False) -> Union[list, frozenlist]:; ls = [self.element_type._convert_from_json_na(elt, _should_freeze) for elt in x]; if _should_freeze:; return frozenlist(ls); return ls. def _convert_to_json(self, x):; return [self.element_type._convert_to_json_na(elt) for elt in x]. def _propagate_jtypes(self, jtype):; self._element_type._add_jtype(jtype.elementType()). def unify(self, t):; return isinstance(t, tstream) and self.element_type.unify(t.element_type). def subst(self):; return tstream(self.element_type.subst()). def clear(self):; self.element_type.clear(). def _get_context(self):; return self.element_type.get_context(). def is_setlike(maybe_setlike):; return isinstance(maybe_setlike, (set, frozenset)). [docs]class tset(HailType):; """"""Hail type for collections of distinct elements. In Python, these are represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """""". @typecheck_method(element_type=hail_type); def __init__(self, element_type):; self._element_type = element_type; self._array_repr = tarray(element_type); super(tset, self).__init__(). @property; def element_type(self):; """"""Set element type. Returns; -------; :class:`.HailType`; Element type.; """"""; return self._element_type. def _traverse(self, obj, f):; if f(self, obj):; for elt in obj:; self.element_type._traverse(elt, f). def _typecheck_one_level(self, annotation):; if annotation is not None:; if not is_setlike(annotation):; raise TypeError(""type 'set' expected Python 'set', but found type '%s'"" % type(annotation)). def __str__(self):; return ""set<{}>"".format(self.element_type). def _eq(self, other):; return isinstance(other, tse",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:23288,parameteriz,parameterized,23288,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['parameteriz'],['parameterized']
Modifiability,"utorial — Batch documentation. Batch; . Getting Started; Tutorial; Import; f-strings; Hello World; File Dependencies; Scatter / Gather; Nested Scatters; Input Files; Output Files; Resource Groups; Resource File Extensions; Python Jobs; Backends. Docker Resources; Batch Service; Cookbooks; Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Tutorial. View page source. Tutorial; This tutorial goes through the basic concepts of Batch with examples. Import; Batch is located inside the hailtop module, which can be installed; as described in the Getting Started section.; >>> import hailtop.batch as hb. f-strings; f-strings were added to Python in version 3.6 and are denoted by the ‘f’ character; before a string literal. When creating the string, Python evaluates any expressions; in single curly braces {…} using the current variable scope. When Python compiles; the example below, the string ‘Alice’ is substituted for {name} because the variable; name is set to ‘Alice’ in the line above.; >>> name = 'Alice'; >>> print(f'hello {name}'); hello Alice. You can put any arbitrary Python code inside the curly braces and Python will evaluate; the expression correctly. For example, below we evaluate x + 1 first before compiling; the string. Therefore, we get ‘x = 6’ as the resulting string.; >>> x = 5; >>> print(f'x = {x + 1}'); x = 6. To use an f-string and output a single curly brace in the output string, escape the curly; brace by duplicating the character. For example, { becomes {{ in the string definition,; but will print as {. Likewise, } becomes }}, but will print as }.; >>> x = 5; >>> print(f'x = {{x + 1}} plus {x}'); x = {x + 1} plus 5. To learn more about f-strings, check out this tutorial. Hello World; A Batch consists of a set of Job to execute. There can be; an arbitrary number of jobs in the batch that are executed in order of their dependencies.; A dependency between two jobs states that",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/tutorial.html:1027,variab,variable,1027,docs/batch/tutorial.html,https://hail.is,https://hail.is/docs/batch/tutorial.html,1,['variab'],['variable']
Modifiability,"valid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """""". if isinstance(dataset, MatrixTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44451,config,config,44451,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"vals(; path, reference_genome='default', skip_invalid_intervals=False, contig_recoding=None, **kwargs; ) -> Table:; """"""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:24203,parameteriz,parameterized,24203,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['parameteriz'],['parameterized']
Modifiability,"ved: cloud={cloud!r}.\n'; f'Valid cloud platforms are {DB._valid_clouds}.'; ); if (region, cloud) not in DB._valid_combinations:; raise ValueError(; f'The {region!r} region is not available for'; f' the {cloud!r} cloud platform. '; f'Valid region, cloud combinations are'; f' {DB._valid_combinations}.'; ); if config is not None and url is not None:; raise ValueError(; f'Only specify one of the parameters url and' f' config, received: url={url} and config={config}'; ); if config is None:; if url is None:; config = get_datasets_metadata(); else:; session = external_requests_client_session(); response = retry_response_returning_functions(session.get, url); config = response.json(); assert isinstance(config, dict); elif not isinstance(config, dict):; raise ValueError(f'expected a dict mapping dataset names to ' f'configurations, but found {config}'); config = {k: v for k, v in config.items() if 'annotation_db' in v}; self.region = region; self.cloud = cloud; self.url = url; self.config = config; self.__by_name = {; k: Dataset.from_name_and_json(k, v, region, cloud); for k, v in config.items(); if Dataset.from_name_and_json(k, v, region, cloud) is not None; }. @property; def available_datasets(self) -> List[str]:; """"""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""; return sorted(self.__by_name.keys()). @staticmethod; def _row_lens(rel: Union[Table, MatrixTable]) -> Union[TableRows, MatrixRows]:; """"""Get row lens from relational object. Parameters; ----------; rel : :class:`Table` or :class:`MatrixTable`. Returns; -------; :class:`TableRows` or :class:`MatrixRows`; """"""; if isinstance(rel, MatrixTable):; return MatrixRows(rel); elif isinstance(rel, Table):; return TableRows(rel); else:; raise ValueError('annotation database can only annotate Hail' ' MatrixTable or Table'). def _dataset_by_name(self, name: str) -> Dataset:; """"""Retrieve :class:`Dataset` object by name. Parameters; ----------; name : :ob",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:12134,config,config,12134,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,4,['config'],['config']
Modifiability,"vided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""; if Env._hc:; if idempotent:; return; else:; warning(; 'Hail has already been initialized. If this call was intended to change configuration,'; ' close the session with hl.stop() first.'; ). if default_reference is not None:; warnings.warn(; 'Using hl.init with a default_reference argument is deprecated. '; 'To set a default reference genome after initializing hail, '; 'call `hl.default_reference` with an argument to set the '; 'default reference genome.'; ); else:; default_reference = 'GRCh37'. backend = choose_backend(backend). if backend == 'service':; warnings.warn(; 'The ""service"" backend is now called the ""batch"" backend. Support for ""service"" will be removed in a '; 'future release.'; ); backend = 'batch'. if backend == 'batch':; return hail_event_loop().run_until_complete(; init_batch(; log=log,; quiet=quiet,; append=append,; tmpdir=tmp_dir,; local_tmpdir=local_tmpdir,; default_reference=default_reference,; global_seed=global_seed,; driver_cores=driver_cores,; driver_memory=driver_memory,; worker_cores=worker_cores,; worker_memory=worker_memory,; name_prefix=app_name,; gcs_requester_pays_configuration=gcs_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:12206,config,configuration,12206,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['config'],['configuration']
Modifiability,"vmat.T) @ xvec).reshape((-1, 1)); fisher10 = fisher01.T; fisher11 = hl.nd.array([[(mu * xvec.T) @ xvec]]); fisher = hl.nd.vstack([hl.nd.hstack([fisher00, fisher01]), hl.nd.hstack([fisher10, fisher11])]). fisher_div_score = hl.nd.solve(fisher, score, no_crash=True); chi_sq = hl.or_missing(~fisher_div_score.failed, score @ fisher_div_score.solution); p = hl.pchisqtail(chi_sq, dof); return chi_sq, p. [docs]def linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True):; r""""""Initialize a linear mixed model from a matrix table. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). [docs]@typecheck(; entry_expr=expr_float64,; model=LinearMixedModel,; pa_t_path=nullable(str),; a_t_path=nullable(str),; mean_impute=bool,; partition_size=nullable(int),; pass_through=sequenceof(oneof(str, Expression)),; ); def linear_mixed_regression_rows(; entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, partition_size=None, pass_through=(); ):; """"""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""; raise NotImplementedError(""linear_mixed_model is no longer implemented/supported as of Hail 0.2.94""). @typecheck(; group=expr_any,; weight=expr_float64,; y=expr_float64,; x=expr_float64,; covariates=sequenceof(expr_float64),; max_size=int,; accuracy=numeric,; iterations=int,; ); def _linear_skat(; group, weight, y, x, covariates, max_size: int = 46340, accuracy: float = 1e-6, iterations: int = 10000; ):; r""""""The linear sequence kernel association test (SKAT). Linear SKAT tests if the phenotype, `y`, is significantly associated with the genotype, `x`. For; :math:`N` samples, in a group of :math:`M` variants, with :math:`K` covariates, the model is; given by:. .. math::. \",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:71061,variab,variable,71061,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"w of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to a file containing a Pandas data; frame where the variables in the data frame are the observed and expected variant; count ratio.; We write a function that runs the random forest model and leaves the window; of interest out of the model window_name.; An important thing to note in the code below is the number of cores is a parameter; to the function and matches the number of cores we give the job in the Batch control; code below.; def random_forest(df_x_path: str, df_y_path: str, window_name: str, cores: int = 1) -> Tuple[str, float, float]:; # read in data; df_x = pd.read_table(df_x_path, header=0, index_col=0); df_y = pd.read_table(df_y_path, header=0, index_col=0). # split training and testing data for the current window; x_train = df_x[df_x.index != window_name]; x_test = df_x[df_x.index == window_name]. y_train = df_y[df_y.index != window_name]; y_test = df_y[df_y.index == window_name]. # run random forest; rf = RandomForestRegressor(n_estimators=100,; n_jobs=cores,; max_features=3/4,; oob_score=True,; verbose=False). rf.fit(x_train, y",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:2337,variab,variables,2337,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['variab'],['variables']
Modifiability,"ware Developers. View page source. For Software Developers; Hail is an open-source project. We welcome contributions to the repository. Requirements. Java 11 JDK . If you have a Mac, you must use a; compatible architecture (uname -m prints your architecture).; The Python and non-pip installation requirements in Getting Started.; Note: These instructions install the JRE but that is not necessary as the JDK should already; be installed which includes the JRE.; If you are setting HAIL_COMPILE_NATIVES=1, then you need the LZ4 library; header files. On Debian and Ubuntu machines run: apt-get install liblz4-dev. Building Hail; The Hail source code is hosted on GitHub:; git clone https://github.com/hail-is/hail.git; cd hail/hail. By default, Hail uses pre-compiled native libraries that are compatible with; recent Mac OS X and Debian releases. If you’re not using one of these OSes, set; the environment (or Make) variable HAIL_COMPILE_NATIVES to any value. This; variable tells GNU Make to build the native libraries from source.; Build and install a wheel file from source with local-mode pyspark:; make install HAIL_COMPILE_NATIVES=1. As above, but explicitly specifying the Scala and Spark versions:; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.11.12 SPARK_VERSION=2.4.5. Building the Docs and Website; Install build dependencies listed in the docs style guide.; Build without rendering the notebooks (which is slow):; make hail-docs-do-not-render-notebooks. Build while rendering the notebooks:; make hail-docs. Serve the built website on http://localhost:8000/; (cd build/www && python3 -m http.server). Running the tests; Install development dependencies:; make -C .. install-dev-requirements. A couple Hail tests compare to PLINK 1.9 (not PLINK 2.0 [ignore the confusing; URL]):. PLINK 1.9. Execute every Hail test using at most 8 parallel threads:; make -j8 test. Contributing; Chat with the dev team on our Zulip chatroom or; development forum if you have an idea for a con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/getting_started_developing.html:1491,variab,variable,1491,docs/0.2/getting_started_developing.html,https://hail.is,https://hail.is/docs/0.2/getting_started_developing.html,1,['variab'],['variable']
Modifiability,"widehat{\sigma} G W G^T \widehat{\sigma} h \\; A &= \widehat{\sigma} G W^{1/2} \\; B &= A A^T \\; \\; Q &= h^T B h \\; \end{align*}. This expression is a `""quadratic form"" <https://en.wikipedia.org/wiki/Quadratic_form>`__ of the; vector :math:`h`. Because :math:`B` is a real symmetric matrix, we can eigendecompose it into an; orthogonal matrix and a diagonal matrix of eigenvalues:. .. math::. \begin{align*}; U \Lambda U^T &= B \quad\quad \Lambda \textrm{ diagonal } U \textrm{ orthogonal} \\; Q &= h^T U \Lambda U^T h; \end{align*}. An orthogonal matrix transforms a vector of i.i.d. standard normal variables into a new vector; of different i.i.d standard normal variables, so we can interpret :math:`Q` as a weighted sum of; i.i.d. standard normal variables:. .. math::. \begin{align*}; \tilde{h} &= U^T h \\; Q &= \sum_s \Lambda_{ss} \tilde{h}_s^2; \end{align*}. The distribution of such sums (indeed, any quadratic form of i.i.d. standard normal variables); is governed by the generalized chi-squared distribution (the CDF is available in Hail as; :func:`.pgenchisq`):. .. math::. \begin{align*}; \lambda_i &= \Lambda_{ii} \\; Q &\sim \mathrm{GeneralizedChiSquared}(\lambda, \vec{1}, \vec{0}, 0, 0); \end{align*}. Therefore, we can test the null hypothesis by calculating the probability of receiving values; larger than :math:`Q`. If that probability is very small, then the residual phenotypes are; likely not i.i.d. normal variables with variance :math:`\widehat{\sigma}^2`. The SKAT method was originally described in:. Wu MC, Lee S, Cai T, Li Y, Boehnke M, Lin X. *Rare-variant association testing for; sequencing data with the sequence kernel association test.* Am J Hum Genet. 2011 Jul; 15;89(1):82-93. doi: 10.1016/j.ajhg.2011.05.029. Epub 2011 Jul 7. PMID: 21737059; PMCID:; PMC3135811. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/. Examples; --------. Generate a dataset with a phenotype noisily computed from the genotypes:. >>> hl.reset_global_randomness(); >>> mt = hl.ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:74690,variab,variables,74690,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variables']
Modifiability,"x('ATT', 'GCAC')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; return numeric_allele_type(ref, alt) == AlleleType.COMPLEX. [docs]@typecheck(ref=expr_str, alt=expr_str); def is_strand_ambiguous(ref, alt) -> BooleanExpression:; """"""Returns ``True`` if the alleles are strand ambiguous. Strand ambiguous allele pairs are ``A/T``, ``T/A``,; ``C/G``, and ``G/C`` where the first allele is `ref`; and the second allele is `alt`. Examples; --------. >>> hl.eval(hl.is_strand_ambiguous('A', 'T')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""; alleles = hl.literal({('A', 'T'), ('T', 'A'), ('G', 'C'), ('C', 'G')}); return alleles.contains((ref, alt)). [docs]@typecheck(ref=expr_str, alt=expr_str); def allele_type(ref, alt) -> StringExpression:; """"""Returns the type of the polymorphism as a string. Examples; --------. >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes; -----; The possible return values are:; - ``""SNP""``; - ``""MNP""``; - ``""Insertion""``; - ``""Deletion""``; - ``""Complex""``; - ``""Star""``; - ``""Symbolic""``; - ``""Unknown""``. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.StringExpression`; """"""; return hl.literal(AlleleType.strings())[numeric_allele_type(ref, alt)]. [docs]@typecheck(s1=expr_str, s2=expr_str); def hamming(s1, s2) -> Int32Expression:; """"""Returns the Hamming distance between the two strings. Examples; --------. >>> hl.eval(hl.hamming('ATATA', 'ATGCA')); 2. >>> hl.eval(hl.hamming('abcdefg', 'zzcdefz')); 3. Notes; -----; This method will fail if the two strings have different length. Parameters; ----------; s1 : :class:`.Strin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:102470,polymorphi,polymorphism,102470,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['polymorphi'],['polymorphism']
Modifiability,"xTable):; require_row_key_variant(dataset, 'vep'); ht = dataset.select_rows().rows(); else:; require_table_key_variant(dataset, 'vep'); ht = dataset.select(). ht = ht.distinct(). backend = hl.current_backend(); if isinstance(backend, ServiceBackend):; with hl.TemporaryDirectory(prefix='qob/vep/inputs/') as vep_input_path:; with hl.TemporaryDirectory(prefix='qob/vep/outputs/') as vep_output_path:; annotations = _service_vep(; backend, ht, config, block_size, csq, tolerate_parse_error, vep_input_path, vep_output_path; ); annotations = annotations.checkpoint(new_temp_file()); else:; if config is None:; maybe_cloud_spark_provider = guess_cloud_spark_provider(); maybe_config = os.getenv(""VEP_CONFIG_URI""); if maybe_config is not None:; config = maybe_config; elif maybe_cloud_spark_provider == 'hdinsight':; warning(; 'Assuming you are in a hailctl hdinsight cluster. If not, specify the config parameter to `hl.vep`.'; ); config = 'file:/vep_data/vep-azure.json'; else:; raise ValueError(""No config set and VEP_CONFIG_URI was not set.""). annotations = Table(; TableToTableApply(; ht._tir,; {; 'name': 'VEP',; 'config': config,; 'csq': csq,; 'blockSize': block_size,; 'tolerateParseError': tolerate_parse_error,; },; ); ).persist(). if csq:; dataset = dataset.annotate_globals(**{name + '_csq_header': annotations.index_globals()['vep_csq_header']}). if isinstance(dataset, MatrixTable):; vep = annotations[dataset.row_key]; return dataset.annotate_rows(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}); else:; vep = annotations[dataset.key]; return dataset.annotate(**{name: vep.vep, name + '_proc_id': vep.vep_proc_id}). [docs]@typecheck(dataset=oneof(Table, MatrixTable), config=str, block_size=int, name=str); def nirvana(dataset: Union[MatrixTable, Table], config, block_size=500000, name='nirvana'):; """"""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:44708,config,config,44708,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['config'],['config']
Modifiability,"x_entries() using; square brackets (the Python __getitem__ syntax). This syntax is; preferred.; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2[dataset.row_key, dataset.col_key].GQ). Returns:; StructExpression. index_globals()[source]; Return this matrix table’s global variables for use in another; expression context.; Examples; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns:; StructExpression. index_rows(*exprs, all_matches=False)[source]; Expose the row values as if looked up in a dictionary, indexing; with exprs.; Examples; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Notes; index_rows(exprs) is equivalent to rows().index(exprs); or rows()[exprs].; The type of the resulting struct is the same as the type of; row_value(). Returns:; Expression. key_cols_by(*keys, **named_keys)[source]; Key columns by a new set of fields.; See Table.key_by() for more information on defining a key. Parameters:. keys (varargs of str or Expression.) – Column fields to key by.; named_keys (keyword args of Expression.) – Column fields to key by. Returns:; MatrixTable. key_rows_by(*keys, **named_keys)[source]; Key rows by a new set of fields.; Examples; >>> dataset_result = dataset.key_rows_by('locus'); >>> dataset_result = dataset.key_rows_by(dataset['locus']); >>> dataset_result = dataset.key_rows_by(**dataset.row_key.drop('alleles')). All of these expressions key the dataset by the ‘locus’ field, dropping; the ‘alleles’ field from the row key.; >>> ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:43627,variab,variable-length,43627,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['variab'],['variable-length']
Modifiability,"xamples; >>> hl.dtype('int'); dtype('int32'). >>> hl.dtype('float'); dtype('float64'). >>> hl.dtype('array<int32>'); dtype('array<int32>'). >>> hl.dtype('dict<str, bool>'); dtype('dict<str, bool>'). >>> hl.dtype('struct{a: int32, `field with spaces`: int64}'); dtype('struct{a: int32, `field with spaces`: int64}'). Notes; This function is able to reverse str(t) on a HailType.; The grammar is defined as follows:; type = _ ( array / bool / call / dict / interval / int64 / int32 / float32 / float64 / locus / ndarray / rng_state / set / stream / struct / str / tuple / union / void / variable ) _; variable = ""?"" simple_identifier ("":"" simple_identifier)?; void = ""void"" / ""tvoid""; int64 = ""int64"" / ""tint64""; int32 = ""int32"" / ""tint32"" / ""int"" / ""tint""; float32 = ""float32"" / ""tfloat32""; float64 = ""float64"" / ""tfloat64"" / ""tfloat"" / ""float""; bool = ""tbool"" / ""bool""; call = ""tcall"" / ""call""; str = ""tstr"" / ""str""; locus = (""tlocus"" / ""locus"") _ ""<"" identifier "">""; array = (""tarray"" / ""array"") _ ""<"" type "">""; ndarray = (""tndarray"" / ""ndarray"") _ ""<"" type "","" nat "">""; set = (""tset"" / ""set"") _ ""<"" type "">""; stream = (""tstream"" / ""stream"") _ ""<"" type "">""; dict = (""tdict"" / ""dict"") _ ""<"" type "","" type "">""; struct = (""tstruct"" / ""struct"") _ ""{"" (fields / _) ""}""; union = (""tunion"" / ""union"") _ ""{"" (fields / _) ""}""; tuple = (""ttuple"" / ""tuple"") _ ""("" ((type ("","" type)*) / _) "")""; fields = field ("","" field)*; field = identifier "":"" type; interval = (""tinterval"" / ""interval"") _ ""<"" type "">""; identifier = _ (simple_identifier / escaped_identifier) _; simple_identifier = ~r""\w+""; escaped_identifier = ~""`([^`\\\\]|\\\\.)*`""; nat = _ (nat_literal / nat_variable) _; nat_literal = ~""[0-9]+""; nat_variable = ""?nat""; rng_state = ""rng_state""; _ = ~r""\s*"". Parameters:; type_str (str) – String representation of type. Returns:; HailType. hail.expr.types.tint = dtype('int32'); Alias for tint32. hail.expr.types.tint32 = dtype('int32'); Hail type for signed 32-bit integers.; Their values can range fr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:5552,variab,variable,5552,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,2,['variab'],['variable']
Modifiability,"xpr_float64))); @ndarray_broadcasting; def expit(x) -> Float64Expression:; """"""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""; return hl.if_else(x >= 0, 1 / (1 + hl.exp(-x)), hl.rbind(hl.exp(x), lambda exped: exped / (exped + 1))). [docs]@typecheck(args=expr_any); def coalesce(*args):; """"""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Also; --------; :func:`.or_else`. Parameters; ----------; args : variable-length args of :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; if builtins.len(args) < 1:; raise ValueError(""'coalesce' requires at least one expression argument""); *exprs, success = unify_exprs(*args); if not success:; arg_types = ''.join([f""\n argument {i}: type '{arg.dtype}'"" for i, arg in builtins.enumerate(exprs)]); raise TypeError(f""'coalesce' requires all arguments to have the same type or compatible types"" f""{arg_types}""); indices, aggregations = unify_all(*exprs); return construct_expr(ir.Coalesce(*(e._ir for e in exprs)), exprs[0].dtype, indices, aggregations). [docs]@typecheck(a=expr_any, b=expr_any); def or_else(a, b):; """"""If `a` is missing, return `b`. Examples; --------. >>> hl.eval(hl.or_else(5, 7)); 5. >>> hl.eval(hl.or_else(hl.missing(hl.tint32), 7)); 7. See Also; --------; :func:`.coalesce`. Parameters; ----------; a: :class:`.Expression`; b: :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""; a, b, success = unify_exprs(a, b)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:58777,variab,variable-length,58777,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['variab'],['variable-length']
Modifiability,"xprs)`` is equivalent to ``rows().index(exprs)``; or ``rows()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.row_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.rows()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable row key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_cols(self, *exprs, all_matches=False) -> 'Expression':; """"""Expose the column values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.s).pheno). Or equivalently:. >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.col_key).pheno). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_cols(cols)`` is equivalent to ``cols().index(exprs)``; or ``cols()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.col_value`. Returns; -------; :class:`.Expression`; """"""; try:; return self.cols()._index(*exprs, all_matches=all_matches); except TableIndexKeyError as err:; raise ExpressionException(; f""Key type mismatch: cannot index matrix table with given expressions:\n""; f"" MatrixTable col key: {', '.join(str(t) for t in err.key_type.values()) or '<<<empty key>>>'}\n""; f"" Index expressions: {', '.join(str(e.dtype) for e in err.index_expressions)}""; ). [docs] def index_entries(self, row_exprs, col_exprs):; """"""Expose the entries as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_entries(GQ2 = dataset2.index_entries(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:92394,variab,variable-length,92394,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['variab'],['variable-length']
Modifiability,"y leak when processing tables. Could trigger unnecessarily high; memory use and out of memory errors when there are many rows per; partition or large key fields.; (#12565) Fixed a bug; that prevented exploding on a field of a Table whose value is a; random value. Version 0.2.107; Released 2022-12-14. Bug fixes. (#12543) Fixed; hl.vds.local_to_global error when LA array contains non-ascending; allele indices. Version 0.2.106; Released 2022-12-13. New Features. (#12522) Added; hailctl config setting 'batch/backend' to specify the default; backend to use in batch scripts when not specified in code.; (#12497) Added; support for scales, nrow, and ncol arguments, as well as; grouped legends, to hail.ggplot.facet_wrap.; (#12471) Added; hailctl batch submit command to run local scripts inside batch; jobs.; (#12525) Add support; for passing arguments to hailctl batch submit.; (#12465) Batch jobs’; status now contains the region the job ran in. The job itself can; access which region it is in through the HAIL_REGION environment; variable.; (#12464) When using; Query-on-Batch, all jobs for a single hail session are inserted into; the same batch instead of one batch per action.; (#12457) pca and; hwe_normalized_pca are now supported in Query-on-Batch.; (#12376) Added; hail.query_table function for reading tables with indices from; Python.; (#12139) Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:41913,variab,variable,41913,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['variab'],['variable']
Modifiability,"y; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the; filters row field. This annotation is a set<str> and can be; queried for filter membership with expressions like; ds.filters.contains(""VQSRTranche99.5...""). Variants that are flagged; as “PASS” will have no filters applied; for these variants,; hl.len(ds.filters) is 0. Thus, filtering to PASS variants; can be done with MatrixTable.filter_rows() as follows:; >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). Column Fields. s (tstr) – Column key. This is the sample ID. Row Fields. locus (tlocus or tstruct) – Row key. The; chromosome (CHROM field) and position (POS field). If reference_genome; is defined, the type will be tlocus parameterized by; reference_genome. Otherwise, the type will be a tstruct with; two fields: contig with type tstr and position with type; tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and oth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:43763,parameteriz,parameterized,43763,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['parameteriz'],['parameterized']
Modifiability,"ys of elements.; In Python, these are represented as list.; Notes; Arrays contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of array. See also; ArrayExpression, CollectionExpression, array(), Collection functions. class hail.expr.types.tndarray(element_type, ndim)[source]; Hail type for n-dimensional arrays. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. In Python, these are represented as NumPy numpy.ndarray.; Notes; NDArrays contain elements of only one type, which is parameterized by; element_type. Parameters:. element_type (HailType) – Element type of array.; ndim (int32) – Number of dimensions. See also; NDArrayExpression, nd.array. class hail.expr.types.tset(element_type)[source]; Hail type for collections of distinct elements.; In Python, these are represented as set.; Notes; Sets contain elements of only one type, which is parameterized by; element_type. Parameters:; element_type (HailType) – Element type of set. See also; SetExpression, CollectionExpression, set(), Collection functions. class hail.expr.types.tdict(key_type, value_type)[source]; Hail type for key-value maps.; In Python, these are represented as dict.; Notes; Dicts parameterize the type of both their keys and values with; key_type and value_type. Parameters:. key_type (HailType) – Key type.; value_type (HailType) – Value type. See also; DictExpression, dict(), Collection functions. class hail.expr.types.tstruct(**field_types)[source]; Hail type for structured groups of heterogeneous fields.; In Python, these are represented as Struct.; Hail’s tstruct type is commonly used to compose types together to form nested; structures. Structs can contain any combination of types, and are ordered mappings; from field name to field type. Each field name must be unique.; Structs are very common in Hail. Each component of a Table and Mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/types.html:9171,parameteriz,parameterized,9171,docs/0.2/types.html,https://hail.is,https://hail.is/docs/0.2/types.html,1,['parameteriz'],['parameterized']
Modifiability,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:39189,plugin,plugin,39189,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['plugin'],['plugin']
Modifiability,"zone as default for hailctl dataproc connect; and hailctl dataproc modify. Version 0.2.40; Released 2020-05-12. VCF Combiner. (#8706) Add option to; key by both locus and alleles for final output. Bug fixes. (#8729) Fix assertion; error in Table.group_by(...).aggregate(...); (#8708) Fix assertion; error in reading tables and matrix tables with _intervals option.; (#8756) Fix return; type of LocusExpression.window to use locus’s reference genome; instead of default RG. Version 0.2.39; Released 2020-04-29. Bug fixes. (#8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:71938,config,configuration,71938,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['config'],['configuration']
Modifiability,"{height} = \beta_0 + \beta_1 \, \mathrm{genotype}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female}; + \varepsilon,; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:5496,variab,variable,5496,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"| 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; Table.index() is used to expose one table’s fields for use in; expressions involving the another table or matrix table’s fields. The; result of the method call is a struct expression that is usable in the; same scope as exprs, just as if exprs were used to look up values of; the table in a dictionary.; The type of the struct expression is the same as the indexed table’s; row_value() (the key fields are removed, as they are available; in the form of the index expressions). Note; There is a shorthand syntax for Table.index() using square; brackets (the Python __getitem__ syntax). This syntax is preferred.; >>> table_result = table1.select(B = table2[table1.ID].B). Parameters:. exprs (variable-length args of Expression) – Index expressions.; all_matches (bool) – Experimental. If True, value of expression is array of all matches. Returns:; Expression. index_globals()[source]; Return this table’s global variables for use in another; expression context.; Examples; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns:; StructExpression. join(right, how='inner', _mangle=<function Table.<lambda>>, _join_key=None)[source]; Join two tables together.; Examples; Join table1 to table2 to produce table_joined:; >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. inner – Only rows with a matching key in the opposite table are included; in the resulting table.; left – All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; right – All rows from the right table are included in the resulting table.; If a row in the right ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:36538,variab,variables,36538,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['variab'],['variables']
Modifiability,"| NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; accuracy : :obj:`float`; The accuracy of the p-value if fault value is zero. Defaults to 1e-6.; iterations : :obj:`int`; The maximum number of iterations used to calculate the p-value (which has no closed; form). Defaults to 1e5. Returns; -------; :class:`.Table`; One row per-group. The key is `group`. The row fields are:. - group : the `group` parameter. - size : :obj:`.tint64`, the number of variants in this group. - q_stat : :obj:`.tfloat64`, the :math:`Q` statistic, see Notes for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:79261,variab,variable,79261,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['variab'],['variable']
Modifiability,"|; +---------------------------+--------+--------------------------------------------------------+; | ``gqMean`` | Double | The average genotype quality across all samples |; +---------------------------+--------+--------------------------------------------------------+; | ``gqStDev`` | Double | Genotype quality standard deviation across all samples |; +---------------------------+--------+--------------------------------------------------------+. Missing values ``NA`` may result (for example, due to division by zero) and are handled properly ; in filtering and written as ""NA"" in export modules. The empirical standard deviation is computed; with zero degrees of freedom. :param str root: Variant annotation root for computed struct. :return: Annotated variant dataset with new variant QC annotations.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.variantQC(root); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(config=strlike,; block_size=integral,; root=strlike,; csq=bool); def vep(self, config, block_size=1000, root='va.vep', csq=False):; """"""Annotate variants with VEP. :py:meth:`~hail.VariantDataset.vep` runs `Variant Effect Predictor <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ with; the `LOFTEE plugin <https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:221158,config,config,221158,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['config'],['config']
Modifiability," fam[, ...]); Import a PLINK dataset (BED, BIM, FAM) as a MatrixTable. import_table(paths[, key, min_partitions, ...]); Import delimited text file (text table) as Table. import_vcf(path[, force, force_bgz, ...]); Import VCF file(s) as a MatrixTable. index_bgen(path[, index_file_map, ...]); Index BGEN files as required by import_bgen(). read_matrix_table(path, *[, _intervals, ...]); Read in a MatrixTable written with MatrixTable.write(). read_table(path, *[, _intervals, ...]); Read in a Table written with Table.write(). Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. Genetics. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. genetic_relatedness_matrix(call_expr);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:3722,variab,variable,3722,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,2,['variab'],['variable']
Modifiability,"; Creates a smoothed density plot.; This method uses the hl.agg.approx_cdf aggregator to compute a sketch; of the distribution of the values of x. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf.; Note: this function currently does not support same interface as R’s ggplot.; Supported aesthetics: x, color, fill. Parameters:. mapping (Aesthetic) – Any aesthetics specific to this geom.; k (int) – Passed to the approx_cdf aggregator. The size of the aggregator scales; linearly with k. The default value of 1000 is likely sufficient for; most uses.; smoothing (float) – Controls the amount of smoothing applied.; fill – A single fill color for all density plots, overrides fill aesthetic.; color – A single line color for all density plots, overrides color aesthetic.; alpha (float) – A measure of transparency between 0 and 1.; smoothed (boolean) – If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_hline(yintercept, *, linetype='solid', color=None)[source]; Plots a horizontal line at yintercept. Parameters:. yintercept (float) – Location to draw line.; linetype (str) – Type of line to draw. Choose from “solid”, “dashed”, “dotted”, “longdash”, “dotdash”.; color (str) – Color of line to draw, black by default. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_vline(xintercept, *, linetype='solid', color=None)[source]; Plots a vertical line at xintercept. Parameters:. xintercept (float) – Location to draw line.; linetype (str) – Type of line to draw. Choose from “solid”, “dashed”, “dotted”, “longdash”, “dotdash”.; color (str) – Color of line to draw, black by default. Returns:; FigureAttribute – The geom to be applied. hail.ggplot.geom_area(mapping={}, fill=None, color=None)[source]; Creates a line plot with the area between the line and ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/ggplot/index.html:6555,variab,variable,6555,docs/0.2/ggplot/index.html,https://hail.is,https://hail.is/docs/0.2/ggplot/index.html,1,['variab'],['variable']
Modifiability,"﻿. . AltAllele — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; AltAllele. View page source. AltAllele¶. class hail.representation.AltAllele(ref, alt)[source]¶; An object that represents an allele in a polymorphism deviating from the reference allele. Parameters:; ref (str) – reference allele; alt (str) – alternate allele. Attributes. alt; Alternate allele. ref; Reference allele. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. category; Returns the type of alt, i.e one of SNP, Insertion, Deletion, Star, MNP, Complex. is_MNP; True if this alternate allele is a multiple nucleotide polymorphism (MNP). is_SNP; True if this alternate allele is a single nucleotide polymorphism (SNP). is_complex; True if this alternate allele does not fit into the categories of SNP, MNP, Insertion, or Deletion. is_deletion; True if this alternate allele is a deletion of one or more bases. is_indel; True if this alternate allele is either an insertion or deletion of one or more bases. is_insertion; True if this alternate allele is an insertion of one or more bases. is_transition; True if this alternate allele is a transition SNP. is_transversion; True if this alternate allele is a transversion SNP. num_mismatch; Returns the number of mismatched bases in this alternate allele. stripped_snp; Returns the one-character reduced SNP. alt¶; Alternate allele. Return type:str. category()[source]¶. Returns the type of alt, i.e one of; SNP,; Insertion,; Deletion,; Star,; MNP,; Complex. Return type:str. is_MNP()[source]¶; True if this alternate allele is a multiple nucleotide polymorphis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html:593,polymorphi,polymorphism,593,docs/0.1/representation/hail.representation.AltAllele.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.AltAllele.html,2,['polymorphi'],['polymorphism']
Modifiability,"﻿. . HailContext — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; HailContext. View page source. HailContext¶. class hail.HailContext(sc=None, app_name='Hail', master=None, local='local[*]', log='hail.log', quiet=False, append=False, parquet_compression='snappy', min_block_size=1, branching_factor=50, tmp_dir='/tmp')[source]¶; The main entry point for Hail functionality. Warning; Only one Hail context may be running in a Python session at any time. If you; need to reconfigure settings, restart the Python session or use the HailContext.stop() method.; If passing in a Spark context, ensure that the configuration parameters spark.sql.files.openCostInBytes; and spark.sql.files.maxPartitionBytes are set to as least 50GB. Parameters:; sc (pyspark.SparkContext) – Spark context, one will be created if None.; appName – Spark application identifier.; master – Spark cluster master.; local – Local resources to use.; log – Log path.; quiet (bool) – Don’t write logging information to standard error.; append – Write to end of log file instead of overwriting.; parquet_compression – Level of on-disk annotation compression.; min_block_size – Minimum file split size in MB.; branching_factor – Branching factor for tree aggregation.; tmp_dir – Temporary directory for file merging. Variables:sc (pyspark.SparkContext) – Spark context. Attributes. version; Return the version of Hail associated with this HailContext. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. balding_nichols_model; Simulate a variant dataset using the Balding-Nichols model. eval_expr; Evaluate an expression. eval_expr_typed; Evaluate an expression and return the result as well as its type. get_run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:900,config,configuration,900,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['config'],['configuration']
Modifiability,"﻿. . Introduction to the Expression Language — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials »; Introduction to the Expression Language. View page source. Introduction to the Expression Language¶; This notebook starts with the basics of the Hail expression language,; and builds up practical experience with the type system, syntax, and; functionality. By the end of this notebook, we hope that you will be; comfortable enough to start using the expression language to slice,; dice, filter, and query genetic data. These are covered in the next; notebook!; The best part about a Jupyter Notebook is that you don’t just have to; run what we’ve written - you can and should change the code and see; what happens!. Setup¶; Every Hail practical notebook starts the same: import the necessary; modules, and construct a; HailContext.; This is the entry point for Hail functionality. This object also wraps a; SparkContext, which can be accessed with hc.sc.; As always, visit the documentation; on the Hail website for full reference. In [1]:. from hail import *; hc = HailContext(). Running on Apache Spark version 2.0.2; SparkUI available at http://10.56.135.40:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-5a67787. Hail Expression Language¶; The Hail expression language is used everywhere in Hail: filtering; conditions, describing covariates and phenotypes, storing summary",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html:460,variab,variables,460,docs/0.1/tutorials/introduction-to-the-expression-language.html,https://hail.is,https://hail.is/docs/0.1/tutorials/introduction-to-the-expression-language.html,1,['variab'],['variables']
Modifiability,"﻿. . Language Constructs — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Language Constructs; Operators; Types; Functions. Python API; Annotation Database; Other Resources. Hail. Docs »; Expression Language »; Language Constructs. View page source. Language Constructs¶. va.foo = 5 + va.bar. Annotation expression. Bind variable va.foo to the result of evaluating 5 + va.bar. if (p) a else b. The value of the conditional is the value of a or b depending on p. If p is missing, the value of the conditional is missing.; if (5 % 2 == 0) 5 else 7; 7. if (5 > NA: Int) 5 else 7; NA: Int. let v1 = e1 and v2 = e2 and … and vn = en in b. Bind variables v1 through vn to result of evaluating the ei. The value of the let is the value of b. v1 is visible in e2 through en, etc.; let v1 = 5 and v2 = 7 and v3 = 2 in v1 * v2 * v3; 70. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/language_constructs.html:446,variab,variable,446,docs/0.1/language_constructs.html,https://hail.is,https://hail.is/docs/0.1/language_constructs.html,2,['variab'],"['variable', 'variables']"
Modifiability,"﻿. . Tutorials — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Hail Overview; Overview; Check for tutorial data or download if necessary; Loading data from disk; Getting to know our data; Integrate sample annotations; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Eplilogue. Introduction to the expression language; Introduction to the Expression Language; Setup; Hail Expression Language; Hail Types; Primitive Types; Missingness; Let; Conditionals; Compound Types; Numeric Arrays; Exercise; Structs; Genetic Types; Demo variables; Wrangling complex nested types; Learn more!; Exercises. Expression language: query, annotate, and aggregate; Using the expression language to slice, dice, and query genetic data; Check for tutorial data or download if necessary; Types in action; Filtering with expressions; Filtering variants and genotypes; Annotating with expressions; Aggregables; Count; Sum; Fraction; Stats; Counter; FlatMap; Take; Collect; takeBy; Aggregating by key. Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Tutorials. View page source. Tutorials¶; To take Hail for a test drive, go through our tutorials. These can be viewed here in the documentation,; but we recommend instead that you run them yourself with Jupyter.; Download the Hail distribution from our getting started page, and follow; the instructions there to set up the Hail. Inside the unzipped distribution folder, you’ll find; a tutorials/ directory. cd to this directory and run jhail to start the notebook; server, then click a notebook to begin!. Hail Overview¶; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the; functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association; test, and demonstrate the need to c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials-landing.html:702,variab,variables,702,docs/0.1/tutorials-landing.html,https://hail.is,https://hail.is/docs/0.1/tutorials-landing.html,1,['variab'],['variables']
Modifiability,"﻿. . Variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation »; Variant. View page source. Variant¶. class hail.representation.Variant(contig, start, ref, alts)[source]¶; An object that represents a genomic polymorphism. Parameters:; contig (str or int) – chromosome identifier; start (int) – chromosomal position (1-based); ref (str) – reference allele; alts (str or list of str) – single alternate allele, or list of alternate alleles. Attributes. alt_alleles; List of alternate allele objects in this polymorphism. contig; Chromosome identifier. ref; Reference allele at this locus. start; Chromosomal position (1-based). Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. allele; Returns the string allele representation for the ith allele. alt; Returns the alternate allele string, assumes biallelic. alt_allele; Returns the alternate allele object, assumes biallelic. in_X_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome X. in_X_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome X. in_Y_PAR; True of this polymorphism is found on the pseudoautosomal region of chromosome Y. in_Y_non_PAR; True of this polymorphism is found on the non-pseudoautosomal region of chromosome Y. is_autosomal; True if this polymorphism is located on an autosome. is_autosomal_or_pseudoautosomal; True if this polymorphism is found on an autosome, or the PAR on X or Y. is_biallelic; True if there is only one alternate allele in this polymorphism. is_mitochondrial; True if this polymorphism is mapped to mitochondrial D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/hail.representation.Variant.html:596,polymorphi,polymorphism,596,docs/0.1/representation/hail.representation.Variant.html,https://hail.is,https://hail.is/docs/0.1/representation/hail.representation.Variant.html,2,['polymorphi'],['polymorphism']
Modifiability,"﻿. . hail.representation.variant — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Other Resources. Hail. Docs »; Module code »; hail.representation.variant. Source code for hail.representation.variant; from hail.java import scala_object, Env, handle_py4j; from hail.typecheck import *. [docs]class Variant(object):; """"""; An object that represents a genomic polymorphism. .. testsetup::. v_biallelic = Variant.parse('16:20012:A:TT'); v_multiallelic = Variant.parse('16:12311:T:C,TTT,A'). :param contig: chromosome identifier; :type contig: str or int; :param int start: chromosomal position (1-based); :param str ref: reference allele; :param alts: single alternate allele, or list of alternate alleles; :type alts: str or list of str; """""". @handle_py4j; def __init__(self, contig, start, ref, alts):; if isinstance(contig, int):; contig = str(contig); jrep = scala_object(Env.hail().variant, 'Variant').apply(contig, start, ref, alts); self._init_from_java(jrep); self._contig = contig; self._start = start; self._ref = ref. def __str__(self):; return self._jrep.toString(). def __repr__(self):; return 'Variant(contig=%s, start=%s, ref=%s, alts=%s)' % (self.contig, self.start, self.ref, self._alt_alleles). def __eq__(self, other):; return self._jrep.equals(other._jrep). def __hash__(self):; return self._jrep.hashCode(). def _init_from_java(self, jrep):; self._jrep = jrep; self._alt_alleles = map(AltAllele._from_java, [jrep.altAlleles().apply(i) for i in xrange(jrep.nAltAlleles())]). @classmethod; def _from_java(cls, jrep):; v = Variant.__new__(cls); v._init_from_java(jrep); v._contig = jrep.contig(); v._start = jrep.start(); v._ref = jrep.ref(); return v. [docs] @staticmethod; @handle_py4j; @typecheck(string=strlike); def parse(string):; """"""Parses a variant object from a string. There are two acceptable formats: CHR:POS:REF:ALT,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/representation/variant.html:515,polymorphi,polymorphism,515,docs/0.1/_modules/hail/representation/variant.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/representation/variant.html,1,['polymorphi'],['polymorphism']
Modifiability,"﻿. . representation — Hail. Toggle navigation. HOME. DOCS. 0.2 (Stable); 0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; HailContext; VariantDataset; KeyTable; KinshipMatrix; LDMatrix; representation; Variant; AltAllele; Genotype; Call; Locus; Interval; Trio; Pedigree; Struct. expr; utils. Annotation Database; Other Resources. Hail. Docs »; Python API »; representation. View page source. representation¶. Classes. hail.representation.Variant; An object that represents a genomic polymorphism. hail.representation.AltAllele; An object that represents an allele in a polymorphism deviating from the reference allele. hail.representation.Genotype; An object that represents an individual’s genotype at a genomic locus. hail.representation.Call; An object that represents an individual’s call at a genomic locus. hail.representation.Locus; An object that represents a location in the genome. hail.representation.Interval; A genomic interval marked by start and end loci. hail.representation.Trio; Class containing information about nuclear family relatedness and sex. hail.representation.Pedigree; Class containing a list of trios, with extra functionality. hail.representation.Struct; Nested annotation structure. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/representation/index.html:567,polymorphi,polymorphism,567,docs/0.1/representation/index.html,https://hail.is,https://hail.is/docs/0.1/representation/index.html,2,['polymorphi'],['polymorphism']
Modifiability,"﻿. Clumping GWAS Results — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Clumping GWAS Results; Introduction; Hail GWAS Script; Docker Image; Batch Script; Functions; Control Code. Synopsis. Random Forest. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:285,Config,Configuration,285,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; Configuration Reference. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Supported Configuration Variables. Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Configuration Reference. View page source. Configuration Reference; Configuration variables can be set for Hail Query by:. passing them as keyword arguments to init(),; running a command of the form hailctl config set <VARIABLE_NAME> <VARIABLE_VALUE> from the command line, or; setting them as shell environment variables by running a command of the form; export <VARIABLE_NAME>=<VARIABLE_VALUE> in a terminal, which will set the variable for the current terminal; session. Each method for setting configuration variables listed above overrides variables set by any and all methods below it.; For example, setting a configuration variable by passing it to init() will override any values set for the; variable using either hailctl or shell environment variables. Warning; Some environment variables are shared between Hail Query and Hail Batch. Setting one of these variables via; init(), hailctl, or environment variables will affect both Query and Batch. However, when; instantiating a class specific to one of the two, passing configuration to that class will not affect the other.; For example, if one value for gcs_bucket_allow_list is passed to init(), a different value; may be passed to the constructor for Batch’s ServiceBackend, which will only affect that instance of the; class (which can only be used within Batch), and won’t affect Query. Supported Configuration Variables. GCS Bucket Allowlist. Keyword Argument Name; gcs_bucket_allow_list. Keyword Argument Format; [""bucket1"", ""bucket2""]. hailctl Variable Name; gcs/bucket_allow",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/configuration_reference.html:537,variab,variables,537,docs/0.2/configuration_reference.html,https://hail.is,https://hail.is/docs/0.2/configuration_reference.html,7,"['config', 'variab']","['config', 'configuration', 'variable', 'variables']"
Modifiability,"﻿. Hail | ; Core language functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Functions; Core language functions. View page source. Core language functions. literal(x[, dtype]); Captures and broadcasts a Python variable or object as an expression. cond(condition, consequent, alternate[, ...]); Deprecated in favor of if_else(). if_else(condition, consequent, alternate[, ...]); Expression for an if/else statement; tests a condition and returns one of two options based on the result. switch(expr); Build a conditional tree on the value of an expression. case([missing_false]); Chain multiple if-else statements with a CaseBuilder. bind(f, *exprs[, _ctx]); Bind a temporary variable and use it in a function. rbind(*exprs[, _ctx]); Bind a temporary variable and use it in a function. missing(t); Creates an expression representing a missing value of a specified type. null(t); Deprecated in favor of missing(). str(x); Returns the string representation of x. is_missing(expression); Returns True if the argument is missing. is_defined(expression); Returns True if the argument is not missing. coalesce(*args); Returns the first non-missing value of args. or_else(a, b); If a is missing, return b. or_missing(predicate, value); Returns value if predicate is True, otherwise returns missing. range(start[, stop, step]); Returns an array of integers from start to stop by step. query_table(path, point_or_interval); Query rec",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/core.html:789,variab,variable,789,docs/0.2/functions/core.html,https://hail.is,https://hail.is/docs/0.2/functions/core.html,1,['variab'],['variable']
Modifiability,"﻿. Hail | ; DB. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Experimental; DB. View page source. DB. class hail.experimental.DB[source]; An annotation database instance.; This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python dict describing an; Annotation DB configuration. User must specify the region (aws: 'us', gcp:; 'us-central1' or 'europe-west1') in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the cloud platform that they are using; ('gcp' or 'aws'). Parameters:. region (str) – Region cluster is running in, either 'us', 'us-central1', or 'europe-west1'; (default is 'us-central1').; cloud (str) – Cloud platform, either 'gcp' or 'aws' (default is 'gcp').; url (str, optional) – Optional URL to annotation DB configuration, if using custom configuration; (default is None).; config (str, optional) – Optional dict describing an annotation DB configuration, if using; custom configuration (default is None). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Examples; Create an annotation database connecting to the default Hail Annotation DB:; >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'). Attributes. available_datasets; List of names of available annotation datasets. Methods. annotate_rows_db; Add",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html:884,config,configuration,884,docs/0.2/experimental/hail.experimental.DB.html,https://hail.is,https://hail.is/docs/0.2/experimental/hail.experimental.DB.html,2,['config'],['configuration']
Modifiability,"﻿. Hail | ; Genetics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Genetics. View page source. Genetics. VEPConfig(); Base class for configuring VEP. VEPConfigGRCh37Version85(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh37 for VEP version 85. VEPConfigGRCh38Version95(*, data_bucket, ...); The Hail-maintained VEP configuration for GRCh38 for VEP version 95. balding_nichols_model(n_populations, ...[, ...]); Generate a matrix table of variants, samples, and genotypes using the Balding-Nichols or Pritchard-Stephens-Donnelly model. concordance(left, right, *[, ...]); Calculate call concordance with another dataset. filter_intervals(ds, intervals[, keep]); Filter rows with a list of intervals. filter_alleles(mt, f); Filter alternate alleles. filter_alleles_hts(mt, f[, subset]); Filter alternate alleles and update standard GATK entry fields. hwe_normalized_pca(call_expr[, k, ...]); Run principal component analysis (PCA) on the Hardy-Weinberg-normalized genotype call matrix. genetic_relatedness_matrix(call_expr); Compute the genetic relatedness matrix (GRM). realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). impute_sex(call[, aaf_threshold, ...]); Impute sex of samples by calculating inbreeding coefficient on the X chromosome. ld_matrix(entry_expr, locus_expr, radius[, ...]); Computes the windowed correlation (linkage disequilibrium) matrix b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:716,config,configuring,716,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,3,['config'],"['configuration', 'configuring']"
Modifiability,"﻿. Hail | ; Install Hail on Mac OS X. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Mac OS X; hailctl Autocompletion (Optional). Linux; Google Dataproc; Azure HDInsight; Other Spark Clusters; After installation, try your first Hail query. Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Installing Hail; Install Hail on Mac OS X. View page source. Install Hail on Mac OS X. Install Java 11. We recommend using a packaged installation from Azul; (make sure the OS version and architecture match your system) or using Homebrew:; brew tap homebrew/cask-versions; brew install --cask temurin8. You must pick a Java installation with a compatible architecture. If you have an Apple M1 or M2; you must use an “arm64” Java, otherwise you must use an “x86_64” Java. You can check if you have; an M1 or M2 either in the “Apple Menu > About This Mac” or by running uname -m Terminal.app. Install Python 3.9 or later. We recommend Miniconda.; Open Terminal.app and execute pip install hail. If this command fails with a message about “Rust”, please try this instead: pip install hail --only-binary=:all:.; Run your first Hail query!. hailctl Autocompletion (Optional). Install autocompletion with hailctl --install-completion zsh; Ensure this line is in your zsh config file (~/.zshrc) and then reload your terminal.; autoload -Uz compinit && compinit. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/install/macosx.html:1542,config,config,1542,docs/0.2/install/macosx.html,https://hail.is,https://hail.is/docs/0.2/install/macosx.html,1,['config'],['config']
Modifiability,"﻿. Hail | ; Microsoft Azure. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; General Advice; Query-on-Batch; Google Cloud; Microsoft Azure; hailctl hdinsight; Variant Effect Predictor (VEP). Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Microsoft Azure. View page source. Microsoft Azure. hailctl hdinsight; As of version 0.2.82, pip installations of Hail come bundled with a command-line tool, hailctl; hdinsight for working with Microsoft Azure HDInsight Spark clusters configured for; Hail.; This tool requires the Azure CLI.; An HDInsight cluster always consists of two “head” nodes, two or more “worker” nodes, and an Azure; Blob Storage container. The head nodes are automatically configured to serve Jupyter Notebooks at; https://CLUSTER_NAME.azurehdinsight.net/jupyter . The Jupyter server is protected by a; username-password combination. The username and password are printed to the terminal after the; cluster is created.; Every HDInsight cluster is associated with one storage account which your Jupyter notebooks may; access. In addition, HDInsight will create a container within this storage account (sharing a name; with the cluster) for its own purposes. When a cluster is stopped using hailctl hdinsight stop,; this container will be deleted.; To start a cluster, you must specify the cluster name, a storage account, and a resource group. The; storage account must be in the given resource group.; hailctl hdinsight start CLUSTER_NAME STORAGE_ACCOUNT RESOURCE_GROUP. To submit a Python job to that cluster, use:; hailctl hdinsight submit CLUSTER_NAME STORAGE_ACCOUNT HTTP_PASSWORD SCRIPT [optional args to your python script...]. To list run",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/azure.html:814,config,configured,814,docs/0.2/cloud/azure.html,https://hail.is,https://hail.is/docs/0.2/cloud/azure.html,1,['config'],['configured']
Modifiability,"﻿. Hail | ; Plot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Plot. View page source. Plot. Warning; Plotting functionality is in early stages and is experimental. Interfaces will change regularly. Plotting in Hail is easy. Hail’s plot functions utilize Bokeh plotting libraries to create attractive,; interactive figures. Plotting functions in this module return a Bokeh Figure, so you can call; a method to plot your data and then choose to extend the plot however you like by interacting; directly with Bokeh. See the GWAS tutorial for examples.; Plot functions in Hail accept data in the form of either Python objects or Table and MatrixTable fields. cdf; Create a cumulative density plot. pdf. smoothed_pdf; Create a density plot. histogram; Create a histogram. cumulative_histogram; Create a cumulative histogram. histogram2d; Plot a two-dimensional histogram. scatter; Create an interactive scatter plot. qq; Create a Quantile-Quantile plot. manhattan; Create a Manhattan plot. output_notebook; Configure the Bokeh output state to generate output in notebook cells when bokeh.io.show() is called. visualize_missingness; Visualize missingness in a MatrixTable. hail.plot.cdf(data, k=350, legend=None, title=None, normalize=True, log=False)[source]; Create a cumulative density plot. Parameters:. data (Struct or Float64Expression) – Sequence of data to plot.; k (int) – Accuracy parameter (passed to approx_cdf()).; leg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/plot.html:1018,extend,extend,1018,docs/0.2/plot.html,https://hail.is,https://hail.is/docs/0.2/plot.html,1,['extend'],['extend']
Modifiability,"﻿. Hail | ; Relatedness. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Relatedness. View page source. Relatedness; The relatedness of two individuals characterizes their biological; relationship. For example, two individuals might be siblings or; parent-and-child. All notions of relatedness implemented in Hail are rooted in; the idea of alleles “inherited identically by descent”. Two alleles in two; distinct individuals are inherited identically by descent if both alleles were; inherited by the same “recent,” common ancestor. The term “recent” distinguishes; alleles shared IBD from family members from alleles shared IBD from “distant”; ancestors. Distant ancestors are thought of contributing to population structure; rather than relatedness.; Relatedness is usually quantified by two quantities: kinship coefficient; (\(\phi\) or PI_HAT) and probability-of-identity-by-descent-zero; (\(\pi_0\) or Z0). The kinship coefficient is the probability that any; two alleles selected randomly from the same locus are identical by; descent. Twice the kinship coefficient is the coefficient of relationship which; is the percent of genetic material shared identically by descent.; Probability-of-identity-by-descent-zero is the probability that none of the; alleles at a randomly chosen locus were inherited identically by descent.; Hail provides three methods for the inference of relatedness: PLINK-style; identity by ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:930,inherit,inherited,930,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['inherit'],['inherited']
Modifiability,"﻿. Hail | ; Statistics. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; Methods; Statistics. View page source. Statistics. linear_mixed_model(y, x[, z_t, k, p_path, ...]); Initialize a linear mixed model from a matrix table. linear_mixed_regression_rows(entry_expr, model); For each row, test an input variable for association using a linear mixed model. linear_regression_rows(y, x, covariates[, ...]); For each row, test an input variable for association with response variables using linear regression. logistic_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a binary response variable using logistic regression. poisson_regression_rows(test, y, x, covariates); For each row, test an input variable for association with a count response variable using Poisson regression. pca(entry_expr[, k, compute_loadings]); Run principal component analysis (PCA) on numeric columns derived from a matrix table. row_correlation(entry_expr[, block_size]); Computes the correlation matrix between row vectors. hail.methods.linear_mixed_model(y, x, z_t=None, k=None, p_path=None, overwrite=False, standardize=True, mean_impute=True)[source]; Initialize a linear mixed model from a matrix table. Warning; This functionality is no longer implemented/supported as of Hail 0.2.94. hail.methods.linear_mixed_regression_rows(entry_expr, model, pa_t_path=None, a_t_path=None, mean_impute=True, par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:873,variab,variable,873,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['variab'],['variable']
Modifiability,"﻿. Hail | ; hail.context. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.context. Source code for hail.context; import os; import sys; import warnings; from contextlib import contextmanager; from random import Random; from types import TracebackType; from typing import Dict, List, Optional, Tuple, Type, Union; from urllib.parse import urlparse, urlunparse. from pyspark import SparkContext. import hail; from hail.backend import Backend; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import dictof, enumeration, nullable, oneof, sequenceof, sized_tupleof, typecheck, typecheck_method; from hail.utils import get_env_or_default; from hail.utils.java import BackendType, Env, choose_backend, warning; from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration, get_gcs_requester_pays_configuration; from hailtop.fs.fs import FS; from hailtop.hail_event_loop import hail_event_loop; from hailtop.utils import secret_alnum_string. from . import __resource_str; from .backend.backend import local_jar_information; from .builtin_references import BUILTIN_REFERENCES. def _get_tmpdir(tmpdir):; if tmpdir is None:; tmpdir = '/tmp'; return tmpdir. def _get_local_tmpdir(local_tmpdir):; local_tmpdir = get_env_or_default(local_tmpdir, 'TMPDIR', 'file:///tmp'); r = urlparse(local_tmpdir); if not r.scheme:; r = r._replace(scheme='file'); elif r.scheme != 'file':; raise ValueError('invalid local_tmpfile: must use scheme file, got scheme {r.scheme}'); return urlunparse(r). def _get_log(log):; if log is None:; py_version = version(); log_dir = os.environ.get('HAIL_LOG_DIR'); if log_dir i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:218,Config,Configuration,218,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:232,Config,Configuration,232,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.db. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.db. Source code for hail.experimental.db; import warnings; from typing import ClassVar, Iterable, List, Optional, Set, Tuple, Union. import hail as hl; from hailtop.utils import external_requests_client_session, retry_response_returning_functions. from ..expr import StructExpression; from ..matrixtable import MatrixTable, matrix_table_type; from ..table import Table, table_type; from ..typecheck import oneof, typecheck_method; from ..utils.java import Env, info; from .datasets_metadata import get_datasets_metadata; from .lens import MatrixRows, TableRows. class DatasetVersion:; """""":class:`DatasetVersion` has two constructors: :func:`.from_json` and; :func:`.get_region`. Parameters; ----------; url : :obj:`dict` or :obj:`str`; Nested dictionary of URLs containing key: value pairs, like; ``cloud: {region: url}`` if using :func:`.from_json` constructor,; or a string with the URL from appropriate region if using the; :func:`.get_region` constructor.; version : :obj:`str`, optional; String of dataset version, if not ``None``.; reference_genome : :obj:`str`, optional; String of dataset reference genome, if not ``None``.; """""". @staticmethod; def from_json(doc: dict, cloud: str) -> Optional['DatasetVersion']:; """"""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/db.html:226,Config,Configuration,226,docs/0.2/_modules/hail/experimental/db.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/db.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.export_entries_by_col. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.export_entries_by_col. Source code for hail.experimental.export_entries_by_col; import hail as hl; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck. [docs]@typecheck(; mt=MatrixTable, path=str, batch_size=int, bgzip=bool, header_json_in_file=bool, use_string_key_as_file_name=bool; ); def export_entries_by_col(; mt: MatrixTable,; path: str,; batch_size: int = 256,; bgzip: bool = True,; header_json_in_file: bool = True,; use_string_key_as_file_name: bool = False,; ):; """"""Export entries of the `mt` by column as separate text files. Examples; --------; >>> range_mt = hl.utils.range_matrix_table(10, 10); >>> range_mt = range_mt.annotate_entries(x = hl.rand_unif(0, 1)); >>> hl.experimental.export_entries_by_col(range_mt, 'output/cols_files'). Notes; -----; This function writes a directory with one file per column in `mt`. The; files contain one tab-separated field (with header) for each row field; and entry field in `mt`. The column fields of `mt` are written as JSON; in the first line of each file, prefixed with a ``#``. The above will produce a directory at ``output/cols_files`` with the; following files:. .. code-block:: text. $ ls -l output/cols_files; total 80; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 index.tsv; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-00.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-01.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-02.tsv.bgz; -rw-r--r-- 1 hail-dev wheel 712 Jan 25 17:19 part-03.tsv.bgz; -rw-r--r-- 1 hail-dev ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html:245,Config,Configuration,245,docs/0.2/_modules/hail/experimental/export_entries_by_col.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/export_entries_by_col.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.expressions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.expressions. Source code for hail.experimental.expressions; import hail as hl; from hail.expr.expressions import analyze, expr_any; from hail.expr.table_type import ttable; from hail.expr.types import hail_type; from hail.typecheck import nullable, typecheck. [docs]@typecheck(expr=expr_any, path=str, overwrite=bool); def write_expression(expr, path, overwrite=False):; """"""Write an Expression. In the same vein as Python's pickle, write out an expression; that does not have a source (such as one that comes from; Table.aggregate with _localize=False). Example; -------; >>> ht = hl.utils.range_table(100).annotate(x=hl.rand_norm()); >>> mean_norm = ht.aggregate(hl.agg.mean(ht.x), _localize=False); >>> mean_norm; >>> hl.eval(mean_norm); >>> hl.experimental.write_expression(mean_norm, 'output/expression.he'). Parameters; ----------. expr : :class:`~.Expression`; Expression to write.; path : :class:`str`; Path to which to write expression.; Suggested extension: .he (hail expression).; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination. Returns; -------; None; """"""; source = expr._indices.source; if source is not None:; analyze('write_expression.expr', expr, source._global_indices); source = source.select_globals(__expr=expr); expr = source.index_globals().__expr; hl.utils.range_table(1).filter(False).key_by().drop('idx').annotate_globals(expr=expr).write(; path, overwrite=overwrite; ). [docs]@typecheck(path=str, _assert_type=nullable(hail_type)); def read_expression(path, _assert_type=None):; """"""R",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html:235,Config,Configuration,235,docs/0.2/_modules/hail/experimental/expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/expressions.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.filtering_allele_frequency. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.filtering_allele_frequency. Source code for hail.experimental.filtering_allele_frequency; from hail.expr.expressions import Float64Expression, expr_float64, expr_int32; from hail.expr.functions import _func; from hail.expr.types import tfloat64; from hail.typecheck import typecheck. [docs]@typecheck(ac=expr_int32, an=expr_int32, ci=expr_float64); def filtering_allele_frequency(ac, an, ci) -> Float64Expression:; """"""; Computes a filtering allele frequency (described below); for `ac` and `an` with confidence `ci`. The filtering allele frequency is the highest true population allele frequency; for which the upper bound of the `ci` (confidence interval) of allele count; under a Poisson distribution is still less than the variant's observed; `ac` (allele count) in the reference sample, given an `an` (allele number). This function defines a ""filtering AF"" that represents; the threshold disease-specific ""maximum credible AF"" at or below which; the disease could not plausibly be caused by that variant. A variant with; a filtering AF >= the maximum credible AF for the disease under consideration; should be filtered, while a variant with a filtering AF below the maximum; credible remains a candidate. This filtering AF is not disease-specific:; it can be applied to any disease of interest by comparing with a; user-defined disease-specific maximum credible AF. For more details, see: `Whiffin et al., 2017 <https://www.nature.com/articles/gim201726>`__. Parameters; ----------; ac : int or :class:`.Expression` of type :p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html:250,Config,Configuration,250,docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/filtering_allele_frequency.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.full_outer_join_mt. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.full_outer_join_mt. Source code for hail.experimental.full_outer_join_mt; import hail as hl; from hail.matrixtable import MatrixTable. [docs]def full_outer_join_mt(left: MatrixTable, right: MatrixTable) -> MatrixTable:; """"""Performs a full outer join on `left` and `right`. Replaces row, column, and entry fields with the following:. - `left_row` / `right_row`: structs of row fields from left and right.; - `left_col` / `right_col`: structs of column fields from left and right.; - `left_entry` / `right_entry`: structs of entry fields from left and right. Examples; --------. The following creates and joins two random datasets with disjoint sample ids; but non-disjoint variant sets. We use :func:`.or_else` to attempt to find a; non-missing genotype. If neither genotype is non-missing, then the genotype; is set to missing. In particular, note that Samples `2` and `3` have missing; genotypes for loci 1:1 and 1:2 because those loci are not present in `mt2`; and these samples are not present in `mt1`. >>> hl.reset_global_randomness(); >>> mt1 = hl.balding_nichols_model(1, 2, 3); >>> mt2 = hl.balding_nichols_model(1, 2, 3); >>> mt2 = mt2.key_rows_by(locus=hl.locus(mt2.locus.contig,; ... mt2.locus.position+2),; ... alleles=mt2.alleles); >>> mt2 = mt2.key_cols_by(sample_idx=mt2.sample_idx+2); >>> mt1.show(); +---------------+------------+------+------+; | locus | alleles | 0.GT | 1.GT |; +---------------+------------+------+------+; | locus<GRCh37> | array<str> | call | call |; +---------------+------------+------+------+; | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html:242,Config,Configuration,242,docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/full_outer_join_mt.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.import_gtf. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.import_gtf. Source code for hail.experimental.import_gtf; import functools; import operator. import hail as hl; from hail.genetics.reference_genome import reference_genome_type; from hail.table import Table; from hail.typecheck import nullable, sequenceof, typecheck; from hail.utils import new_temp_file; from hail.utils.java import info. [docs]@typecheck(; path=str,; reference_genome=nullable(reference_genome_type),; skip_invalid_contigs=bool,; min_partitions=nullable(int),; force_bgz=bool,; force=bool,; ); def import_gtf(; path, reference_genome=None, skip_invalid_contigs=False, min_partitions=None, force_bgz=False, force=False; ) -> Table:; """"""Import a GTF file. The GTF file format is identical to the GFF version 2 file format,; and so this function can be used to import GFF version 2 files as; well. See https://www.ensembl.org/info/website/upload/gff.html for more; details on the GTF/GFF2 file format. The :class:`.Table` returned by this function will be keyed by the; ``interval`` row field and will include the following row fields:. .. code-block:: text. 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. If the ``refer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:234,Config,Configuration,234,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.ld_score_regression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ld_score_regression. Source code for hail.experimental.ld_score_regression; import hail as hl; from hail.expr.expressions import analyze, expr_float64, expr_numeric; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; weight_expr=expr_float64,; ld_score_expr=expr_numeric,; chi_sq_exprs=oneof(expr_float64, sequenceof(expr_float64)),; n_samples_exprs=oneof(expr_numeric, sequenceof(expr_numeric)),; n_blocks=int,; two_step_threshold=int,; n_reference_panel_variants=nullable(int),; ); def ld_score_regression(; weight_expr,; ld_score_expr,; chi_sq_exprs,; n_samples_exprs,; n_blocks=200,; two_step_threshold=30,; n_reference_panel_variants=None,; ) -> Table:; r""""""Estimate SNP-heritability and level of confounding biases from genome-wide association study; (GWAS) summary statistics. Given a set or multiple sets of GWAS summary statistics, :func:`.ld_score_regression` estimates the heritability; of a trait or set of traits and the level of confounding biases present in; the underlying studies by regressing chi-squared statistics on LD scores,; leveraging the model:. .. math::. \mathrm{E}[\chi_j^2] = 1 + Na + \frac{Nh_g^2}{M}l_j. * :math:`\mathrm{E}[\chi_j^2]` is the expected chi-squared statistic; for variant :math:`j` resulting from a test of association between; variant :math:`j` and a trait.; * :math:`l_j = \sum_{k} r_{jk}^2` is the LD score of variant; :math:`j`, calculated as the sum of squared correlat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html:243,Config,Configuration,243,docs/0.2/_modules/hail/experimental/ld_score_regression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ld_score_regression.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.ldscore. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscore. Source code for hail.experimental.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.bi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:231,Config,Configuration,231,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.ldscsim. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscsim. Source code for hail.experimental.ldscsim; #!/usr/bin/env python3; # -*- coding: utf-8 -*-; """"""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """""". import numpy as np; import pandas as pd; from scipy import stats. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_float64, expr_int32; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(; mt=MatrixTable,; genotype=oneof(expr_int32, expr_float64, expr_call),; h2=(oneof(float, int, list, np.ndarray)),; pi=nullable(oneof(float, int, list, np.ndarray)),; rg=nullable(oneof(float, int, list, np.ndarray)),; annot=nullable(oneof(expr_float64, expr_int32)),; popstrat=nullable(oneof(expr_int32, expr_float64)),; popstrat_var=nullable(oneof(float, int)),; exact_h2=bool,; ); def simulate_phenotypes(; mt, genotype, h2, pi=None, rg=None, annot=None, popstrat=None, popstrat_var=None, exact",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html:231,Config,Configuration,231,docs/0.2/_modules/hail/experimental/ldscsim.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscsim.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.loop. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.loop. Source code for hail.experimental.loop; from typing import Callable. from hail import ir; from hail.expr.expressions import construct_expr, construct_variable, expr_any, to_expr, unify_all; from hail.expr.types import hail_type; from hail.typecheck import anytype, typecheck; from hail.utils.java import Env. [docs]@typecheck(f=anytype, typ=hail_type, args=expr_any); def loop(f: Callable, typ, *args):; r""""""Define and call a tail-recursive function with given arguments. Notes; -----; The argument `f` must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. .. math::. f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}. we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)). Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called. This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let's; consider two different recursive definitions for the triangle function; :math:`f(x) = 0 + 1 + \dots + x`:. >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1). >>> def triangle2(x, total):; ... if x == 0:; ... return total; ... return triangle2(x - 1, total + x). The first function definition, `triangle1`, will call itself and then add x.; This is an example of a n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html:228,Config,Configuration,228,docs/0.2/_modules/hail/experimental/loop.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/loop.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:227,Config,Configuration,227,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.phase_by_transmission. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.phase_by_transmission. Source code for hail.experimental.phase_by_transmission; from typing import List. import hail as hl; from hail.expr.expressions import expr_array, expr_call, expr_locus, expr_str; from hail.matrixtable import MatrixTable; from hail.typecheck import sequenceof, typecheck. [docs]@typecheck(; locus=expr_locus(),; alleles=expr_array(expr_str),; proband_call=expr_call,; father_call=expr_call,; mother_call=expr_call,; ); def phase_by_transmission(; locus: hl.expr.LocusExpression,; alleles: hl.expr.ArrayExpression,; proband_call: hl.expr.CallExpression,; father_call: hl.expr.CallExpression,; mother_call: hl.expr.CallExpression,; ) -> hl.expr.ArrayExpression:; """"""Phases genotype calls in a trio based allele transmission. Notes; -----; In the phased calls returned, the order is as follows:; - Proband: father_allele | mother_allele; - Parents: transmitted_allele | untransmitted_allele. Phasing of sex chromosomes:; - Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html:245,Config,Configuration,245,docs/0.2/_modules/hail/experimental/phase_by_transmission.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/phase_by_transmission.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.plots. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.plots. Source code for hail.experimental.plots; import json. import numpy as np; import pandas as pd; from bokeh.layouts import gridplot; from bokeh.models import ColumnDataSource, Div, HoverTool, TabPanel, Tabs, Title; from bokeh.palettes import Spectral8; from bokeh.plotting import figure; from bokeh.transform import factor_cmap. import hail as hl; from hail.typecheck import typecheck; from hail.utils.hadoop_utils import hadoop_ls, hadoop_open; from hail.utils.java import warning. [docs]def plot_roc_curve(ht, scores, tp_label='tp', fp_label='fp', colors=None, title='ROC Curve', hover_mode='mouse'):; """"""Create ROC curve from Hail Table. One or more `score` fields must be provided, which are assessed against `tp_label` and `fp_label` as truth data. High scores should correspond to true positives. Parameters; ----------; ht : :class:`.Table`; Table with required data; scores : :class:`str` or :obj:`list` of :obj:`.str`; Top-level location of scores in ht against which to generate PR curves.; tp_label : :class:`str`; Top-level location of true positives in ht.; fp_label : :class:`str`; Top-level location of false positives in ht.; colors : :obj:`dict` of :class:`str`; Optional colors to use (score -> desired color).; title : :class:`str`; Title of plot.; hover_mode : :class:`str`; Hover mode; one of 'mouse' (default), 'vline' or 'hline'. Returns; -------; :obj:`tuple` of :class:`bokeh.plotting.figure` and :obj:`list` of :class:`str`; Figure, and list of AUCs corresponding to scores.; """"""; if colors is None:; # Get a palette aut",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html:229,Config,Configuration,229,docs/0.2/_modules/hail/experimental/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/plots.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.tidyr. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.tidyr. Source code for hail.experimental.tidyr; import hail as hl; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(ht=Table, key=str, value=str, fields=str); def gather(ht, key, value, *fields) -> Table:; """"""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields."""""". ht = ht.annotate(_col_val=hl.array([hl.struct(field_name=field, value=ht[field]) for field in fields])); ht = ht.drop(*fields); ht = ht.explode(ht['_col_val']); ht = ht.annotate(**{key: ht['_col_val'][0], value: ht['_col_val'][1]}); ht = ht.drop('_col_val'). ht_tmp = new_temp_file(); ht.write(ht_tmp). return hl.read_table(ht_tmp). [docs]@typecheck(ht=Table, field=str, value=str, key=nullable(oneof(str, sequenceof(str)))); def spread(ht, field, value, key=None) -> Table:; """"""Spread a key-value pair of fields across multiple fields. :func:`.spread` mimics the functionality of the `spread()` function in R's; `tidyr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html:229,Config,Configuration,229,docs/0.2/_modules/hail/experimental/tidyr.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/tidyr.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.experimental.time. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.time. Source code for hail.experimental.time; import hail as hl; from hail.expr.expressions import expr_int64, expr_str; from hail.expr.functions import _func; from hail.typecheck import typecheck. [docs]@typecheck(format=expr_str, time=expr_int64, zone_id=expr_str); def strftime(format, time, zone_id):; """"""; Convert Unix timestamp to a formatted datetime string. Examples; --------. >>> hl.eval(hl.experimental.strftime(""%Y.%m.%d %H:%M:%S %z"", 1562569201, ""America/New_York"")); '2019.07.08 03:00:01 -04:00'. >>> hl.eval(hl.experimental.strftime(""%A, %B %e, %Y. %r"", 876541523, ""GMT+2"")); 'Saturday, October 11, 1997. 05:45:23 AM'. >>> hl.eval(hl.experimental.strftime(""%A, %B %e, %Y. %r"", 876541523, ""+08:00"")); 'Saturday, October 11, 1997. 11:45:23 AM'. Notes; -----; The following formatting characters are supported in format strings: A a B b D d e F H I j k l M m n p R r S s T t U u V v W Y y z; See documentation here: https://linux.die.net/man/3/strftime. A zone id can take one of three forms. It can be an explicit offset, like ""+01:00"", a relative offset, like ""GMT+2"",; or a IANA timezone database (TZDB) identifier, like ""America/New_York"". Wikipedia maintains a list of TZDB identifiers here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. Currently, the formatter implicitly uses the ""en_US"" locale. Parameters; ----------; format : str or :class:`.Expression` of type :py:data:`.tstr`; The format string describing how to render the time.; time : int of :class:`.Expression` of type :py:data:`.tint64`; A long represent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/time.html:228,Config,Configuration,228,docs/0.2/_modules/hail/experimental/time.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/time.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.aggregators.aggregators. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.aggregators.aggregators. Source code for hail.expr.aggregators.aggregators; import difflib; from functools import update_wrapper, wraps. import hail as hl; from hail import ir; from hail.expr import (; Aggregation,; ArrayExpression,; BooleanExpression,; DictExpression,; Expression,; ExpressionException,; Float64Expression,; Indices,; Int64Expression,; NDArrayNumericExpression,; NumericExpression,; SetExpression,; StringExpression,; StructExpression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; to_expr,; unify_all,; unify_types,; ); from hail.expr.expressions.typed_expressions import construct_variable; from hail.expr.functions import _quantile_from_cdf, _result_from_raw_cdf, float32, rbind; from hail.expr.types import (; hail_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import TypeChecker, func_spec, identity, nullable, oneof, sequenceof, typecheck, typecheck_method; from hail.utils import wrap_to_list; from hail.utils.java import Env. class AggregableChecker(TypeChecker):; def __init__(self, coercer):; self.coercer = coercer; super(AggregableChecker, self).__init__(). def expects(self):; return self.coercer.expects(). def format(self, arg):; return self.coercer.format(arg). def check(self, x, caller, param):; x = self.coercer.check(x, caller, param); if len(x._ir.search(lambda node: isinstance(nod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:239,Config,Configuration,239,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.builders. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.builders. Source code for hail.expr.builders; import hail as hl; from hail import ir; from hail.expr.expressions import (; ExpressionException,; construct_expr,; expr_any,; expr_bool,; expr_str,; unify_types,; unify_types_limited,; ); from hail.typecheck import typecheck_method. class ConditionalBuilder(object):; def __init__(self):; self._ret_type = None; self._cases = []. def _unify_type(self, t):; if self._ret_type is None:; self._ret_type = t; else:; r = unify_types_limited(self._ret_type, t); if not r:; raise TypeError(""'then' expressions must have same type, found '{}' and '{}'"".format(self._ret_type, t)). [docs]class SwitchBuilder(ConditionalBuilder):; """"""Class for generating conditional trees based on value of an expression. Examples; --------. >>> csq = hl.literal('loss of function'); >>> expr = (hl.switch(csq); ... .when('synonymous', 1); ... .when('SYN', 1); ... .when('missense', 2); ... .when('MIS', 2); ... .when('loss of function', 3); ... .when('LOF', 3); ... .or_missing()); >>> hl.eval(expr); 3. Notes; -----; All expressions appearing as the `then` parameters to; :meth:`~hail.expr.builders.SwitchBuilder.when` or; :meth:`~hail.expr.builders.SwitchBuilder.default` method calls must be the; same type. See Also; --------; :func:`.case`, :func:`.cond`, :func:`.switch`. Parameters; ----------; expr : :class:`.Expression`; Value to match against.; """""". @typecheck_method(base=expr_any); def __init__(self, base):; self._base = base; self._when_missing_case = None; super(SwitchBuilder, self).__init__(). def _finish(self, default):; a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/builders.html:224,Config,Configuration,224,docs/0.2/_modules/hail/expr/builders.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/builders.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.expressions.base_expression. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.base_expression. Source code for hail.expr.expressions.base_expression; from typing import Any, List, Mapping, Tuple, overload. import numpy as np; import pandas as pd. import hail; import hail as hl; from hail import ir; from hail.expr import expressions; from hail.expr.types import (; HailType,; from_numpy,; is_compound,; is_numeric,; is_setlike,; summary_type,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; tset,; tstr,; tstruct,; ttuple,; ); from hail.typecheck import anyfunc, linked_list, nullable, typecheck_method; from hail.utils.java import Env; from hail.utils.linkedlist import LinkedList. from .indices import Aggregation, Indices. class Summary(object):; def __init__(self, type, count, summ_fields, nested, header=None):; self.count = count; self.summ_fields = summ_fields; self.nested = nested; self.type = type; self.header = header. @staticmethod; def pct(x):; return f'{x*100:.2f}%'. @staticmethod; def format(x):; if isinstance(x, float):; return f'{x:.2f}'; else:; return str(x). def __str__(self):; return self._ascii_string(depth=0, prefix=None). def __repr__(self):; return self.__str__(). def _repr_html_(self):; return self._html_string(prefix=None). def _ascii_string(self, depth, prefix):; spacing = ' ' * depth. summary = ''; if self.header:; summary += f'\n{spacing}{self.header}'. if prefix is not None:; summary += f'\n\n{spacing}- {prefix} ({summary_type(self.type)}):'. if len(self.summ_fields) > 0:; max_n_len = max(len(n) for n in self",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html:243,Config,Configuration,243,docs/0.2/_modules/hail/expr/expressions/base_expression.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/base_expression.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.expressions.expression_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.expression_utils. Source code for hail.expr.expressions.expression_utils; from typing import Dict, Set. from hail.typecheck import setof, typecheck. from ...ir import MakeTuple; from ..expressions import Expression, ExpressionException, expr_any; from .indices import Aggregation, Indices. @typecheck(caller=str, expr=Expression, expected_indices=Indices, aggregation_axes=setof(str), broadcast=bool); def analyze(caller: str, expr: Expression, expected_indices: Indices, aggregation_axes: Set = set(), broadcast=True):; from hail.utils import error, warning. indices = expr._indices; source = indices.source; axes = indices.axes; aggregations = expr._aggregations. warnings = []; errors = []. expected_source = expected_indices.source; expected_axes = expected_indices.axes. if source is not None and source is not expected_source:; bad_refs = []; for name, inds in get_refs(expr).items():; if inds.source is not expected_source:; bad_refs.append(name); errors.append(; ExpressionException(; ""'{caller}': source mismatch\n""; "" Expected an expression from source {expected}\n""; "" Found expression derived from source {actual}\n""; "" Problematic field(s): {bad_refs}\n\n""; "" This error is commonly caused by chaining methods together:\n""; "" >>> ht.distinct().select(ht.x)\n\n""; "" Correct usage:\n""; "" >>> ht = ht.distinct()\n""; "" >>> ht = ht.select(ht.x)"".format(; caller=caller, expected=expected_source, actual=source, bad_refs=list(bad_refs); ); ); ). # check for stray indices by subtracting expected axes from observed; if broadc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html:244,Config,Configuration,244,docs/0.2/_modules/hail/expr/expressions/expression_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/expression_utils.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.expressions.typed_expressions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.expressions.typed_expressions. Source code for hail.expr.expressions.typed_expressions; from typing import Dict, Mapping, Optional, Sequence, Union. import numpy as np; from deprecated import deprecated. import hail as hl; from hail import ir; from hail.expr.types import (; HailType,; is_numeric,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.typecheck import (; anyfunc,; dictof,; func_spec,; identity,; nullable,; oneof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils.java import Env, warning; from hail.utils.linkedlist import LinkedList; from hail.utils.misc import get_nice_attr_error, get_nice_field_error, wrap_to_list, wrap_to_tuple. from .base_expression import Expression, ExpressionException, to_expr, unify_all, unify_types; from .expression_typecheck import (; coercer_from_dtype,; expr_any,; expr_array,; expr_bool,; expr_dict,; expr_int32,; expr_int64,; expr_interval,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_tuple,; ); from .indices import Aggregation, Indices. [docs]class CollectionExpression(Expression):; """"""Expression of type :class:`.tarray` or :class:`.tset`. >>> a = hl.literal([1, 2, 3, 4, 5]). >>> s3 = hl.literal({'Alice', 'Bob', 'Charlie'}); """""". def _filter_missing_method(self, filter_missing: bool, name: str, ret_type: HailType, *args):; collection = self; if filter_missing:; collection = self.filter(hl.is_defined); return collection._me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:245,Config,Configuration,245,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.functions. Source code for hail.expr.functions; import builtins; import functools; import itertools; import operator; from typing import Any, Callable, Iterable, Optional, TypeVar, Union. import numpy as np; import pandas as pd; from deprecated import deprecated. import hail; import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; ArrayNumericExpression,; BooleanExpression,; CallExpression,; DictExpression,; Expression,; ExpressionException,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; IntervalExpression,; LocusExpression,; NumericExpression,; SetExpression,; StreamExpression,; StringExpression,; StructExpression,; TupleExpression,; apply_expr,; cast_expr,; coercer_from_dtype,; construct_expr,; construct_variable,; expr_any,; expr_array,; expr_bool,; expr_call,; expr_dict,; expr_float32,; expr_float64,; expr_int32,; expr_int64,; expr_interval,; expr_locus,; expr_ndarray,; expr_numeric,; expr_oneof,; expr_set,; expr_str,; expr_stream,; expr_struct,; expr_tuple,; impute_type,; to_expr,; unify_all,; unify_exprs,; unify_types_limited,; ); from hail.expr.types import (; HailType,; hail_type,; is_float32,; is_float64,; is_int32,; is_int64,; is_numeric,; is_primitive,; tarray,; tbool,; tcall,; tdict,; tfloat32,; tfloat64,; tint32,; tint64,; tinterval,; tlocus,; tndarray,; trngstate,; tset,; tstr,; tstream,; tstruct,; ttuple,; ); from hail.genetics.allele_type import AlleleType; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typec",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:225,Config,Configuration,225,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.expr.types. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.expr.types. Source code for hail.expr.types; import abc; import builtins; import json; import math; import pprint; from collections.abc import Mapping, Sequence; from typing import ClassVar, Union. import numpy as np; import pandas as pd. import hail as hl; from hailtop.frozendict import frozendict; from hailtop.hail_frozenlist import frozenlist. from .. import genetics; from ..genetics.reference_genome import reference_genome_type; from ..typecheck import nullable, oneof, transformed, typecheck, typecheck_method; from ..utils.byte_reader import ByteReader, ByteWriter; from ..utils.java import escape_parsable; from ..utils.misc import lookup_bit; from ..utils.struct import Struct; from .nat import NatBase, NatLiteral; from .type_parsing import type_grammar, type_grammar_str, type_node_visitor. __all__ = [; 'dtype',; 'dtypes_from_pandas',; 'HailType',; 'hail_type',; 'is_container',; 'is_compound',; 'is_numeric',; 'is_primitive',; 'types_match',; 'tint',; 'tint32',; 'tint64',; 'tfloat',; 'tfloat32',; 'tfloat64',; 'tstr',; 'tbool',; 'tarray',; 'tstream',; 'tndarray',; 'tset',; 'tdict',; 'tstruct',; 'tunion',; 'ttuple',; 'tinterval',; 'tlocus',; 'tcall',; 'tvoid',; 'tvariable',; 'hts_entry_schema',; ]. def summary_type(t):; if isinstance(t, hl.tdict):; return f'dict<{summary_type(t.key_type)}, {summary_type(t.value_type)}>'; elif isinstance(t, hl.tset):; return f'set<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tarray):; return f'array<{summary_type(t.element_type)}>'; elif isinstance(t, hl.tstruct):; return f'struct with {len(t)} fields';",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:221,Config,Configuration,221,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.genetics.allele_type. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.allele_type. Source code for hail.genetics.allele_type; from enum import IntEnum, auto. _ALLELE_STRS = (; ""Unknown"",; ""SNP"",; ""MNP"",; ""Insertion"",; ""Deletion"",; ""Complex"",; ""Star"",; ""Symbolic"",; ""Transition"",; ""Transversion"",; ). [docs]class AlleleType(IntEnum):; """"""An enumeration for allele type. Notes; -----; The precise values of the enumeration constants are not guarenteed; to be stable and must not be relied upon.; """""". UNKNOWN = 0; """"""Unknown Allele Type""""""; SNP = auto(); """"""Single-nucleotide Polymorphism (SNP)""""""; MNP = auto(); """"""Multi-nucleotide Polymorphism (MNP)""""""; INSERTION = auto(); """"""Insertion""""""; DELETION = auto(); """"""Deletion""""""; COMPLEX = auto(); """"""Complex Polymorphism""""""; STAR = auto(); """"""Star Allele (``alt=*``)""""""; SYMBOLIC = auto(); """"""Symbolic Allele. e.g. ``alt=<INS>``; """"""; TRANSITION = auto(); """"""Transition SNP. e.g. ``ref=A alt=G``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """"""; TRANSVERSION = auto(); """"""Transversion SNP. e.g. ``ref=A alt=C``. Note; ----; This is only really used internally in :func:`hail.vds.sample_qc` and; :func:`hail.methods.sample_qc`.; """""". def __str__(self):; return str(self.value). @property; def pretty_name(self):; """"""A formatted (as opposed to uppercase) version of the member's name,; to match :func:`~hail.expr.functions.allele_type`. Examples; --------; >>> AlleleType.INSERTION.pretty_name; 'Insertion'; >>> at = AlleleType(hl.eval(hl.numeric_allele_type('a', 'att'))); >>> at.pretty_name == hl.eval(hl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html:231,Config,Configuration,231,docs/0.2/_modules/hail/genetics/allele_type.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/allele_type.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.genetics.call. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.call. Source code for hail.genetics.call; from collections.abc import Sequence. from hail.typecheck import typecheck_method. [docs]class Call(object):; """"""; An object that represents an individual's call at a genomic locus. Parameters; ----------; alleles : :obj:`list` of :obj:`int`; List of alleles that compose the call.; phased : :obj:`bool`; If ``True``, the alleles are phased and the order is specified by; `alleles`. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.GT.take(5`)``. This is rare; it is much; more common to manipulate the :class:`.CallExpression` object, which is; constructed using the following functions:. - :func:`.call`; - :func:`.unphased_diploid_gt_index_call`; - :func:`.parse_call`; """""". def __init__(self, alleles, phased=False):; # Intentionally not using the type check annotations which are too slow.; assert isinstance(alleles, Sequence); assert isinstance(phased, bool). if len(alleles) > 2:; raise NotImplementedError(""Calls with greater than 2 alleles are not supported.""); self._phased = phased; ploidy = len(alleles); if phased or ploidy < 2:; self._alleles = alleles; else:; assert ploidy == 2; a0 = alleles[0]; a1 = alleles[1]; if a1 < a0:; a0, a1 = a1, a0; self._alleles = [a0, a1]. def __str__(self):; n = self.ploidy; if n == 0:; if self._phased:; return '|-'; return '-'. if n == 1:; if self._phased:; return f'|{self._alleles[0]}'; return str(self._alleles[0]). assert n == 2; a0 = self._alleles[0]; a1 = self._alleles[1]; if self._phased:; return",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/call.html:224,Config,Configuration,224,docs/0.2/_modules/hail/genetics/call.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/call.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.genetics.locus. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.locus. Source code for hail.genetics.locus; from typing import Union. import hail as hl; from hail.genetics.reference_genome import ReferenceGenome, reference_genome_type; from hail.typecheck import typecheck_method. [docs]class Locus(object):; """"""An object that represents a location in the genome. Parameters; ----------; contig : :class:`str`; Chromosome identifier.; position : :obj:`int`; Chromosomal position (1-indexed).; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.locus.take(5)``. This is rare; it is much; more common to manipulate the :class:`.LocusExpression` object, which is; constructed using the following functions:. - :func:`.locus`; - :func:`.parse_locus`; - :func:`.locus_from_global_position`; """""". def __init__(self, contig, position, reference_genome: Union[str, ReferenceGenome] = 'default'):; if isinstance(contig, int):; contig = str(contig). if isinstance(reference_genome, str):; reference_genome = hl.get_reference(reference_genome). assert isinstance(contig, str); assert isinstance(position, int); assert isinstance(reference_genome, ReferenceGenome). self._contig = contig; self._position = position; self._rg = reference_genome. def __str__(self):; return f'{self._contig}:{self._position}'. def __repr__(self):; return 'Locus(contig=%s, position=%s, reference_genome=%s)' % (self.contig, self.position, self._rg). def __eq__(self, other):; return (; (self._contig == ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html:225,Config,Configuration,225,docs/0.2/_modules/hail/genetics/locus.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/locus.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.genetics.pedigree. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.pedigree. Source code for hail.genetics.pedigree; import re; from collections import Counter. from hail.typecheck import nullable, sequenceof, typecheck_method; from hail.utils.java import Env, FatalError, warning. [docs]class Trio(object):; """"""Class containing information about nuclear family relatedness and sex. :param str s: Sample ID of proband. :param fam_id: Family ID.; :type fam_id: str or None. :param pat_id: Sample ID of father.; :type pat_id: str or None. :param mat_id: Sample ID of mother.; :type mat_id: str or None. :param is_female: Sex of proband.; :type is_female: bool or None; """""". @typecheck_method(s=str, fam_id=nullable(str), pat_id=nullable(str), mat_id=nullable(str), is_female=nullable(bool)); def __init__(self, s, fam_id=None, pat_id=None, mat_id=None, is_female=None):; self._fam_id = fam_id; self._s = s; self._pat_id = pat_id; self._mat_id = mat_id; self._is_female = is_female. def __repr__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; repr(self.s),; repr(self.fam_id),; repr(self.pat_id),; repr(self.mat_id),; repr(self.is_female),; ). def __str__(self):; return 'Trio(s=%s, fam_id=%s, pat_id=%s, mat_id=%s, is_female=%s)' % (; str(self.s),; str(self.fam_id),; str(self.pat_id),; str(self.mat_id),; str(self.is_female),; ). def __eq__(self, other):; return (; isinstance(other, Trio); and self._s == other._s; and self._mat_id == other._mat_id; and self._pat_id == other._pat_id; and self._fam_id == other._fam_id; and self._is_female == other._is_female; ). def __hash__(self):; retur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html:228,Config,Configuration,228,docs/0.2/_modules/hail/genetics/pedigree.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/pedigree.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.genetics.reference_genome. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.genetics.reference_genome. Source code for hail.genetics.reference_genome; import json; import re; from bisect import bisect_right. import hail as hl; from hail.typecheck import dictof, lazy, nullable, oneof, sequenceof, sized_tupleof, transformed, typecheck_method; from hail.utils.java import Env; from hail.utils.misc import wrap_to_list. rg_type = lazy(); reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type). [docs]class ReferenceGenome:; """"""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:236,Config,Configuration,236,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.aes. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.aes. Source code for hail.ggplot.aes; from collections.abc import Mapping. from hail.expr import Expression, literal. [docs]class Aesthetic(Mapping):; def __init__(self, properties):; self.properties = properties. def __getitem__(self, item):; return self.properties[item]. def __len__(self):; return len(self.properties). def __contains__(self, item):; return item in self.properties. def __iter__(self):; return iter(self.properties). def __repr__(self):; return self.properties.__repr__(). def merge(self, other):; return Aesthetic({**self.properties, **other.properties}). [docs]def aes(**kwargs):; """"""Create an aesthetic mapping. Parameters; ----------; kwargs:; Map aesthetic names to hail expressions based on table's plot. Returns; -------; :class:`.Aesthetic`; The aesthetic mapping to be applied. """"""; hail_field_properties = {}. for k, v in kwargs.items():; _v = v; if not isinstance(v, Expression):; _v = literal(v); hail_field_properties[k] = _v; return Aesthetic(hail_field_properties). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html:221,Config,Configuration,221,docs/0.2/_modules/hail/ggplot/aes.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/aes.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.coord_cartesian. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.coord_cartesian. Source code for hail.ggplot.coord_cartesian; from .geoms import FigureAttribute. class CoordCartesian(FigureAttribute):; def __init__(self, xlim, ylim):; self.xlim = xlim; self.ylim = ylim. def apply_to_fig(self, fig_so_far):; if self.xlim is not None:; fig_so_far.update_xaxes(range=list(self.xlim)); if self.ylim is not None:; fig_so_far.update_yaxes(range=list(self.ylim)). [docs]def coord_cartesian(xlim=None, ylim=None):; """"""Set the boundaries of the plot. Parameters; ----------; xlim : :obj:`tuple` with two int; The minimum and maximum x value to show on the plot.; ylim : :obj:`tuple` with two int; The minimum and maximum y value to show on the plot. Returns; -------; :class:`.FigureAttribute`; The coordinate attribute to be applied. """"""; return CoordCartesian(xlim, ylim). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html:233,Config,Configuration,233,docs/0.2/_modules/hail/ggplot/coord_cartesian.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/coord_cartesian.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.facets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.facets. Source code for hail.ggplot.facets; import abc; import math; from typing import ClassVar, Dict, Optional, Tuple. import hail as hl; from hail.expr import Expression, StructExpression. from .geoms import FigureAttribute; from .utils import n_partitions. [docs]def vars(*args: Expression) -> StructExpression:; """""". Parameters; ----------; *args: :class:`hail.expr.Expression`; Fields to facet by. Returns; -------; :class:`hail.expr.StructExpression`; A struct to pass to a faceter. """"""; return hl.struct(**{f""var_{i}"": arg for i, arg in enumerate(args)}). [docs]def facet_wrap(; facets: StructExpression, *, nrow: Optional[int] = None, ncol: Optional[int] = None, scales: str = ""fixed""; ) -> ""FacetWrap"":; """"""Introduce a one dimensional faceting on specified fields. Parameters; ----------; facets: :class:`hail.expr.StructExpression` created by `hl.ggplot.vars` function.; The fields to facet on.; nrow: :class:`int`; The number of rows into which the facets will be spread. Will be ignored if `ncol` is set.; ncol: :class:`int`; The number of columns into which the facets will be spread.; scales: :class:`str`; Whether the scales are the same across facets. For more information and a list of supported options, see `the ggplot documentation <https://ggplot2-book.org/facet.html#controlling-scales>`__. Returns; -------; :class:`FigureAttribute`; The faceter. """"""; return FacetWrap(facets, nrow, ncol, scales). class Faceter(FigureAttribute):; @abc.abstractmethod; def get_expr_to_group_by(self) -> StructExpression:; pass. class FacetWrap(Faceter):; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html:224,Config,Configuration,224,docs/0.2/_modules/hail/ggplot/facets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/facets.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.geoms. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.geoms. Source code for hail.ggplot.geoms; import abc; from typing import Any, ClassVar, Dict, Optional. import numpy as np; import plotly.graph_objects as go. from .aes import aes; from .stats import StatBin, StatCDF, StatCount, StatFunction, StatIdentity, StatNone; from .utils import bar_position_plotly_to_gg, linetype_plotly_to_gg. [docs]class FigureAttribute(abc.ABC):; pass. class Geom(FigureAttribute):; def __init__(self, aes):; self.aes = aes. @abc.abstractmethod; def apply_to_fig(; self, agg_result, fig_so_far: go.Figure, precomputed, facet_row, facet_col, legend_cache, is_faceted: bool; ):; """"""Add this geometry to the figure and indicate if this geometry demands a static figure.""""""; pass. @abc.abstractmethod; def get_stat(self):; pass. def _add_aesthetics_to_trace_args(self, trace_args, df):; for aes_name, (plotly_name, default) in self.aes_to_arg.items():; if hasattr(self, aes_name) and getattr(self, aes_name) is not None:; trace_args[plotly_name] = getattr(self, aes_name); elif aes_name in df.attrs:; trace_args[plotly_name] = df.attrs[aes_name]; elif aes_name in df.columns:; trace_args[plotly_name] = df[aes_name]; elif default is not None:; trace_args[plotly_name] = default. def _update_legend_trace_args(self, trace_args, legend_cache):; if ""name"" in trace_args:; trace_args[""legendgroup""] = trace_args[""name""]; if trace_args[""name""] in legend_cache:; trace_args[""showlegend""] = False; else:; trace_args[""showlegend""] = True; legend_cache[trace_args[""name""]] = {}. class GeomLineBasic(Geom):; aes_to_arg: ClassVar = {; ""color"": (""line",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html:223,Config,Configuration,223,docs/0.2/_modules/hail/ggplot/geoms.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/geoms.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.ggplot. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.ggplot. Source code for hail.ggplot.ggplot; import itertools; from pprint import pprint. from plotly.subplots import make_subplots. import hail as hl. from .aes import Aesthetic, aes; from .coord_cartesian import CoordCartesian; from .facets import Faceter; from .geoms import FigureAttribute, Geom; from .labels import Labels; from .scale import (; Scale,; ScaleContinuous,; ScaleDiscrete,; scale_color_continuous,; scale_color_discrete,; scale_fill_continuous,; scale_fill_discrete,; scale_shape_auto,; scale_x_continuous,; scale_x_discrete,; scale_x_genomic,; scale_y_continuous,; scale_y_discrete,; ); from .utils import check_scale_continuity, is_continuous_type, is_genomic_type. [docs]class GGPlot:; """"""The class representing a figure created using the ``hail.ggplot`` module. Create one by using :func:`.ggplot`. .. automethod:: to_plotly; .. automethod:: show; .. automethod:: write_image; """""". def __init__(self, ht, aes, geoms=[], labels=Labels(), coord_cartesian=None, scales=None, facet=None):; if scales is None:; scales = {}. self.ht = ht; self.aes = aes; self.geoms = geoms; self.labels = labels; self.coord_cartesian = coord_cartesian; self.scales = scales; self.facet = facet. self.add_default_scales(aes). def __add__(self, other):; assert isinstance(other, (FigureAttribute, Aesthetic)). copied = self.copy(); if isinstance(other, Geom):; copied.geoms.append(other); copied.add_default_scales(other.aes); elif isinstance(other, Labels):; copied.labels = copied.labels.merge(other); elif isinstance(other, CoordCartesian):; copied.coord_cartes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html:224,Config,Configuration,224,docs/0.2/_modules/hail/ggplot/ggplot.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/ggplot.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.labels. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.labels. Source code for hail.ggplot.labels; from .geoms import FigureAttribute. class Labels(FigureAttribute):; def __init__(self, title=None, xlabel=None, ylabel=None, group_labels={}, **kwargs):; self.title = title; self.xlabel = xlabel; self.ylabel = ylabel; self.group_labels = group_labels. def apply_to_fig(self, fig_so_far):; layout_updates = {}; if self.title is not None:; layout_updates[""title""] = self.title; if self.xlabel is not None:; layout_updates[""xaxis_title""] = self.xlabel; if self.ylabel is not None:; layout_updates[""yaxis_title""] = self.ylabel. fig_so_far.update_layout(**layout_updates). for legend_group, label in self.group_labels.items():; fig_so_far.update_traces({""legendgrouptitle_text"": label}, {""legendgroup"": legend_group}). def merge(self, other):; new_title = other.title if other.title is not None else self.title; new_xlabel = other.xlabel if other.xlabel is not None else self.xlabel; new_ylabel = other.ylabel if other.ylabel is not None else self.ylabel; new_group_labels = {**self.group_labels, **other.group_labels}. return Labels(title=new_title, xlabel=new_xlabel, ylabel=new_ylabel, group_labels=new_group_labels). [docs]def ggtitle(label):; """"""Sets the title of a plot. Parameters; ----------; label : :class:`str`; The desired title of the plot. Returns; -------; :class:`.FigureAttribute`; Label object to change the title.; """"""; return Labels(title=label). [docs]def xlab(label):; """"""Sets the x-axis label of a plot. Parameters; ----------; label : :class:`str`; The desired x-axis label of the plot. Returns; ---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html:224,Config,Configuration,224,docs/0.2/_modules/hail/ggplot/labels.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/labels.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.ggplot.scale. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.ggplot.scale. Source code for hail.ggplot.scale; import abc; from collections.abc import Mapping. import plotly; import plotly.express as px. from hail.context import get_reference; from hail.expr.types import tstr. from .geoms import FigureAttribute; from .utils import continuous_nums_to_colors, is_continuous_type, is_discrete_type. class Scale(FigureAttribute):; def __init__(self, aesthetic_name):; self.aesthetic_name = aesthetic_name. @abc.abstractmethod; def transform_data(self, field_expr):; pass. def create_local_transformer(self, groups_of_dfs):; return lambda x: x. @abc.abstractmethod; def is_discrete(self):; pass. @abc.abstractmethod; def is_continuous(self):; pass. def valid_dtype(self, dtype):; pass. class PositionScale(Scale):; def __init__(self, aesthetic_name, name, breaks, labels):; super().__init__(aesthetic_name); self.name = name; self.breaks = breaks; self.labels = labels. def update_axis(self, fig):; if self.aesthetic_name == ""x"":; return fig.update_xaxes; elif self.aesthetic_name == ""y"":; return fig.update_yaxes. # What else do discrete and continuous scales have in common?; def apply_to_fig(self, parent, fig_so_far):; if self.name is not None:; self.update_axis(fig_so_far)(title=self.name). if self.breaks is not None:; self.update_axis(fig_so_far)(tickvals=self.breaks). if self.labels is not None:; self.update_axis(fig_so_far)(ticktext=self.labels). def valid_dtype(self, dtype):; return True. class PositionScaleGenomic(PositionScale):; def __init__(self, aesthetic_name, reference_genome, name=None):; super().__init__(aesth",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html:223,Config,Configuration,223,docs/0.2/_modules/hail/ggplot/scale.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/ggplot/scale.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.linalg.blockmatrix. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.blockmatrix. Source code for hail.linalg.blockmatrix; import itertools; import math; import os; import re. import numpy as np; import scipy.linalg as spla. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import construct_expr, construct_variable; from hail.expr.blockmatrix_type import tblockmatrix; from hail.expr.expressions import (; expr_array,; expr_float64,; expr_int32,; expr_int64,; expr_ndarray,; expr_tuple,; matrix_table_source,; raise_unless_entry_indexed,; ); from hail.ir import (; F64,; ApplyBinaryPrimOp,; ApplyUnaryPrimOp,; BandSparsifier,; BlockMatrixAgg,; BlockMatrixBroadcast,; BlockMatrixCollect,; BlockMatrixDensify,; BlockMatrixDot,; BlockMatrixFilter,; BlockMatrixMap,; BlockMatrixMap2,; BlockMatrixRandom,; BlockMatrixRead,; BlockMatrixSlice,; BlockMatrixSparsify,; BlockMatrixToTable,; BlockMatrixToValueApply,; BlockMatrixWrite,; ExportType,; PerBlockSparsifier,; RectangleSparsifier,; RowIntervalSparsifier,; TableFromBlockMatrixNativeReader,; TableRead,; ValueToBlockMatrix,; tensor_shape_to_matrix_shape,; ); from hail.ir.blockmatrix_reader import BlockMatrixBinaryReader, BlockMatrixNativeReader; from hail.ir.blockmatrix_writer import BlockMatrixBinaryWriter, BlockMatrixNativeWriter, BlockMatrixRectanglesWriter; from hail.table import Table; from hail.typecheck import (; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; sliceof,; tupleof,; typecheck,; typecheck_method,; ); from hail.utils import local_path_uri, new_local_temp_file, new_temp_file, storage_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:229,Config,Configuration,229,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.linalg.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.linalg.utils.misc. Source code for hail.linalg.utils.misc; import numpy as np. import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, raise_unless_row_indexed; from hail.typecheck import nullable, oneof, typecheck; from hail.utils.java import Env. [docs]@typecheck(a=np.ndarray, radius=oneof(int, float)); def array_windows(a, radius):; """"""Returns start and stop indices for window around each array value. Examples; --------. >>> hl.linalg.utils.array_windows(np.array([1, 2, 4, 4, 6, 8]), 2); (array([0, 0, 1, 1, 2, 4]), array([2, 4, 5, 5, 6, 6])). >>> hl.linalg.utils.array_windows(np.array([-10.0, -2.5, 0.0, 0.0, 1.2, 2.3, 3.0]), 2.5); (array([0, 1, 1, 1, 2, 2, 4]), array([1, 4, 6, 6, 7, 7, 7])). Notes; -----; For an array ``a`` in ascending order, the resulting ``starts`` and ``stops``; arrays have the same length as ``a`` and the property that, for all indices; ``i``, ``[starts[i], stops[i])`` is the maximal range of indices ``j`` such; that ``a[i] - radius <= a[j] <= a[i] + radius``. Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; :meth:`.BlockMatrix.sparsify_row_intervals`. Parameters; ----------; a: :obj:`numpy.ndarray` of signed integer or float values; 1-dimensional array of values, non-decreasing with respect to index.; radius: :obj:`float`; Non-negative radius of window for values. Returns; -------; (:class:`numpy.ndarray` of :obj:`int`, :class:`numpy.ndarray` of :obj:`int`); Tuple of start indices array and stop indices array.; """"""; if radius < 0:;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html:228,Config,Configuration,228,docs/0.2/_modules/hail/linalg/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/utils/misc.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.matrixtable. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.matrixtable. Source code for hail.matrixtable; import itertools; import warnings; from collections import Counter; from typing import Any, Dict, Iterable, List, Optional, Tuple. from deprecated import deprecated. import hail as hl; from hail import ir; from hail.expr.expressions import (; Expression,; ExpressionException,; Indices,; StructExpression,; TupleExpression,; analyze,; construct_expr,; construct_reference,; expr_any,; expr_bool,; expr_struct,; extract_refs_by_indices,; unify_all,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.types import tarray, tset, types_match; from hail.table import ExprContainer, Table, TableIndexKeyError; from hail.typecheck import (; anyfunc,; anytype,; dictof,; enumeration,; lazy,; nullable,; numeric,; oneof,; sequenceof,; typecheck,; typecheck_method,; ); from hail.utils import deduplicate, default_handler, storage_level; from hail.utils.java import Env, info, warning; from hail.utils.misc import check_annotate_exprs, get_key_by_exprs, get_select_exprs, process_joins, wrap_to_tuple. [docs]class GroupedMatrixTable(ExprContainer):; """"""Matrix table grouped by row or column that can be aggregated into a new matrix table."""""". def __init__(; self,; parent: 'MatrixTable',; row_keys=None,; computed_row_key=None,; col_keys=None,; computed_col_key=None,; entry_fields=None,; row_fields=None,; col_fields=None,; partitions=None,; ):; super(GroupedMatrixTable, self).__init__(); self._parent = parent; self._copy_fields_from(parent); self._row_keys = row_keys; self._computed_row_key = computed_row_key; self._c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:222,Config,Configuration,222,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.family_methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.family_methods. Source code for hail.methods.family_methods; from typing import Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail.expr import expr_call, expr_float64; from hail.genetics.pedigree import Pedigree; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import numeric, typecheck; from hail.utils.java import Env. from .misc import require_biallelic, require_col_key_str. [docs]@typecheck(dataset=MatrixTable, pedigree=Pedigree, complete_trios=bool); def trio_matrix(dataset, pedigree, complete_trios=False) -> MatrixTable:; """"""Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. .. include:: ../_templates/req_tstring.rst. Examples; --------. Create a trio matrix:. >>> pedigree = hl.Pedigree.read('data/case_control_study.fam'); >>> trio_dataset = hl.trio_matrix(dataset, pedigree, complete_trios=True). Notes; -----. This method builds a new matrix table with one column per trio. If; `complete_trios` is ``True``, then only trios that satisfy; :meth:`.Trio.is_complete` are included. In this new dataset, the column; identifiers are the sample IDs of the trio probands. The column fields and; entries of the matrix are changed in the following ways:. The new column fields consist of three structs (`proband`, `father`,; `mother`), a Boolean field, and a string field:. - **proband** (:class:`.tstruct`) - Column fields on the proband.; - **father** (:class:`.tstruct`) - Column fields on the father.; - **mother** (:clas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:233,Config,Configuration,233,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.impex. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.impex. Source code for hail.methods.impex; import os; import re; from collections import defaultdict. import avro.schema; from avro.datafile import DataFileReader; from avro.io import DatumReader. import hail as hl; from hail import ir; from hail.expr import (; LocusExpression,; StructExpression,; analyze,; expr_array,; expr_bool,; expr_call,; expr_float64,; expr_int32,; expr_numeric,; expr_str,; to_expr,; ); from hail.expr.matrix_type import tmatrix; from hail.expr.table_type import ttable; from hail.expr.types import hail_type, tarray, tbool, tcall, tfloat32, tfloat64, tint32, tint64, tstr, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.ir.utils import parse_type; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str, require_row_key_variant; from hail.table import Table; from hail.typecheck import (; anytype,; char,; dictof,; enumeration,; nullable,; numeric,; oneof,; sequenceof,; sized_tupleof,; table_key_type,; typecheck,; ); from hail.utils import new_temp_file; from hail.utils.deduplicate import deduplicate; from hail.utils.java import Env, FatalError, info, jindexed_seq_args, warning; from hail.utils.misc import plural, wrap_to_list. from .import_lines_helpers import should_remove_line, split_lines. def locus_interval_expr(contig, start, end, includes_start, includes_end, reference_genome, skip_invalid_intervals):; includes_start = hl.bool(includes_start); includes_end = hl.bool(includes_end). if reference_genome:; return hl.locus_int",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:224,Config,Configuration,224,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.misc. Source code for hail.methods.misc; from typing import Union. import hail as hl; from hail import ir; from hail.expr import Expression, construct_expr, construct_variable, expr_any, expr_array, expr_interval, expr_numeric; from hail.expr.types import tarray, tlocus, tstr, tstruct, ttuple; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import func_spec, nullable, oneof, typecheck; from hail.utils import Interval, Struct, deduplicate, new_temp_file; from hail.utils.java import Env, info; from hail.utils.misc import plural. [docs]@typecheck(i=Expression, j=Expression, keep=bool, tie_breaker=nullable(func_spec(2, expr_numeric)), keyed=bool); def maximal_independent_set(i, j, keep=True, tie_breaker=None, keyed=True) -> Table:; """"""Return a table containing the vertices in a near; `maximal independent set <https://en.wikipedia.org/wiki/Maximal_independent_set>`_; of an undirected graph whose edges are given by a two-column table. Examples; --------; Run PC-relate and compute pairs of closely related individuals:. >>> pc_rel = hl.pc_relate(dataset.GT, 0.001, k=2, statistics='kin'); >>> pairs = pc_rel.filter(pc_rel['kin'] > 0.125). Starting from the above pairs, prune individuals from a dataset until no; close relationships remain:. >>> related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j, False); >>> result = dataset.filter_cols(; ... hl.is_defined(related_samples_to_remove[dataset.col_key]), keep=False). Starting from the above pairs, prune individuals from a dataset until no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:223,Config,Configuration,223,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.pca. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.pca. Source code for hail.methods.pca; from typing import List, Tuple. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.experimental import mt_to_table_of_ndarray; from hail.expr import expr_call, expr_float64, matrix_table_source, raise_unless_entry_indexed; from hail.expr.expressions import construct_expr; from hail.table import Table; from hail.typecheck import nullable, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info. def hwe_normalize(call_expr):; mt = matrix_table_source('hwe_normalize/call_expr', call_expr); mt = mt.select_entries(__gt=call_expr.n_alt_alleles()); mt = mt.annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:222,Config,Configuration,222,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.qc. Source code for hail.methods.qc; import abc; import logging; import os; from collections import Counter; from shlex import quote as shq; from typing import Dict, List, Optional, Tuple, Union. import hail as hl; import hailtop.batch_client as bc; from hail.backend.service_backend import ServiceBackend; from hail.expr import Float64Expression; from hail.expr.expressions.expression_typecheck import expr_float64; from hail.expr.functions import numeric_allele_type; from hail.expr.types import tarray, tfloat, tint32, tstr, tstruct; from hail.genetics.allele_type import AlleleType; from hail.ir import TableToTableApply; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import anytype, nullable, numeric, oneof, typecheck; from hail.utils import FatalError; from hail.utils.java import Env, info, warning; from hail.utils.misc import divide_null, guess_cloud_spark_provider, new_temp_file; from hailtop import pip_version, yamlx; from hailtop.config import get_deploy_config; from hailtop.utils import async_to_blocking. from .misc import (; require_alleles_field,; require_biallelic,; require_col_key_str,; require_row_key_variant,; require_table_key_variant,; ). log = logging.getLogger('methods.qc'). HAIL_GENETICS_VEP_GRCH37_85_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH37_85_IMAGE', f'hailgenetics/vep-grch37-85:{pip_version()}'; ); HAIL_GENETICS_VEP_GRCH38_95_IMAGE = os.environ.get(; 'HAIL_GENETICS_VEP_GRCH38_95_IMAGE', f'hailgenetics/vep-grch38-95:{pip_version()}'; ). def _qc_allele_type(ref, alt):; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:221,Config,Configuration,221,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.relatedness.identity_by_descent. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.identity_by_descent. Source code for hail.methods.relatedness.identity_by_descent; import hail as hl; from hail import ir; from hail.backend.spark_backend import SparkBackend; from hail.expr import analyze; from hail.expr.expressions import expr_float64; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_col_key_str; from hail.table import Table; from hail.typecheck import nullable, numeric, typecheck; from hail.utils.java import Env. [docs]@typecheck(dataset=MatrixTable, maf=nullable(expr_float64), bounded=bool, min=nullable(numeric), max=nullable(numeric)); def identity_by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:250,Config,Configuration,250,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.relatedness.king. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.king. Source code for hail.methods.relatedness.king; import hail as hl; from hail.expr.expressions import expr_call, matrix_table_source; from hail.typecheck import nullable, typecheck; from hail.utils import deduplicate; from hail.utils.java import Env. [docs]@typecheck(call_expr=expr_call, block_size=nullable(int)); def king(call_expr, *, block_size=None):; r""""""Compute relatedness estimates between individuals using a KING variant. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate the kinship coefficient for every pair of samples. >>> kinship = hl.king(dataset.GT). Notes; -----. The following presentation summarizes the methods section of `Manichaikul,; et. al. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3025716/>`__, but; adopts a more consistent notation for matrices. Let. - :math:`i` and :math:`j` be two individuals in the dataset. - :math:`N^{Aa}_{i}` be the number of heterozygote genotypes for individual; :math:`i`. - :math:`N^{Aa,Aa}_{i,j}` be the number of variants at which a pair of; individuals both have heterozygote genotypes. - :math:`N^{AA,aa}_{i,j}` be the number of variants at which a pair of; individuals have opposing homozygote genotypes. - :math:`S_{i,j}` be the set of single-nucleotide variants for which both; individuals :math:`i` and :math:`j` have a non-missing genotype. - :math:`X_{i,s}` be the genotype score matrix. Each entry corresponds to; the genotype of individual :math:`i` at variant; :math:`s`. Homozygous-reference genotypes are represented as 0,; hetero",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html:235,Config,Configuration,235,docs/0.2/_modules/hail/methods/relatedness/king.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/king.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.relatedness.mating_simulation. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.mating_simulation. Source code for hail.methods.relatedness.mating_simulation; import hail as hl; from hail.matrixtable import MatrixTable; from hail.typecheck import numeric, typecheck. [docs]@typecheck(mt=MatrixTable, n_rounds=int, generation_size_multiplier=numeric, keep_founders=bool); def simulate_random_mating(mt, n_rounds=1, generation_size_multiplier=1.0, keep_founders=True):; """"""Simulate random diploid mating to produce new individuals. Parameters; ----------; mt; n_rounds : :obj:`int`; Number of rounds of mating.; generation_size_multiplier : :obj:`float`; Ratio of number of offspring to current population for each round of mating.; keep_founders :obj:`bool`; If true, keep all founders and intermediate generations in the final sample list. If; false, keep only offspring in the last generation. Returns; -------; :class:`.MatrixTable`; """"""; if generation_size_multiplier <= 0:; raise ValueError(; f""simulate_random_mating: 'generation_size_multiplier' must be greater than zero: got {generation_size_multiplier}""; ); if n_rounds < 1:; raise ValueError(f""simulate_random_mating: 'n_rounds' must be positive: got {n_rounds}""). ck = next(iter(mt.col_key)). mt = mt.select_entries('GT'). ht = mt.localize_entries('__entries', '__cols'). ht = ht.annotate_globals(; generation_0=hl.range(hl.len(ht.__cols)).map(; lambda i: hl.struct(; s=hl.str('generation_0_idx_') + hl.str(i),; original=hl.str(ht.__cols[i][ck]),; mother=hl.missing('int32'),; father=hl.missing('int32'),; ); ); ). def make_new_generation(prev_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html:248,Config,Configuration,248,docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/mating_simulation.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.relatedness.pc_relate. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.relatedness.pc_relate. Source code for hail.methods.relatedness.pc_relate; from typing import Optional. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.backend.spark_backend import SparkBackend; from hail.expr import (; ArrayNumericExpression,; BooleanExpression,; CallExpression,; Float64Expression,; analyze,; expr_array,; expr_call,; expr_float64,; matrix_table_source,; ); from hail.expr.types import tarray; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import enumeration, nullable, numeric, typecheck; from hail.utils import new_temp_file; from hail.utils.java import Env. from ..pca import _hwe_normalized_blanczos, hwe_normalized_pca. [docs]@typecheck(; call_expr=expr_call,; min_individual_maf=numeric,; k=nullable(int),; scores_expr=nullable(expr_array(expr_float64)),; min_kinship=nullable(numeric),; statistics=enumeration('kin', 'kin2', 'kin20', 'all'),; block_size=nullable(int),; include_self_kinship=bool,; ); def pc_relate(; call_expr: CallExpression,; min_individual_maf: float,; *,; k: Optional[int] = None,; scores_expr: Optional[ArrayNumericExpression] = None,; min_kinship: Optional[float] = None,; statistics: str = 'all',; block_size: Optional[int] = None,; include_self_kinship: bool = False,; ) -> Table:; r""""""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: ../_templates/req_diploid_gt.rst. Examples; --------; Estimate kinship, identity-by-descent two, identity-by-descent one, a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:240,Config,Configuration,240,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.methods.statgen. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.methods.statgen. Source code for hail.methods.statgen; import builtins; import itertools; import math; from typing import Callable, Dict, List, Optional, Tuple, Union. import hail as hl; import hail.expr.aggregators as agg; from hail import ir; from hail.expr import (; Expression,; ExpressionException,; NDArrayNumericExpression,; StructExpression,; analyze,; expr_any,; expr_call,; expr_float64,; expr_locus,; expr_numeric,; matrix_table_source,; raise_unless_column_indexed,; raise_unless_entry_indexed,; raise_unless_row_indexed,; table_source,; ); from hail.expr.functions import expit; from hail.expr.types import tarray, tbool, tfloat64, tint32, tndarray, tstruct; from hail.genetics.reference_genome import reference_genome_type; from hail.linalg import BlockMatrix; from hail.matrixtable import MatrixTable; from hail.methods.misc import require_biallelic, require_row_key_variant; from hail.stats import LinearMixedModel; from hail.table import Table; from hail.typecheck import anytype, enumeration, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils import FatalError, new_temp_file, wrap_to_list; from hail.utils.java import Env, info, warning. from ..backend.spark_backend import SparkBackend; from . import pca, relatedness. pc_relate = relatedness.pc_relate; identity_by_descent = relatedness.identity_by_descent; _blanczos_pca = pca._blanczos_pca; _hwe_normalized_blanczos = pca._hwe_normalized_blanczos; _spectral_moments = pca._spectral_moments; _pca_and_moments = pca._pca_and_moments; hwe_normalized_pca = pca.hwe_nor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:226,Config,Configuration,226,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.nd.nd. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.nd.nd. Source code for hail.nd.nd; from functools import reduce. import hail as hl; from hail.expr.expressions import (; Int64Expression,; cast_expr,; construct_expr,; expr_any,; expr_array,; expr_bool,; expr_int32,; expr_int64,; expr_ndarray,; expr_numeric,; expr_tuple,; unify_all,; ); from hail.expr.expressions.typed_expressions import NDArrayNumericExpression; from hail.expr.functions import _ndarray; from hail.expr.functions import array as aarray; from hail.expr.types import HailType, tfloat32, tfloat64, tndarray, ttuple; from hail.ir import Apply, NDArrayConcat, NDArrayEigh, NDArrayInv, NDArrayQR, NDArraySVD; from hail.typecheck import nullable, oneof, sequenceof, tupleof, typecheck. tsequenceof_nd = oneof(sequenceof(expr_ndarray()), expr_array(expr_ndarray())); shape_type = oneof(expr_int64, tupleof(expr_int64), expr_tuple()). [docs]def array(input_array, dtype=None):; """"""Construct an :class:`.NDArrayExpression`. Examples; --------. >>> hl.eval(hl.nd.array([1, 2, 3, 4])); array([1, 2, 3, 4], dtype=int32). >>> hl.eval(hl.nd.array([[1, 2, 3], [4, 5, 6]])); array([[1, 2, 3],; [4, 5, 6]], dtype=int32). >>> hl.eval(hl.nd.array(np.identity(3))); array([[1., 0., 0.],; [0., 1., 0.],; [0., 0., 1.]]). >>> hl.eval(hl.nd.array(hl.range(10, 20))); array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype=int32). Parameters; ----------; input_array : :class:`.ArrayExpression`, numpy ndarray, or nested python lists/tuples; The array to convert to a Hail ndarray.; dtype : :class:`.HailType`; Desired hail type. Default: `float64`. Returns; -------; :class:`.NDArray",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:216,Config,Configuration,216,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.plot.plots. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.plot.plots. Source code for hail.plot.plots; import collections; import math; import warnings; from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple, Union. import bokeh; import bokeh.io; import bokeh.models; import bokeh.palettes; import bokeh.plotting; import numpy as np; import pandas as pd; from bokeh.layouts import gridplot; from bokeh.models import (; BasicTicker,; CategoricalColorMapper,; CDSView,; ColorBar,; ColorMapper,; Column,; ColumnDataSource,; CustomJS,; DataRange1d,; GridPlot,; GroupFilter,; HoverTool,; IntersectionFilter,; Label,; Legend,; LegendItem,; LinearColorMapper,; LogColorMapper,; LogTicker,; Plot,; Renderer,; Select,; Slope,; Span,; ); from bokeh.plotting import figure; from bokeh.transform import transform. import hail; from hail.expr import aggregators; from hail.expr.expressions import (; Expression,; Float32Expression,; Float64Expression,; Int32Expression,; Int64Expression,; LocusExpression,; NumericExpression,; StringExpression,; expr_any,; expr_float64,; expr_locus,; expr_numeric,; expr_str,; raise_unless_row_indexed,; ); from hail.expr.functions import _error_from_cdf_python; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, nullable, numeric, oneof, sequenceof, sized_tupleof, typecheck; from hail.utils.java import warning; from hail.utils.struct import Struct. palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']. [docs]def output_notebook():; """"""Configure the Bokeh out",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/plot/plots.html:221,Config,Configuration,221,docs/0.2/_modules/hail/plot/plots.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/plot/plots.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.stats.linear_mixed_model. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.stats.linear_mixed_model. Source code for hail.stats.linear_mixed_model; [docs]class LinearMixedModel(object):; r""""""Class representing a linear mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94. """""". def __init__(self, py, px, s, y=None, x=None, p_path=None):; raise NotImplementedError(""LinearMixedModel is no longer implemented/supported as of Hail 0.2.94""). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html:235,Config,Configuration,235,docs/0.2/_modules/hail/stats/linear_mixed_model.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/stats/linear_mixed_model.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.table. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.table. Source code for hail.table; import collections; import itertools; import pprint; import shutil; from typing import Callable, ClassVar, Dict, List, Optional, Sequence, Union, overload. import numpy as np; import pandas; import pyspark. import hail as hl; from hail import ir; from hail.expr.expressions import (; ArrayExpression,; BooleanExpression,; CallExpression,; CollectionExpression,; DictExpression,; Expression,; ExpressionException,; Indices,; IntervalExpression,; LocusExpression,; NDArrayExpression,; NumericExpression,; StringExpression,; StructExpression,; TupleExpression,; analyze,; construct_expr,; construct_reference,; expr_any,; expr_array,; expr_bool,; expr_stream,; expr_struct,; extract_refs_by_indices,; to_expr,; unify_all,; ); from hail.expr.table_type import ttable; from hail.expr.types import dtypes_from_pandas, hail_type, tarray, tset, tstruct, types_match; from hail.typecheck import (; anyfunc,; anytype,; dictof,; enumeration,; func_spec,; lazy,; nullable,; numeric,; oneof,; sequenceof,; table_key_type,; typecheck,; typecheck_method,; ); from hail.utils import deduplicate; from hail.utils.interval import Interval; from hail.utils.java import Env, info, warning; from hail.utils.misc import (; check_annotate_exprs,; check_collisions,; check_keys,; get_key_by_exprs,; get_nice_attr_error,; get_nice_field_error,; get_select_exprs,; plural,; process_joins,; storage_level,; wrap_to_tuple,; ); from hail.utils.placement_tree import PlacementTree. table_type = lazy(). class TableIndexKeyError(Exception):; def __init__(self, key_type, in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:216,Config,Configuration,216,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.utils.hadoop_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.hadoop_utils. Source code for hail.utils.hadoop_utils; import gzip; import io; import os.path; import sys; from typing import Any, Dict, List. from hail.fs.hadoop_fs import HadoopFS; from hail.typecheck import enumeration, typecheck; from hail.utils.java import Env, info. [docs]@typecheck(path=str, mode=enumeration('r', 'w', 'x', 'rb', 'wb', 'xb'), buffer_size=int); def hadoop_open(path: str, mode: str = 'r', buffer_size: int = 8192):; """"""Open a file through the Hadoop filesystem API. Supports distributed; file systems like hdfs, gs, and s3. Warning; -------; Due to an implementation limitation, :func:`hadoop_open` may be quite; slow for large data sets (anything larger than 50 MB). Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported mo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html:229,Config,Configuration,229,docs/0.2/_modules/hail/utils/hadoop_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/hadoop_utils.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.utils.interval. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.interval. Source code for hail.utils.interval; import hail as hl; from hail.typecheck import anytype, lazy, nullable, typecheck_method. interval_type = lazy(). [docs]class Interval(object):; """"""; An object representing a range of values between `start` and `end`. >>> interval2 = hl.Interval(3, 6). Parameters; ----------; start : any type; Object with type `point_type`.; end : any type; Object with type `point_type`.; includes_start : :obj:`bool`; Interval includes start.; includes_end : :obj:`bool`; Interval includes end. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.interval.take(5)``. This is rare; it is much; more common to manipulate the :class:`.IntervalExpression` object, which is; constructed using the following functions:. - :func:`.interval`; - :func:`.locus_interval`; - :func:`.parse_locus_interval`; """""". @typecheck_method(; start=anytype,; end=anytype,; includes_start=bool,; includes_end=bool,; point_type=nullable(lambda: hl.expr.types.hail_type),; ); def __init__(self, start, end, includes_start=True, includes_end=False, point_type=None):; if point_type is None:; from hail.expr.expressions import impute_type, unify_types_limited. start_type = impute_type(start); end_type = impute_type(end); point_type = unify_types_limited(start_type, end_type); if point_type is None:; raise TypeError(""'start' and 'end' have incompatible types: '{}', '{}'."".format(start_type, end_type)). self._point_type = point_type; self._start = start; self._end = end; self._includes_start",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/interval.html:225,Config,Configuration,225,docs/0.2/_modules/hail/utils/interval.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/interval.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.utils.misc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.misc. Source code for hail.utils.misc; import atexit; import datetime; import difflib; import json; import os; import re; import secrets; import shutil; import string; import tempfile; from collections import Counter, defaultdict; from contextlib import contextmanager; from io import StringIO; from typing import Literal, Optional; from urllib.parse import urlparse. import hail; import hail as hl; from hail.typecheck import enumeration, nullable, typecheck; from hail.utils.java import Env, error. [docs]@typecheck(n_rows=int, n_cols=int, n_partitions=nullable(int)); def range_matrix_table(n_rows, n_cols, n_partitions=None) -> 'hail.MatrixTable':; """"""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:221,Config,Configuration,221,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.utils.struct. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.struct. Source code for hail.utils.struct; import pprint; from collections import OrderedDict; from collections.abc import Mapping; from typing import Any, Dict. from hail.typecheck import anytype, typecheck, typecheck_method; from hail.utils.misc import get_nice_attr_error, get_nice_field_error. [docs]class Struct(Mapping):; """"""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """""". def __init__(self, **kwargs):; # Set this way to avoid an infinite recursion in `__getattr__`.; self.__dict__[""_fields""] = kwargs. def __contains__(self, item):; return item in self._fields. def __getstate__(self) -> Dict[str, Any]:; return self._fields. def __setstate__(self, state: Dict[str, Any]):; self.__dict__[""_fields""] = st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/struct.html:223,Config,Configuration,223,docs/0.2/_modules/hail/utils/struct.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/struct.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.utils.tutorial. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.utils.tutorial. Source code for hail.utils.tutorial; import os; import zipfile; from urllib.request import urlretrieve. import hail as hl; from hailtop.utils import sync_retry_transient_errors. from .java import Env, info; from .misc import local_path_uri, new_local_temp_dir, new_temp_file. __all__ = ['get_1kg', 'get_hgdp', 'get_movie_lens']. resources = {; '1kg_annotations': 'https://storage.googleapis.com/hail-tutorial/1kg_annotations.txt',; '1kg_matrix_table': 'https://storage.googleapis.com/hail-tutorial/1kg.vcf.bgz',; '1kg_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/ensembl_gene_annotations.txt',; 'HGDP_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_pop_and_sex_annotations.tsv',; 'HGDP_matrix_table': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_subset.vcf.bgz',; 'HGDP_ensembl_gene_annotations': 'https://storage.googleapis.com/hail-tutorial/hgdp/hgdp_gene_annotations.tsv',; 'movie_lens_100k': 'https://files.grouplens.org/datasets/movielens/ml-100k.zip',; }. tmp_dir: str = None. def init_temp_dir():; global tmp_dir; if tmp_dir is None:; tmp_dir = new_local_temp_dir(). def _dir_exists(fs, path):; return fs.exists(path) and fs.is_dir(path). def _file_exists(fs, path):; return fs.exists(path) and fs.is_file(path). def _copy_to_tmp(fs, src, extension=None):; dst = new_temp_file(extension=extension); fs.copy(src, dst); return dst. [docs]def get_1kg(output_dir, overwrite: bool = False):; """"""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html:225,Config,Configuration,225,docs/0.2/_modules/hail/utils/tutorial.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/tutorial.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.vds.combiner.variant_dataset_combiner. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.combiner.variant_dataset_combiner. Source code for hail.vds.combiner.variant_dataset_combiner; import collections; import hashlib; import json; import os; import sys; import uuid; from itertools import chain; from math import floor, log; from typing import ClassVar, Collection, Dict, List, NamedTuple, Optional, Union. import hail as hl; from hail.expr import HailType, tmatrix; from hail.genetics.reference_genome import ReferenceGenome; from hail.utils import FatalError, Interval; from hail.utils.java import info, warning. from ..variant_dataset import VariantDataset; from .combine import (; calculate_even_genome_partitioning,; calculate_new_intervals,; combine,; combine_r,; combine_variant_datasets,; defined_entry_fields,; make_reference_stream,; make_variant_stream,; transform_gvcf,; ). [docs]class VDSMetadata(NamedTuple):; """"""The path to a Variant Dataset and the number of samples within. Parameters; ----------; path : :class:`str`; Path to the variant dataset.; n_samples : :class:`int`; Number of samples contained within the Variant Dataset at `path`. """""". path: str; n_samples: int. class CombinerOutType(NamedTuple):; """"""A container for the types of a VDS"""""". reference_type: tmatrix; variant_type: tmatrix. FAST_CODEC_SPEC = """"""{; ""name"": ""LEB128BufferSpec"",; ""child"": {; ""name"": ""BlockingBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""ZstdBlockBufferSpec"",; ""blockSize"": 65536,; ""child"": {; ""name"": ""StreamBlockBufferSpec""; }; }; }; }"""""". [docs]class VariantDatasetCombiner: # pylint: disable=too-many-instanc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:248,Config,Configuration,248,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.vds.functions. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.functions. Source code for hail.vds.functions; import hail as hl; from hail.expr.expressions import expr_any, expr_array, expr_call, expr_int32; from hail.expr.functions import _func; from hail.typecheck import enumeration, typecheck. [docs]@typecheck(lgt=expr_call, la=expr_array(expr_int32)); def lgt_to_gt(lgt, la):; """"""Transform LGT into GT using local alleles array. Parameters; ----------; lgt : :class:`.CallExpression`; LGT value.; la : :class:`.ArrayExpression`; Local alleles array. Returns; -------; :class:`.CallExpression`; """"""; return hl.rbind(lgt, lambda lgt: hl.if_else(lgt.is_non_ref(), _func(""lgt_to_gt"", hl.tcall, lgt, la), lgt)). [docs]@typecheck(; array=expr_array(),; local_alleles=expr_array(expr_int32),; n_alleles=expr_int32,; fill_value=expr_any,; number=enumeration('A', 'R', 'G'),; ); def local_to_global(array, local_alleles, n_alleles, fill_value, number):; """"""Reindex a locally-indexed array to globally-indexed. Examples; --------; >>> local_alleles = hl.array([0, 2]); >>> local_ad = hl.array([9, 10]); >>> local_pl = hl.array([94, 0, 123]). >>> hl.eval(local_to_global(local_ad, local_alleles, n_alleles=3, fill_value=0, number='R')); [9, 0, 10]. >>> hl.eval(local_to_global(local_pl, local_alleles, n_alleles=3, fill_value=999, number='G')); [94, 999, 999, 0, 999, 123]. Notes; -----; The `number` parameter matches the `VCF specification <https://samtools.github.io/hts-specs/VCFv4.3.pdf>`__; number definitions:. - ``A`` indicates one value per allele, excluding the reference.; - ``R`` indicates one value per allele, including",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/functions.html:224,Config,Configuration,224,docs/0.2/_modules/hail/vds/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/functions.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.vds.methods. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.methods. Source code for hail.vds.methods; import hail as hl; from hail import ir; from hail.expr import expr_any, expr_array, expr_bool, expr_interval, expr_locus, expr_str; from hail.matrixtable import MatrixTable; from hail.table import Table; from hail.typecheck import dictof, enumeration, func_spec, nullable, oneof, sequenceof, typecheck; from hail.utils.java import Env, info, warning; from hail.utils.misc import new_temp_file, wrap_to_list; from hail.vds.variant_dataset import VariantDataset. def write_variant_datasets(vdss, paths, *, overwrite=False, stage_locally=False, codec_spec=None):; """"""Write many `vdses` to their corresponding path in `paths`.""""""; ref_writer = ir.MatrixNativeMultiWriter(; [f""{p}/reference_data"" for p in paths], overwrite, stage_locally, codec_spec; ); var_writer = ir.MatrixNativeMultiWriter([f""{p}/variant_data"" for p in paths], overwrite, stage_locally, codec_spec); Env.backend().execute(ir.MatrixMultiWrite([vds.reference_data._mir for vds in vdss], ref_writer)); Env.backend().execute(ir.MatrixMultiWrite([vds.variant_data._mir for vds in vdss], var_writer)). [docs]@typecheck(vds=VariantDataset); def to_dense_mt(vds: 'VariantDataset') -> 'MatrixTable':; """"""Creates a single, dense :class:`.MatrixTable` from the split; :class:`.VariantDataset` representation. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation. Returns; -------; :class:`.MatrixTable`; Dataset in dense MatrixTable representation.; """"""; ref = vds.reference_data; # FIXME(chrisvittal) consider changing END ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/methods.html:222,Config,Configuration,222,docs/0.2/_modules/hail/vds/methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/methods.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.vds.sample_qc. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.sample_qc. Source code for hail.vds.sample_qc; from collections.abc import Sequence; from typing import Optional. import hail as hl; from hail.expr.expressions import Expression; from hail.expr.expressions.typed_expressions import (; ArrayExpression,; CallExpression,; LocusExpression,; NumericExpression,; StructExpression,; ); from hail.genetics.allele_type import AlleleType; from hail.methods.misc import require_first_key_field_locus; from hail.methods.qc import _qc_allele_type; from hail.table import Table; from hail.typecheck import nullable, sequenceof, typecheck; from hail.utils.java import Env; from hail.utils.misc import divide_null; from hail.vds.variant_dataset import VariantDataset. @typecheck(global_gt=Expression, alleles=ArrayExpression); def vmt_sample_qc_variant_annotations(; *,; global_gt: 'Expression',; alleles: 'ArrayExpression',; ) -> tuple['Expression', 'Expression']:; """"""Compute the necessary variant annotations for :func:`.vmt_sample_qc`, that is,; allele count (AC) and an integer representation of allele type. Parameters; ----------; global_gt : :class:`.Expression`; Call expression of the global GT of a variants matrix table usually generated; by :func:`..lgt_to_gt`; alleles : :class:`.ArrayExpression`; Array expression of the alleles of a variants matrix table; (generally ``vds.variant_data.alleles``). Returns; -------; :class:`tuple`; Tuple of expressions representing the AC (first element) and allele type; (second element).; """""". return (hl.agg.call_stats(global_gt, alleles).AC, alleles[1:].map(lambda alt: _qc_all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html:224,Config,Configuration,224,docs/0.2/_modules/hail/vds/sample_qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/sample_qc.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hail.vds.variant_dataset. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.vds.variant_dataset. Source code for hail.vds.variant_dataset; import json; import os. import hail as hl; from hail.genetics import ReferenceGenome; from hail.matrixtable import MatrixTable; from hail.typecheck import typecheck_method; from hail.utils.java import info, warning. extra_ref_globals_file = 'extra_reference_globals.json'. [docs]def read_vds(; path,; *,; intervals=None,; n_partitions=None,; _assert_reference_type=None,; _assert_variant_type=None,; _warn_no_ref_block_max_length=True,; ) -> 'VariantDataset':; """"""Read in a :class:`.VariantDataset` written with :meth:`.VariantDataset.write`. Parameters; ----------; path: :obj:`str`. Returns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:230,Config,Configuration,230,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hailtop.frozendict. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.frozendict. Source code for hailtop.frozendict; from collections.abc import Mapping; from typing import Dict, Generic, TypeVar. T = TypeVar(""T""); U = TypeVar(""U""). [docs]class frozendict(Mapping, Generic[T, U]):; """"""; An object representing an immutable dictionary. >>> my_frozen_dict = hl.utils.frozendict({1:2, 7:5}). To get a normal python dictionary with the same elements from a `frozendict`:. >>> dict(frozendict({'a': 1, 'b': 2})). Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.my_dict.take(5)``. This is rare; it is much; more common to manipulate the :class:`.DictExpression` object, which is; constructed using :func:`.dict`. This class is necessary because hail; supports using dicts as keys to other dicts or as elements in sets, while; python does not. """""". def __init__(self, d: Dict[T, U]):; self.d = d.copy(). def __getitem__(self, k: T) -> U:; return self.d[k]. def __hash__(self) -> int:; return hash(frozenset(self.items())). def __len__(self) -> int:; return len(self.d). def __iter__(self):; return iter(self.d). def __repr__(self):; return f'frozendict({self.d!r})'. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html:224,Config,Configuration,224,docs/0.2/_modules/hailtop/frozendict.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/frozendict.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; hailtop.fs.fs_utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hailtop.fs.fs_utils. Source code for hailtop.fs.fs_utils; import io; from typing import List, Optional. from hailtop.aiocloud.aiogoogle import GCSRequesterPaysConfiguration; from hailtop.utils.gcs_requester_pays import GCSRequesterPaysFSCache. from .router_fs import RouterFS; from .stat_result import FileListEntry. _fses = GCSRequesterPaysFSCache(fs_constructor=RouterFS). [docs]def open(; path: str,; mode: str = 'r',; buffer_size: int = 8192,; *,; requester_pays_config: Optional[GCSRequesterPaysConfiguration] = None,; ) -> io.IOBase:; """"""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Writ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html:225,Config,Configuration,225,docs/0.2/_modules/hailtop/fs/fs_utils.html,https://hail.is,https://hail.is/docs/0.2/_modules/hailtop/fs/fs_utils.html,1,['Config'],['Configuration']
Modifiability,"﻿. Hail | ; linalg/utils. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); hail; Classes; Modules; expressions; types; functions; aggregators; scans; methods; nd; utils; linalg; stats; genetics; plot; ggplot; vds; experimental. Top-Level Functions. hailtop.fs; hailtop.batch. Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Python API; Hail Query Python API; linalg; linalg/utils. View page source. linalg/utils. array_windows(a, radius); Returns start and stop indices for window around each array value. locus_windows(locus_expr, radius[, ...]); Returns start and stop indices for window around each locus. hail.linalg.utils.array_windows(a, radius)[source]; Returns start and stop indices for window around each array value.; Examples; >>> hl.linalg.utils.array_windows(np.array([1, 2, 4, 4, 6, 8]), 2); (array([0, 0, 1, 1, 2, 4]), array([2, 4, 5, 5, 6, 6])). >>> hl.linalg.utils.array_windows(np.array([-10.0, -2.5, 0.0, 0.0, 1.2, 2.3, 3.0]), 2.5); (array([0, 1, 1, 1, 2, 2, 4]), array([1, 4, 6, 6, 7, 7, 7])). Notes; For an array a in ascending order, the resulting starts and stops; arrays have the same length as a and the property that, for all indices; i, [starts[i], stops[i]) is the maximal range of indices j such; that a[i] - radius <= a[j] <= a[i] + radius.; Index ranges are start-inclusive and stop-exclusive. This function is; especially useful in conjunction with; BlockMatrix.sparsify_row_intervals(). Parameters:. a (numpy.ndarray of signed integer or float values) – 1-dimensional array of values, non-decreasing with respect to index.; radius (float) – Non-negative radius of window for values. Returns:; (numpy.ndarray of int, numpy.ndarray of int) – Tuple of start indices array and stop in",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/utils/index.html:419,Config,Configuration,419,docs/0.2/linalg/utils/index.html,https://hail.is,https://hail.is/docs/0.2/linalg/utils/index.html,1,['Config'],['Configuration']
Performance," 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interval))); if verbose:; info(; f'get_gene_intervals found {len(gene_info)} entries:\n'; + ""\n"".join(map(lambda x: f'{x[0]}: {x[1]} ({x[2] if x[0] == ""gene"" else x[3]})', gene_info)); ); intervals = list(map(lambda x: x[-1], gene_info)); return intervals. def _load_gencode_gtf(gtf_file=None, reference_genome=None):; """"""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""; GTFS = {; 'GRCh37': 'gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz',; 'GRCh38': 'gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz',; }; if reference_genome is None:; reference_genome = hl.default_reference().name; else:; reference_genome = reference_genome.name; if gtf_file is None:; gtf_file = GTFS.get(reference_genome); if gtf_file is None:; raise ValueError(; 'get_gene_intervals requires a GTF file, or the reference genome be one of GRCh37 or GRCh38 (when on Google Cloud Platform)'; ); ht = hl.experimental.import_gtf(; gtf_file, reference_genome=reference_genome, skip_invalid_contigs=True, min_partitions=12; ); ht = ht.annotate(gene_id=ht.gene_id.split('\\.')[0], transcript_id=ht.transcript_id.split('\\.')[0]); return ht. © Copyright 2015-2024, Hail Team.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:9372,load,load,9372,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['load'],['load']
Performance," (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87117,perform,performance,87117,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101974,load,load,101974,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance," ),; _row_fields=sequenceof(enumeration('varid', 'rsid')),; ); def import_bgen(; path,; entry_fields,; sample_file=None,; n_partitions=None,; block_size=None,; index_file_map=None,; variants=None,; _row_fields=['varid', 'rsid'],; ) -> MatrixTable:; """"""Import BGEN file(s) as a :class:`.MatrixTable`. Examples; --------. Import a BGEN file as a matrix table with GT and GP entry fields:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38624,Load,Load,38624,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Load'],['Load']
Performance," + ''.join(f'\n ""{pre}"" => ""{post}""' for pre, post in mapping); ); else:; info('No duplicate sample IDs found.'); return dataset.annotate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:12495,load,loaded,12495,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,['load'],['loaded']
Performance," 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:. .. code-block:: text. (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:. .. code-block:: text. (AB > 0.2). HIGH-quality indel:. .. code-block:: text. (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:. .. code-block:: text. (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:. .. code-block:: text. (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the `min_gq` parameter, if the proband allele balance is; lower than the `min_child_ab` parameter, if the depth ratio between the; proband and parents is smaller than the `min_depth_ratio` parameter, if; the allele balance in a parent is above the `max_parent_ab` parameter, or; if the posterior probability `p` is smaller than the `min_p` parameter. Parameters; ----------; mt : :class:`.MatrixTable`; High-throughput sequencing dataset.; pedigree : :class:`.Pedigree`; Sample pedigree.; pop_frequency_prior : :class:`.Float64Expression`; Expression for population alternate allele frequency prior.; min_gq; Minimum proband GQ to be considered for *de novo* calling.; min_p; Minimum posterior probability to be considered for *de novo* calling.; max_parent_ab; Maximum parent allele balance.; min_child_ab; Minimum proband allele balance/; min_dp_ratio; Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency; Ignore in-sample allele frequency in computing site prior. Experimental.; Returns; -------; :class:`.Table`; """"""; DE_NOVO_PRIOR = 1 / 30000000; MIN_POP_PRIOR = 100 / 30000000. required_entry_fields = {'GT', 'AD', 'DP', 'GQ', 'PL'}; missing_fields = required_entry_fields - set(mt.entry); if missing_fields:; raise ValueError(; f""'de_novo': expected 'MatrixTable' to have at least {required_entry_fields}, "" f""missing {missing_fields}""; ). pop_frequency_prio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:26566,throughput,throughput,26566,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['throughput'],['throughput']
Performance," 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the functions we wrote above to create new jobs; on a Batch which can be executed with the ServiceBackend.; First, we import the Batch module as hb.; import hailtop.batch as hb. Next, we create a Batch specifying the backend is the ServiceBackend; and give it the name ‘clumping’.; backend = hb.ServiceBackend(); batch = hb.Batch(backend=backend, name='clumping'). We create InputResourceFile objects for the VCF file and; phenotypes file using the Batch.read_input() method. These; are the inputs to the entire Batch and are not outputs of a BashJob.; vcf = batch.read_input('gs://hail-tutorial/1kg.vcf.bgz'); phenotypes = batch.read_input('gs://hail-tutorial/1kg_annotations.txt'). We use the gwas function defined above to create a new job on the batch to; perform a GWAS that outputs a binary PLINK file and association results:; g = gwas(batch, vcf, phenotypes). We call the clump function once per chromosome and aggregate a list of the; clumping results files passing the outputs from the g job defined above; as inputs to the clump function:; results = []; for chr in range(1, 23):; c = clump(batch, g.ofile, g.ofile.assoc, chr); results.append(c.clumped). Finally, we use the merge function to concatenate the results into a single file; and then write this output to a permanent location using Batch.write_output().; The inputs to the merge function are the clumped output files from each of the clump; jobs.; m = merge(batch, results); batch.write_output(m.ofile, 'gs://<MY_BUCKET>/batch-clumping/1kg-caffeine-consumption.clumped'). The last thing we do is submit the Batch to the service and then close the Backend:; batch.run(open=True, wait=False) # doctest: +SKIP; backend.close(). Synopsis; We provide the code used above in one place for your reference:. ru",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:10811,perform,perform,10811,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['perform']
Performance," : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.bool`; If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns; -------; - u: :class:`.NDArrayNumericExpression`; The left singular vectors.; - s: :class:`.NDArrayNumericExpression`; The singular values.; - vt: :class:`.NDArrayNumericExpression`; The right singular vectors.; """"""; float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArraySVD(float_nd._ir, full_matrices, compute_uv). return_type = (; ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1), tndarray(tfloat64, 2)); if compute_uv; else tndarray(tfloat64, 1); ); return construct_expr(ir, return_type, nd._indices, nd._aggregations). @typecheck(nd=expr_ndarray(), eigvals_only=bool); def eigh(nd, eigvals_only=False):; """"""Performs an eigenvalue decomposition of a symmetric matrix. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(N, N).; eigvals_only: :class:`.bool`; If False (default), compute the eigenvectors and eigenvalues. Otherwise, only compute eigenvalues. Returns; -------; - w: :class:`.NDArrayNumericExpression`; The eigenvalues, shape(N).; - v: :class:`.NDArrayNumericExpression`; The eigenvectors, shape(N, N). Only returned if eigvals_only is false.; """"""; float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayEigh(float_nd._ir, eigvals_only). return_type = tndarray(tfloat64, 1) if eigvals_only else ttuple(tndarray(tfloat64, 1), tndarray(tfloat64, 2)); return construct_expr(ir, return_type, nd._indices, nd._aggregations). [docs]@typecheck(nd=expr_ndarray()); def inv(nd):; """"""Performs a matrix inversion. Parameters; ----------. nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray. Returns; -------; :class:`.NDArrayNumericE",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:12402,Perform,Performs,12402,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Perform'],['Performs']
Performance," = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters:; always_copy_output (bool) – If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Return type:; Self. Returns:; Same job object set to always copy output. always_run(always_run=True); Set the job to always run, even if dependencies fail. Warning; Jobs set to always run are not cancellable!. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters:; always_run (bool) – If True, set job to always run. Return type:; Self. Returns:; Same job object set to always run. cloudfuse(bucket, mount_point, *, read_only=True); Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. Warning; There are performance and cost implications of using gcsfuse; or blobfuse. Examples; Google Cloud Platform:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters:. bucket (str) – Name of the google storage bucket to mount or the path to an Azure container in the; format of <account>/<container>.; mount_point (str) – The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only (bool) – If True, mount the cloud blob storage in read-only mode. Return type:; Self. Returns:; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse. cpu(cores); Set the job’s CPU requirements.; Notes; The string expression must be of the form {number}{suffix}; where th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:2565,perform,performance,2565,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['perform'],['performance']
Performance," = drop(va, vep)'). # subset VEP annotations if needed; subset = ','.join([x.rsplit('.')[-1] for x in annotations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global conc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40712,cache,cache,40712,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['cache'],"['cache', 'cached']"
Performance," Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90205,perform,performance,90205,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89286,perform,performance,89286,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is current",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1259,Load,Load,1259,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['Load'],['Load']
Performance," MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79208,perform,performance,79208,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," NDArray of positional quotients.; """"""; return self._bin_op_numeric(""/"", other, self._div_ret_type_f). def __rtruediv__(self, other):; return self._bin_op_numeric_reverse(""/"", other, self._div_ret_type_f). [docs] def __floordiv__(self, other):; """"""Positionally divide by a ndarray or a scalar using floor division. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression`; """"""; return self._bin_op_numeric('//', other). def __rfloordiv__(self, other):; return self._bin_op_numeric_reverse('//', other). def __rmatmul__(self, other):; if not isinstance(other, NDArrayNumericExpression):; other = hl.nd.array(other); return other.__matmul__(self). [docs] def __matmul__(self, other):; """"""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression` or :class:`.NumericExpres",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:107657,perform,performs,107657,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['perform'],['performs']
Performance," Notes; -----; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns; -------; :obj:`bool`; """"""; return Env.backend()._to_java_blockmatrix_ir(self._bmir).typ().isSparse(). @property; def T(self):; """"""Matrix transpose. Returns; -------; :class:`.BlockMatrix`; """"""; if self.n_rows == 1 and self.n_cols == 1:; return self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42022,Cache,Cached,42022,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,1,['Cache'],['Cached']
Performance," Notes; The key type of the table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the table to keys not present in another table.; To restrict to keys present in other, use semi_join().; Examples; >>> table_result = table1.anti_join(table2). It may be expensive to key the left-side table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> table_result = table1.filter(hl.is_missing(table2.index(table1['ID']))). See also; semi_join(), filter(). any(expr)[source]; Evaluate whether a Boolean expression is true for at least one row.; Examples; Test whether C1 is equal to 5 any row in any row of the table:; >>> if table1.any(table1.C1 == 5):; ... print(""At least one row has C1 equal 5.""). Parameters:; expr (BooleanExpression) – Boolean expression. Returns:; bool – True if the predicate evaluated for True for any row, otherwise False. cache()[source]; Persist this table in memory.; Examples; Persist the table in memory:; >>> table = table.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; Table – Cached table. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False)[source]; Checkpoint the table to disk by writing and reading. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; Table. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_table(). It is; possible to read the file at this path later with read_table().; Examples; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). collect(_locali",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:15064,cache,cache,15064,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['cache'],['cache']
Performance," Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83805,perform,performance,83805,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Policy; Change Log. Batch. Python API; BatchPoolFuture. View page source. BatchPoolFuture. class hailtop.batch.batch_pool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This fu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1360,concurren,concurrent,1360,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,2,['concurren'],['concurrent']
Performance," Random; number generation has been updated, but shouldn’t affect most users.; If you need to manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details.; (#11884) Added; Job.always_copy_output when using the ServiceBackend. The; default behavior is False, which is a breaking change from the; previous behavior to always copy output files regardless of the job’s; completion state.; (#12139) Brand new; random number generation, shouldn’t affect most users. If you need to; manually set seeds, see; https://hail.is/docs/0.2/functions/random.html for details. Bug Fixes. (#12487) Fixed a bug; causing rare but deterministic job failures deserializing data in; Query-on-Batch.; (#12535) QoB will; now error if the user reads from and writes to the same path. QoB; also now respects the user’s configuration of; disable_progress_bar. When disable_progress_bar is; unspecified, QoB only disables the progress bar for non-interactive; sessions.; (#12517) Fix a; performance regression that appears when using hl.split_multi_hts; among other methods. Version 0.2.105; Released 2022-10-31 🎃. New Features. (#12293) Added; support for hail.MatrixTables to hail.ggplot. Bug Fixes. (#12384) Fixed a; critical bug that disabled tree aggregation and scan executions in; 0.2.104, leading to out-of-memory errors.; (#12265) Fix; long-standing bug wherein hl.agg.collect_as_set and; hl.agg.counter error when applied to types which, in Python, are; unhashable. For example, hl.agg.counter(t.list_of_genes) will not; error when t.list_of_genes is a list. Instead, the counter; dictionary will use FrozenList keys from the frozenlist; package. Version 0.2.104; Release 2022-10-19. New Features. (#12346): Introduced; new progress bars which include total time elapsed and look cool. Version 0.2.103; Release 2022-10-18. Bug Fixes. (#12305): Fixed a; rare crash reading tables/matrixtables with _intervals. Version 0.2.102; Released 2022-10-06. New Features. (#12218) Missing; value",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:43220,perform,performance,43220,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Table(TableRead(reader)). [docs] @typecheck_method(n_partitions=nullable(int), maximum_cache_memory_in_bytes=nullable(int)); def to_matrix_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""; t = self.to_table_row_major(n_partitions, maximum_cache_memory_in_bytes); t = t.transmute(entries=t.entries.map(lambda i: hl.struct(element=i))); t = t.annotate_globals(cols=hl.range(self.n_cols).map(lambda i: hl.struct(col_idx=hl.int64(i)))); return t._unlocalize_entries('entries', 'cols', ['col_idx']). [docs] @staticmethod; @typecheck(; path_in=str,; path_out=str,; de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:56960,cache,cache,56960,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['cache'],['cache']
Performance," Returns; -------; :obj:`list` of :class:`.Interval`; """""". return self._par. [docs] @typecheck_method(contig=str); def contig_length(self, contig):; """"""Contig length. Parameters; ----------; contig : :class:`str`; Contig name. Returns; -------; :obj:`int`; Length of contig.; """"""; if contig in self.lengths:; return self.lengths[contig]; else:; raise KeyError(""Contig `{}' is not in reference genome."".format(contig)). @property; def global_positions_dict(self):; """"""Get a dictionary mapping contig names to their global genomic positions. Returns; -------; :class:`dict`; A dictionary of contig names to global genomic positions.; """"""; if self._global_positions is None:; gp = {}; lengths = self._lengths; x = 0; for c in self.contigs:; gp[c] = x; x += lengths[c]; self._global_positions = gp; return self._global_positions. @typecheck_method(contig=str); def _contig_global_position(self, contig):; return self.global_positions_dict[contig]. [docs] @classmethod; @typecheck_method(path=str); def read(cls, path):; """"""Load reference genome from a JSON file. Notes; -----. The JSON file must have the following format:. .. code-block:: text. {""name"": ""my_reference_genome"",; ""contigs"": [{""name"": ""1"", ""length"": 10000000},; {""name"": ""2"", ""length"": 20000000},; {""name"": ""X"", ""length"": 19856300},; {""name"": ""Y"", ""length"": 78140000},; {""name"": ""MT"", ""length"": 532}],; ""xContigs"": [""X""],; ""yContigs"": [""Y""],; ""mtContigs"": [""MT""],; ""par"": [{""start"": {""contig"": ""X"",""position"": 60001},""end"": {""contig"": ""X"",""position"": 2699521}},; {""start"": {""contig"": ""Y"",""position"": 10001},""end"": {""contig"": ""Y"",""position"": 2649521}}]; }. `name` must be unique and not overlap with Hail's pre-instantiated; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``.; The contig names in `xContigs`, `yContigs`, and `mtContigs` must be; present in `contigs`. The intervals listed in `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:7347,Load,Load,7347,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['Load'],['Load']
Performance," VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45462,load,load,45462,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance," Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the information content; does not change. To avoid this superlinear scaling, the SVCR renames these; fields to their “local” versions: LGT, LAD, LPL, etc, and adds a new field,; LA (local alleles). The information in the local fields refers to the alleles; defined per row of the matrix indirectly through the LA list.; For instance, if a sample has the following information in its GVCF:; Ref=G Alt=T GT=0/1 AD=5,6 PL=102,0,150. If the alternate alleles A,C,T are discovered in the cohort, this sample’s; entry would look like:; LA=0,2 LGT=0/1 LAD=5,6 LPL=102,0,150. The “1” allele referred to in LGT, and the allele to which the reads in the; second position of LAD belong to, is not the allele with absolute index 1; (C), but rather the allele whose i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:4803,scalab,scalable,4803,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance," [*self._parent.globals, *self._parent.col]; else:; assert indices == self._parent._col_indices; fixed_fields = [*self._parent.globals, *self._parent.row]. bound_fields = set(; itertools.chain(; iter_option(self._row_keys),; iter_option(self._col_keys),; iter_option(self._col_fields),; iter_option(self._row_fields),; iter_option(self._entry_fields),; fixed_fields,; ); ). for k in new_bindings:; if k in bound_fields:; raise ExpressionException(f""{caller!r} cannot assign duplicate field {k!r}""). [docs] def partition_hint(self, n: int) -> 'GroupedMatrixTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """""". self._partitions = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate_cols(self, **named_exprs) -> 'GroupedMatrixTable':; """"""Aggregate cols by group. Examples; --------; Aggregate to a matrix with cohort as column keys, computing the mean height; per cohort as a new column field:. >>> dataset_result = (dataset.group_cols_by(dataset.cohort); ... .aggregate_cols(mean_height = hl.agg.mean(dataset.pheno.height)); ... .result()). Notes; -----;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:8072,optimiz,optimizer,8072,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['optimiz'],['optimizer']
Performance," [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1719,load,loadings,1719,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance," a FASTA file. Parameters:. name (str) – Name for new reference genome.; fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (str) – Path to FASTA index file. Must be uncompressed.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Returns:; ReferenceGenome. property global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. Returns:; dict – A dictionary of contig names to global genomic positions. has_liftover(dest_reference_genome)[source]; True if a liftover chain file is available from this reference; genome to the destination reference. Parameters:; dest_reference_genome (str or ReferenceGenome). Returns:; bool. has_sequence()[source]; True if the reference sequence has been loaded. Returns:; bool. property lengths; Dict of contig name to contig length. Returns:; dict of str to int. locus_from_global_position(global_pos)[source]; ”; Constructs a locus from a global position in reference genome.; The inverse of Locus.position().; Examples; >>> rg = hl.get_reference('GRCh37'); >>> rg.locus_from_global_position(0); Locus(contig=1, position=1, reference_genome=GRCh37). >>> rg.locus_from_global_position(2824183054); Locus(contig=21, position=42584230, reference_genome=GRCh37). >>> rg = hl.get_reference('GRCh38'); >>> rg.locus_from_global_position(2824183054); Locus(contig=chr22, position=1, reference_genome=GRCh38). Parameters:; global_pos (int) – Zero-based global base position along the reference genome. Returns:; Locus. property mt_contigs; Mitochondrial contigs. Returns:; list of str. property name; Name of reference genome. Returns:; str. property par; Pseudoautosomal regions. Returns:; list of Interval. classmethod read(p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:7450,load,loaded,7450,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance," a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered entry); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset and the second index is; the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:16804,perform,performs,16804,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['performs']
Performance," annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:87177,perform,performance,87177,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," bool}'; ). return _agg_func('ImputeType', [expr], ret_type, []). class ScanFunctions(object):; def __init__(self, scope):; self._functions = {name: self._scan_decorator(f) for name, f in scope.items()}. def _scan_decorator(self, f):; @wraps(f); def wrapper(*args, **kwargs):; func = getattr(f, '__wrapped__'); af = func.__globals__['_agg_func']; as_scan = getattr(af, '_as_scan'); setattr(af, '_as_scan', True); try:; res = f(*args, **kwargs); except Exception as e:; setattr(af, '_as_scan', as_scan); raise e; setattr(af, '_as_scan', as_scan); return res. update_wrapper(wrapper, f); return wrapper. def __getattr__(self, field):; if field in self._functions:; return self._functions[field]; else:; field_matches = difflib.get_close_matches(field, self._functions.keys(), n=5); raise AttributeError(; ""hl.scan.{} does not exist. Did you mean:\n {}"".format(field, ""\n "".join(field_matches)); ). @typecheck(initial_value=expr_any, seq_op=func_spec(1, expr_any), comb_op=func_spec(2, expr_any)); def fold(initial_value, seq_op, comb_op):; """"""; Perform an arbitrary aggregation in terms of python functions. Examples; --------. Start with a range table with its default `idx` field:. >>> ht = hl.utils.range_table(100). Now, using fold, can reimplement `hl.agg.sum` (for non-missing values) as:. >>> ht.aggregate(hl.agg.fold(0, lambda accum: accum + ht.idx, lambda comb_left, comb_right: comb_left + comb_right)); 4950. Parameters; ----------; initial_value : :class:`.Expression`; The initial value to start the aggregator with. This is a value of type `A`.; seq_op : function ( (:class:`.Expression`) -> :class:`.Expression`); The function used to combine the current aggregator state with the next element you're aggregating over. Type is; `A => A`; comb_op : function ( (:class:`.Expression`, :class:`.Expression`) -> :class:`.Expression`); The function used to combine two aggregator states together and produce final result. Type is `(A, A) => A`.; """""". return _agg_func._fold(initial_value, seq_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:61012,Perform,Perform,61012,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['Perform'],['Perform']
Performance," columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns:; MatrixTable – Dataset with columns from both datasets. union_rows(*, _check_cols=True)[source]; Take the union of dataset rows.; Examples; Union the rows of two datasets:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:; >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:; >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; In order to combine two datasets, three requirements must be met:. The column keys must be ide",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:66735,perform,perform,66735,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,['perform'],['perform']
Performance," compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; identical types, and zips them together into one table. File Format. The native file format version is now 1.1.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.16; Released 2019-06-19. hailctl. (#6357) Accommodated; Google Dataproc bug causing cluster creation failures. Bug fixes. (#637",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90875,optimiz,optimization,90875,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance," disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86062,perform,performance,86062,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," familiar if you’ve used R or pandas, but Table differs in 3 important ways:. It is distributed. Hail tables can store far more data than can fit on a single computer.; It carries global fields.; It is keyed. A Table has two different kinds of fields:. global fields; row fields. Importing and Reading; Hail can import data from many sources: TSV and CSV files, JSON files, FAM files, databases, Spark, etc. It can also read (and write) a native Hail format.; You can read a dataset with hl.read_table. It take a path and returns a Table. ht stands for Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:1806,load,load,1806,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['load'],['load']
Performance," genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; acce",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:10158,perform,performance,10158,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['performance']
Performance," hl.eval(locus.in_autosome_or_par()); True. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isAutosomalOrPseudoAutosomal"", tbool). [docs] def in_mito(self):; """"""Returns ``True`` if the locus is on mitochondrial DNA. Examples; --------. >>> hl.eval(locus.in_mito()); False. Returns; -------; :class:`.BooleanExpression`; """"""; return self._method(""isMitochondrial"", tbool). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def sequence_context(self, before=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89118,load,load,89118,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['load'],['load']
Performance," hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:12103,perform,performs,12103,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performs']
Performance," in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized linear algebra libraries. These should load automatically on recent versions of Mac OS X and Google Dataproc. On Linux, these must be explicitly installed; on Ubuntu 14.04, run; $ apt-get install libatlas-base-dev. If natives are not found, hail.log will contain the warnings; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS. See netlib-java for more information. Running the tests¶; Several Hail tests have additional dependencies:. PLINK 1.9; QCTOOL 1.4; R 3.3.1 with packages jsonlite and logistf, which depends on mice and Rcpp. Other recent versions of QCTOOL and R should suffice, but PLINK 1.7 will not.; To execute all Hail tests, run; $ ./gradlew -Dspark.home=$SPARK_HOME test. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:7709,optimiz,optimized,7709,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,4,"['load', 'optimiz']","['load', 'optimized']"
Performance," in the dataset at the site.; p refers to \(\mathrm{P_{\text{de novo}}}\).; min_p refers to the min_p function parameter. HIGH-quality SNV:; (p > 0.99) AND (AB > 0.3) AND (AC == 1); OR; (p > 0.99) AND (AB > 0.3) AND (DR > 0.2); OR; (p > 0.5) AND (AB > 0.3) AND (AC < 10) AND (DP > 10). MEDIUM-quality SNV:; (p > 0.5) AND (AB > 0.3); OR; (AC == 1). LOW-quality SNV:; (AB > 0.2). HIGH-quality indel:; (p > 0.99) AND (AB > 0.3) AND (AC == 1). MEDIUM-quality indel:; (p > 0.5) AND (AB > 0.3) AND (AC < 10). LOW-quality indel:; (AB > 0.2). Additionally, de novo candidates are not considered if the proband GQ is; smaller than the min_gq parameter, if the proband allele balance is; lower than the min_child_ab parameter, if the depth ratio between the; proband and parents is smaller than the min_depth_ratio parameter, if; the allele balance in a parent is above the max_parent_ab parameter, or; if the posterior probability p is smaller than the min_p parameter. Parameters:. mt (MatrixTable) – High-throughput sequencing dataset.; pedigree (Pedigree) – Sample pedigree.; pop_frequency_prior (Float64Expression) – Expression for population alternate allele frequency prior.; min_gq – Minimum proband GQ to be considered for de novo calling.; min_p – Minimum posterior probability to be considered for de novo calling.; max_parent_ab – Maximum parent allele balance.; min_child_ab – Minimum proband allele balance/; min_dp_ratio – Minimum ratio between proband read depth and parental read depth.; ignore_in_sample_allele_frequency – Ignore in-sample allele frequency in computing site prior. Experimental. Returns:; Table. hail.methods.nirvana(dataset, config, block_size=500000, name='nirvana')[source]; Annotate variants using Nirvana. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:58312,throughput,throughput,58312,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance," interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:21314,perform,perform,21314,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['perform'],['perform']
Performance," key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fixed crash when using repeated calls to hl.filter_intervals. New features. (#9101) Add; hl.nd.{concat, hstack, vstack} to concatenate ndarrays.; (#9105) Add; hl.nd.{eye, identity} to create identity matrix ndarrays.; (#9093) Add; hl.nd.inv to invert ndarrays.; (#9063) Add; BlockMatrix.tree_matmul to improve matrix multiply performance; with a large inner dimension. Version 0.2.49; Released 2020-07-08. Bug fixes. (#9058) Fixed memory; leak affecting Table.aggregate, MatrixTable.annotate_cols; aggregations, and hl.sample_qc. Version 0.2.48; Released 2020-07-07. Bug fixes. (#9029) Fix crash; when using hl.agg.linreg with no aggregated data records.; (#9028) Fixed memory; leak affecting Table.annotate with scans,; hl.experimental.densify, and Table.group_by / aggregate.; (#8978) Fixed; aggregation behavior of; MatrixTable.{group_rows_by, group_cols_by} to skip filtered; entries. Version 0.2.47; Released 2020-06-23. Bug fixes. (#9009) Fix memory; leak when counting per-partition. This caused excessive memory use in; BlockMatrix.write_from_entry_expr, and likely in many other; places.; (#9006) Fix memory; leak in hl.export_bgen.; (#9001) Fix double; close error that showed up on Azure Cloud. Version 0.2.46; Released 2020-06-17. Site. (#8955) Natural; language documentation search. Bug fixes. (#8981) Fix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:67976,perform,performance,67976,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," less than \(2^{31}\). Parameters:. ndarray (numpy.ndarray) – ndarray with two dimensions, each of non-zero size.; block_size (int, optional) – Block size. Default given by default_block_size(). Returns:; BlockMatrix. classmethod fromfile(uri, n_rows, n_cols, block_size=None, *, _assert_type=None)[source]; Creates a block matrix from a binary file.; Examples; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') . To create a block matrix of the same dimensions:; >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) . Notes; This method, analogous to numpy.fromfile,; reads a binary file of float64 values in row-major order, such as that; produced by numpy.tofile; or BlockMatrix.tofile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; A NumPy ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:24370,load,load,24370,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['load']
Performance," missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; paths,; key=key,; min_partitions=min_partitions,; impute=impute,; no_header=no_header,; comment=comment,; missing=missing,; types=types,; skip_blank_lines=skip_blank_lines,; force_bgz=force_bgz,; filter=filter,; find_replace=find_replace,; force=force,; source_file_field=source_file_field,; delimiter="","",; quote=quote,; ); return ht. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:118718,load,load,118718,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance," mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86149,perform,performance,86149,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," ndarray must have type float64 for the output of; func:numpy.tofile to be a valid binary input to fromfile().; This is not checked.; The number of entries must be less than \(2^{31}\). Parameters:. uri (str, optional) – URI of binary input file.; n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – Block size. Default given by default_block_size(). See also; from_numpy(). property is_sparse; Returns True if block-sparse.; Notes; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns:; bool. log()[source]; Element-wise natural logarithm. Returns:; BlockMatrix. property n_cols; Number of columns. Returns:; int. property n_rows; Number of rows. Returns:; int. persist(storage_level='MEMORY_AND_DISK')[source]; Persists this block matrix in memory or on disk.; Notes; The BlockMatrix.persist() and BlockMatrix.cache(); methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; BlockMatrix.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; BlockMatrix – Persisted block matrix. classmethod random(n_rows, n_cols, block_size=None, seed=None, gaussian=True)[source]; Creates a block matrix with standard normal or uniform random entries.; Examples; Create a block matrix with 10 rows, 20 columns, and standard normal entries:; >>> bm = BlockMatrix.random(10, 20). Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; block_size (int, optional) – ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:25405,cache,cache,25405,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance," ndarrays of; booleans. Version 0.2.71; Released 2021-07-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:59754,perform,performance,59754,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18240,load,loadings,18240,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance," of a new; n_rows parameter. Version 0.2.120; Released 2023-07-27. New Features. (#13206) The VDS; Combiner now works in Query-on-Batch. Bug Fixes. (#13313) Fix bug; introduced in 0.2.119 which causes a serialization error when using; Query-on-Spark to read a VCF which is sorted by locus, with split; multi-allelics, in which the records sharing a single locus do not; appear in the dictionary ordering of their alternate alleles.; (#13264) Fix bug; which ignored the partition_hint of a Table; group-by-and-aggregate.; (#13239) Fix bug; which ignored the HAIL_BATCH_REGIONS argument when determining in; which regions to schedule jobs when using Query-on-Batch.; (#13253) Improve; hadoop_ls and hfs.ls to quickly list globbed files in a; directory. The speed improvement is proportional to the number of; files in the directory.; (#13226) Fix the; comparison of an hl.Struct to an hl.struct or field of type; tstruct. Resolves; (#13045) and; (Hail#13046).; (#12995) Fixed bug; causing poor performance and memory leaks for; MatrixTable.annotate_rows aggregations. Version 0.2.119; Released 2023-06-28. New Features. (#12081) Hail now; uses Zstandard as the default; compression algorithm for table and matrix table storage. Reducing; file size around 20% in most cases.; (#12988) Arbitrary; aggregations can now be used on arrays via; ArrayExpression.aggregate. This method is useful for accessing; functionality that exists in the aggregator library but not the basic; expression library, for instance, call_stats.; (#13166) Add an; eigh ndarray method, for finding eigenvalues of symmetric; matrices (“h” is for Hermitian, the complex analogue of symmetric). Bug Fixes. (#13184) The; vds.to_dense_mt no longer densifies past the end of contig; boundaries. A logic bug in to_dense_mt could lead to reference; data toward’s the end of one contig being applied to the following; contig up until the first reference block of the contig.; (#13173) Fix; globbing in scala blob storage filesystem i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:30115,perform,performance,30115,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance," one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},\]; with \(M_{ij} = 0\) for \(C_{ij}\) missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value \(1/m\) for variants in Hardy-Weinberg equilibrium and is further motivated in the paper cited above. (The r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138885,load,loadings,138885,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance," partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110418,cache,cache,110418,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['cache'],['cache']
Performance," redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it.; """"""; self._jkt.unpersist(). [docs] @handle_py4j; @typecheck_method(cols=tupleof(oneof(strlike, Ascending, Descending))); def order_by(self, *cols):; """"""Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. :param cols: Columns to sort by.; :type: str or asc(str) or desc(str). :return: Key table sorted by ``cols``.; :rtype: :class:`.KeyTable`; """""". jsort_columns = [asc(col)._jrep if isinstance(col, str) else col._jrep for col in cols]; return KeyTable(self.hc,; self._jkt.orderBy(jarray(Env.hail().keytable.SortColumn, jsort_columns))). [docs] @handle_py4j; def num_partitions(self):; """"""Returns the number of partitions in the key table.; ; :rtype: int; """"""; return self._jkt.nPartitions(). [docs] @staticmethod; @handle_py4j; @typecheck(path=strlike); de",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:24890,perform,performed,24890,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['perform'],['performed']
Performance," reference genome (``'GRCh37'`` by default).; With an argument, sets the default reference genome to the argument. Returns; -------; :class:`.ReferenceGenome`; """"""; if new_default_reference is not None:; Env.hc().default_reference = new_default_reference; return None; return Env.hc().default_reference. [docs]def get_reference(name) -> ReferenceGenome:; """"""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""; Env.hc(); if name == 'default':; return default_reference(); else:; return Env.backend().get_reference(name). [docs]@typecheck(seed=int); def set_global_seed(seed):; """"""Deprecated. Has no effect. To ensure reproducible randomness, use the `global_seed`; argument to :func:`.init` and :func:`.reset_global_randomness`. See the :ref:`random functions <sec-random-functions>` reference docs for more. Parameters; ----------; seed : :obj:`int`; Integer used to seed Hail's random number generator; """""". warning(; 'hl.set_global_seed has no effect. See '; 'https://hail.is/docs/0.2/functions/random.html for details on '; 'ensuring reproducibility of randomness.'; ); pass. [docs]@typecheck(); def reset_global_randomness():; """"""Restore global randomness to initial state for test reproducibility."""""". Env.reset_global_randomness(). def _set_flags(**f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/context.html:26173,load,loaded,26173,docs/0.2/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/context.html,2,['load'],['loaded']
Performance," require us to choose only one region per; continent and we have chosen US-CENTRAL1 and EUROPE-WEST1. Documentation. (#14113) Add; examples to Table.parallelize, Table.key_by,; Table.annotate_globals, Table.select_globals,; Table.transmute_globals, Table.transmute, Table.annotate,; and Table.filter.; (#14242) Add; examples to Table.sample, Table.head, and; Table.semi_join. New Features. (#14206) Introduce; hailctl config set http/timeout_in_seconds which Batch and QoB; users can use to increase the timeout on their laptops. Laptops tend; to have flaky internet connections and a timeout of 300 seconds; produces a more robust experience.; (#14178) Reduce VDS; Combiner runtime slightly by computing the maximum ref block length; without executing the combination pipeline twice.; (#14207) VDS; Combiner now verifies that every GVCF path and sample name is unique. Bug Fixes. (#14300) Require; orjson<3.9.12 to avoid a segfault introduced in orjson 3.9.12; (#14071) Use indexed; VEP cache files for GRCh38 on both dataproc and QoB.; (#14232) Allow use; of large numbers of fields on a table without triggering; ClassTooLargeException: Class too large:.; (#14246)(#14245); Fix a bug, introduced in 0.2.114, in which; Table.multi_way_zip_join and Table.aggregate_by_key could; throw “NoSuchElementException: Ref with name __iruid_...” when; one or more of the tables had a number of partitions substantially; different from the desired number of output partitions.; (#14202) Support; coercing {} (the empty dictionary) into any Struct type (with all; missing fields).; (#14239) Remove an; erroneous statement from the MatrixTable tutorial.; (#14176); hailtop.fs.ls can now list a bucket,; e.g. hailtop.fs.ls(""gs://my-bucket"").; (#14258) Fix; import_avro to not raise NullPointerException in certain rare; cases (e.g. when using _key_by_assert_sorted).; (#14285) Fix a; broken link in the MatrixTable tutorial. Deprecations. (#14293) Support for; the hail-az:// scheme, deprecated in 0.2.116, i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:16497,cache,cache,16497,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['cache'],['cache']
Performance," row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31997,perform,perform,31997,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['perform'],['perform']
Performance," samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail now uses; the correct unicode string encoding which resolves a number of issues; when a Table or MatrixTable has a key field containing unicode; characters.; (#5692) When; keyed is True, hl.maximal_independent_set now does not; produce duplicates.; (#5725) Docs now; consistently refer to hl.agg not agg.; (#5730)(#5782); Taught import_bgen to optimize its variants argument. Experimental. (#5732) The; hl.agg.approx_quantiles aggregate computes an approximation of; the quantiles of an expression.; (#5693)(#5396); Table._multi_way_zip_join now correctly handles keys that have; been truncated. Version 0.2.12; Released 2019-03-28. New features. (#5614) Add support; for multiple missing values in hl.import_table.; (#5666) Produce HTML; table output for Table.show() when running in Jupyter notebook. Bug fixes. (#5603)(#5697); Fixed issue where min_partitions on hl.import_table was; non-functional.; (#5611) Fix; hl.nirvana crash. Experimental. (#5524) Add; summarize functions to Table, MatrixTable, and Expression.; (#5570) Add; hl.agg.approx_cdf aggregator for approximate density calculation.; (#5571) Add log; parameter to hl.plot.histogram.; (#5601) Add; hl.plot.joint_plot, extend functionality of hl.plot.scatter.; (#5608) Add LD score; simulation framework.; (#5628) Add; hl.experimental.full_outer_join_mt for full outer ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:97100,optimiz,optimize,97100,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimize']
Performance," specified by the left-hand side (must; begin with ``g``). This is analogous to :py:meth:`~hail.VariantDataset.annotate_variants_expr` and; :py:meth:`~hail.VariantDataset.annotate_samples_expr` where the annotation paths are ``va`` and ``sa`` respectively. ``expr`` is in genotype context so the following symbols are in scope:. - ``g``: genotype annotation; - ``v`` (*Variant*): :ref:`variant`; - ``va``: variant annotations; - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations. For more information, see the documentation on writing `expressions <overview.html#expressions>`__; and using the `Hail Expression Language <exprlang.html>`__. .. warning::. - If the resulting genotype schema is not :py:class:`~hail.expr.TGenotype`,; subsequent function calls on the annotated variant dataset may not work such as; :py:meth:`~hail.VariantDataset.pca` and :py:meth:`~hail.VariantDataset.linreg`. - Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to :py:class:`~hail.expr.TGenotype`. - Genotypes are immutable. For example, if ``g`` is initially of type ``Genotype``, the expression; ``g.gt = g.gt + 1`` will return a ``Struct`` with one field ``gt`` of type ``Int`` and **NOT** a ``Genotype``; with the ``gt`` incremented by 1. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = "","".join(expr). jvds = self._jvdf.annotateGenotypesExpr(expr); vds = VariantDataset(self.hc, jvds); if isinstance(vds.genotype_schema, TGenotype):; return VariantDataset(self.hc, vds._jvdf.toVDS()); else:; return vds. [docs] @handle_py4j; @typecheck_method(expr=oneof(strlike, listof(strlike))); def annotate_global_expr(self, expr):; """"""Annotate global with expression. **Example**. Annotate global with an array of populations:. >>> vds = vds.annotate_global_expr('global.pops = [""FI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:10603,perform,performance,10603,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance," tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; D",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23245,cache,cache,23245,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['cache'],['cache']
Performance," transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23439,concurren,concurrently,23439,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['concurren'],['concurrently']
Performance," type array<tfloat64>). Returns:; Float64Expression. hail.expr.functions.uniroot(f, min, max, *, max_iter=1000, epsilon=2.220446049250313e-16, tolerance=0.0001220703)[source]; Finds a root of the function f within the interval [min, max].; Examples; >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; f(min) and f(max) must not have the same sign.; If no root can be found, the result of this call will be NA (missing).; uniroot() returns an estimate for a root with accuracy; 4 * epsilon * abs(x) + tolerance.; 4*EPSILON*abs(x) + tol. Parameters:. f (function ( (arg) -> Float64Expression)) – Must return a Float64Expression.; min (Float64Expression); max (Float64Expression); max_iter (int) – The maximum number of iterations before giving up.; epsilon (float) – The scaling factor in the accuracy of the root found.; tolerance (float) – The constant factor in approximate accuracy of the root found. Returns:; Float64Expression – The root of the function f. hail.expr.functions.binary_search(array, elem)[source]; Binary search array for the insertion point of elem. Parameters:. array (Expression of type tarray); elem (Expression). Returns:; Int32Expression. Notes; This function assumes that array is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last.; The returned index is the lower bound on the insertion point of elem into; the ordered array, or the index of the first element in array not smaller; than elem. This is a value between 0 and the length of array, inclusive; (if all elements in array are smaller than elem, the returned value is; the length of array or the index of the first missing value, if one; exists).; If either elem or array is missing, the result is missing.; Examples; >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/numeric.html:18601,perform,perform,18601,docs/0.2/functions/numeric.html,https://hail.is,https://hail.is/docs/0.2/functions/numeric.html,1,['perform'],['perform']
Performance," use sparse genotype vector in rotation (advanced).; use_dosages (bool) – If true, use dosages rather than hard call genotypes.; n_eigs (int) – Number of eigenvectors of the kinship matrix used to fit the model.; dropped_variance_fraction (float) – Upper bound on fraction of sample variance lost by dropping eigenvectors with small eigenvalues. Returns:Variant dataset with linear mixed regression annotations. Return type:VariantDataset. logreg(test, y, covariates=[], root='va.logreg', use_dosages=False)[source]¶; Test each variant for association using logistic regression. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run the logistic regression Wald test per variant using a Boolean phenotype and two covariates stored; in sample annotations:; >>> vds_result = vds.logreg('wald', 'sa.pheno.isCase', covariates=['sa.pheno.age', 'sa.pheno.isFemale']). Notes; The logreg() method performs,; for each variant, a significance test of the genotype in; predicting a binary (case-control) phenotype based on the; logistic regression model. The phenotype type must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’), Rao score test (‘score’),; and Firth test (‘firth’). Hail only includes samples for which the phenotype and all covariates are; defined. For each variant, Hail imputes missing genotypes as the mean of called genotypes.; By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; The example above considers a model of the form. \[\mathrm{Prob}(\mat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:109365,perform,performs,109365,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance," value of 50. [4]:. p = hl.plot.histogram(mt.DP, range=(0, 30), bins=30); show(p). [Stage 4:> (0 + 1) / 1]. Cumulative Histogram; The cumulative_histogram() method works in a similar way to histogram(). [5]:. p = hl.plot.cumulative_histogram(mt.DP, range=(0,30), bins=30); show(p). Scatter; The scatter() method can also take in either Python types or Hail fields as arguments for x and y. [6]:. p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate'); show(p). [Stage 7:> (0 + 1) / 1]. We can also pass in a Hail field as a label argument, which determines how to color the data points. [7]:. mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD); filter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |; (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |; (mt.GT.is_hom_var() & (ab >= 0.9))); mt = mt.filter_entries(filter_condition_ab); mt = hl.variant_qc(mt).cache(); common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01); gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0]); pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT). [Stage 16:> (0 + 1) / 1]. [8]:. p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA', xlabel='PC1', ylabel='PC2',; n_divisions=None); show(p). [Stage 121:===> (1 + 15) / 16]. Hail’s downsample aggregator is incorporated into the scatter(), qq(), join_plot and manhattan() functions. The n_divisions parameter controls the factor by which values are downsampled. Using n_divisions=None tells the plot function to collect all values. [9]:. p2 = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],; label=common_mt.cols()[pca_scores.s].SuperPopulation,; title='PCA (downsampled)', xlabel='PC1', ylabel='PC2',; n_divisions=50); show(gridplot([p, p2], ncols=2, width=400, height",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:5237,cache,cache,5237,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['cache'],['cache']
Performance," vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove variants overlapping; an interval if False. Returns:Filtered variant dataset. Return type:VariantDataset. filter_multi()[source]¶; Filter out multi-alle",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54414,latency,latency,54414,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['latency'],['latency']
Performance," want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1623,scalab,scalable,1623,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['scalab'],['scalable']
Performance," while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; Notice the lines highlighted below. Hail will attempt to use all cores on the computer if no; defaults are given. However, with Batch, we only get a subset of the computer, so we must; explicitly specify how much resources Hail can use based on the input argument --cores. run_gwas.py; import argparse. import hail as hl. def run_gwas(vcf_file, phenotypes_file, output_file):; table = hl.import_table(phenotypes_file, impute=True).key_by('Sample'). hl.import_vcf(vcf_file).write('tmp.mt'); mt = hl.read_matrix_table('tmp.mt'). mt = mt.annotate_cols(pheno=table[mt.s]); mt = hl.sample_qc(mt); mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97)); ab = mt.AD[1] / hl.sum(mt.AD);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:2566,perform,performing,2566,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['performing']
Performance," will also introduce noise for rare variants; common practice is to filter out variants with minor allele frequency below some cutoff.) The factor \(1/m\) gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply \(MM^T\).; PCA then computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2, \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\). Typically one computes only the first \(k\) singular vectors and values, yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are \(n \times k\), \(k \times k\) and \(m \times k\) respectively.; From the perspective of the samples or rows of \(M\) as data, \(V_k\) contains the variant loadings for the first \(k\) PCs while \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:140971,load,loadings,140971,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance," will never be missing, even if the `missing` string appears; in the column IDs. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; row_fields: :obj:`dict` of :class:`str` to :class:`.HailType`; Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key: :class:`str` or :obj:`list` of :obj:`str`; Key fields(s). If empty, creates an index `row_id` to use as key.; entry_type: :class:`.HailType`; Type of entries in matrix table. Must be one of: :py:data:`.tint32`,; :py:data:`.tint64`, :py:data:`.tfloat32`, :py:data:`.tfloat64`, or; :py:data:`.tstr`. Default: :py:data:`.tint32`.; missing: :class:`str`; Identifier to be treated as missing. Default: NA; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header: :obj:`bool`; If ``True``, assume the file has no header and name the row fields `f0`,; `f1`, ... `fK` (0-indexed) and the column keys 0, 1, ... N.; force_bgz : :obj:`bool`; If ``True``, load **.gz** files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep : :class:`str`; This parameter is a deprecated name for `delimiter`, please use that; instead.; delimiter : :class:`str`; A single character string which separates values in the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns; -------; :class:`.MatrixTable`; MatrixTable constructed from imported data.; """"""; row_key = wrap_to_list(row_key); comment = wrap_to_list(comment); paths = [hl.current_backend().fs.canonicalize_path(p) for p in wrap_to_list(paths)]; missing_list = wrap_to_list(missing). def comment_filter(table):; return (; hl.rbind(; hl.array(comment),; lambda hl_comment: hl_comment.any(; lambda com: hl.if_else(hl.len(com) == 1, tab",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:72287,load,load,72287,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance," will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. Ne",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88869,perform,performance,88869,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"', 'GP', 'dosage')),; n_partitions=nullable(int),; block_size=nullable(int),; index_file_map=nullable(dictof(str, str)),; variants=nullable(; oneof(sequenceof(hl.utils.Struct), sequenceof(hl.genetics.Locus), StructExpression, LocusExpression, Table); ),; _row_fields=sequenceof(enumeration('varid', 'rsid')),; ); def import_bgen(; path,; entry_fields,; sample_file=None,; n_partitions=None,; block_size=None,; index_file_map=None,; variants=None,; _row_fields=['varid', 'rsid'],; ) -> MatrixTable:; """"""Import BGEN file(s) as a :class:`.MatrixTable`. Examples; --------. Import a BGEN file as a matrix table with GT and GP entry fields:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes mu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38391,Load,Load,38391,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Load'],['Load']
Performance,"', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144280,cache,cache,144280,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"(*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from VCF file(s); :rtype: :py:class:`.VariantDataset`. """""". if generic:; jvds = self._jhc.importVCFsGeneric(jindexed_seq_args(path), force, force_bgz, joption(header_file),; joption(min_partitions), drop_samples, jset_args(call_fields)); else:; jvds = self._jhc.importVCFs(jindexed_seq_args(path",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:25280,load,load,25280,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"(*jobs); Explicitly set dependencies on other jobs.; Examples; Initialize the batch:; >>> b = Batch(). Create the first job:; >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job j2 that depends on j1:; >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:; >>> b.run(). Notes; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters:; jobs (Job) – Sequence of jobs to depend on. Return type:; Self. Returns:; Same job object with dependencies set. env(variable, value). gcsfuse(bucket, mount_point, read_only=True); Add a bucket to mount with gcsfuse.; Notes; Can only be used with the backend.ServiceBackend. This method can; be called more than once. This method has been deprecated. Use Job.cloudfuse(); instead. Warning; There are performance and cost implications of using gcsfuse. Examples; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters:. bucket – Name of the google storage bucket to mount.; mount_point – The path at which the bucket should be mounted to in the Docker; container.; read_only – If True, mount the bucket in read-only mode. Return type:; Self. Returns:; Same job object set with a bucket to mount with gcsfuse. memory(memory); Set the job’s memory requirements.; Examples; Set the job’s memory requirement to be 3Gi:; >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:5140,perform,performance,5140,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['perform'],['performance']
Performance,") – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linreg('sa.phenotype') ; >>> vds.persist() . The above code does NOT persist vds. Instead, it copies vds and persists that result. ; The proper usage is this:; >>> vds = vds.pca().persist() . Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:VariantDataset. query_genotypes(exprs)[source]¶; Performs aggregation queries over genotypes, and returns Python object(s).; Examples; Compute global GQ histogram; >>> gq_hist = vds.query_genotypes('gs.map(g => g.gq).hist(0, 100, 100)'). Compute call rate; >>> call_rate = vds.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:144439,cache,cache,144439,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"), KeyTable(self.hc, kts._2()), \; KeyTable(self.hc, kts._3()), KeyTable(self.hc, kts._4()). [docs] @handle_py4j; @typecheck_method(max_shift=integral); def min_rep(self, max_shift=100):; """"""; Gives minimal, left-aligned representation of alleles. Note that this can change the variant position. **Examples**. 1. Simple trimming of a multi-allelic site, no change in variant position; `1:10000:TAA:TAA,AA` => `1:10000:TA:T,A`. 2. Trimming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:161652,load,loadings,161652,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"); .or_error(""Found non-left-aligned variant in split_multi""); ). return hl.bind(error_on_moved, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). return split_rows(hl.sorted(kept_alleles.map(make_struct)), permit_shuffle); else:. def make_struct(i, cond):; def struct_or_empty(v):; return hl.case().when(cond(v.locus), hl.array([new_struct(v, i)])).or_missing(). return hl.bind(struct_or_empty, hl.min_rep(old_row.locus, [old_row.alleles[0], old_row.alleles[i]])). def make_array(cond):; return hl.sorted(kept_alleles.flatmap(lambda i: make_struct(i, cond))). left = split_rows(make_array(lambda locus: locus == ds['locus']), permit_shuffle); moved = split_rows(make_array(lambda locus: locus != ds['locus']), True); return left.union(moved) if is_table else left.union_rows(moved, _check_cols=False). [docs]@typecheck(ds=oneof(Table, MatrixTable), keep_star=bool, left_aligned=bool, vep_root=str, permit_shuffle=bool); def split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False):; """"""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split tho",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:117351,throughput,throughput,117351,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['throughput'],['throughput']
Performance,"++ standard libraries if not already installed. Detailed instructions. Hail Query. Simplified Analysis. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowled",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1888,scalab,scalable,1888,index.html,https://hail.is,https://hail.is/index.html,1,['scalab'],['scalable']
Performance,"+------------+------+---------+----------+--------------+; <BLANKLINE>; +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+; <BLANKLINE>; +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; -----. Hail is designed to be maximally compatible with files in the `VCF v4.2; spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :func:`.import_vcf` takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); cont",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:98365,load,load,98365,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,", Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:74562,latency,latency,74562,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['latency'],['latency']
Performance,", \sigma^2)\]; Boolean covariates like \(\mathrm{is\_female}\) are encoded as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:5643,perform,perform,5643,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['perform']
Performance,", and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file set with three files; ending in .bed, .bim, and .fam. We also export a file with two columns SNP and P which; contain the GWAS p-values per variant.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1821,perform,performs,1821,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['performs']
Performance,", n_rows, n_cols, block_size). [docs] @classmethod; @typecheck_method(; entry_expr=expr_float64,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def from_entry_expr(; cls, entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14308,perform,performance,14308,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['perform'],['performance']
Performance,", pair adenine (A) with uracil (U) instead of thymine (T). Returns; -------; :class:`.StringExpression`; """"""; s = s.reverse(). if rna:; pairs = [('A', 'U'), ('U', 'A'), ('T', 'A'), ('G', 'C'), ('C', 'G')]; else:; pairs = [('A', 'T'), ('T', 'A'), ('G', 'C'), ('C', 'G')]. d = {}; for b1, b2 in pairs:; d[b1] = b2; d[b1.lower()] = b2.lower(). return s.translate(d). [docs]@typecheck(; contig=expr_str, position=expr_int32, before=expr_int32, after=expr_int32, reference_genome=reference_genome_type; ); def get_sequence(contig, position, before=0, after=0, reference_genome='default') -> StringExpression:; """"""Return the reference sequence at a given locus. Examples; --------. Return the reference allele for ``'GRCh37'`` at the locus ``'1:45323'``:. >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) # doctest: +SKIP; ""T"". Notes; -----; This function requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.na",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:160651,load,load,160651,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['load'],['load']
Performance,",5,10,20 to; 25,20.; DP: Unchanged.; PL: Columns involving filtered alleles are eliminated and; the remaining columns’ values are shifted so the minimum; value is 0.; GQ: The second-lowest PL (after shifting). Warning; filter_alleles_hts() does not update any row fields other than; locus and alleles. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; annotate_rows(). See also; filter_alleles(). Parameters:. mt (MatrixTable); f (callable) – Function from (allele: StringExpression, allele_index:; Int32Expression) to BooleanExpression; subset (bool) – Subset PL field if True, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns:; MatrixTable. hail.methods.hwe_normalized_pca(call_expr, k=10, compute_loadings=False)[source]; Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix.; Examples; >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; This method specializes pca() for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See pca() for more details.; Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; \(ij\) entry of the GRM \(MM^T\) is simply the dot product of rows; \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In; PLINK/GCTA the denominator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:28766,load,loadings,28766,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,",; gvcf_paths: Optional[List[str]] = None,; vds_paths: Optional[List[str]] = None,; vds_sample_counts: Optional[List[int]] = None,; intervals: Optional[List[Interval]] = None,; import_interval_size: Optional[int] = None,; use_genome_default_intervals: bool = False,; use_exome_default_intervals: bool = False,; gvcf_external_header: Optional[str] = None,; gvcf_sample_names: Optional[List[str]] = None,; gvcf_info_to_keep: Optional[Collection[str]] = None,; gvcf_reference_entry_fields_to_keep: Optional[Collection[str]] = None,; call_fields: Collection[str] = ['PGT'],; branch_factor: int = VariantDatasetCombiner._default_branch_factor,; target_records: int = VariantDatasetCombiner._default_target_records,; gvcf_batch_size: Optional[int] = None,; batch_size: Optional[int] = None,; reference_genome: Union[str, ReferenceGenome] = 'default',; contig_recoding: Optional[Dict[str, str]] = None,; force: bool = False,; ) -> VariantDatasetCombiner:; """"""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""; if not (gvcf_paths or vds_paths):; raise ValueError(""at least one of 'gvcf_paths' or 'vds_paths' must be nonempty""); if gvcf_paths is None:; gvcf_paths = []; if len(gvcf_paths) > 0:; if len(set(gvcf_paths)) != len(gvcf_paths):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_paths).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(f'gvcf paths should be unique, the following paths are repeated:{duplicates}'); if gvcf_sample_names is not None and len(set(gvcf_sample_names)) != len(gvcf_sample_names):; duplicates = [gvcf for gvcf, count in collections.Counter(gvcf_sample_names).items() if count > 1]; duplicates = '\n '.join(duplicates); raise ValueError(; ""provided sample names ('gvcf_sample_names') should be unique, ""; f'the following names are repeated:{duplicates}'; ). if vds_paths is None:; vds_paths = []; if vds_sample_counts is not None and len(vds_paths) != len(vds_sample_counts):; raise ValueError(; ""'vds_pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:23667,load,load,23667,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['load'],['load']
Performance,",; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; -----; This method assumes the GATK high-throughput sequencing fields exist:; `GT`, `AD`, `DP`, `GQ`, `PL`. This method replicates the functionality of `Kaitlin Samocha's de novo; caller <https://github.com/ksamocha/de_novo_scripts>`__. The version; corresponding to git commit ``bde3e40`` is implemented in Hail with her; permission and assistance. This method produces a :class:`.Table` with the following fields:. - `locus` (``locus``) -- Variant locus.; - `alleles` (``array<str>``) -- Variant alleles.; - `id` (``str``) -- Proband sample ID.; - `prior` (``float64``) -- Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; `pop_frequency_prior` parameter, and the global prior; ``1 / 3e7``. If the `ignore_in_sample_allele_frequency` parameter is ``True``,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the `pop_frequency_prior` and ``1 / 3e7``.; - `proband` (``struct``) -- Proband column fields from `mt`.; - `father` (``struct``) -- Fath",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:20555,throughput,throughput,20555,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['throughput'],['throughput']
Performance,", *[, ...]); Cap reference blocks at a maximum length in order to permit faster interval filtering. merge_reference_blocks(ds, equivalence_function); Merge adjacent reference blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3768,scalab,scalable,3768,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"-+----------+--------------+; | 1:123456 | [""A"",""C""] | NA | NA | NA | [1,NA] |; +---------------+------------+------+---------+----------+--------------+. +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+. +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; Hail is designed to be maximally compatible with files in the VCF v4.2; spec.; import_vcf() takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:41921,load,load,41921,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"-- number of samples used; - **va.linreg.AC** (*Double*) -- sum of the genotype values ``x``; - **va.linreg.ytx** (*Array[Double]*) -- array of dot products of each phenotype vector ``y`` with the genotype vector ``x``; - **va.linreg.beta** (*Array[Double]*) -- array of fit genotype coefficients, :math:`\hat\beta_1`; - **va.linreg.se** (*Array[Double]*) -- array of estimated standard errors, :math:`\widehat{\mathrm{se}}`; - **va.linreg.tstat** (*Array[Double]*) -- array of :math:`t`-statistics, equal to :math:`\hat\beta_1 / \widehat{\mathrm{se}}`; - **va.linreg.pval** (*Array[Double]*) -- array of :math:`p`-values. :param ys: list of one or more response expressions.; :type covariates: list of str. :param covariates: list of covariate expressions.; :type covariates: list of str. :param str root: Variant annotation path to store result of linear regression. :param bool use_dosages: If true, use dosage genotypes rather than hard call genotypes. :param int variant_block_size: Number of variant regressions to perform simultaneously. Larger block size requires more memmory. :return: Variant dataset with linear regression variant annotations.; :rtype: :py:class:`.VariantDataset`. """""". jvds = self._jvdf.linreg3(jarray(Env.jvm().java.lang.String, ys),; jarray(Env.jvm().java.lang.String, covariates), root, use_dosages, variant_block_size); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(kinshipMatrix=KinshipMatrix,; y=strlike,; covariates=listof(strlike),; global_root=strlike,; va_root=strlike,; run_assoc=bool,; use_ml=bool,; delta=nullable(numeric),; sparsity_threshold=numeric,; use_dosages=bool,; n_eigs=nullable(integral),; dropped_variance_fraction=(nullable(float))); def lmmreg(self, kinshipMatrix, y, covariates=[], global_root=""global.lmmreg"", va_root=""va.lmmreg"",; run_assoc=True, use_ml=False, delta=None, sparsity_threshold=1.0, use_dosages=False,; n_eigs=None, dropped_variance_fraction=None):; """"""Use a kinship-based line",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:114002,perform,perform,114002,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"---+; | HT_DESCRIPTION |; +----------------+; | str |; +----------------+; | ""sixty-five"" |; | ""seventy-two"" |; | ""seventy"" |; | ""sixty"" |; +----------------+. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Expressions for new fields. Returns; -------; :class:`.Table`; Table with new fields. """"""; caller = ""Table.annotate""; check_annotate_exprs(caller, named_exprs, self._row_indices, set()); return self._select(caller, self.row.annotate(**named_exprs)). [docs] @typecheck_method(expr=expr_bool, keep=bool); def filter(self, expr, keep: bool = True) -> 'Table':; """"""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:42454,optimiz,optimization,42454,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['optimiz'],['optimization']
Performance,"-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | A | B |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | 65 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | 72 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | 70 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | 60 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+----------+. In addition to the Table.join() method, Hail provides another; join syntax using Python’s bracket indexing syntax. The syntax looks like; right_table[left_table.key], which will return an Expression; instead of a Table. This expression is a dictionary mapping the; keys in the left table to the rows in the right table.; We can annotate the left table with this expression to perform a left join:; left_table.annotate(x = right_table[left_table.key].x]. For example, below; we add the field ‘B’ from ht2 to ht:; >>> ht1 = ht.annotate(B = ht2[ht.ID].B); >>> ht1.show(width=120); +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | B |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | str |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 | ""cat"" |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 | ""dog"" |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 | ""mouse"" |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 | ""rabbit"" |; +-------+-------+-----+-------+-------+-------+-------+-------+----------+. Interacting with Tables Locally; Hail has many useful methods for interacting with tables locally such as in an; Jupyter notebook. Use the Table.show() method to see ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:7297,perform,perform,7297,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['perform'],['perform']
Performance,"---------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_interv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:26855,load,load,26855,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"-08. New Features. (#10632) Added; support for weighted linear regression to; hl.linear_regression_rows.; (#10635) Added; hl.nd.maximum and hl.nd.minimum.; (#10602) Added; hl.starmap. Bug fixes. (#10038) Fixed; crashes when writing/reading matrix tables with 0 partitions.; (#10624) Fixed out; of bounds bug with _quantile_from_cdf. hailctl dataproc. (#10633) Added; --scopes parameter to hailctl dataproc start. Version 0.2.70; Released 2021-06-21. Version 0.2.69; Released 2021-06-14. New Features. (#10592) Added; hl.get_hgdp function.; (#10555) Added; hl.hadoop_scheme_supported function.; (#10551) Indexing; ndarrays now supports ellipses. Bug fixes. (#10553) Dividing; two integers now returns a float64, not a float32.; (#10595) Don’t; include nans in lambda_gc_agg. hailctl dataproc. (#10574) Hail logs; will now be stored in /home/hail by default. Version 0.2.68; Released 2021-05-27. Version 0.2.67. Critical performance fix; Released 2021-05-06. (#10451) Fixed a; memory leak / performance bug triggered by; hl.literal(...).contains(...). Version 0.2.66; Released 2021-05-03. New features. (#10398) Added new; method BlockMatrix.to_ndarray.; (#10251) Added; suport for haploid GT calls to VCF combiner. Version 0.2.65; Released 2021-04-14. Default Spark Version Change. Starting from version 0.2.65, Hail uses Spark 3.1.1 by default. This; will also allow the use of all python versions >= 3.6. By building; hail from source, it is still possible to use older versions of; Spark. New features. (#10290) Added; hl.nd.solve.; (#10187) Added; NDArrayNumericExpression.sum. Performance improvements. (#10233) Loops; created with hl.experimental.loop will now clean up unneeded; memory between iterations. Bug fixes. (#10227); hl.nd.qr now supports ndarrays that have 0 rows or columns. Version 0.2.64; Released 2021-03-11. New features. (#10164) Add; source_file_field parameter to hl.import_table to allow lines to be; associated with their original source file. Bug fixes",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:59825,perform,performance,59825,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"-compatible location.; Examples; Specify a manual path:; >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') ; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:; >>> hl.copy_log('gs://my-bucket/') ; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes.; If path is a directory, then the log file will be copied using its; base name to the directory (e.g. /home/hail.log would be copied as; gs://my-bucket/hail.log if path is gs://my-bucket. Parameters:; path (str). hail.utils.range_table(n, n_partitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, ov",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:10033,optimiz,optimized,10033,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88735,perform,performance,88735,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,". BatchPoolExecutor — Batch documentation. Batch; . Getting Started; Tutorial; Docker Resources; Batch Service; Cookbooks; Reference (Python API); Batches; Resources; Batch Pool Executor; BatchPoolExecutor; BatchPoolExecutor. BatchPoolFuture. Backends; Utilities. Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python API; BatchPoolExecutor. View page source. BatchPoolExecutor. class hailtop.batch.batch_pool_executor.BatchPoolExecutor(*, name=None, backend=None, image=None, cpus_per_job=None, wait_on_exit=True, cleanup_bucket=True, project=None); Bases: object; An executor which executes Python functions in the cloud.; concurrent.futures.ProcessPoolExecutor and; concurrent.futures.ThreadPoolExecutor enable the use of all the; computer cores available on a single computer. BatchPoolExecutor; enables the use of an effectively arbitrary number of cloud computer cores.; Functions provided to submit() are serialized using dill, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which submit() was; called. The Python version in the docker container will share a major and; minor verison with the local process. The image parameter overrides this; behavior.; When used as a context manager (the with syntax), the executor will wait; for all jobs to finish before finishing the with statement. This; behavior can be controlled by the wait_on_exit parameter.; This class creates a folder batch-pool-executor at the root of the; bucket specified by the backend. This folder can be safely deleted after; all jobs have completed.; Examples; Add 3 to 6 on a machine in the cloud and send the result back to; this machine:; >>> with BatchPoolExecutor() as bpe: ; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() ; 9. map() facilitates the common case of executing a function on many; values in parallel:; >>> with BatchPoolExecutor(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html:686,concurren,concurrent,686,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolExecutor.html,4,['concurren'],['concurrent']
Performance,". Notes; The resulting ndarray will have the same shape as the block matrix. Returns:; numpy.ndarray. to_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; row_idx (:py:data.`tint64`, key field) – Row index; entries (tarray of tfloat64) – Entries for the row. Examples; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters:. n_partitions (int or None) – Number of partitions of the table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by function",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:42269,cache,cache,42269,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,".MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'MatrixTable':; """"""; Unpersists this dataset from memory/disk. Notes; -----; This function will have no effect on a dataset that was not previously; persisted. Returns; -------; :class:`.MatrixTable`; Unpersisted dataset.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:111007,cache,cache,111007,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,4,"['cache', 'perform']","['cache', 'performance']"
Performance,".T @ G2); Q1, R1 = hl.nd.qr(G2)._persist(); fact2 = _krylov_factorization(A, Q1, p, compute_U=False); moments_and_stdevs = fact2.spectral_moments(num_moments, R1); # Add back exact moments; moments = moments_and_stdevs.moments + hl.nd.array([; fact.S.map(lambda x: x ** (2 * i)).sum() for i in range(1, num_moments + 1); ]); moments_and_stdevs = hl.eval(hl.struct(moments=moments, stdevs=moments_and_stdevs.stdevs)); moments = moments_and_stdevs.moments; stdevs = moments_and_stdevs.stdevs. scores = V * S; eigens = hl.eval(S * S); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). hail_array_scores = scores._data_array(); cols_and_scores = hl.zip(A.source_table.index_globals().cols, hail_array_scores).map(; lambda tup: tup[0].annotate(scores=tup[1]); ); st = hl.Table.parallelize(cols_and_scores, key=A.col_key). if compute_loadings:; lt = A.source_table.select(); lt = lt.annotate_globals(U=U); idx_name = '_tmp_pca_loading_index'; lt = lt.add_index(idx_name); lt = lt.annotate(loadings=hl.array(lt.U[lt[idx_name], :])).select_globals(); lt = lt.drop(lt[idx_name]); else:; lt = None. return eigens, st, lt, moments, stdevs. @typecheck(; A=oneof(expr_float64, TallSkinnyMatrix),; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; compute_scores=bool,; transpose=bool,; ); def _blanczos_pca(; A,; k=10,; compute_loadings=False,; q_iterations=10,; oversampling_param=None,; block_size=128,; compute_scores=True,; transpose=False,; ):; r""""""Run randomized principal component analysis approximation (PCA); on numeric columns derived from a matrix table. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl._blanczos_pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:17702,load,loadings,17702,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,".default_reference with an argument to set new default; references usually shortly after hl.init. Version 0.2.126; Released 2023-10-30. Bug Fixes. (#13939) Fix a bug; introduced in 0.2.125 which could cause dict literals created in; python to be decoded incorrectly, causing runtime errors or,; potentially, incorrect results.; (#13751) Correct the; broadcasting of ndarrays containing at least one dimension of length; zero. This previously produced incorrect results. Version 0.2.125; Released 2023-10-26. New Features. (#13682); hl.export_vcf now clearly reports all Table or Matrix Table; fields which cannot be represented in a VCF.; (#13355) Improve the; Hail compiler to more reliably rewrite Table.filter and; MatrixTable.filter_rows to use hl.filter_intervals. Before; this change some queries required reading all partitions even though; only a small number of partitions match the filter.; (#13787) Improve; speed of reading hail format datasets from disk. Simple pipelines may; see as much as a halving in latency.; (#13849) Fix; (#13788), improving; the error message when hl.logistic_regression_rows is provided; row or entry annotations for the dependent variable.; (#13888); hl.default_reference can now be passed an argument to change the; default reference genome. Bug Fixes. (#13702) Fix; (#13699) and; (#13693). Since; 0.2.96, pipelines that combined random functions; (e.g. hl.rand_unif) with index(..., all_matches=True) could; fail with a ClassCastException.; (#13707) Fix; (#13633).; hl.maximum_independent_set now accepts strings as the names of; individuals. It has always accepted structures containing a single; string field.; (#13713) Fix; (#13704), in which; Hail could encounter an IllegalArgumentException if there are too; many transient errors.; (#13730) Fix; (#13356) and; (#13409). In QoB; pipelines with 10K or more partitions, transient “Corrupted block; detected” errors were common. This was caused by incorrect retry; logic. That logic has been fixed.; (#",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:21357,latency,latency,21357,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['latency'],['latency']
Performance,".eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; -----; The odds ratio is given by ``(c1 / c2) / (c3 / c4)``. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, odds_ratio=tfloat64); return _func(""chi_squared_test"", ret_type, c1, c2, c3, c4). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32, min_cell_count=expr_int32); def contingency_table_test(c1, c2, c3, c4, min_cell_count) -> StructExpression:; """"""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :cla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:20416,Perform,Performs,20416,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Perform'],['Performs']
Performance,".filter_cols(hl.is_missing(cols_to_remove.index(ds['s']))). See also; semi_join_cols(), filter_cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:17941,cache,cache,17941,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['cache'],['cache']
Performance,".pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1492,load,loadings,1492,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance,"/en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: arr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:47076,cache,cache,47076,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,3,"['Cache', 'cache']","['Cache', 'cache']"
Performance,"0.1 (Deprecated). FORUM; CHAT; CODE; JOBS. Hail; . ; . 0.1; . Getting Started; Overview; Tutorials; Expression Language; Python API; Annotation Database; Database Query; Documentation; Important Notes; Multiallelic variants; VEP annotations; Gene-level annotations. Suggest additions or edits. Other Resources. Hail. Docs »; Annotation Database. View page source. Annotation Database¶; This database contains a curated collection of variant annotations in Hail-friendly format, for use in Hail analysis pipelines.; Currently, the annotate_variants_db() VDS method associated with this database works only if you are running Hail on the; Google Cloud Platform.; To incorporate these annotations in your own Hail analysis pipeline, select which annotations you would like to query from the; documentation below and then copy-and-paste the Hail code generated into your own analysis script.; For example, a simple Hail script to load a VCF into a VDS, annotate the VDS with CADD raw and PHRED scores using this database,; and inspect the schema could look something like this:; import hail; from pprint import pprint. hc = hail.HailContext(). vds = (; hc; .import_vcf('gs://annotationdb/test/sample.vcf'); .split_multi(); .annotate_variants_db([; 'va.cadd'; ]); ). pprint(vds.variant_schema). This code would return the following schema:; Struct{; rsid: String,; qual: Double,; filters: Set[String],; info: Struct{; ...; },; cadd: Struct{; RawScore: Double,; PHRED: Double; }; }. Database Query¶; Select annotations by clicking on the checkboxes in the documentation, and the appropriate Hail command will be generated; in the panel below.; Use the “Copy to clipboard” button to copy the generated Hail code, and paste the command into your; own Hail script. Database Query. Copy to clipboard. vds = ( hc .read('my.vds') .split_multi(); .annotate_variants_db([ ... ]); ). Documentation¶; These annotations have been collected from a variety of publications and their accompanying datasets (usually text f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/annotationdb.html:1004,load,load,1004,docs/0.1/annotationdb.html,https://hail.is,https://hail.is/docs/0.1/annotationdb.html,1,['load'],['load']
Performance,"0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantgq_means, bins = np.arange(0, 84, 2)); plt.xlabel(""Variant Mean GQ""); pl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:18668,cache,cache,18668,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['cache'],['cache']
Performance,"0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[st",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34886,perform,perform,34886,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['perform'],['perform']
Performance,"0].p_value,; 'p_value_excess_het': hwe[1].p_value,; }),; ),; ). return mt.annotate_rows(**{name: result}). [docs]@typecheck(left=MatrixTable, right=MatrixTable, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14032,perform,performs,14032,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['perform'],['performs']
Performance,"3 array of bi-allelic Phred-scaled genotype likelihoods. Returns:; Expression of type tfloat64. hail.expr.functions.gp_dosage(gp)[source]; Return expected genotype dosage from array of genotype probabilities.; Examples; >>> hl.eval(hl.gp_dosage([0.0, 0.5, 0.5])); 1.5. Notes; This function is only defined for bi-allelic variants. The gp argument; must be length 3. The value is gp[1] + 2 * gp[2]. Parameters:; gp (Expression of type tarray of tfloat64) – Length 3 array of bi-allelic genotype probabilities. Returns:; Expression of type tfloat64. hail.expr.functions.get_sequence(contig, position, before=0, after=0, reference_genome='default')[source]; Return the reference sequence at a given locus.; Examples; Return the reference allele for 'GRCh37' at the locus '1:45323':; >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) ; ""T"". Notes; This function requires reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome.; Returns None if contig and position are not valid coordinates in; reference_genome. Parameters:. contig (Expression of type tstr) – Locus contig.; position (Expression of type tint32) – Locus position.; before (Expression of type tint32, optional) – Number of bases to include before the locus of interest. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome (str or ReferenceGenome) – Reference genome to use. Must have a reference sequence available. Returns:; StringExpression. hail.expr.functions.mendel_error_code(locus, is_female, father, mother, child)[source]; Compute a Mendelian violation code for genotypes.; >>> father = hl.call(0, 0); >>> mother = hl.call(1, 1); >>> child1 = hl.call(0, 1) # consistent; >>> child2 = hl.call(0, 0) # Mendel error; >>> locus = hl.locus('2', 2000000). >>> hl.eval(hl.mendel_error_cod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:19045,load,load,19045,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['load']
Performance,"30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_indepen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101956,optimiz,optimizer,101956,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"32)(#6115); hl.import_bed abd hl.import_locus_intervals now accept; keyword arguments to pass through to hl.import_table, which is; used internally. This permits parameters like min_partitions to; be set.; (#5980) Added log; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93363,optimiz,optimized,93363,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimized']
Performance,"4) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use split_multi(); to split multiallelic variants beforehand. Parameters:. expr (CallExpression) – Call to test for Hardy-Weinberg equilibrium.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – Struct expression with fields het_freq_hwe and p_value. hail.expr.aggregators.explode(f, array_agg_expr)[source]; Explode an array or set expression to aggregate the elements of all records.; Examples; Compute the mean of all elements in fields C1, C2, and C3:; >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of all observed elements in the filters field (Set[String]):; >>> dataset.aggregate_rows(hl.agg.explode(lambda elt: hl.agg.collect_as_set(elt), dataset.filters)); set(). Notes; This method can be used with aggregator functions to aggregate the elements; of collection types (tarray and tset). Parameters:. f (Function from Expression to Expression) – Aggregation function to apply to each element of the exploded array.; array_agg_expr (CollectionExpression) – Expression of type tarray or tset. Returns:; Expression – Aggregation express",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:16703,perform,perform,16703,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['perform'],['perform']
Performance,"4,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_new = mt.annotate_rows(gnomad_ann = gnomad_data[mt.locus, mt.alleles]). If we only cared about adding one new row field such as AF from gnomad_data,; we could do the following:; >>> mt_new = mt.annotate_rows(gnomad_af = gnomad_data[mt.locus, mt.alleles]['AF']). To add all fields as top-level row fields, the following syntax unpacks the gnomad_data; row as keyword arguments to MatrixTable.annotate_rows():; >>> mt_new = mt.annotate_rows(**gnomad_data[mt.locus, mt.alleles]). Interacting with Matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:12494,perform,performed,12494,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performed']
Performance,"4`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""; return hl.rbind(; hl.rbind(; expr,; lambda call: filter(; call.ploidy == 2,; counter(call.n_alt_alleles()).map_values(; lambda i: hl.case(); .when(i < 1 << 31, hl.int(i)); .or_error('hardy_weinberg_test: count greater than MAX_INT'); ),; ),; _ctx=_agg_func.context,; ),; lambda counts: hl.hardy_weinberg_test(; counts.get(0, 0), counts.get(1, 0), counts.get(2, 0), one_sided=one_sided; ),; ). [docs]@typecheck(f=func_spec(1, agg_expr(expr_any)), array_agg_expr=expr_oneof(expr_array(), expr_set())); def explode(f, array_agg_expr) -> Expression:; """"""Explode an array or set expression to aggregate the elements of all records. Examples; --------; Compute the mean of all elements in fields `C1`, `C2`, and `C3`:. >>> table1.aggregate(hl.agg.explode(lambda elt: hl.agg.mean(elt), [table1.C1, table1.C2, table1.C3])); 24.833333333333332. Compute the set of ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:32627,perform,perform,32627,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['perform'],['perform']
Performance,"8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:78956,perform,performance,78956,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"8615) Fix contig; ordering in the CanFam3 (dog) reference genome.; (#8622) Fix bug that; causes inscrutable JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72481,perform,performance,72481,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79680,perform,performance,79680,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"95) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88098,perform,performance,88098,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,": hl.tstr} solves this problem. Parameters:. paths (str or list of str) – Files to import.; key (str or list of str) – Key fields(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – If True`, assume the file has no header and name the N fields f0,; f1, … fN (0-indexed).; impute (bool) – If True, Impute field types from the file.; comment (str or list of str) – Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:35780,load,load,35780,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,": str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters:. path (str) – File to import.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_contigs (bool) – If True and reference_genome is not None, skip lines where; seqname is not consistent with the reference genome.; min_partitions (int or None) – Minimum number of partitions (passed to import_table).; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns:; Table. hail.experimental.get_gene_intervals(gene_symbols=None, gene_ids=None, transcript_ids=None, verbose=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:27141,load,load,27141,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,":. >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:. >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:. >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). .. note::. For optimal performance, be sure to explicitly give the alternative (``NA``) the same type as the consequent (``sa.pheno.bloodPressure``). **Notes**. ``expr`` is in sample context so the following symbols are in scope:. - ``s`` (*Sample*): sample; - ``sa``: sample annotations; - ``global``: global annotations; - ``gs`` (*Aggregable[Genotype]*): aggregable of :ref:`genotype` for sample ``s``. :param expr: Annotation expression.; :type expr: str or list of str. :return: Annotated variant dataset.; :rtype: :class:`.VariantDataset`; """""". if isinstance(expr, list):; expr = ','.join(expr). jvds = self._jvds.annotateSamplesExpr(expr); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; root=nullable(strlike),; expr=nullable(strlike),; vds_key=nullable(strlike),; product=bool); def annotate_samples_table(self, table, root=None, expr=None, vds_key=None, product=False):; """"""Annotate samples with a key table. **Examples**. To annotates samples using `samples1.tsv` with type imputation::. >>> table = hc.import_t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:14718,perform,performance,14718,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,":; Expression of type tint64 or tfloat64 – Product of records of expr. hail.expr.aggregators.fraction(predicate)[source]; Compute the fraction of records where predicate is True.; Examples; Compute the fraction of rows where SEX is “F” and HT > 65:; >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; Missing values for predicate are treated as False. Parameters:; predicate (BooleanExpression) – Boolean predicate. Returns:; Expression of type tfloat64 – Fraction of records where predicate is True. hail.expr.aggregators.hardy_weinberg_test(expr, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; Test each row of a dataset:; >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:; >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; This method performs the test described in functions.hardy_weinberg_test() based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls.; The resulting struct expression has two fields:. het_freq_hwe (tfloat64) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium.; p_value (tfloat64) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this document for; details on the Levene-Haldane distribution and references.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Warning; Non-diploid calls (ploidy != 2) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/aggregators.html:15490,perform,performs,15490,docs/0.2/aggregators.html,https://hail.is,https://hail.is/docs/0.2/aggregators.html,1,['perform'],['performs']
Performance,":`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ._insert_field(; 'transcript_consequences',; tarray(; vep_json_typ['transcript_consequences'].element_type._insert_fields(; appris=tstr,; tsl=tint32,; ); ),; ). def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh38 \; --fasta {self.data_mount}homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz \; --plugin ""LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:{self.data_mount}/gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:{self.data_mount}/human_ancestor.fa.gz,conservation_file:{self.data_mount}/loftee.sql"" \; --dir_plugins /vep/ensembl-vep/Plugins/ \; --dir_cache {self.data_mount} \; -o STDOUT; """""". supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh37Version85(; data_bucket='hail-qob-vep-grch37-us-central1',; data_mount='/vep_data/',; image=HAIL_GENETICS_VEP_GRCH37_85_IMAGE,; regions=['us-central1'],; cloud='gcp',; data_bucket_is_requester_pays=True,; ),; ('GRCh38', 'gcp', 'us-central1', 'hail.is'): VEPConfigGRCh38Version95(;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:30485,cache,cache,30485,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['cache'],['cache']
Performance,":data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, odds_ratio=tfloat64); return _func(""contingency_table_test"", ret_type, c1, c2, c3, c4, min_cell_count). # We use 64-bit integers.; # It is relatively easy to encounter an integer overflow bug with 32-bit integers.; [docs]@typecheck(a=expr_array(expr_int64), b=expr_array(expr_int64), c=expr_array(expr_int64), d=expr_array(expr_int64)); def cochran_mantel_haenszel_test(; a: Union[tarray, list], b: Union[tarray, list], c: Union[tarray, list], d: Union[tarray, list]; ) -> StructExpression:; """"""Perform the Cochran-Mantel-Haenszel test for association. Examples; --------; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locus == hl.Locus(20, 10633237)); >>> mt.count_rows(); 1; >>> a, b, c, d = mt.aggregate_entries(; ... hl.tuple([; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(~mt.GT.is_non_ref() & mt.pheno.is_case & ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:22174,Perform,Perform,22174,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Perform'],['Perform']
Performance,"; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.pca. Source code for hail.experimental.pca; import hail as hl; from hail.expr.expressions import (; expr_array,; expr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1128,load,loadings,1128,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance,"; :class:`.Table`; Table of SKAT results. """"""; if hl.current_backend().requires_lowering:; if logistic:; kwargs = {'accuracy': accuracy, 'iterations': iterations}; if logistic is not True:; null_max_iterations, null_tolerance = logistic; kwargs['null_max_iterations'] = null_max_iterations; kwargs['null_tolerance'] = null_tolerance; ht = hl._logistic_skat(key_expr, weight_expr, y, x, covariates, max_size, **kwargs); else:; ht = hl._linear_skat(key_expr, weight_expr, y, x, covariates, max_size, accuracy, iterations); ht = ht.select_globals(); return ht; mt = matrix_table_source('skat/x', x); raise_unless_entry_indexed('skat/x', x). analyze('skat/key_expr', key_expr, mt._row_indices); analyze('skat/weight_expr', weight_expr, mt._row_indices); analyze('skat/y', y, mt._col_indices). all_exprs = [key_expr, weight_expr, y]; for e in covariates:; all_exprs.append(e); analyze('skat/covariates', e, mt._col_indices). _warn_if_no_intercept('skat', covariates). # FIXME: remove this logic when annotation is better optimized; if x in mt._fields_inverse:; x_field_name = mt._fields_inverse[x]; entry_expr = {}; else:; x_field_name = Env.get_uid(); entry_expr = {x_field_name: x}. y_field_name = '__y'; weight_field_name = '__weight'; key_field_name = '__key'; cov_field_names = list(f'__cov{i}' for i in range(len(covariates))). mt = mt._select_all(; col_exprs=dict(**{y_field_name: y}, **dict(zip(cov_field_names, covariates))),; row_exprs={weight_field_name: weight_expr, key_field_name: key_expr},; entry_exprs=entry_expr,; ). if logistic is True:; use_logistic = True; max_iterations = 25; tolerance = 1e-6; elif logistic is False:; use_logistic = False; max_iterations = 0; tolerance = 0.0; else:; assert isinstance(logistic, tuple) and len(logistic) == 2; use_logistic = True; max_iterations, tolerance = logistic. config = {; 'name': 'Skat',; 'keyField': key_field_name,; 'weightField': weight_field_name,; 'xField': x_field_name,; 'yField': y_field_name,; 'covFields': cov_field_names,; 'logi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:108486,optimiz,optimized,108486,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['optimiz'],['optimized']
Performance,"; Matrix slicing, and more generally filter(), filter_rows(),; and filter_cols(). These following methods always result in a block-dense matrix:. fill(); Addition or subtraction of a scalar or broadcasted vector.; Matrix multiplication, @. The following methods fail if any operand is block-sparse, but can be forced; by first applying densify(). Element-wise division between two block matrices.; Multiplication by a scalar or broadcasted vector which includes an; infinite or nan value.; Division by a scalar or broadcasted vector which includes a zero, infinite; or nan value.; Division of a scalar or broadcasted vector by a block matrix.; Element-wise exponentiation by a negative exponent.; Natural logarithm, log(). Attributes. T; Matrix transpose. block_size; Block size. element_type; The type of the elements. is_sparse; Returns True if block-sparse. n_cols; Number of columns. n_rows; Number of rows. shape; Shape of matrix. Methods. abs; Element-wise absolute value. cache; Persist this block matrix in memory. ceil; Element-wise ceiling. checkpoint; Checkpoint the block matrix. default_block_size; Default block side length. densify; Restore all dropped blocks as explicit blocks of zeros. diagonal; Extracts diagonal elements as a row vector. entries; Returns a table with the indices and value of each block matrix entry. export; Exports a stored block matrix as a delimited text file. export_blocks; Export each block of the block matrix as its own delimited text or binary file. export_rectangles; Export rectangular regions from a block matrix to delimited text or binary files. fill; Creates a block matrix with all elements the same value. filter; Filters matrix rows and columns. filter_cols; Filters matrix columns. filter_rows; Filters matrix rows. floor; Element-wise floor. from_entry_expr; Creates a block matrix using a matrix table entry expression. from_ndarray; Create a BlockMatrix from an ndarray. from_numpy; Distributes a NumPy ndarray as a block matrix. fromfile; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:7844,cache,cache,7844,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"; Returns True if the locus is in a pseudoautosomal region; of chromosome Y.; Examples; >>> hl.eval(locus.in_y_par()); False. Note; Many variant callers only generate variants on chromosome X for the; pseudoautosomal region. In this case, all loci mapped to chromosome; Y are non-pseudoautosomal. Returns:; BooleanExpression. property position; Returns the position along the chromosome.; Examples; >>> hl.eval(locus.position); 1034245. Returns:; Expression of type tint32 – This locus’s position along its chromosome. sequence_context(before=0, after=0)[source]; Return the reference genome sequence at the locus.; Examples; Get the reference allele at a locus:; >>> hl.eval(locus.sequence_context()) ; ""G"". Get the reference sequence at a locus including the previous 5 bases:; >>> hl.eval(locus.sequence_context(before=5)) ; ""ACTCGG"". Notes; This function requires that this locus’ reference genome has an attached; reference sequence. Use ReferenceGenome.add_sequence() to; load and attach a reference sequence to a reference genome. Parameters:. before (Expression of type tint32, optional) – Number of bases to include before the locus. Truncates at; contig boundary.; after (Expression of type tint32, optional) – Number of bases to include after the locus. Truncates at; contig boundary. Returns:; StringExpression. show(n=None, width=None, truncate=None, types=True, handler=None, n_rows=None, n_cols=None); Print the first few records of the expression to the console.; If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records.; Examples; >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; The output can be passed piped to another output source using the handl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.LocusExpression.html:9165,load,load,9165,docs/0.2/hail.expr.LocusExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.LocusExpression.html,1,['load'],['load']
Performance,"; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46737,perform,performance,46737,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matrix with 1 row, 10 columns,; and elements from row 2 of bm.; bm[:3, -1] is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of bm.; bm[::2, ::2] is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of bm. Use fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3811,cache,cache,3811,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_iterations=q_iterations,; oversampling_param=oversampling_param,; block_size=block_size,; ). © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23973,load,loadings,23973,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"<https://github.com/konradjk/loftee>`__; on the current variant dataset and adds the result as a variant annotation. **Examples**. Add VEP annotations to the dataset:. >>> vds_result = vds.vep(""data/vep.properties"") # doctest: +SKIP. **Configuration**. :py:meth:`~hail.VariantDataset.vep` needs a configuration file to tell it how to run; VEP. The format is a `.properties file <https://en.wikipedia.org/wiki/.properties>`__.; Roughly, each line defines a property as a key-value pair of the form `key = value`. `vep` supports the following properties:. - **hail.vep.perl** -- Location of Perl. Optional, default: perl.; - **hail.vep.perl5lib** -- Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; - **hail.vep.path** -- Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; - **hail.vep.location** -- Location of the VEP Perl script. Required.; - **hail.vep.cache_dir** -- Location of the VEP cache dir, passed to VEP with the `--dir` option. Required.; - **hail.vep.fasta** -- Location of the FASTA file to use to look up the reference sequence, passed to VEP with the `--fasta` option. Required.; - **hail.vep.assembly** -- Genome assembly version to use. Optional, default: GRCh37; - **hail.vep.plugin** -- VEP plugin, passed to VEP with the `--plugin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:222465,cache,cache,222465,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"= {row.ID : row.SEX for row in kt1.collect()}. **Notes**. This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. :rtype: list of :py:class:`.hail.representation.Struct`; """""". return TArray(self.schema)._convert_to_py(self._jkt.collect()). @handle_py4j; def _typecheck(self):; """"""Check if all values with the schema."""""". self._jkt.typeCheck(). [docs] @handle_py4j; @typecheck_method(output=strlike,; overwrite=bool); def write(self, output, overwrite=False):; """"""Write as KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-dep",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23159,cache,cache,23159,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,['cache'],"['cache', 'cached']"
Performance,"=False). Caution; The double quotes on ""1"" are necessary because v.contig is of type String. Notes; The following symbols are in scope for expr:. v (Variant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:59690,perform,performs,59690,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"=True, reference_genome=None, gtf_file=None)[source]; Get intervals of genes or transcripts.; Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable.; On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz; Example; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) . Parameters:. gene_symbols (list of str, optional) – Gene symbols (e.g. PCSK9).; gene_ids (list of str, optional) – Gene IDs (e.g. ENSG00000223972).; transcript_ids (list of str, optional) – Transcript IDs (e.g. ENSG00000223972).; verbose (bool) – If True, print which genes and transcripts were matched in the GTF file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use (passed along to import_gtf).; gtf_file (str) – GTF file to load. If none is provided, but reference_genome is one of; GRCh37 or GRCh38, a default will be used (on Google Cloud Platform). Returns:; list of Interval. hail.experimental.export_entries_by_col(mt, path, batch_size=256, bgzip=True, header_json_in_file=True, use_string_key_as_file_name=False)[source]; Export entries of the mt by column as separate text files.; Examples; >>> range_mt = hl.utils.range_matrix_table(10, 10); >>> range_mt = range_mt.annotate_entries(x = hl.rand_unif(0, 1)); >>> hl.experimental.export_entries_by_col(range_mt, 'output/cols_files'). Notes; This function writes a directory with one file per column in mt. The; files contain one tab-separated field (with header) for each row field; and entry field in mt. The column fields of mt are written as JSON; in the first line of each file, prefixed with a #.; The above will produce a directory at output/cols_files with the; following files:; $ ls -l output/cols_files; total 80; -rw-r--r-- 1 hail-dev wheel 71",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:28430,load,load,28430,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_regions}.'; ). valid_clouds = {'gcp', 'aws'}; if cloud not in valid_clouds:; raise ValueError(; f'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1728,load,load,1728,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['load'],['load']
Performance,"ASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``.; :type call_fields: str or list of str. :return: Variant dataset imported from V",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:25017,load,load,25017,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"A` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162233,load,loadings,162233,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"BS}^{(0)}_{ij}}; {\sum_{s \in S_{ij}} \\widehat{\\mu_{is}}^2(1 - \\widehat{\\mu_{js}})^2 + (1 - \\widehat{\\mu_{is}})^2\\widehat{\\mu_{js}}^2}; & \\widehat{\phi_{ij}} > 2^{-5/2} \\\\; 1 - 4 \\widehat{\phi_{ij}} + k^{(2)}_{ij}; & \\widehat{\phi_{ij}} \le 2^{-5/2}; \\end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \\widehat{k^{(1)}_{ij}} := 1 - \\widehat{k^{(2)}_{ij}} - \\widehat{k^{(0)}_{ij}}. **Details**. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :py:meth:`~hail.VariantDataset.pc_relate` differs from the reference; implementation in a couple key ways:. - the principal components analysis does not use an unrelated set of; individuals. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). **Notes**. The ``block_size`` controls memory usage and parallelism. If it is large; enough to hold an entire sample-by-sample matrix of 64-bit doubles in; memory, then only one Spark worker node can be used to compute matrix; operations. If it is too small, communication overhead will begin to; dominate the computation's time. The author has found that on Google; Dataproc (where each core has about 3.75GB of memory), setting; ``block_size`` larger than 512 tends to cause memory exhaustion errors. The minimum allele frequency filter is applied per-pair: if either of; the two individual's individual-specific minor allele frequency is below; the threshold, then the variant's contribution to relatedness estimates; is zero. Under the PC-Relate model, ki",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:174389,perform,perform,174389,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:23378,perform,performance,23378,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['perform'],['performance']
Performance,"Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77673,perform,performance,77673,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"CF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a waste of resources, so indexing should generally be done; as a one-time step separately from large analyses. Parameters:path (str or list of str) – .bgen files to index. read(path, drop_samples=False, drop_variants=False)[source]¶; Read .vds files as variant dataset.; When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. Parameters:; path (str or list of str) – VDS files to read.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations; or gneotypes.; drop_variants (bool) – If True, create samples-only variant; dataset (no variants or genotypes). Returns:Variant dataset read from disk. Return type:VariantDataset. read_table(path)[source]¶; Read a KT file as key table. Parameters:path (str) – KT file to read. Returns:Key table read from disk. Return type:KeyTable. report()[source]¶; Print information and warnings about VCF + GEN import and deduplication. stop()[source]¶; Shut down the Hail context.; It is not possible to have multiple Hail contexts running in a; single Python session, so use this if you need to reconfigure the Hail; context. Note that this also stops a running Spark context. version¶; Return the version of Hail associated with this HailContext. Return type:str. write_partitioning(path)[source]¶; Write partitioning.json.gz file for legacy VDS file. Parameters:path (str) – path to VDS file. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:25071,load,load,25071,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"DArrayExpression`; """"""; ir = BlockMatrixCollect(self._bmir); return construct_expr(ir, hl.tndarray(hl.tfloat64, 2)). @property; def is_sparse(self):; """"""Returns ``True`` if block-sparse. Notes; -----; A block matrix is block-sparse if at least of its blocks is dropped,; i.e. implicitly a block of zeros. Returns; -------; :obj:`bool`; """"""; return Env.backend()._to_java_blockmatrix_ir(self._bmir).typ().isSparse(). @property; def T(self):; """"""Matrix transpose. Returns; -------; :class:`.BlockMatrix`; """"""; if self.n_rows == 1 and self.n_cols == 1:; return self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:41819,cache,cache,41819,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['cache'],['cache']
Performance,"If you are using hailctl dataproc as mentioned above, you can just use the; default argument for config and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below.; The format of the configuration file is JSON, and vep(); expects a JSON object with three fields:. command (array of string) – The VEP command line to run. The string literal __OUTPUT_FORMAT_FLAG__ is replaced with –json or –vcf depending on csq.; env (object) – A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; vep_json_schema (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the –json option). Note: This is the old-style ‘parseable’ Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in /vep with the Loftee plugin:; {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:103172,cache,cache,103172,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"K behavior. Use missing='-9' to interpret this; value as missing. Parameters:. path (str) – Path to FAM file.; quant_pheno (bool) – If True, phenotype is interpreted as quantitative.; delimiter (str) – Field delimiter regex.; missing (str) – The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns:; Table. hail.methods.import_gen(path, sample_file=None, tolerance=0.2, min_partitions=None, chromosome=None, reference_genome='default', contig_recoding=None, skip_invalid_loci=False)[source]; Import GEN file(s) as a MatrixTable.; Examples; >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; For more information on the GEN file format, see here.; If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the chromosome parameter.; To load multiple files at the same time, use Hadoop Glob Patterns.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file. Row Fields. locus (tlocus or tstruct) – Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by chromosome) and position (4th column if chromosome is not; defined). If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An array; containing the alleles of the variant. The reference allele (4th column if; chromosome is not defined) is the first element of the array and the; alternate allele (5th column if chromosome is not defined) is the second; element.; varid (tstr) – The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; rsid (tstr) – The rsID. 3rd column of GEN file if; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:17075,load,load,17075,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"KT file. ***Examples***. >>> kt1.write('output/kt1.kt'). .. note:: The write path must end in "".kt"". . :param str output: Path of KT file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23746,cache,cache,23746,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"PUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """""". return self.annotate(**{name: hl.scan.count()}). [docs] @typecheck_method(tables=table_type, unify=bool); def union(self, *tables, unify: bool = False) -> 'Table':; """"""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an err",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:84723,perform,performance,84723,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['perform'],['performance']
Performance,"Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_level)). [docs] def unpersist(self):; """"""; Unpersists this VDS from memory/disk.; ; **Notes**; This function will have no effect on a VDS that was not previously persisted.; ; There's nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it.; ; """"""; self._jvds.unpersist(). @property; @handle_py4j; def global_schema(self):; """"""; Returns the signature of the global annotations contained in this VDS. **Examples**. >>> print(vds.global_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.global_schema). :rtype: :class:`.Type`; """""". if self._global_schema is None:; self._global_schema = Type._from_java(self._jvds.globalSignature()); return self._global_schema. @property; @handle_py4j; def colkey_schema(self):; """"""; Returns the signature of the column key (sample) contained in this VDS. **Examples**. >>> print(vds.colkey_schema). The ``pprint`` module can be used to print the schema in a more human-readable format:. >>> from pprint import pprint; >>> pprint(vds.colkey_schema). :rtype: ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:179613,perform,performed,179613,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performed']
Performance,"Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Clumping GWAS Results. View page source. Clumping GWAS Results. Introduction; After performing a genome-wide association study (GWAS) for a given phenotype,; an analyst might want to clump the association results based on the correlation; between variants and p-values. The goal is to get a list of independent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1234,perform,performing,1234,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,4,"['perform', 'scalab']","['performing', 'scalable']"
Performance,"Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) A",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86370,perform,performance,86370,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """""". ht = hl.import_table(; path,; min_partitions=min_partitions,; comment='#',; no_header=True,; types={'f3': hl.tint, 'f4': hl.tint, 'f5': hl.tfloat, 'f7': hl.tint},; missing='.',; delimiter='\t',; force_bgz=force_bgz,; force=force,; ). ht = ht.rename({; 'f0': 'seqname',; 'f1': 'source',; 'f2': 'feature',; 'f3': 'start',; 'f4': 'end',; 'f5': 'score',; 'f6': 'strand',; 'f7': 'frame',; 'f8': 'attribute',; }). def parse_attributes(unparsed_attributes):; def parse_attribute(attribute):; key_and_value = attribute.split(' '); key = k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:3800,load,load,3800,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['load'],['load']
Performance,"Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Filter; Annotate; Select and Transmute; Global Fields; Exercises. Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Filtering and Annotation Tutorial. View page source. Filtering and Annotation Tutorial. Filter; You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True. [1]:. import hail as hl. hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2009-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:44.088 Hail: INFO: Movie Lens files found!. [2]:. users.filter(users.occupation == 'programmer').count(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 66. We can also express this query in multiple ways using aggregations:. [3]:. users.aggregate(hl.agg.filter(users.occupation == 'programmer', hl.agg.count())). [3]:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html:1124,load,load,1124,docs/0.2/tutorials/05-filter-annotate.html,https://hail.is,https://hail.is/docs/0.2/tutorials/05-filter-annotate.html,1,['load'],['load']
Performance,"Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is Fals",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:25117,cache,cache,25117,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['cache'],['cache']
Performance,"The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """""". def __init__(; self,; *,; data_bucket: str,; data_mount: str,; image: str,; regions: List[str],; cloud: str,; data_bucket_is_requester_pays: bool,; ):; self.data_bucket = data_bucket; self.data_mount = data_mount; self.image = image; self.regions = regions; self.env = {}; self.data_bucket_is_requester_pays = data_bucket_is_requester_pays; self.cloud = cloud; self.batch_run_command = ['python3', '/hail-vep/vep.py', 'vep']; self.batch_run_csq_header_command = ['python3', '/hail-vep/vep.py', 'csq_header']; self.json_typ = vep_json_typ. def command(; self,; *,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str,; ) -> str:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f""""""/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; """""". [docs]class VEPConfigGRCh38Version95(VEPConfig):; """"""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is locate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:28361,cache,cache,28361,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['cache'],['cache']
Performance,"\beta^0_v, \\beta^1_v, \\ldots, \\beta^c_v), \sigma_{g_v}^2)`, with :math:`\delta` fixed at :math:`\\hat\delta` in both. The latter fit is simply that of the global model, :math:`((0, \\hat{\\beta}^1, \\ldots, \\hat{\\beta}^c), \\hat{\sigma}_g^2)`. The likelihood ratio test statistic is given by. .. math::. \chi^2 = n \\, \\mathrm{ln}\left(\\frac{\hat{\sigma}^2_g}{\\hat{\sigma}_{g,v}^2}\\right). and follows a chi-squared distribution with one degree of freedom. Here the ratio :math:`\\hat{\sigma}^2_g / \\hat{\sigma}_{g,v}^2` captures the degree to which adding the variant :math:`v` to the global model reduces the residual phenotypic variance. **Kinship Matrix**. FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with :py:meth:`~hail.VariantDataset.rrm`. However, any instance of :py:class:`KinshipMatrix` may be used, so long as ``sample_list`` contains the complete samples of the caller variant dataset in the same order. **Low-rank approximation of kinship for improved performance**. :py:meth:`.lmmreg` can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter ``n_eigs`` to use only the top ``n_eigs`` eigenvectors. Alternatively, specify ``dropped_variance_fraction`` to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, ``dropped_variance_fraction=0.01`` will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two. **Further background**. For the history and mathematics of linear mixed models in genetics, including `FastLMM <https://www.microsoft.com/en-us/research/project/fas",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:136237,perform,performance,136237,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,"^{-5/2}; \end{cases}\]; The estimator for identity-by-descent one is given by:. \[\widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}\]; Note that, even if present, phase information is ignored by this method.; The PC-Relate method is described in “Model-free Estimation of Recent; Genetic Relatedness”. Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the GENESIS Bioconductor package .; pc_relate() differs from the reference implementation in a few; ways:. if k is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples.; the estimators do not perform small sample correction; the algorithm does not provide an option to use population-wide; allele frequency estimates; the algorithm does not provide an option to not use “overall; standardization” (see R pcrelate documentation). Under the PC-Relate model, kinship, \(\phi_{ij}\), ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection.; Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, \(k^{(2)}_{ij}\),; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs.; Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation; “Third degree relatives” are those pairs sharing; \(2^{-3} = 12.5 %\) of their genetic material, the results of; PCRelate are often too noisy to reliably distinguish these pairs from; higher-degree-relative-pair",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:18061,perform,perform,18061,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['perform'],['perform']
Performance,"_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See also; split_multi_hts(). Parameters:. ds (MatrixTable or Table) – An unsplit dataset.; keep_star (bool) – Do not filter out * alleles.; left_aligned (bool) – If True, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle (bool) – If True, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns:; MatrixTable or Table. hail.methods.split_multi_hts(ds, keep_star=False, left_aligned=False, vep_root='vep', *, permit_shuffle=False)[source]; Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema.; struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; MatrixTable.annotate_entries().; Examples; >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; This method assumes ds contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving split_multi_hts; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset.; If each locus in ds contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants.; For example, the following cod",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:86800,throughput,throughput,86800,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Run the logistic regression Wald test per variant using a list of binary (0/1); phenotypes, intercept and two covariates stored in column-indexed; fields:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:48475,perform,performs,48475,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['performs']
Performance,"_format keyword argument. Deprecations. (#12230) The; python-dill Batch images in gcr.io/hail-vdc are no longer; supported. Use hailgenetics/python-dill instead. Bug fixes. (#12215) Fix search; bar in the Hail Batch documentation. Version 0.2.100; Released 2022-09-23. New Features. (#12207) Add support; for the shape aesthetic to hail.ggplot.geom_point. Deprecations. (#12213) The; batch_size parameter of vds.new_combiner is deprecated in; favor of gvcf_batch_size. Bug fixes. (#12216) Fix bug; that caused make install-on-cluster to fail with a message about; sys_platform.; (#12164) Fix bug; that caused Query on Batch pipelines to fail on datasets with indexes; greater than 2GiB. Version 0.2.99; Released 2022-09-13. New Features. (#12091) Teach; Table to write_many, which writes one table per provided; field.; (#12067) Add; rand_int32 and rand_int64 for generating random 32-bit and; 64-bit integers, respectively. Performance Improvements. (#12159) Improve; performance of MatrixTable reads when using _intervals argument. Bug fixes. (#12179) Fix; incorrect composition of interval filters with unordered interval; lists that could lead to over- or under-filtering.; (#12162) Fixed crash; in collect_cols_by_key with preceding random functions. Version 0.2.98; Released 2022-08-22. New Features. (#12062); hl.balding_nichols_model now supports an optional boolean; parameter, phased, to control the phasedness of the generated; genotypes. Performance improvements. (#12099) Make; repeated VCF/PLINK queries much faster by caching compiler data; structures.; (#12038) Speed up; hl.import_matrix_table by caching header line computation. Bug fixes. (#12115) When using; use_new_shuffle=True, fix a bug when there are more than 2^31; rows; (#12074) Fix bug; where hl.init could silently overwrite the global random seed.; (#12079) Fix bug in; handling of missing (aka NA) fields in grouped aggregation and; distinct by key.; (#12056) Fix; hl.export_vcf to actually create tabix f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:46359,perform,performance,46359,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"_right_row_fields=bool); def union_cols(; self, other: 'MatrixTable', row_join_type: str = 'inner', drop_right_row_fields: bool = True; ) -> 'MatrixTable':; """"""Take the union of dataset columns. Warning; -------. This method does not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120046,perform,performed,120046,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['perform'],['performed']
Performance,"` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:166328,load,loadings,166328,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167531,load,loadings,167531,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"`, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167986,load,loadings,167986,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102323,load,load,102323,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"``global.lmmreg.fit.logDeltaGrid`` | Array[Double] | values of :math:`\\mathrm{ln}(\delta)` used in the grid search |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+; | ``global.lmmreg.fit.logLkhdVals`` | Array[Double] | (restricted) log likelihood of :math:`y` given :math:`X` and :math:`\\mathrm{ln}(\delta)` at the (RE)ML fit of :math:`\\beta` and :math:`\sigma_g^2` |; +----------------------------------------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+. These global annotations are also added to ``hail.log``, with the ranked evals and :math:`\delta` grid with values in .tsv tabular form. Use ``grep 'lmmreg:' hail.log`` to find the lines just above each table. If Step 5 is performed, :py:meth:`.lmmreg` also adds four linear regression variant annotations. +------------------------+--------+-------------------------------------------------------------------------+; | Annotation | Type | Value |; +========================+========+=========================================================================+; | ``va.lmmreg.beta`` | Double | fit genotype coefficient, :math:`\hat\\beta_0` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.sigmaG2`` | Double | fit coefficient of genetic variance component, :math:`\hat{\sigma}_g^2` |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.chi2`` | Double | :math:`\chi^2` statistic of the likelihood ratio test |; +------------------------+--------+-------------------------------------------------------------------------+; | ``va.lmmreg.pval`` | Double | :math:`p`-value |; +-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:122434,perform,performed,122434,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performed']
Performance,"a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-val",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:94405,perform,perform,94405,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['perform']
Performance,"a version of Spark. The Cloudera Spark version string is the Spark version string followed by “.cloudera”. For example, to build a Hail JAR compatible with Cloudera Spark version 2.0.2, execute:; ./gradlew shadowJar -Dspark.version=2.0.2.cloudera1. Similarly, a Hail JAR compatible with Cloudera Spark version 2.1.0 is built by executing:; ./gradlew shadowJar -Dspark.version=2.1.0.cloudera1. On a Cloudera cluster, SPARK_HOME should be set as:; SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2,. On Cloudera, you can create an interactive Python shell using pyspark2:; $ pyspark2 --jars build/libs/hail-all-spark.jar \; --py-files build/distributions/hail-python.zip \; --conf spark.sql.files.openCostInBytes=1099511627776 \; --conf spark.sql.files.maxPartitionBytes=1099511627776 \; --conf spark.hadoop.parquet.block.size=1099511627776. Cloudera’s version of spark-submit is called spark2-submit. Running in the cloud¶; Google and Amazon offer optimized Spark performance; and exceptional scalability to many thousands of cores without the overhead; of installing and managing an on-prem cluster.; Hail publishes pre-built JARs for Google Cloud Platform’s Dataproc Spark; clusters. If you would prefer to avoid building Hail from source, learn how to; get started on Google Cloud Platform by reading this forum post. You; can use cloudtools to simplify using; Hail on GCP even further, including via interactive Jupyter notebooks (also discussed here). Building with other versions of Spark 2¶; Hail is compatible with Spark 2.0.x and 2.1.x. To build against Spark 2.1.0,; modify the above instructions as follows:. Set the Spark version in the gradle command; $ ./gradlew -Dspark.version=2.1.0 shadowJar. SPARK_HOME should point to an installation of the desired version of Spark, such as spark-2.1.0-bin-hadoop2.7. The version of the Py4J ZIP file in the hail alias must match the version in $SPARK_HOME/python/lib in your version of Spark. BLAS and LAPACK¶; Hail uses BLAS and LAPACK optimized",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/getting_started.html:6666,optimiz,optimized,6666,docs/0.1/getting_started.html,https://hail.is,https://hail.is/docs/0.1/getting_started.html,3,"['optimiz', 'perform', 'scalab']","['optimized', 'performance', 'scalability']"
Performance,"ability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:40490,perform,performance,40490,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['perform'],['performance']
Performance,"able) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[allele_index - 1] > 0). Update the AC field of the resulting dataset:; >>> updated_info = ds_result.info.annotate(AC =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:20774,latency,latency,20774,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['latency'],['latency']
Performance,"able.; There are only two operations on a grouped table, GroupedTable.partition_hint(); and GroupedTable.aggregate().; Attributes. Methods. aggregate; Aggregate by group, used after Table.group_by(). partition_hint; Set the target number of partitions for aggregation. aggregate(**named_exprs)[source]; Aggregate by group, used after Table.group_by().; Examples; Compute the mean value of X and the sum of Z per unique ID:; >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:; >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; The resulting table has a key field for each group and a value field for; each aggregation. The names of the aggregation expressions must be; distinct from the names of the groups. Parameters:; named_exprs (varargs of Expression) – Aggregation expressions. Returns:; Table – Aggregated table. partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a Table.group_by() / GroupedTable.aggregate(); pipeline:; >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedTable.aggregate() is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedTable – Same grouped table with a partition hint. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedTable.html:2095,optimiz,optimizer,2095,docs/0.2/hail.GroupedTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedTable.html,1,['optimiz'],['optimizer']
Performance,"agg.sum(ht.global_value * ht.a)); 30. Warning; Parallelizing very large local arrays will be slow. Parameters:. rows – List of row values, or expression of type array<struct{...}>.; schema (str or a hail type (see Types), optional) – Value type.; key (Union[str, List[str]]], optional) – Key field(s).; n_partitions (int, optional); partial_type (dict, optional) – A value type which may elide fields or have None in arbitrary places. The partial; type is used by hail where the type cannot be imputed.; globals (dict of str to any or StructExpression, optional) – A dict or struct{..} containing supplementary global data. Returns:; Table – A distributed Hail table created from the local collection of rows. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the table to both memory and disk:; >>> table = table.persist() . Notes; The Table.persist() and Table.cache() methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for Table.write(), which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; Table – Persisted table. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:54073,cache,cache,54073,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"ail's initial version of :py:meth:`.lmmreg` scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used :py:meth:`.lmmreg` in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on `Google cloud <http://discuss.hail.is/t/using-hail-on-the-google-cloud-platform/80>`__. While :py:meth:`.lmmreg` computes the kinship matrix :math:`K` using distributed matrix multiplication (Step 2), the full `eigendecomposition <https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix>`__ (Step 3) is currently run on a single core of master using the `LAPACK routine DSYEVD <http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_ga694ddc6e5527b6223748e3462013d867.html>`__, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in :math:`n` are available `here <https://github.com/hail-is/hail/pull/906>`__. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see ""BLAS and LAPACK"" in Getting Started). Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector :math:`v` by the matrix of eigenvectors :math:`U^T` as described below, which we accelerate with a sparse representation of :math:`v`. The matrix :math:`U^T` has size about :math:`8n^2` bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing :math:`U^T` consumes about 3.6GB o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:125277,perform,performant,125277,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['perform'],"['performance', 'performant']"
Performance,"ail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9894,load,loaded,9894,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['load'],['loaded']
Performance,"aively decrease the number of partitions.; Example; Naively repartition to 10 partitions:; >>> dataset_result = dataset.naive_coalesce(10). Warning; naive_coalesce() simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; repartition(), so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters:; max_partitions (int) – Desired number of partitions. If the current number of partitions is; less than or equal to max_partitions, do nothing. Returns:; MatrixTable – Matrix table with at most max_partitions partitions. persist(storage_level='MEMORY_AND_DISK')[source]; Persist this table in memory or on disk.; Examples; Persist the dataset to both memory and disk:; >>> dataset = dataset.persist() . Notes; The MatrixTable.persist() and MatrixTable.cache(); methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for Table.write(),; which stores a permanent file.; Most users should use the “MEMORY_AND_DISK” storage level. See the Spark; documentation; for a more in-depth discussion of persisting data. Parameters:; storage_level (str) – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns:; MatrixTable – Persisted dataset. rename(fields)[source]; Rename fields of a matrix table.; Examples; Rename column key s to SampleID, still keying by SampleID.; >>> dataset_result = dataset.rename({'s': 'SampleID'}). You can rename a field to a field name that already exists, as long as; that field also gets renamed (no name collisions). Here, we rename the; column key s to info, and the row field info to vcf_info:; >>> dataset_result =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:49723,cache,cache,49723,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"alue_ir, res_ir, is_scan=True); else:; res_ir = ir.Let(uid, value_ir, res_ir). return construct_expr(res_ir, lambda_result.dtype, indices, aggregations). [docs]def rbind(*exprs, _ctx=None):; """"""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """""". *args, f = exprs; args = [expr_any.check(arg, 'rbind', f'argument {index}') for index, arg in builtins.enumerate(args)]. return hl.bind(f, *args, _ctx=_ctx). [docs]@typecheck(c1=expr_int32, c2=expr_int32, c3=expr_int32, c4=expr_int32); def chi_squared_test(c1, c2, c3, c4) -> StructExpression:; """"""Performs chi-squared test of independence on a 2x2 contingency table. Examples; --------. >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odds_ratio=1.0). >>> hl.eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; -----; The odds ratio is given by ``(c1 / c2) / (c3 / c4)``. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(p_value=tfloat64, o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:19182,Perform,Performs,19182,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Perform'],['Performs']
Performance,"amples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; 'Population': str; 'SuperPopulation': str; 'isFemale': bool; 'PurpleHair': bool; 'CaffeineConsumption': int32; 'sample_qc': struct {; dp_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64; },; call_rate: float64,; n_called: int64,; n_not_called: int64,; n_filtered: int64,; n_hom_ref: int64,; n_het: int64,; n_hom_var: int64,; n_non_ref: int64,; n_singleton: int64,; n_snp: int64,; n_insertion: int64,; n_deletion: int64,; n_transition: int64,; n_transversion: int64,; n_star: int64,; r_ti_tv: float64,; r_het_hom_var: float64,; r_insertion_deletion: float64; }",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:1860,load,load,1860,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['load'],['load']
Performance,"an join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new joined table j has a field t2_x that comes from the rows of t2. The tables could be joined, because they shared the same number of keys (1) and the same key type (string). The keys do not need to share the same name. Notice that the rows with keys present in t2 but not in t1 do not show up in the final result.; This join syntax performs a left join. Tables also have a SQL-style inner/left/right/outer join() method.; The magic of keys is that they can be used to create a mapping, like a Python dictionary, between the keys of one table and the row values of another table: table[expr] will refer to the row of table that has a key value of expr. If the row is not unique, one such row is chosen arbitrarily.; Here’s a subtle bit: if expr is an expression indexed by a row of table2, then table[expr] is also an expression indexed by a row of table2.; Also note that while they look similar, table['field'] and table1[table2.key] are doing very different things!; table['field'] selects a field from the table, while table1[table2.key] creates a mapping between the keys of table2 and the rows of table1. [9]:. t1['a'].describe(). --------------------------------------------------------; Type:; str; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5bee73d130>; Index:; ['row']",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:5299,perform,performs,5299,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['perform'],['performs']
Performance,"an; min_kinship are excluded from the results.; statistics (str) – the set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For eac",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138389,load,loadings,138389,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"ance=numeric,; sample_file=nullable(strlike),; min_partitions=nullable(integral)); def import_bgen(self, path, tolerance=0.2, sample_file=None, min_partitions=None):; """"""Import .bgen file(s) as variant dataset. **Examples**. Importing a BGEN file as a VDS (assuming it has already been indexed). >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). **Notes**. Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see `here <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only **unphased** and **diploid** genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed. Before importing, ensure that:. - The sample file has the same number of samples as the BGEN file.; - No duplicate sample IDs are present. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. .. _gpfilters:. **Genotype probability (``gp``) representation**:. The following modifications are made to genotype probabilities in BGEN v1.1 files:. - Since genotype probabilities are understood to define a probability distribution, :py:meth:`~hail.HailContext.import_bgen` automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the ``tolerance`` parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains. - :py:meth:`~hail.HailContext.import_bgen` normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. **Annotations**. :py:meth:`~hail.HailContext.import_bgen` adds the following variant annotations:. - **va.varid** (*String*) -- 2nd column of .gen file if chromosome present, otherwise 1st column. - **va.rsid** (*St",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:6373,load,load,6373,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"and aggregate to produce a new table:; >>> table3 = (table1.group_by(table1.SEX); ... .aggregate(mean_height_data = hl.agg.mean(table1.HT))); >>> table3.show(). Join tables together inside an annotation expression:; >>> table2 = table2.key_by('ID'); >>> table1 = table1.annotate(B = table2[table1.ID].B); >>> table1.show(). Attributes. globals; Returns a struct expression including all global fields. key; Row key struct. row; Returns a struct expression of all row-indexed fields, including keys. row_value; Returns a struct expression including all non-key row-indexed fields. Methods. add_index; Add the integer index of each row as a new row field. aggregate; Aggregate over rows into a local value. all; Evaluate whether a boolean expression is true for all rows. annotate; Add new fields. annotate_globals; Add new global fields. anti_join; Filters the table to rows whose key does not appear in other. any; Evaluate whether a Boolean expression is true for at least one row. cache; Persist this table in memory. checkpoint; Checkpoint the table to disk by writing and reading. collect; Collect the rows of the table into a local list. collect_by_key; Collect values for each unique key into an array. count; Count the number of rows in the table. describe; Print information about the fields in the table. distinct; Deduplicate keys, keeping exactly one row for each unique key. drop; Drop fields from the table. expand_types; Expand complex types into structs and arrays. explode; Explode rows along a field of type array or set, copying the entire row for each element. export; Export to a text file. filter; Filter rows conditional on the value of each row's fields. flatten; Flatten nested structs. from_pandas; Create table from Pandas DataFrame. from_spark; Convert PySpark SQL DataFrame to a table. group_by; Group by a new key for use with GroupedTable.aggregate(). head; Subset table to first n rows. index; Expose the row values as if looked up in a dictionary, indexing with exprs. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:3639,cache,cache,3639,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['cache'],['cache']
Performance,"ange this behavior. Annotations; The below annotations can be accessed with sa.imputesex. isFemale (Boolean) – True if the imputed sex is female, false if male, missing if undetermined; Fstat (Double) – Inbreeding coefficient; nTotal (Long) – Total number of variants considered; nCalled (Long) – Number of variants with a genotype call; expectedHoms (Double) – Expected number of homozygotes; observedHoms (Long) – Observed number of homozygotes. Parameters:; maf_threshold (float) – Minimum minor allele frequency threshold.; include_par (bool) – Include pseudoautosomal regions.; female_threshold (float) – Samples are called females if F < femaleThreshold; male_threshold (float) – Samples are called males if F > maleThreshold; pop_freq (str) – Variant annotation for estimate of MAF.; If None, MAF will be computed. Returns:Annotated dataset. Return type:VariantDataset. join(right)[source]¶; Join two variant datasets.; Notes; This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self).; The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). Parameters:right (VariantDataset) – right-hand variant dataset. Returns:Joined variant dataset. Return type:VariantDataset. ld_matrix(force_local=False)[source]¶; Computes the linkage disequilibrium (correlation) matrix for the variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries be",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:73696,perform,performs,73696,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance,"annotate_rows(__AC=agg.sum(mt.__gt), __n_called=agg.count_where(hl.is_defined(mt.__gt))); mt = mt.filter_rows((mt.__AC > 0) & (mt.__AC < 2 * mt.__n_called)). n_variants = mt.count_rows(); if n_variants == 0:; raise FatalError(""hwe_normalize: found 0 variants after filtering out monomorphic sites.""); info(f""hwe_normalize: found {n_variants} variants after filtering out monomorphic sites.""). mt = mt.annotate_rows(__mean_gt=mt.__AC / mt.__n_called); mt = mt.annotate_rows(__hwe_scaled_std_dev=hl.sqrt(mt.__mean_gt * (2 - mt.__mean_gt) * n_variants / 2)); mt = mt.unfilter_entries(). normalized_gt = hl.or_else((mt.__gt - mt.__mean_gt) / mt.__hwe_scaled_std_dev, 0.0); return normalized_gt. [docs]@typecheck(call_expr=expr_call, k=int, compute_loadings=bool); def hwe_normalized_pca(call_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on the Hardy-Weinberg-normalized; genotype call matrix. Examples; --------. >>> eigenvalues, scores, loadings = hl.hwe_normalized_pca(dataset.GT, k=5). Notes; -----; This method specializes :func:`.pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:2110,load,loadings,2110,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"ap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\); are then mean-centered and variance-normalized as. \[M_{ij} = \frac{C_{ij}-2p_j}{\s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:30712,load,loadings,30712,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,"are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:. >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; -------; :func:`.logistic_regression_rows` considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which **all** response variables and covariates are defined. For each row, missing values of; `x` are mean-imputed over these columns. As in the example, the; intercept covariate ``1`` must be included **explicitly** if desired. Notes; -----; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively. Hail supports the Wald test ('wald'), likelihood ratio test ('lrt'),; Rao score test ('score'), and Firth test ('firth'). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values. The example above considers a model of the form. .. math::. \mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2). where :math:`\mathrm{sigmoid}` is the `sigmoid function`_, the genotype; :math:`\mathrm{gt}` is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:27985,perform,performs,27985,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['performs']
Performance,"are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A p",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45649,load,loaded,45649,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"arget** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). t = import_table(; path,; comment=""@"",; impute=False,; no_header=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; **kwargs,; ). if t.row.dtype == tstruct(f0=tstr):; if reference_genome:; t = t.select(interval=hl.parse_locus_interval(t['f0'], reference_genome)); else:; interval_regex = r""([^:]*):(\d+)\-(\d+)"". def checked_match",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:25602,load,loaded,25602,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['loaded']
Performance,"arly uncorrelated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. realized_relationship_matrix(call_expr); Computes the realized relationship matrix (RRM). sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. Relatedness; Hail provides three methods for the inference of relatedness: PLINK-style; identity by descent [1], KING [2], and PC-Relate [3]. identity_by_descent() is appropriate for datasets containing one; homogeneous population.; king() is appropriate for datasets containing multiple homogeneous; populations and no admixture. It is also used to prune close relatives before; using pc_relate().; pc_relate() is appropriate for datasets containing multiple homogeneous; populations and admixture. identity_by_d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/index.html:6289,throughput,throughput,6289,docs/0.2/methods/index.html,https://hail.is,https://hail.is/docs/0.2/methods/index.html,1,['throughput'],['throughput']
Performance,"artitions=None)[source]; Construct a table with the row index and no other fields.; Examples; >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; The resulting table contains one field:. idx (tint32) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n (int) – Number of rows.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; Table. hail.utils.range_matrix_table(n_rows, n_cols, n_partitions=None)[source]; Construct a matrix table with row and column indices and no entry fields.; Examples; >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; The resulting matrix table contains the following fields:. row_idx (tint32) - Row index (row key).; col_idx (tint32) - Column index (column key). It contains no entry fields.; This method is meant for testing and learning, and is not optimized for; production performance. Parameters:. n_rows (int) – Number of rows.; n_cols (int) – Number of columns.; n_partitions (int, optional) – Number of partitions (uses Spark default parallelism if None). Returns:; MatrixTable. hail.utils.get_1kg(output_dir, overwrite=False)[source]; Download subset of the 1000 Genomes; dataset and sample annotations.; Notes; The download is about 15M. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_hgdp(output_dir, overwrite=False)[source]; Download subset of the Human Genome Diversity Panel; dataset and sample annotations.; Notes; The download is about 30MB. Parameters:. output_dir – Directory in which to write data.; overwrite – If True, overwrite any existing files/directories at output_dir. hail.utils.get_movie_lens(output_dir, overwrite=False)[source]; Download public Movie Lens dataset.; Notes; The download is about 6M.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/utils/index.html:10754,optimiz,optimized,10754,docs/0.2/utils/index.html,https://hail.is,https://hail.is/docs/0.2/utils/index.html,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"ass:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True); ; Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; ; >>> kt = hc.import_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:76379,latency,latency,76379,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['latency'],['latency']
Performance,"ated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102413,load,load,102413,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"ated within each window. compute_charr(ds[, min_af, max_af, min_dp, ...]); Compute CHARR, the DNA sample contamination estimator. mendel_errors(call, pedigree); Find Mendel errors; count per variant, individual and nuclear family. de_novo(mt, pedigree, pop_frequency_prior, *); Call putative de novo events from trio data. nirvana(dataset, config[, block_size, name]); Annotate variants using Nirvana. sample_qc(mt[, name]); Compute per-sample metrics useful for quality control. _logistic_skat(group, weight, y, x, covariates); The logistic sequence kernel association test (SKAT). skat(key_expr, weight_expr, y, x, covariates); Test each keyed group of rows for association by linear or logistic SKAT test. lambda_gc(p_value[, approximate]); Compute genomic inflation factor (lambda GC) from an Expression of p-values. split_multi(ds[, keep_star, left_aligned, ...]); Split multiallelic variants. split_multi_hts(ds[, keep_star, ...]); Split multiallelic variants for datasets that contain one or more fields from a standard high-throughput sequencing entry schema. summarize_variants(mt[, show, handler]); Summarize the variants present in a dataset and print the results. transmission_disequilibrium_test(dataset, ...); Performs the transmission disequilibrium test on trios. trio_matrix(dataset, pedigree[, complete_trios]); Builds and returns a matrix where columns correspond to trios and entries contain genotypes for the trio. variant_qc(mt[, name]); Compute common variant statistics (quality control metrics). vep(dataset[, config, block_size, name, ...]); Annotate variants with VEP. class hail.methods.VEPConfig[source]; Base class for configuring VEP.; To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from VEPConfig; and has the following parameters defined:. json_type (HailType): The type of the VEP JSON schema (as produced by VEP when invoked with the –json option).; data_bucket (str) – The location where the VEP data is stored.; da",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:3159,throughput,throughput,3159,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"ath}'); print('An old version of this state may be there.'); print(; 'Dumping current state as json to standard output, you may wish '; 'to save this output in order to resume the combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; interval",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:12926,load,load,12926,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,3,"['Load', 'load']","['Load', 'load']"
Performance,"ation is a ``Set`` and can be queried for filter membership with expressions ; like ``va.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged as ""PASS"" ; will have no filters applied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:24750,load,load,24750,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"ation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the significance of x or more successes, and; - 'two-sided': a two-tailed test of the significance of x or any equivalent or more unlikely outcome.; Examples; All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads.; Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:; >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12569,perform,perform,12569,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['perform']
Performance,"ations if x.startswith('va.vep.')]); if subset:; self = self.annotate_variants_expr('va.vep = select(va.vep, {})'.format(subset)). # iterate through files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant c",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40810,cache,cache,40810,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"bal.lmmreg.evals; Array[Double]; all eigenvalues of the kinship matrix in descending order. global.lmmreg.fit.seH2; Double; standard error of \(\hat{h}^2\) under asymptotic normal approximation. global.lmmreg.fit.normLkhdH2; Array[Double]; likelihood function of \(h^2\) normalized on the discrete grid 0.01, 0.02, ..., 0.99. Index i is the likelihood for percentage i. global.lmmreg.fit.maxLogLkhd; Double; (restricted) maximum log likelihood corresponding to \(\hat{\delta}\). global.lmmreg.fit.logDeltaGrid; Array[Double]; values of \(\mathrm{ln}(\delta)\) used in the grid search. global.lmmreg.fit.logLkhdVals; Array[Double]; (restricted) log likelihood of \(y\) given \(X\) and \(\mathrm{ln}(\delta)\) at the (RE)ML fit of \(\beta\) and \(\sigma_g^2\). These global annotations are also added to hail.log, with the ranked evals and \(\delta\) grid with values in .tsv tabular form. Use grep 'lmmreg:' hail.log to find the lines just above each table.; If Step 5 is performed, lmmreg() also adds four linear regression variant annotations. Annotation; Type; Value. va.lmmreg.beta; Double; fit genotype coefficient, \(\hat\beta_0\). va.lmmreg.sigmaG2; Double; fit coefficient of genetic variance component, \(\hat{\sigma}_g^2\). va.lmmreg.chi2; Double; \(\chi^2\) statistic of the likelihood ratio test. va.lmmreg.pval; Double; \(p\)-value. Those variants that don’t vary across the included samples (e.g., all genotypes; are HomRef) will have missing annotations.; The simplest way to export all resulting annotations is:; >>> lmm_vds.export_variants('output/lmmreg.tsv.bgz', 'variant = v, va.lmmreg.*'); >>> lmmreg_results = lmm_vds.globals['lmmreg']. By default, genotypes values are given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:95097,perform,performed,95097,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performed']
Performance,"before converting to; DataFrame.; flatten (bool) – If true, flatten before converting to; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Return type:pyspark.sql.DataFrame. to_pandas(expand=True, flatten=True)[source]¶; Converts this key table into a Pandas DataFrame. Parameters:; expand (bool) – If true, expand_types before converting to; Pandas DataFrame.; flatten (bool) – If true, flatten before converting to Pandas; DataFrame. If both are true, flatten is run after expand so; that expanded types are flattened. Returns:Pandas DataFrame constructed from the key table. Return type:pandas.DataFrame. union(*kts)[source]¶; Union the rows of multiple tables.; Examples; Take the union of rows from two tables:; >>> other = hc.import_table('data/kt_example1.tsv', impute=True); >>> union_kt = kt1.union(other). Notes; If a row appears in both tables identically, it is duplicated in; the result. The left and right tables must have the same schema; and key. Parameters:kts (args of type KeyTable) – Tables to merge. Returns:A table with all rows from the left and right tables. Return type:KeyTable. unpersist()[source]¶; Unpersists this table from memory/disk.; Notes; This function will have no effect on a table that was not previously persisted.; There’s nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist; a table when you are done with it. write(output, overwrite=False)[source]¶; Write as KT file.; *Examples*; >>> kt1.write('output/kt1.kt'). Note; The write path must end in “.kt”. Parameters:; output (str) – Path of KT file to write.; overwrite (bool) – If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. Next ; Previous. © Copyright 2016, Hail Team. . Built with Sphinx using a theme provided by Read the Docs. . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:29871,perform,performed,29871,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['perform'],['performed']
Performance,"ble Overview. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; Expressions; Tables; Import; Global Fields; Keys; Referencing Fields; Updating Fields; Aggregation; Joins; Interacting with Tables Locally. MatrixTables. How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Overview; Table Overview. View page source. Table Overview; A Table is the Hail equivalent of a SQL table, a Pandas Dataframe, an; R Dataframe, a dyplr Tibble, or a Spark Dataframe. It consists of rows of data; conforming to a given schema where each column (row field) in the dataset is of; a specific type. Import; Hail has functions to create tables from a variety of data sources.; The most common use case is to load data from a TSV or CSV file, which can be; done with the import_table() function.; >>> ht = hl.import_table(""data/kt_example1.tsv"", impute=True). Examples of genetics-specific import methods are; import_locus_intervals(), import_fam(), and import_bed().; Many Hail methods also return tables.; An example of a table is below. We recommend ht as a variable name for; tables, referring to a “Hail table”.; >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Global Fields; In addition to row fields, Hail tables also have global fie",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/table.html:988,load,load,988,docs/0.2/overview/table.html,https://hail.is,https://hail.is/docs/0.2/overview/table.html,1,['load'],['load']
Performance,"ble, _localize_global_statistics=bool); def concordance(left, right, *, _localize_global_statistics=True) -> Tuple[List[List[int]], Table, Table]:; """"""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or filtered entry); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var. The first index is the state in the left dataset and the secon",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:14193,perform,performs,14193,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,2,['perform'],['performs']
Performance,"ble1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""; return Env.backend().persist(self). [docs] def unpersist(self) -> 'Table':; """"""; Unpersists this table from memory/disk. Notes; -----; This function will have no effect on a table that was not previously; persisted. Returns; -------; :class:`.Table`; Unpersisted table.; """"""; return Env.backend().",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:80028,cache,cache,80028,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,4,"['cache', 'perform']","['cache', 'performance']"
Performance,"blocks according to user equivalence criteria. lgt_to_gt(lgt, la); Transform LGT into GT using local alleles array. local_to_global(array, local_alleles, ...); Reindex a locally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain info",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:3926,scalab,scalable,3926,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"brium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""; ret_type = tstruct(het_freq_hwe=tfloat64, p_value=tfloat64); return _func(""hardy_weinberg_test"", ret_type, n_hom_ref, n_het, n_hom_var, one_sided). [docs]@typecheck(contig=expr_str, pos=expr_int32, reference_genome=reference_genome_type); def locus(contig, pos, reference_genome: Union[str, ReferenceGenome] = 'default') -> LocusExpression:; """"""Construct a locus expression from a chromosome and position. Examples; --------. >>> hl.eval(hl.locus(""1"", 10000, reference_genome='GRCh37')); Locus(contig=1, position=10000, reference_genome=GRCh37). Parameters; ----------; contig : str or :class:`.StringExpression`; Chromosome.; pos : int or :class:`.Expression` of type :py:data:`.tint32`; Base position along the chromosome.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:35469,perform,perform,35469,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['perform'],['perform']
Performance,"bucket1"", ""bucket2""].; copy_spark_log_on_error (bool, optional) – Spark backend only. If True, copy the log from the spark driver node to tmp_dir on error. hail.asc(col)[source]; Sort by col ascending. hail.desc(col)[source]; Sort by col descending. hail.stop()[source]; Stop the currently running Hail session. hail.spark_context()[source]; Returns the active Spark context. Returns:; pyspark.SparkContext. hail.tmp_dir()[source]; Returns the Hail shared temporary directory. Returns:; str. hail.default_reference(new_default_reference=None)[source]; With no argument, returns the default reference genome ('GRCh37' by default).; With an argument, sets the default reference genome to the argument. Returns:; ReferenceGenome. hail.get_reference(name)[source]; Returns the reference genome corresponding to name.; Notes; Hail’s built-in references are 'GRCh37', GRCh38', 'GRCm38', and; 'CanFam3'.; The contig names and lengths come from the GATK resource bundle:; human_g1k_v37.dict; and Homo_sapiens_assembly38.dict.; If name='default', the value of default_reference() is returned. Parameters:; name (str) – Name of a previously loaded reference genome or one of Hail’s built-in; references: 'GRCh37', 'GRCh38', 'GRCm38', 'CanFam3', and; 'default'. Returns:; ReferenceGenome. hail.set_global_seed(seed)[source]; Deprecated.; Has no effect. To ensure reproducible randomness, use the global_seed; argument to init() and reset_global_randomness().; See the random functions reference docs for more. Parameters:; seed (int) – Integer used to seed Hail’s random number generator. hail.reset_global_randomness()[source]; Restore global randomness to initial state for test reproducibility. hail.citation(*, bibtex=False)[source]; Generate a Hail citation. Parameters:; bibtex (bool) – Generate a citation in BibTeX form. Returns:; str. hail.version()[source]; Get the installed Hail version. Returns:; str. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/api.html:8231,load,loaded,8231,docs/0.2/api.html,https://hail.is,https://hail.is/docs/0.2/api.html,1,['load'],['loaded']
Performance,bute). aggregate_by_key() (hail.KeyTable method). (hail.VariantDataset method). allele() (hail.representation.Variant method). alt (hail.representation.AltAllele attribute). alt() (hail.representation.Variant method). alt_allele() (hail.representation.Variant method). alt_alleles (hail.representation.Variant attribute). AltAllele (class in hail.representation). annotate() (hail.KeyTable method). annotate_alleles_expr() (hail.VariantDataset method). annotate_genotypes_expr() (hail.VariantDataset method). annotate_global() (hail.VariantDataset method). annotate_global_expr() (hail.VariantDataset method). annotate_samples_expr() (hail.VariantDataset method). annotate_samples_table() (hail.VariantDataset method). annotate_variants_db() (hail.VariantDataset method). annotate_variants_expr() (hail.VariantDataset method). annotate_variants_table() (hail.VariantDataset method). annotate_variants_vds() (hail.VariantDataset method). B. balding_nichols_model() (hail.HailContext method). C. cache() (hail.KeyTable method). (hail.VariantDataset method). Call (class in hail.representation). category() (hail.representation.AltAllele method). colkey_schema (hail.VariantDataset attribute). collect() (hail.KeyTable method). columns (hail.KeyTable attribute). complete_trios() (hail.representation.Pedigree method). concordance() (hail.VariantDataset method). contains() (hail.representation.Interval method). contig (hail.representation.Locus attribute). (hail.representation.Variant attribute). count() (hail.KeyTable method). (hail.VariantDataset method). count_variants() (hail.VariantDataset method). D. deduplicate() (hail.VariantDataset method). delete_va_attribute() (hail.VariantDataset method). dosage() (hail.representation.Genotype method). dp (hail.representation.Genotype attribute). drop() (hail.KeyTable method). drop_samples() (hail.VariantDataset method). drop_variants() (hail.VariantDataset method). E. end (hail.representation.Interval attribute). eval_expr() (hail.HailContext me,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/genindex.html:1411,cache,cache,1411,docs/0.1/genindex.html,https://hail.is,https://hail.is/docs/0.1/genindex.html,1,['cache'],['cache']
Performance,"by_descent(dataset, maf=None, bounded=True, min=None, max=None) -> Table:; """"""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Fo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html:2169,perform,perform,2169,docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/identity_by_descent.html,2,['perform'],['perform']
Performance,"c, vds._jvdf.toVDS()); return func(coerced_vds, *args, **kwargs); else:; raise TypeError(""genotype signature must be Genotype, but found '%s'"" % type(vds.genotype_schema)). return func(vds, *args, **kwargs). @decorator; def convertVDS(func, vds, *args, **kwargs):; if vds._is_generic_genotype:; if isinstance(vds.genotype_schema, TGenotype):; vds = VariantDataset(vds.hc, vds._jvdf.toVDS()). return func(vds, *args, **kwargs). vds_type = lazy(). [docs]class VariantDataset(object):; """"""Hail's primary representation of genomic data, a matrix keyed by sample and variant. Variant datasets may be generated from other formats using the :py:class:`.HailContext` import methods,; constructed from a variant-keyed :py:class:`KeyTable` using :py:meth:`.VariantDataset.from_table`,; and simulated using :py:meth:`~hail.HailContext.balding_nichols_model`. Once a variant dataset has been written to disk with :py:meth:`~hail.VariantDataset.write`,; use :py:meth:`~hail.HailContext.read` to load the variant dataset into the environment. >>> vds = hc.read(""data/example.vds""). :ivar hc: Hail Context.; :vartype hc: :class:`.HailContext`; """""". def __init__(self, hc, jvds):; self.hc = hc; self._jvds = jvds. self._globals = None; self._sample_annotations = None; self._colkey_schema = None; self._sa_schema = None; self._rowkey_schema = None; self._va_schema = None; self._global_schema = None; self._genotype_schema = None; self._sample_ids = None; self._num_samples = None; self._jvdf_cache = None. [docs] @staticmethod; @handle_py4j; @typecheck(table=KeyTable); def from_table(table):; """"""Construct a sites-only variant dataset from a key table. **Examples**. Import a text table and construct a sites-only VDS:. >>> table = hc.import_table('data/variant-lof.tsv', types={'v': TVariant()}).key_by('v'); >>> sites_vds = VariantDataset.from_table(table). **Notes**. The key table must be keyed by one column of type :py:class:`.TVariant`. All columns in the key table become variant annotations in the result.;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:2004,load,load,2004,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['load']
Performance,"cache_memory_in_bytes=nullable(int)); def to_table_row_major(self, n_partitions=None, maximum_cache_memory_in_bytes=None):; """"""Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; - **row_idx** (:py:data.`tint64`, key field) -- Row index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""; path = new_temp_file(); if maximum_cache_memory_in_bytes and maximum_cache_memory_in_bytes > (1 << 31) - 1:; raise ValueError(; f'maximum_cache_memory_in_bytes must be less than 2^31 -1, was: {maximum_cache_memory_in_bytes}'; ). self.write(path, overwrite=True, force_row_major=True); reader = TableFromBlockMatrixNativeReader(path, n_partitions, maximum_cache_memory_in_bytes); return Tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:55455,cache,cache,55455,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['cache'],['cache']
Performance,"ce]; Calculate call concordance with another dataset. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Note; Requires the dataset to contain only diploid and unphased genotype calls.; Use call() to recode genotype calls or missing() to set genotype; calls to missing. Examples; Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:; >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; This method computes the genotype call concordance (from the entry; field GT) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be diploid and unphased.; It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with “no data”. For example, if a variant is only in one dataset,; then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. No Data (missing variant or filtered en",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:16650,perform,performs,16650,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['performs']
Performance,"cf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45542,load,load,45542,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ch should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig re",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101742,optimiz,optimization,101742,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886); Fix BlockMatrix conversion methods to correctly handle filtered; entries.; (#5884)(#4874); Fix longstanding crash when reading Hail data files under certain; conditions.; (#5855)(#5786); Fix hl.mendel_errors incorrectly reporting children counts in the; presence of entry filtering.; (#5830)(#5835); Fix Nirvana support; (#5773) Fix; hl.sample_qc to use correct number of total rows when calculating; call rate.; (#5763)(#5764); Fix hl.agg.array_agg to work inside mt.annotate_rows and; similar functions.; (#5770) Hail n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95754,perform,performance,95754,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,ckMatrix from_numpy correctness bug; Bug fixes; Versioning. Version 0.2.89; Version 0.2.88; Version 0.2.87; Bug fixes. Version 0.2.86; Bug fixes; Performance improvements. Version 0.2.85; Bug fixes; New features. Version 0.2.84; Bug fixes; New features. Version 0.2.83; Bug fixes; New features; hailctl dataproc. Version 0.2.82; Bug fixes; New features; Performance Improvements; Python and Java Support; File Format. Version 0.2.81; hailctl dataproc. Version 0.2.80; New features; hailctl dataproc. Version 0.2.79; Bug fixes; New features. Version 0.2.78; Bug fixes; New features; Performance Improvements. Version 0.2.77; Bug fixes. Version 0.2.76; Bug fixes. Version 0.2.75; Bug fixes; New features; Performance improvements. Version 0.2.74; Bug fixes. Version 0.2.73; Bug fixes. Version 0.2.72; New Features; Bug fixes. Version 0.2.71; New Features; Bug fixes; hailctl dataproc. Version 0.2.70; Version 0.2.69; New Features; Bug fixes; hailctl dataproc. Version 0.2.68; Version 0.2.67; Critical performance fix. Version 0.2.66; New features. Version 0.2.65; Default Spark Version Change; New features; Performance improvements; Bug fixes. Version 0.2.64; New features; Bug fixes. Version 0.2.63; Bug fixes; Performance Improvements. Version 0.2.62; New features; Bug fixes; Performance improvements. Version 0.2.61; New features; Bug fixes. Version 0.2.60; New features; Bug fixes; hailctl dataproc. Version 0.2.59; Datasets / Annotation DB; hailctl dataproc. Version 0.2.58; New features; Bug fixes; Performance improvements; hailctl dataproc; Deprecations. Version 0.2.57; New features. Version 0.2.56; New features; Performance; Bug fixes; hailctl dataproc. Version 0.2.55; Performance; Bug fixes; File Format. Version 0.2.54; VCF Combiner; New features; Bug fixes. Version 0.2.53; Bug fixes. Version 0.2.52; Bug fixes. Version 0.2.51; Bug fixes. Version 0.2.50; Bug fixes; New features. Version 0.2.49; Bug fixes. Version 0.2.48; Bug fixes. Version 0.2.47; Bug fixes. Version 0.2.46; Site; Bug,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:3693,perform,performance,3693,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ckpointing; Add Batching of Jobs; Synopsis. Reference (Python API); Configuration Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Cookbooks; Random Forest Model. View page source. Random Forest Model. Introduction; We want to use a random forest model to predict regional mutability of; the genome (at a scale of 50kb) using a series of genomic features. Specifically,; we divide the genome into non-overlapping 50kb windows and we regress; the observed/expected variant count ratio (which indicates the mutability; of a specific window) against a number of genomic features measured on each; corresponding window (such as replication timing, recombination rate, and; various histone marks). For each window under investigation, we fit the; model using all the rest of the windows and then apply the model to; that window to predict its mutability as a function of its genomic features.; To perform this analysis with Batch, we will first use a PythonJob; to execute a Python function directly for each window of interest. Next,; we will add a mechanism for checkpointing files as the number of windows; of interest is quite large (~52,000). Lastly, we will add a mechanism to batch windows; into groups of 10 to amortize the amount of time spent copying input; and output files compared to the time of the actual computation per window; (~30 seconds). Batch Code. Imports; We import all the modules we will need. The random forest model code comes; from the sklearn package.; import hailtop.batch as hb; import hailtop.fs as hfs; from hailtop.utils import grouped; import pandas as pd; from typing import List, Optional, Tuple; import argparse; import sklearn. Random Forest Function; The inputs to the random forest function are two data frame files. df_x; is the path to a file containing a Pandas data frame where the variables; in the data frame represent the number of genomic features measured on each; corresponding window. df_y is the path to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/random_forest.html:1224,perform,perform,1224,docs/batch/cookbook/random_forest.html,https://hail.is,https://hail.is/docs/batch/cookbook/random_forest.html,2,['perform'],['perform']
Performance,"cols(), anti_join_rows(). anti_join_rows(other)[source]; Filters the table to rows whose key does not appear in other. Parameters:; other (Table) – Table with compatible key field(s). Returns:; MatrixTable. Notes; The row key type of the matrix table must match the key type of other.; This method does not change the schema of the table; it is a method of; filtering the matrix table to row keys not present in another table.; To restrict to rows whose key is present in other, use; semi_join_rows().; Examples; >>> ds_result = ds.anti_join_rows(rows_to_remove). It may be expensive to key the matrix table by the right-side key.; In this case, it is possible to implement an anti-join using a non-key; field as follows:; >>> ds_result = ds.filter_rows(hl.is_missing(rows_to_remove.index(ds['locus'], ds['alleles']))). See also; anti_join_rows(), filter_rows(), anti_join_cols(). cache()[source]; Persist the dataset in memory.; Examples; Persist the dataset in memory:; >>> dataset = dataset.cache() . Notes; This method is an alias for persist(""MEMORY_ONLY""). Returns:; MatrixTable – Cached dataset. checkpoint(output, overwrite=False, stage_locally=False, _codec_spec=None, _read_if_exists=False, _intervals=None, _filter_intervals=False, _drop_cols=False, _drop_rows=False)[source]; Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters:. output (str) – Path at which to write.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output; overwrite (bool) – If True, overwrite an existing file at the destination. Returns:; MatrixTable. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Notes; An alias for write() followed by read_matrix_table(). It is; possible to read the file at this path later with; read_matrix_table(). A faster, but less efficient, codec is used; or writing the data so the file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:18055,cache,cache,18055,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['cache'],['cache']
Performance,"combiner.'; ); json.dump(self, sys.stdout, indent=2, cls=Encoder); print(); raise e. [docs] def run(self):; """"""Combine the specified GVCFs and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13104,load,load,13104,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['load'],['load']
Performance,"compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is computed as half the mean of the non-missing entries of column \(j\). Entries of \(M\) are then m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138545,load,loadings,138545,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"containing name of each shard; written successfully to the directory. These filenames are relative; to the export directory.; (#13007) In; Query-on-Batch and hailtop.batch, memory and storage request; strings may now be optionally terminated with a B for bytes. Bug Fixes. (#13065) In Azure; Query-on-Batch, fix a resource leak that prevented running pipelines; with >500 partitions and created flakiness with >250 partitions.; (#13067) In; Query-on-Batch, driver and worker logs no longer buffer so messages; should arrive in the UI after a fixed delay rather than proportional; to the frequency of log messages.; (#13028) Fix crash; in hl.vds.filter_intervals when using a table to filter a VDS; that stores the max ref block length.; (#13060) Prevent 500; Internal Server Error in Jupyter Notebooks of Dataproc clusters; started by hailctl dataproc.; (#13051) In; Query-on-Batch and hailtop.batch, Azure Blob Storage https; URLs are now supported.; (#13042) In; Query-on-Batch, naive_coalesce no longer performs a full; write/read of the dataset. It now operates identically to the; Query-on-Spark implementation.; (#13031) In; hl.ld_prune, an informative error message is raised when a; dataset does not contain diploid calls instead of an assertion error.; (#13032) In; Query-on-Batch, in Azure, Hail now users a newer version of the Azure; blob storage libraries to reduce the frequency of “Stream is already; closed” errors.; (#13011) In; Query-on-Batch, the driver will use ~1/2 as much memory to read; results as it did in 0.2.115.; (#13013) In; Query-on-Batch, transient errors while streaming from Google Storage; are now automatically retried. Version 0.2.116; Released 2023-05-08. New Features. (#12917) ABS blob; URIs in the format of; https://<ACCOUNT_NAME>.blob.core.windows.net/<CONTAINER_NAME>/<PATH>; are now supported.; (#12731) Introduced; hailtop.fs that makes public a filesystem module that works for; local fs, gs, s3 and abs. This is now used as the Backend.fs for; hail q",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:33234,perform,performs,33234,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performs']
Performance,"contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to :func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167518,queue,queue,167518,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['queue'],['queue']
Performance,"contig lengths.; x_contigs (str or list of str) – Contigs to be treated as X chromosomes.; y_contigs (str or list of str) – Contigs to be treated as Y chromosomes.; mt_contigs (str or list of str) – Contigs to be treated as mitochondrial DNA.; par (list of tuple of (str, int, int)) – List of tuples with (contig, start, end). Attributes. contigs; Contig names. global_positions_dict; Get a dictionary mapping contig names to their global genomic positions. lengths; Dict of contig name to contig length. mt_contigs; Mitochondrial contigs. name; Name of reference genome. par; Pseudoautosomal regions. x_contigs; X contigs. y_contigs; Y contigs. Methods. add_liftover; Register a chain file for liftover. add_sequence; Load the reference sequence from a FASTA file. contig_length; Contig length. from_fasta_file; Create reference genome from a FASTA file. has_liftover; True if a liftover chain file is available from this reference genome to the destination reference. has_sequence; True if the reference sequence has been loaded. locus_from_global_position; "". read; Load reference genome from a JSON file. remove_liftover; Remove liftover to dest_reference_genome. remove_sequence; Remove the reference sequence. write; ""Write this reference genome to a file in JSON format. add_liftover(chain_file, dest_reference_genome)[source]; Register a chain file for liftover.; Examples; Access GRCh37 and GRCh38 using get_reference():; >>> rg37 = hl.get_reference('GRCh37') ; >>> rg38 = hl.get_reference('GRCh38') . Add a chain file from 37 to 38:; >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) . Notes; This method can only be run once per reference genome. Use; has_liftover() to test whether a chain file has been registered.; The chain file format is described; here.; Chain files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:3581,load,loaded,3581,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance,"count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to start.; (#6919) Added; --update-hail-version to modify. Version 0.2.20; Released 2019-08-19. Critical memory management fix. (#6824) Fixed memory; management inside annotate_cols with aggregations. This was; causing memory leaks and segfaults. Bug fixes. (#6769) Fixed; non-functional hl.lambda_gc method.; (#6847) Fixed bug in; handling of NaN in hl.agg.min and hl.agg.max. These will now; properly ignore NaN (the intended semantics). Note that hl.min; and hl.max propagate NaN; use hl.nanmin and hl.nanmax to; ignore NaN. New features. (#6847) Added; hl.nanmin and hl.nanmax functions. Version 0.2.19; Released 2019-08-01. Critical performance bug fix. (#6629) Fixed a; critical performance bug introduced in; (#6266). This bug led; to long hang times when reading in Hail tables and matrix tables; written in version 0.2.18. Bug fixes. (#6757) Fixed; correctness bug in optimizations applied to the combination of; Table.order_by with hl.desc arguments and show(), leading; to tables sorted in ascending, not descending order.; (#6770) Fixed; assertion error caused by Table.expand_types(), which was used by; Table.to_spark and Table.to_pandas. Performance Improvements. (#6666) Slightly; improve performance of hl.pca and hl.hwe_normalized_pca.; (#6669) Improve; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:88146,perform,performance,88146,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"cs for hets:; >>> vds_result = (vds.annotate_samples_expr('sa.gqHetStats = gs.filter(g => g.isHet()).map(g => g.gq).stats()'); ... .export_samples('output/samples.txt', 'sample = s, het_gq_mean = sa.gqHetStats.mean')). Compute the list of genes with a singleton LOF per sample:; >>> variant_annotations_table = hc.import_table('data/consequence.tsv', impute=True).key_by('Variant'); >>> vds_result = (vds.annotate_variants_table(variant_annotations_table, root='va.consequence'); ... .annotate_variants_expr('va.isSingleton = gs.map(g => g.nNonRefAlleles()).sum() == 1'); ... .annotate_samples_expr('sa.LOF_genes = gs.filter(g => va.isSingleton && g.isHet() && va.consequence == ""LOF"").map(g => va.gene).collect()')). To create an annotation for only a subset of samples based on an existing annotation:; >>> vds_result = vds.annotate_samples_expr('sa.newpheno = if (sa.pheno.cohortName == ""cohort1"") sa.pheno.bloodPressure else NA: Double'). Note; For optimal performance, be sure to explicitly give the alternative (NA) the same type as the consequent (sa.pheno.bloodPressure). Notes; expr is in sample context so the following symbols are in scope:. s (Sample): sample; sa: sample annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for sample s. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_samples_table(table, root=None, expr=None, vds_key=None, product=False)[source]¶; Annotate samples with a key table.; Examples; To annotates samples using samples1.tsv with type imputation:; >>> table = hc.import_table('data/samples1.tsv', impute=True).key_by('Sample'); >>> vds_result = vds.annotate_samples_table(table, root='sa.pheno'). Given this file; $ cat data/samples1.tsv; Sample Height Status Age; PT-1234 154.1 ADHD 24; PT-1236 160.9 Control 19; PT-1238 NA ADHD 89; PT-1239 170.3 Control 55. the three new sample annotations are sa.pheno.Height: Double, sa.pheno.Sta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:14261,perform,performance,14261,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"ctors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the curre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143324,load,loadings,143324,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"d VEP annotations to your VDS, make sure to add the initialization action ; :code:`gs://hail-common/vep/vep/vep85-init.sh` when starting your cluster. :param annotations: List of annotations to import from the database.; :type annotations: str or list of str . :param gene_key: Existing variant annotation used to map variants to gene symbols if importing gene-level ; annotations. If not provided, the method will add VEP annotations and parse them as described in the ; database documentation to obtain one gene symbol per variant.; :type gene_key: str. :return: Annotated variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". # import modules needed by this function; import sqlite3. # collect user-supplied annotations, converting str -> list if necessary and dropping duplicates; annotations = list(set(wrap_to_list(annotations))). # open connection to in-memory SQLite database; conn = sqlite3.connect(':memory:'). # load database with annotation metadata, print error if not on Google Cloud Platform; try:; f = hadoop_read('gs://annotationdb/ADMIN/annotationdb.sql'); except FatalError:; raise EnvironmentError('Cannot read from Google Storage. Must be running on Google Cloud Platform to use annotation database.'); else:; curs = conn.executescript(f.read()); f.close(). # parameter substitution string to put in SQL query; like = ' OR '.join('a.annotation LIKE ?' for i in xrange(2*len(annotations))). # query to extract path of all needed database files and their respective annotation exprs ; qry = """"""SELECT file_path, annotation, file_type, file_element, f.file_id; FROM files AS f INNER JOIN annotations AS a ON f.file_id = a.file_id; WHERE {}"""""".format(like). # run query and collect results in a file_path: expr dictionary; results = curs.execute(qry, [x + '.%' for x in annotations] + annotations).fetchall(). # all file_ids to be used; file_ids = list(set([x[4] for x in results])). # parameter substitution string; sub = ','.join('?' for x in file_ids). # query to fetch coun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:35207,load,load,35207,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['load']
Performance,"d a (nonsensical) value greater than one. The `max_size` parameter allows us to skip large genes that would cause ""out of memory"" errors:. >>> skat = hl._linear_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 8.13e+02 | 3.95e-05 | 0 |; +-------+-------+----------+----------+-------+. Notes; -----. In the SKAT R package, the ""weights"" are actually the *square root* of the weight expression; from the paper. This method uses the definition from the paper. The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1. This method does not perform small sample size correction. The `q_stat` return value is *not* the :math:`Q` statistic from the paper. We match the output; of the SKAT R package which returns :math:`\tilde{Q}`:. .. math::. \tilde{Q} = \frac{Q}{2 \widehat{\sigma}^2}. Parameters; ----------; group : :class:`.Expression`; Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight : :class:`.Float64Expression`; Row-indexed expression for weights. Must be non-negative.; y : :class:`.Float64Expression`; Column-indexed response (dependent variable) expression.; x : :class:`.Float64Expression`; Entry-indexed expression for input (independent variable).; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size : :obj:`int`; Maximum size of group on which to run the test. Groups which exceed this size will ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:78681,perform,perform,78681,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['perform']
Performance,"d as 1 for; True and 0 for False. The null model sets \(\beta_1 = 0\).; The standard least-squares linear regression model is derived in Section; 3.2 of The Elements of Statistical Learning, 2nd Edition.; See equation 3.12 for the t-statistic which follows the t-distribution with; \(n - k - 1\) degrees of freedom, under the null hypothesis of no; effect, with \(n\) samples and \(k\) covariates in addition to; x. Note; Use the pass_through parameter to include additional row fields from; matrix table underlying x. For example, to include an “rsid” field, set; pass_through=['rsid'] or pass_through=[mt.rsid]. Parameters:. y (Float64Expression or list of Float64Expression) – One or more column-indexed response expressions.; x (Float64Expression) – Entry-indexed expression for input variable.; covariates (list of Float64Expression) – List of column-indexed covariate expressions.; block_size (int) – Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through (list of str or Expression) – Additional row fields to include in the resulting table.; weights (Float64Expression or list of Float64Expression) – Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns:; Table. hail.methods.logistic_regression_rows(test, y, x, covariates, pass_through=(), *, max_iterations=None, tolerance=None)[source]; For each row, test an input variable for association with a; binary response variable using logistic regression.; Examples; Run the logistic regression Wald test per variant using a Boolean; phenotype, intercept and two covariates stored in column-indexed; fields:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=dataset.pheno.is_case,; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:5727,perform,performance,5727,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['performance']
Performance,"d as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102547,load,loaded,102547,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['loaded']
Performance,"d format (with a “target” column) produces a table with two; fields:. interval (tinterval) - Row key. Same schema as above.; target (tstr). If reference_genome is defined AND the file has one field, intervals; are parsed with parse_locus_interval(). See the documentation for; valid inputs.; If reference_genome is NOT defined and the file has one field,; intervals are parsed with the regex `""([^:]*):(\d+)\-(\d+)""; where contig, start, and end match each of the three capture groups.; start and end match positions inclusively, e.g.; start <= position <= end.; For files with three or five fields, start and end match positions; inclusively, e.g. start <= position <= end. Parameters:. path (str) – Path to file.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; skip_invalid_intervals (bool) – If True and reference_genome is not None, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding (dict of (str, str)) – Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; **kwargs – Additional optional arguments to import_table() are valid; arguments here except: no_header, comment, impute, and; types, as these are used by import_locus_intervals(). Returns:; Table – Interval-keyed table. hail.methods.import_matrix_table(paths, row_fields={}, row_key=[], entry_type=dtype('int32'), missing='NA', min_partitions=None, no_header=False, force_bgz=False, sep=None, delimiter=None, comment=())[source]; Import tab-delimited file(s) as a MatrixTable.; Examples; Consider the following file containing counts from a RNA sequencing; dataset:; $ cat data/matrix1.tsv; Barcode Tissue Days GENE1 GENE2 GENE3 GENE4; TTAGCCA brain 1.0 0 0 1 0; ATCACTT kidney 5.5 3 0 2 0; CTCTTCT kidney 2.5 0 0 0 1; CTATATA brain 7.0 0 0 3 0. The field Days contains floating-point numbers and each of the ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:21496,load,loaded,21496,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"d in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""; if self.entry.dtype != other.entry.dtype:; raise ValueError(; f'entry types differ:\n' f' left: {self.entry.dtype}\n' f' right: {other.entry.dtype}'; ); if self.col.dtype != other.col.dtype:; raise ValueError(f'column types differ:\n' f' left: {self.col.dtype}\n' f' right: {other.col.dtype}'); if self.col_key.keys() != other.col_key.keys():; raise ValueError(; f'column key fields differ:\n'; f' left: {"", "".join(self.col_key.keys())}\n'; f' right: {"", "".join(other.col_key.keys())}'; ); if list(self.row_key.dtype.values()) != list(other.row_key.dtype.values()):; raise ValueError(; f'row key types differ:\n'; f' left: {"", "".j",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:120899,perform,perform,120899,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,4,['perform'],['perform']
Performance,"d stage. Furthermore, due to finite; precision, the zero eigenvalues of \(X^T X\) or \(X X^T\) will; only be approximately zero.; If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to “zero” eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away before an; action which realizes the block-matrix-side singular vectors.; svd() sets the singular values corresponding to negative; eigenvalues to exactly 0.0. Warning; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately.; The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; np.show_config(). For Intel machines, we recommend installing the; MKL package for Anaconda.; Consequently, the optimal value of complexity_bound is highly; configuration-dependent. Parameters:. compute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Ret",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:38972,perform,performance,38972,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"d support; for --no-max-idle, no-max-age, --max-age, and; --expiration-time to hailctl dataproc --modify. Version 0.2.55; Released 2020-08-19. Performance. (#9264); Table.checkpoint now uses a faster LZ4 compression scheme. Bug fixes. (#9250); hailctl dataproc no longer uses deprecated gcloud flags.; Consequently, users must update to a recent version of gcloud.; (#9294) The “Python; 3” kernel in notebooks in clusters started by hailctl   dataproc; now features the same Spark monitoring widget found in the “Hail”; kernel. There is now no reason to use the “Hail” kernel. File Format. The native file format version is now 1.5.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.54; Released 2020-08-07. VCF Combiner. (#9224)(#9237); Breaking change: Users are now required to pass a partitioning; argument to the command-line interface or run_combiner method.; See documentation for details.; (#8963) Improved; performance of VCF combiner by ~4x. New features. (#9209) Add; hl.agg.ndarray_sum aggregator. Bug fixes. (#9206)(#9207); Improved error messages from invalid usages of Hail expressions.; (#9223) Fixed error; in bounds checking for NDArray slicing. Version 0.2.53; Released 2020-07-30. Bug fixes. (#9173) Use less; confusing column key behavior in MT.show.; (#9172) Add a missing; Python dependency to Hail: google-cloud-storage.; (#9170) Change Hail; tree aggregate depth logic to correctly respect the branching factor; set in hl.init. Version 0.2.52; Released 2020-07-29. Bug fixes. (#8944)(#9169); Fixed crash (error 134 or SIGSEGV) in MatrixTable.annotate_cols,; hl.sample_qc, and more. Version 0.2.51; Released 2020-07-28. Bug fixes. (#9161) Fix bug that; prevented concatenating ndarrays that are fields of a table.; (#9152) Fix bounds in; NDArray slicing.; (#9161) Fix bugs; calculating row_id in hl.import_matrix_table. Version 0.2.50; Released 2020-07-23. Bug fixes. (#9114) CHANGELOG:; Fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:66643,perform,performance,66643,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"d) is None:; fields_to_impute_idx.append(idx); fields_to_guess.append(field). hl.utils.info('Reading table to impute column types'); guessed = ht.aggregate(; hl.agg.array_agg(lambda x: hl.agg._impute_type(x), [ht.split_text[i] for i in fields_to_impute_idx]); ). reasons = {f: 'user-supplied type' for f in types}; imputed_types = dict(); for field, s in zip(fields_to_guess, guessed):; if not s['anyNonMissing']:; imputed_types[field] = hl.tstr; reasons[field] = 'no non-missing observations'; else:; if s['supportsBool']:; imputed_types[field] = hl.tbool; elif s['supportsInt32']:; imputed_types[field] = hl.tint32; elif s['supportsInt64']:; imputed_types[field] = hl.tint64; elif s['supportsFloat64']:; imputed_types[field] = hl.tfloat64; else:; imputed_types[field] = hl.tstr; reasons[field] = 'imputed'. strs.append('Finished type imputation'). all_types = dict(**types, **imputed_types). for f_idx, field in enumerate(fields):; strs.append(f' Loading field {field!r} as type {all_types[field]} ({reasons[field]})'); fields_to_value[field] = parse_type(ht.split_text[f_idx], all_types[field]); else:; strs.append('Reading table without type imputation'); for f_idx, field in enumerate(fields):; reason = 'user-supplied' if field in types else 'not specified'; t = types.get(field, hl.tstr); fields_to_value[field] = parse_type(ht.split_text[f_idx], t); strs.append(f' Loading field {field!r} as type {t} ({reason})'). ht = ht.annotate(**fields_to_value).drop('split_text'); if source_file_field is not None:; source_file = {source_file_field: ht.file}; ht = ht.annotate(**source_file); ht = ht.drop('file'). if len(fields) < 30:; hl.utils.info('\n'.join(strs)); else:; from collections import Counter. strs2 = [f'Loading {ht.row} fields. Counts by type:']; for name, count in Counter(ht[f].dtype for f in fields).most_common():; strs2.append(f' {name}: {count}'); hl.utils.info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:63958,Load,Loading,63958,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Load'],['Loading']
Performance,"dCalls()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(maf=nullable(strlike),; bounded=bool,; min=nullable(numeric),; max=nullable(numeric)); def ibd(self, maf=None, bounded=True, min=None, max=None):; """"""Compute matrix of identity-by-descent estimations. .. include:: requireTGenotype.rst. **Examples**. To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:. >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in [0.2, 0.9], using minor allele frequencies stored in; ``va.panel_maf``:. >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). **Notes**. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :py:meth:`~hail.VariantDataset.ibd` requires the dataset to be; bi-allelic (otherwise run :py:meth:`~hail.VariantDataset.split_multi` or otherwise run :py:meth:`~hail.VariantDataset.filter_multi`); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first. The resulting :py:class:`.KeyTable` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... :param maf: Expression for the minor allele frequency.; :type maf: str or None. :param bool bounded: Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]). :param min: Sample pairs with a PI_HAT below this value will; no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:82590,perform,perform,82590,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['perform']
Performance,"d_prune(). If the dataset contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:44461,queue,queue,44461,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"darray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 rows and 10 columns:. - ``bm[0, 0]`` is the element in row 0 and column 0 of ``bm``. - ``bm[0:1, 0]`` is a block matrix with 1 row, 1 column,; and element ``bm[0, 0]``. - ``bm[2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5460,cache,cache,5460,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['cache'],['cache']
Performance,"darray to add. Returns:; NDArrayNumericExpression – NDArray of positional sums. __eq__(other); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:2924,perform,performed,2924,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['perform'],['performed']
Performance,"data on those features. The eigenvalues of the GRM \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2, \ldots\), which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the loadings parameter is specified.; Note: In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the \(ij\) entry of \(MM^T\) is simply the dot product of rows \(i\) and \(j\) of \(M\); in terms of \(C\) it is. \[\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}\]; where \(\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}\). In PLINK/GCTA the denominator \(m\) is replaced with the number of terms in the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:142223,load,loadings,142223,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: array<str>,; medGenIds: array<s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60464,cache,cache,60464,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value di",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85962,perform,performance,85962,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:46644,cache,cache,46644,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['cache'],['cache']
Performance,"declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45625,load,load,45625,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"dia.org/wiki/List_of_tz_database_time_zones; Currently, the parser implicitly uses the “en_US” locale.; This function will fail if there is not enough information in the string to determine a particular timestamp.; For example, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39341,load,loadings,39341,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"doop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688) Added; experimental support for NDArrayExpressions in new hl.nd; module.; (#7608) hl.grep; now has a show argument that allows users to either print the; results (default) or return a dictionary of the results. hailctl dataproc. (#7717) Throw error; when mispelling arguments instead of silently quitting. Version 0.2.28; Released 2019-11-22. Critical correctness bug fix. (#7588) Fixes a bug; where filtering old matrix tables in newer versions of hail did not; work as expected. Please update from 0.2.27. Bug fixes. (#7571) Don’t set GQ; to missing if PL is missing in split_multi_hts.; (#7577) Fixed an; optimizer bug. New Features. (#7561) Added; hl.plot.visualize_missingness() to plot missingness patterns for; MatrixTables.; (#7575) Added; hl.version() to quickly check hail version. hailctl dataproc. (#7586); hailctl dataproc now supports --gcloud_configuration option. Documentation. (#7570) Hail has a; cheatsheet for Tables now. Version 0.2.27; Released 2019-11-15. New Features. (#7379) Add; delimiter argument to hl.import_matrix_table; (#7389) Add force; and force_bgz arguments to hl.experimental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailct",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:81229,optimiz,optimizer,81229,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"ds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = VariantDataset.union(*all_vds). Notes. In order to combine two datasets, these requirements must be met:. the samples must match; the variant annotation schemas must match (field order within structs matters).; the cell (genotype) schemas must match (field order within structs matters). The column annotations in the resulting dataset are simply the column annotations; from the first dataset; the column annotation schemas do not need to match.; This method can trigger a shuffle, if partitions from two datasets overlap. Parameters:vds_type (tuple of VariantDataset) – Datasets to combine. Returns:Dataset with variants from all datasets. Return type:VariantDataset. unpersist()[source]¶; Unpersists this VDS from memory/disk.; Notes; This function will have no effect on a VDS that was not previously persisted.; There’s nothing stopping you from continuing to use a VDS that has been unpersisted, but doing so will result in; all previous steps taken to compute the VDS being performed again since the VDS must be recomputed. Only unpersist; a VDS when you are done with it. variant_qc(root='va.qc')[source]¶; Compute common variant statistics (quality control metrics). Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; >>> vds_result = vds.variant_qc(). Annotations; variant_qc() computes 18 variant statistics from the ; genotype data and stores the results as variant annotations that can be accessed ; with va.qc.<identifier> (or <root>.<identifier> if a non-default root was passed):. Name; Type; Description. callRate; Double; Fraction of samples with called genotypes. AF; Double; Calculated alternate allele frequency (q). AC; Int; Count of alternate alleles. rHeterozygosity; Double; Proportion of heterozygotes. rHetHomVar; Double; Ratio of heterozygotes to homozygous alternates. rExpectedHetFrequency; Double; Expected rHeterozygosity based on HWE. pHWE; Do",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:171567,perform,performed,171567,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performed']
Performance,"e Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using :py:meth:`~hail.KeyTable.filter`. >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). **Method**. The traditional estimator for kinship between a pair of individuals; :math:`i` and :math:`j`, sharing the set :math:`S_{ij}` of; single-nucleotide variants, from a population with allele frequencies; :math:`p_s`, is given by:. .. math::. \\widehat{\phi_{ij}} := \\frac{1}{|S_{ij}|}\\sum_{s \in S_{ij}}\\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}. This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they're common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele wa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:169258,perform,performing,169258,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performing']
Performance,"e UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantile",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90133,load,load,90133,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['load'],['load']
Performance,"e Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40153,load,loadings,40153,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"e called homozygous variant on the right. Parameters:. left (MatrixTable) – First dataset to compare.; right (MatrixTable) – Second dataset to compare. Returns:; (list of list of int, Table, Table) – The global concordance statistics, a table with concordance statistics; per column key, and a table with concordance statistics per row key. hail.methods.filter_intervals(ds, intervals, keep=True)[source]; Filter rows with a list of intervals.; Examples; Filter to loci falling within one interval:; >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:; >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; Based on the keep argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges.; When keep=True, partitions that don’t overlap any supplied interval; will not be loaded at all. This enables filter_intervals() to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters:. ds (MatrixTable or Table) – Dataset to filter.; intervals (ArrayExpression of type tinterval) – Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep (bool) – If True, keep only rows that fall within any interval in intervals.; If False, keep only rows that fall outside all intervals in; intervals. Returns:; MatrixTable or Table. hail.methods.filter_alleles(mt, f)[source]; Filter alternate alleles. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Examples; Keep SNPs:; >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). Keep alleles with AC > 0:; >>> ds_result = hl.filter_alleles(ds, lambda a, allele_index: ds.info.AC[al",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:20696,load,loaded,20696,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loaded']
Performance,"e chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory because PLINK defaults to using half; of the machine’s memory. PLINK’s memory-available detection mechanism is unfortunately; unaware of the memory limit imposed by Batch. Not specifying resource requirements; correctly can cause performance degradations with PLINK.; PLINK creates a hard-coded file plink.clumped. We have to move that file to a temporary; Batch file {c.clumped} in order to use that file in downstream jobs. Merge Clumping Results; The third function concatenates all of the clumping results per chromosome into a single file; with one header line. The inputs are the Batch for which to create a new BashJob; and a list containing all of the individual clumping results files. We use the ubuntu:22.04; Docker image for this job. The return value is the new BashJob created.; def merge(batch, results):; """"""; Merge clumped results files together; """"""; merger = batch.new_job(name='merge-results'); merger.image('ubuntu:22.04'); if results:; merger.command(f'''; head -n 1 {results[0]} > {merger.ofile}; for result in {"" "".join(results)}; do; tail -n +2 ""$result"" >> {merger.ofile}; done; sed -i -e '/^$/d' {merger.ofile}; '''); return merger. Control Code; The last thing we want to do is use the fun",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:9069,perform,performance,9069,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['performance']
Performance,"e eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors :math:`U_k` instead of the component scores :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167437,load,loadings,167437,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"e of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143512,load,loadings,143512,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['load'],['loadings']
Performance,"e parameter lamb.; Examples; >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters:. x (float or Expression of type tfloat64) – Non-negative number at which to compute the probability density.; lamb (float or Expression of type tfloat64) – Poisson rate parameter. Must be non-negative.; log_p (bool or BooleanExpression) – If True, the natural logarithm of the probability density is returned. Returns:; Expression of type tfloat64 – The (log) probability density. hail.expr.functions.hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False)[source]; Performs test of Hardy-Weinberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:11469,perform,performs,11469,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['performs']
Performance,"e pipeline. See also; flatten(), write(). Parameters:. output (str) – URI at which to write exported file.; types_file (str, optional) – URI at which to write file containing field type information.; header (bool) – Include a header in the file.; parallel (str, optional) – If None, a single file is produced, otherwise a; folder of file shards is produced. If ‘separate_header’,; the header file is output separately from the file shards. If; ‘header_per_shard’, each file shard has a header. If set to None; the export will be slower.; delimiter (str) – Field delimiter. filter(expr, keep=True)[source]; Filter rows conditional on the value of each row’s fields. Note; Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using read_table(), _not_; import_table()). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; read_table() and a filter(). For example, a key_by and group_by, both; force reading all the data.; Suppose we previously write() a Hail Table with one million rows keyed by a field; called idx. If we filter this table to one value of idx, the pipeline will be fast; because we read only the rows that have that value of idx:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx == 5) . This also works with inequality conditions:; >>> ht = hl.read_table('large-table.ht') ; >>> ht = ht.filter(ht.idx <= 5) . Examples; Consider this table:; >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where Z is 3:; >>> fil",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:23989,optimiz,optimization,23989,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['optimiz'],['optimization']
Performance,"e same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:99295,perform,perform,99295,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['perform'],['perform']
Performance,"e variants in this VDS.; Examples; >>> ld_mat = vds.ld_matrix(). Notes; Each entry (i, j) in the LD matrix gives the \(r\) value between variants i and j, defined as; Pearson’s correlation coefficient; \(\rho_{x_i,x_j}\) between the two genotype vectors \(x_i\) and \(x_j\). \[\rho_{x_i,x_j} = \frac{\mathrm{Cov}(X_i,X_j)}{\sigma_{X_i} \sigma_{X_j}}\]; Also note that variants with zero variance (\(\sigma = 0\)) will be dropped from the matrix. Caution; The matrix returned by this function can easily be very large with most entries near zero; (for example, entries between variants on different chromosomes in a homogenous population).; Most likely you’ll want to reduce the number of variants with methods like; sample_variants(), filter_variants_expr(), or ld_prune() before; calling this unless your dataset is very small. Parameters:force_local (bool) – If true, the LD matrix is computed using local matrix multiplication on the Spark driver. This may improve performance when the genotype matrix is small enough to easily fit in local memory. If false, the LD matrix is computed using distributed matrix multiplication if the number of genotypes exceeds \(5000^2\) and locally otherwise. Returns:Matrix of r values between pairs of variants. Return type:LDMatrix. ld_prune(r2=0.2, window=1000000, memory_per_core=256, num_cores=1)[source]¶; Prune variants in linkage disequilibrium (LD). Important; The genotype_schema() must be of type TGenotype in order to use this method. Requires was_split equals True.; Examples; Export the set of common LD pruned variants to a file:; >>> vds_result = (vds.variant_qc(); ... .filter_variants_expr(""va.qc.AF >= 0.05 && va.qc.AF <= 0.95""); ... .ld_prune(); ... .export_variants(""output/ldpruned.variants"", ""v"")). Notes; Variants are pruned in each contig from smallest to largest start position. The LD pruning algorithm is as follows:; pruned_set = []; for v1 in contig:; keep = True; for v2 in pruned_set:; if ((v1.position - v2.position) <= window and",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:75149,perform,performance,75149,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"e(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by ‘locus’ and ‘alleles’ from a BGEN file:; >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; Hail supports importing data from v1.2 of the BGEN file format.; Genotypes must be unphased and diploid, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic.; Each BGEN file must have a corresponding index file, which can be generated; with index_bgen(). All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use Hadoop Glob Patterns.; If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size.; Column Fields. s (tstr) – Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from _0, _1, to _N. Row Fields; Between two and four row fields are created. The locus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: con",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:9516,load,load,9516,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"e, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir.StringTableReader(paths, min_partitions, force_bgz, force, file_per_partition); table_type = hl.ttable(global_type=hl.tstruct(), row_type=hl.tstruct(file=hl.tstr, text=hl.tstr), row_key=[]); string_table = Table(ir.TableRead(st_reader, _assert_type=table_type)); return s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:66158,load,load,66158,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"e.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the resu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40074,load,loadings,40074,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"e:; struct {; id: int32; }; --------------------------------------------------------; Source:; <hail.table.Table object at 0x7f5beee034f0>; Index:; ['row']; --------------------------------------------------------. Keys need not be unique or non-missing, although in many applications they will be both.; When tables are joined in Hail, they are joined based on their keys. In order to join two tables, they must share the same number of keys, same key types (i.e. string vs integer), and the same order of keys.; Let’s look at a simple example of a join. We’ll use the Table.parallelize() method to create two small tables, t1 and t2. [4]:. t1 = hl.Table.parallelize([; {'a': 'foo', 'b': 1},; {'a': 'bar', 'b': 2},; {'a': 'bar', 'b': 2}],; hl.tstruct(a=hl.tstr, b=hl.tint32),; key='a'); t2 = hl.Table.parallelize([; {'t': 'foo', 'x': 3.14},; {'t': 'bar', 'x': 2.78},; {'t': 'bar', 'x': -1},; {'t': 'quam', 'x': 0}],; hl.tstruct(t=hl.tstr, x=hl.tfloat64),; key='t'). [5]:. t1.show(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. abstrint32; ""bar""2; ""bar""2; ""foo""1. [6]:. t2.show(). txstrfloat64; ""bar""2.78e+00; ""bar""-1.00e+00; ""foo""3.14e+00; ""quam""0.00e+00. Now, we can join the tables. [7]:. j = t1.annotate(t2_x = t2[t1.a].x); j.show(). [Stage 3:==========================================> (12 + 4) / 16]. abt2_xstrint32float64; ""bar""22.78e+00; ""bar""22.78e+00; ""foo""13.14e+00. Let’s break this syntax down.; t2[t1.a] is an expression referring to the row of table t2 with value t1.a. So this expression will create a map between the keys of t1 and the rows of t2. You can view this mapping directly:. [8]:. t2[t1.a].show(). <expr>axstrfloat64; ""bar""2.78e+00; ""bar""2.78e+00; ""foo""3.14e+00. Since we only want the field x from t2, we can select it with t2[t1.a].x. Then we add this field to t1 with the anntotate_rows() method. The new",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/06-joins.html:3969,load,load,3969,docs/0.2/tutorials/06-joins.html,https://hail.is,https://hail.is/docs/0.2/tutorials/06-joins.html,1,['load'],['load']
Performance,"e` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.Interval`. :param bool keep: Keep variants overlapping an interval if ``True``, remove variants overlapping; an interval if ``False``. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". intervals = wrap_to_list(intervals). jvds = self._jvds.filterIntervals([x._jrep for x in intervals], keep); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(variants=listof(Variant),; keep=bool); def filter_variants_list(self, variants, keep=True):; """"""Filter variants with a list of variants. **Examples**. Filter VDS down to a list of variants:. >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True); ; **Notes**. This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap with any supplied variant will not be loaded at all. This property; enables ``filter_variants_list`` to be used for reasonably low-latency queries of one; or more variants, even on large datasets. ; ; :param variants: List of variants to keep or remove.; :type variants: list of :py:class:`~hail.representation.Variant`. :param bool keep: If true, keep variants in ``variants``, otherwise remove them. :return: Filtered variant dataset.; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(; self.hc, self._jvds.filterVariantsList(; [TVariant()._convert_to_j(v) for v in variants], keep)). [docs] @handle_py4j; @typecheck_method(table=KeyTable,; keep=bool); def filter_variants_table(self, table, keep=True):; """"""Filter variants with a Variant keyed key table. **Example**. Filter variants of a VDS to those appearing in a text file:. >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:76155,perform,performs,76155,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ean of an array of numbers with .mean(), find their max with; .max(), and so on.; But what if we wanted to compute the mean of 5 trillion numbers?; That’s a lot of data, and turns out to be the rough number of genotypes; in the preprocessed gnomAD VCF,; which contained about 20 thousand samples and 250 million variants. Hail; is designed to handle datasets of this size and larger, and does so by; computing in parallel on many computers using Apache; Spark.; But we still want a simple programming model that allows us to query and; transform such distributed data. That is where the Aggregable comes; in. First, an example:. In [24]:. vds.query_genotypes('gs.map(g => g.gq).stats()').mean. Out[24]:. 30.682263230349086. The above statement computes the mean GQ of all genotypes in a dataset.; This code can compute the mean GQ of a megabyte-scale thousand genomes; subset on a laptop, or compute the mean GQ of a 300 TB .vcf on a massive; cloud cluster. Hail is scalable!; An Aggregable[T] is distributed collection of elements of type; T. The interface is modeled on Array[T], but aggregables can be; arbitrarily large and they are unordered, so they don’t support; operations like indexing.; Aggregables support map and filter. Like sum, max, etc. on arrays,; aggregables support operations which we call “aggregators” that operate; on the entire aggregable collection and produce a summary or derived; statistic. See the; documentation for a; complete list of aggregators.; Aggregables are available in expressions on various methods on; VariantDataset.; Above,; query_genotypes; exposes the aggregable gs: Aggregable[Genotype] which is the; collection of all the genotypes in the dataset.; First, we map the genotypes to their GQ values. Then, we use the; stats() aggregator to compute a struct with information like mean; and standard deviation. We can see the other values in the struct; produced as well:. In [25]:. pprint(vds.query_genotypes('gs.map(g => g.gq).stats()')). {u'max': 99.0,;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html:10939,scalab,scalable,10939,docs/0.1/tutorials/expression-language-part-2.html,https://hail.is,https://hail.is/docs/0.1/tutorials/expression-language-part-2.html,1,['scalab'],['scalable']
Performance,"eature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167772,load,loadings,167772,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"efault: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. VEP Invocation; <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. Annotations; Annotations with the following schema are placed in the location specified by root.; The full resulting dataset schema can be queried with variant_schema.; Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_allele: String,; exac_afr_maf: Double,; exac_amr_allele: String,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:176356,cache,cache,176356,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"efining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """"""; if len(delimiter) < 1:; raise ValueError('import_table: empty delimiter is not supported'). paths = wrap_to_list(paths); comment = wrap_to_list(comment); missing = wrap_to_list(missing). ht = hl.import_lines(paths, min_partitions, force_bgz, force). should_remove_line_expr = should_remove_line(; ht.text, filter=filter, comment=comment, skip_blank_lines=skip_blank_lines; ); if should_remove_line_expr is not None:; ht = ht.filter(should_remove_line_expr, keep=False). try:; if len(paths) <= 1:; # With zero or one files and no filters, the first row, if it exists must be in the first; # partiti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:60415,load,load,60415,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"egate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are define",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:31278,perform,performs,31278,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,2,['perform'],['performs']
Performance,"einberg equilibrium.; Examples; >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; By default, this method performs a two-sided exact test with mid-p-value correction of; Hardy-Weinberg equilibrium; via an efficient implementation of the; Levene-Haldane distribution,; which models the number of heterozygous individuals under equilibrium.; The mean of this distribution is (n_ref * n_var) / (2n - 1), where; n_ref = 2*n_hom_ref + n_het is the number of reference alleles,; n_var = 2*n_hom_var + n_het is the number of variant alleles,; and n = n_hom_ref + n_het + n_hom_var is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; het_freq_hwe, is this mean divided by n.; To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set one_sided=True and the p-value returned will be; from the one-sided exact test. Parameters:. n_hom_ref (int or Expression of type tint32) – Number of homozygous reference genotypes.; n_het (int or Expression of type tint32) – Number of heterozygous genotypes.; n_hom_var (int or Expression of type tint32) – Number of homozygous variant genotypes.; one_sided (bool) – False by default. When True, perform one-sided test for excess heterozygosity. Returns:; StructExpression – A struct expression with two fields, het_freq_hwe; (tfloat64) and p_value (tfloat64). hail.expr.functions.binom_test(x, n, p, alternative)[source]; Performs a binomial test on p given x successes in n trials.; Returns the p-value from the exact binomial test of the null hypothesis that; success has probability p, given x successes in n trials.; The alternatives are interpreted as follows:; - 'less': a one-tailed test of the significance of x or fewer successes,; - 'greater': a one-tailed test of the signifi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/stats.html:12076,perform,perform,12076,docs/0.2/functions/stats.html,https://hail.is,https://hail.is/docs/0.2/functions/stats.html,1,['perform'],['perform']
Performance,"eld case_status and then; compute statistics about the entry field GQ for each grouping of case_status.; >>> mt_ann = mt.annotate_cols(case_status = hl.if_else(hl.rand_bool(0.5),; ... ""CASE"",; ... ""CONTROL"")). Next we group the columns by case_status and aggregate:; >>> mt_grouped = (mt_ann.group_cols_by(mt_ann.case_status); ... .aggregate(gq_stats = hl.agg.stats(mt_ann.GQ))); >>> print(mt_grouped.entry.dtype.pretty()); struct {; gq_stats: struct {; mean: float64,; stdev: float64,; min: float64,; max: float64,; n: int64,; sum: float64; }; }; >>> print(mt_grouped.col.dtype); struct{case_status: str}. Joins; Joins on two-dimensional data are significantly more complicated than joins; in one dimension, and Hail does not yet support the full range of; joins on both dimensions of a matrix table.; MatrixTable has methods for concatenating rows or columns:. MatrixTable.union_cols(); MatrixTable.union_rows(). MatrixTable.union_cols() joins matrix tables together by performing an; inner join on rows while concatenating columns together (similar to paste in; Unix). Likewise, MatrixTable.union_rows() performs an inner join on; columns while concatenating rows together (similar to cat in Unix).; In addition, Hail provides support for joining data from multiple sources together; if the keys of each source are compatible. Keys are compatible if they are the; same type, and share the same ordering in the case where tables have multiple keys.; If the keys are compatible, joins can then be performed using Python’s bracket; notation []. This looks like right_table[left_table.key]. The argument; inside the brackets is the key of the destination (left) table as a single value, or a; tuple if there are multiple destination keys.; For example, we can join a matrix table and a table in order to annotate the; rows of the matrix table with a field from the table. Let gnomad_data be a; Table keyed by two row fields with type; locus and array<str>, which matches the row keys of mt:; >>> mt_n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/overview/matrix_table-1.html:11968,perform,performing,11968,docs/0.2/overview/matrix_table-1.html,https://hail.is,https://hail.is/docs/0.2/overview/matrix_table-1.html,2,['perform'],['performing']
Performance,"elding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18856,load,loadings,18856,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['load'],['loadings']
Performance,"else tndarray(tfloat64, 1); ); return construct_expr(ir, return_type, nd._indices, nd._aggregations). @typecheck(nd=expr_ndarray(), eigvals_only=bool); def eigh(nd, eigvals_only=False):; """"""Performs an eigenvalue decomposition of a symmetric matrix. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(N, N).; eigvals_only: :class:`.bool`; If False (default), compute the eigenvectors and eigenvalues. Otherwise, only compute eigenvalues. Returns; -------; - w: :class:`.NDArrayNumericExpression`; The eigenvalues, shape(N).; - v: :class:`.NDArrayNumericExpression`; The eigenvectors, shape(N, N). Only returned if eigvals_only is false.; """"""; float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayEigh(float_nd._ir, eigvals_only). return_type = tndarray(tfloat64, 1) if eigvals_only else ttuple(tndarray(tfloat64, 1), tndarray(tfloat64, 2)); return construct_expr(ir, return_type, nd._indices, nd._aggregations). [docs]@typecheck(nd=expr_ndarray()); def inv(nd):; """"""Performs a matrix inversion. Parameters; ----------. nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray. Returns; -------; :class:`.NDArrayNumericExpression`; The inverted matrix.; """""". assert nd.ndim == 2, ""Matrix inversion requires 2 dimensional ndarray"". float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayInv(float_nd._ir); return construct_expr(ir, tndarray(tfloat64, 2), nd._indices, nd._aggregations). [docs]@typecheck(nds=tsequenceof_nd, axis=int); def concatenate(nds, axis=0):; """"""Join a sequence of arrays along an existing axis. Examples; --------. >>> x = hl.nd.array([[1., 2.], [3., 4.]]); >>> y = hl.nd.array([[5.], [6.]]); >>> hl.eval(hl.nd.concatenate([x, y], axis=1)); array([[1., 2., 5.],; [3., 4., 6.]]); >>> x = hl.nd.array([1., 2.]); >>> y = hl.nd.array([3., 4.]); >>> hl.eval(hl.nd.concatenate((x, y), axis=0)); array([1., 2., 3., 4.]). Parameters; ----------; nds : a sequence of :class:`.NDArrayNumericExpression`; The arrays must have the sam",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:13226,Perform,Performs,13226,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Perform'],['Performs']
Performance,"ely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=Non",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:37410,load,load,37410,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ements.; filters (tset of tstr) – Set containing all filters applied to a; variant.; rsid (tstr) – rsID of the variant.; qual (tfloat64) – Floating-point number in the QUAL field.; info (tstruct) – All INFO fields defined in the VCF header; can be found in the struct info. Data types match the type specified; in the VCF header, and if the declared Number is not 1, the result; will be stored as an array. Entry Fields; import_vcf() generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference – “GT” and other fields; specified in call_fields will be read as tcall. Parameters:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45135,load,load,45135,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ence('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMO",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:79640,Cache,Cached,79640,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,1,['Cache'],['Cached']
Performance,"ence. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Genome-Wide Association Study (GWAS) Tutorial; Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; Histogram; Cumulative Histogram; Scatter; 2-D histogram; Q-Q (Quantile-Quantile); Manhattan. GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; Plotting Tutorial. View page source. Plotting Tutorial; The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial. [1]:. import hail as hl; hl.init(). from bokeh.io import show; from bokeh.layouts import gridplot. Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2012-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_1kg('data/'); mt = hl.read_matrix_table('data/1kg.mt'); table = (hl.import_table('data/1kg_annotations.txt', impute=True); .key_by('Sample')); mt = mt.annotate_cols(**table[mt.s]); mt = hl.sample_qc(mt). mt.describe(). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. ----------------------------------------; Global fields:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/08-plotting.html:1110,load,load,1110,docs/0.2/tutorials/08-plotting.html,https://hail.is,https://hail.is/docs/0.2/tutorials/08-plotting.html,1,['load'],['load']
Performance,"ent; associated loci accounting for linkage disequilibrium between variants.; For example, given a region of the genome with three variants: SNP1, SNP2, and SNP3.; SNP1 has a p-value of 1e-8, SNP2 has a p-value of 1e-7, and SNP3 has a; p-value of 1e-6. The correlation between SNP1 and SNP2 is 0.95, SNP1 and; SNP3 is 0.8, and SNP2 and SNP3 is 0.7. We would want to report SNP1 is the; most associated variant with the phenotype and “clump” SNP2 and SNP3 with the; association for SNP1.; Hail is a highly flexible tool for performing; analyses on genetic datasets in a parallel manner that takes advantage; of a scalable compute cluster. However, LD-based clumping is one example of; many algorithms that are not available in Hail, but are implemented by other; bioinformatics tools such as PLINK.; We use Batch to enable functionality unavailable directly in Hail while still; being able to take advantage of a scalable compute cluster.; To demonstrate how to perform LD-based clumping with Batch, we’ll use the; 1000 Genomes dataset from the Hail GWAS tutorial.; First, we’ll write a Python Hail script that performs a GWAS for caffeine; consumption and exports the results as a binary PLINK file and a TSV; with the association results. Second, we’ll build a docker image containing; the custom GWAS script and Hail pre-installed and then push that image; to the Google Container Repository. Lastly, we’ll write a Python script; that creates a Batch workflow for LD-based clumping with parallelism across; chromosomes and execute it with the Batch Service. The job computation graph; will look like the one depicted in the image below:. Hail GWAS Script; We wrote a stand-alone Python script run_gwas.py that takes a VCF file, a phenotypes file,; the output destination file root, and the number of cores to use as input arguments.; The Hail code for performing the GWAS is described; here.; We export two sets of files to the file root defined by --output-file. The first is; a binary PLINK file",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:1672,perform,perform,1672,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['perform']
Performance,"ent; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_tabl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100937,perform,performance,100937,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'bin",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4335,load,load,4335,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"er for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; These files are hosted by the Hail team in a public Google Storage bucket; the following cell downloads that data locally. [3]:. hl.utils.get_1kg('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 1:==========================================> (12 + 4) / 16]. Importing data from VCF; The data in a VCF file is naturally represented as a Hail MatrixTable. By first importing the VCF file and then writing the resulting MatrixTable in Hail’s native file format, all downstream operations on the VCF’s data will be MUCH faster. [4]:. hl.import_vcf('data/1kg.vcf.bgz').write('data/1kg.mt', overwrite=True). [Stage 3:> (0 + 1) / 1]. Next we read the written file, assigning the variable mt (for matrix table). [5]:. mt = hl.read_matrix_table('data/1kg.mt'). Getting to know our data; It’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of this functionality is demonstrated below.; The rows method can be used to get a table with all the row fields in our MatrixTa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:2456,load,load,2456,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['load'],['load']
Performance,"erage age? Youngest age? Oldest age?; What are all the occupations that appear, and how many times does each appear?. We can answer these questions with aggregation. Aggregation combines many values together to create a summary.; To start, we’ll aggregate all the values in a table. (Later, we’ll learn how to aggregate over subsets.); We can do this with the Table.aggregate method.; A call to aggregate has two parts:. The expression to aggregate over (e.g. a field of a Table).; The aggregator to combine the values into the summary. Hail has a large suite of aggregators for summarizing data. Let’s see some in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:2072,load,load,2072,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['load'],['load']
Performance,"ero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. .. caution::. To process a group with :math:`m` rows, several copies of an; :math:`m \times m` matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the `max_size` parameter to skip; groups larger than `max_size`. Warning; -------; :func:`.skat` considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which **all** covariates are defined.; For each row, missing values of `x` are mean-imputed over these columns.; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----. This method provides a scalable implementation of the score-based; variance-component test originally described in; `Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test; <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3135811/>`__. Row weights must be non-negative. Rows with missing weights are ignored. In; the R package ``skat``---which assumes rows are variants---default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field `AF`, one can use the expression:. >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response `y` must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively. The resulting :class:`.Table` provides the group's key (`id`), thenumber of; rows in the group (`size`), the variance component score `q_stat`, the SKAT; `p-value`, and a `fault` flag. For the toy example above, the table has the; form:. +-------+----",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:103176,scalab,scalable,103176,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['scalab'],['scalable']
Performance,"ers:. path (str or list of str) – One or more paths to VCF files to read. Each path may or may not include glob expressions; like *, ?, or [abc123].; force (bool) – If True, load .vcf.gz files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz (bool) – If True, load .vcf.gz files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file (str, optional) – Optional header override file. If not specified, the first file in; path is used. Glob patterns are not allowed in the header_file.; min_partitions (int, optional) – Minimum partitions to load per file.; drop_samples (bool) – If True, create sites-only dataset. Don’t load sample IDs or; entries.; call_fields (list of str) – List of FORMAT fields to load as tcall. “GT” is; loaded as a call automatically.; reference_genome (str or ReferenceGenome, optional) – Reference genome to use.; contig_recoding (dict of (str, str), optional) – Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the reference_genome, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required (bool) – If True, all elements in an array field must be present. Set this; parameter to False for Hail to allow array fields with missing; values such as 1,.,5. In this case, the second element will be; missing. However, in the case of a single missing element ., the; entire field will be missing and not an array with one missing; element.; skip_invalid_loci (bool) – If True, skip loci that are not consistent with reference_genome.; entry_float_type (HailType) – Type of floating point entries in matrix table. Must be one of:; tfloat32 or tfloat64. Default:; tfloat64.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitutio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:45861,load,loaded,45861,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['loaded']
Performance,"ers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :cla",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1330,Load,Load,1330,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,1,['Load'],['Load']
Performance,"ersampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:22613,load,loadings,22613,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"erview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; Hail Query-on-Batch. View page source. Hail Query-on-Batch. Warning; Hail Query-on-Batch (the Batch backend) is currently in beta. This means some functionality is; not yet working. Please contact us if you would like to use missing; functionality on Query-on-Batch!. Hail Query-on-Batch uses Hail Batch instead of Apache Spark to execute jobs. Instead of a Dataproc; cluster, you will need a Hail Batch cluster. For more information on using Hail Batch, see the Hail; Batch docs. For more information on deploying a Hail Batch cluster,; please contact the Hail Team at our discussion forum. Getting Started. Install Hail version 0.2.93 or later:. pip install 'hail>=0.2.93'. Sign up for a Hail Batch account (currently only available to; Broad affiliates).; Authenticate with Hail Batch. hailctl auth login. Specify a bucket for Hail to use for temporary intermediate files. In Google Cloud, we recommend; using a bucket with automatic deletion after a set period of time. hailctl config set batch/remote_tmpdir gs://my-auto-delete-bucket/hail-query-temporaries. Specify a Hail Batch billing project (these are different from Google Cloud projects). Every new; user has a trial billing project loaded with 10 USD. The name is available on the Hail User; account page. hailctl config set batch/billing_project my-billing-project. Set the default Hail Query backend to batch:. hailctl config set query/backend batch. Now you are ready to try Hail! If you want to switch back to; Query-on-Spark, run the previous command again with “spark” in place of “batch”. Variant Effect Predictor (VEP); More information coming very soon. If you want to use VEP with Hail Query-on-Batch, please contact; the Hail Team at our discussion forum. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/query_on_batch.html:1776,load,loaded,1776,docs/0.2/cloud/query_on_batch.html,https://hail.is,https://hail.is/docs/0.2/cloud/query_on_batch.html,1,['load'],['loaded']
Performance,"es Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""; check_nonnegative_and_in_range('range_matrix_table', 'n_rows', n_rows); check_nonnegative_and_in_range('range_matrix_table', 'n_cols', n_cols); if n_partitions is not None:; check_positive_and_in_range('range_matrix_table', 'n_partitions', n_partitions); return hail.MatrixTable(; hail.ir.MatrixRead(; hail.ir.MatrixRangeReader(n_rows, n_cols, n_partitions),; _assert_type=hl.tmatrix(; hl.tstruct(),; hl.tstruct(col_idx=hl.tint32),; ['col_idx'],; hl.tstruct(row_idx=hl.tint32),; ['row_idx'],; hl.tstruct(),; ),; ); ). [docs]@typecheck(n=int, n_partitions=nullable(int)); def range_table(n, n_partitions=None) -> 'hail.Table':; """"""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""; check_nonnegative_and_in_range('range_table', 'n', n); if n_partitions is not None:; check_positive_and_in_range('range_table', 'n_partitions', n_partitions). return hail.Table(hail.ir.TableRange(n, n_partitions)). def check_positive_and_in_range(caller, name, value):; if value <= 0:; raise ValueError(f""'{caller}': parameter '{name}' must be positive, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}': parameter '{name}' must be less than or equal to {hail.tint32.max_value}, "" f""found {value}""; ). def check_nonnegative_and_in_range(caller, name, value):; if value < 0:; raise ValueError(f""'{caller}': parameter '{name}' must be non-negative, found {value}""); elif value > hail.tint32.max_value:; raise ValueError(; f""'{caller}'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/utils/misc.html:2801,optimiz,optimized,2801,docs/0.2/_modules/hail/utils/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/utils/misc.html,4,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"es a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as Hadoop glob; patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you; have a large compressed VCF that ends in .vcf.gz, it is likely that the; file is actually block-compressed, and you should rename the file to; .vcf.bgz accordingly. If you are unable to rename this file, please use; force_bgz=True to ignore the extension and treat this file as; block-gzipped.; If you have a non-block (aka standard) gzipped file, you may use; force=True; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset.; import_vcf() does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. Note; Using the FILTER field:; The information in the FILTER field of a VCF is contained in the; filters row field. This annotation is a set<str> and can be; queried for filter membership with expressions like; ds.filters.contains(""VQSRTranche99.5...""). Variants that are flagged; as “PASS” will have no filters applied; for these variants,; hl.len(ds.filters) is 0. Thus, filtering to PASS variants; can be done with MatrixTable.filter_rows() as follows:; >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). Column Fields. s (tstr) – Column key. This is the sample ID. Row Fields. locus (tlocus or tstruct) – Row key. The; chromosome (CHROM field) and position (POS field). If reference_genome; is defined, the type will be tlocus parameterized by; reference_genome. Otherwise, the type will be a tstruct with; two fields: contig with type tstr and position with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:42798,perform,perform,42798,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['perform']
Performance,"es of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for Psychiatric Research, which together with; Neale Lab has provided an incredibly supportive and stimulating; home.; . Principal Investigator Benjamin Neale, whose; scientific leadership has been essential for solving the right; problems.; . Principal Investigator Daniel MacArthur and the other members; of the gnomAD council.; . Jeremy Wertheimer, whose strategic advice and generous; philanthropy have been essential for growing the impact of Hail.; . We are grateful for generous support from:. The National Institute of Diabetes and Digestive and Kidney; Diseases; ; The National Institute of Mental Health; The National Human Genome Research Institute. We are grateful for generous past support from:. The ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:2636,queue,queueing,2636,index.html,https://hail.is,https://hail.is/index.html,2,['queue'],"['queue', 'queueing']"
Performance,"es the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.persist(storage_l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178243,cache,cache,178243,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"esourceFile is used; to specify files that are inputs to a Batch. These files are not generated as outputs from a; Job. Likewise, a JobResourceFile is a file that is produced by a job. JobResourceFiles; generated by one job can be used in subsequent job, creating a dependency between the jobs.; A ResourceGroup represents a collection of files that should be treated as one unit. All files; share a common root, but each file has its own extension.; A PythonResult stores the output from running a PythonJob. resource.Resource; Abstract class for resources. resource.ResourceFile; Class representing a single file resource. resource.InputResourceFile; Class representing a resource from an input file. resource.JobResourceFile; Class representing an intermediate file from a job. resource.ResourceGroup; Class representing a mapping of identifiers to a resource file. resource.PythonResult; Class representing a result from a Python job. Batch Pool Executor; A BatchPoolExecutor provides roughly the same interface as the Python; standard library’s concurrent.futures.Executor. It facilitates; executing arbitrary Python functions in the cloud. batch_pool_executor.BatchPoolExecutor; An executor which executes Python functions in the cloud. batch_pool_executor.BatchPoolFuture. Backends; A Backend is an abstract class that can execute a Batch. Currently,; there are two types of backends: LocalBackend and ServiceBackend. The; local backend executes a batch on your local computer by running a shell script. The service; backend executes a batch on Google Compute Engine VMs operated by the Hail team; (Batch Service). You can access the UI for the Batch Service; at https://batch.hail.is. backend.RunningBatchType; The type of value returned by Backend._run(). backend.Backend; Abstract class for backends. backend.LocalBackend; Backend that executes batches on a local computer. backend.ServiceBackend; Backend that executes batches on Hail's Batch Service on Google Cloud. Utilities. docker.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api.html:2731,concurren,concurrent,2731,docs/batch/api.html,https://hail.is,https://hail.is/docs/batch/api.html,1,['concurren'],['concurrent']
Performance,"ethod(max_partitions=int); def naive_coalesce(self, max_partitions: int) -> 'MatrixTable':; """"""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110279,cache,cache,110279,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['cache'],['cache']
Performance,"ethod(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_bad_ad=bool,; generic=bool,; call_fields=oneof(strlike, listof(strlike))); def import_vcf(self, path, force=False, force_bgz=False, header_file=None, min_partitions=None,; drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False,; call_fields=[]):; """"""Import VCF file(s) as variant dataset. **Examples**. >>> vds = hc.import_vcf('data/example2.vcf.bgz'). **Notes**. Hail is designed to be maximally compatible with files in the `VCF v4.2 spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :py:meth:`~hail.HailContext.import_vcf` takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as :ref:`Hadoop glob patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (*.vcf*) or block compressed; (*.vcf.bgz*). If you have a large compressed VCF that ends in *.vcf.gz*, it is likely that the file is actually block-compressed,; and you should rename the file to "".vcf.bgz"" accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the ``force`` optional parameter. However, this is not recommended -- all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:20729,load,load,20729,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"eturned a (nonsensical) value greater than one.; The max_size parameter allows us to skip large genes that would cause “out of memory” errors:; >>> skat = hl._logistic_skat(; ... mt.gene,; ... mt.weight,; ... mt.phenotype,; ... mt.GT.n_alt_alleles(),; ... covariates=[1.0],; ... max_size=10); >>> skat.show(); +-------+-------+----------+----------+-------+; | group | size | q_stat | p_value | fault |; +-------+-------+----------+----------+-------+; | int32 | int64 | float64 | float64 | int32 |; +-------+-------+----------+----------+-------+; | 0 | 11 | NA | NA | NA |; | 1 | 9 | 1.39e+02 | 1.82e-03 | 0 |; +-------+-------+----------+----------+-------+. Notes; In the SKAT R package, the “weights” are actually the square root of the weight expression; from the paper. This method uses the definition from the paper.; The paper includes an explicit intercept term but this method expects the user to specify the; intercept as an extra covariate with the value 1.; This method does not perform small sample size correction.; The q_stat return value is not the \(Q\) statistic from the paper. We match the output; of the SKAT R package which returns \(\tilde{Q}\):. \[\tilde{Q} = \frac{Q}{2}\]. Parameters:. group (Expression) – Row-indexed expression indicating to which group a variant belongs. This is typically a gene; name or an interval.; weight (Float64Expression) – Row-indexed expression for weights. Must be non-negative.; y (Float64Expression) – Column-indexed response (dependent variable) expression.; x (Float64Expression) – Entry-indexed expression for input (independent variable).; covariates (list of Float64Expression) – List of column-indexed covariate expressions. You must explicitly provide an intercept term; if desired. You must provide at least one covariate.; max_size (int) – Maximum size of group on which to run the test. Groups which exceed this size will have a; missing p-value and missing q statistic. Defaults to 46340.; null_max_iterations (int) – The maximu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:75168,perform,perform,75168,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['perform'],['perform']
Performance,"eturns:; BlockMatrix. filter_rows(rows_to_keep)[source]; Filters matrix rows. Parameters:; rows_to_keep (list of int) – Indices of rows to keep. Must be non-empty and increasing. Returns:; BlockMatrix. floor()[source]; Element-wise floor. Returns:; BlockMatrix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:21870,perform,performance,21870,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"eturns; -------; :class:`.VariantDataset`; """"""; if intervals or not n_partitions:; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals); else:; assert n_partitions is not None; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path)); intervals = reference_data._calculate_new_partitions(n_partitions); assert len(intervals) > 0; reference_data = hl.read_matrix_table(VariantDataset._reference_path(path), _intervals=intervals); variant_data = hl.read_matrix_table(VariantDataset._variants_path(path), _intervals=intervals). vds = VariantDataset(reference_data, variant_data); if VariantDataset.ref_block_max_length_field not in vds.reference_data.globals:; fs = hl.current_backend().fs; metadata_file = os.path.join(path, extra_ref_globals_file); if fs.exists(metadata_file):; with fs.open(metadata_file, 'r') as f:; metadata = json.load(f); vds.reference_data = vds.reference_data.annotate_globals(**metadata); elif _warn_no_ref_block_max_length:; warning(; ""You are reading a VDS written with an older version of Hail.""; ""\n Hail now supports much faster interval filters on VDS, but you'll need to run either""; ""\n `hl.vds.truncate_reference_blocks(vds, ...)` and write a copy (see docs) or patch the""; ""\n existing VDS in place with `hl.vds.store_ref_block_max_length(vds_path)`.""; ). return vds. [docs]def store_ref_block_max_length(vds_path):; """"""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make inter",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html:2081,load,load,2081,docs/0.2/_modules/hail/vds/variant_dataset.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/variant_dataset.html,2,['load'],['load']
Performance,"experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). nirvana() runs Nirvana on the current dataset and adds a; new row field in the location specified by name.; Examples; Add Nirvana annotations to the dataset:; >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") . Configuration; nirvana() requires a configuration file. The format is a; .properties file, where each; line defines a property as a key-value pair of the form key = value.; nirvana() supports the following properties:. hail.nirvana.dotnet – Location of dotnet. Optional, default: dotnet.; hail.nirvana.path – Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; hail.nirvana.location – Location of Nirvana.dll. Required.; hail.nirvana.reference – Location of reference genome. Required.; hail.nirvana.cache – Location of cache. Required.; hail.nirvana.supplementaryAnnotationDirectory – Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example nirvana.properties configuration file:; hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. Annotations; A new row field is added in the location specified by name with the; following schema:; struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopSc",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:60068,cache,cache,60068,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,2,['cache'],['cache']
Performance,"ey of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; t = t.add_index(idx_name); t = t.annotate(**{field_name: hl.array(t.X[t[idx_name], :])}).select_globals(); t = t.drop(t[idx_name]); return t. def numpy_to_cols_table(X, field_name):; hail_array = X._data_array(); cols_and_X = hl.zip(A.source_table.index_globals().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); retur",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21526,load,loadings,21526,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"f projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`.pca` for more details. Users of PLINK/GCTA should be aware that Hail computes the GRM slightly; differently with regard to missing data. In Hail, the; :math:`ij` entry of the GRM :math:`MM^T` is simply the dot product of rows; :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \text{ is non-missing}\}`. In; PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:3331,load,loadings,3331,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"f terms in; the sum :math:`\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert`, i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4049,load,loadings,4049,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"f the Hail JAR appears on the; classpath before the Scala JARs.; (#13919) Fix; (#13915) which; prevented using a glob pattern in hl.import_vcf. Version 0.2.124; Released 2023-09-21. New Features. (#13608) Change; default behavior of hl.ggplot.geom_density to use a new method. The; old method is still available using the flag smoothed=True. The new; method is typically a much more accurate representation, and works; well for any distribution, not just smooth ones. Version 0.2.123; Released 2023-09-19. New Features. (#13610) Additional; setup is no longer required when using hail.plot or hail.ggplot in a; Jupyter notebook (calling bokeh.io.output_notebook or; hail.plot.output_notebook and/or setting plotly.io.renderers.default; = ‘iframe’ is no longer necessary). Bug Fixes. (#13634) Fix a bug; which caused Query-on-Batch pipelines with a large number of; partitions (close to 100k) to run out of memory on the driver after; all partitions finish.; (#13619) Fix an; optimization bug that, on some pipelines, since at least 0.2.58; (commit 23813af), resulted in Hail using essentially unbounded; amounts of memory.; (#13609) Fix a bug; in hail.ggplot.scale_color_continuous that sometimes caused errors by; generating invalid colors. Version 0.2.122; Released 2023-09-07. New Features. (#13508) The n; parameter of MatrixTable.tail is deprecated in favor of a new n_rows; parameter. Bug Fixes. (#13498) Fix a bug; where field names can shadow methods on the StructExpression class,; e.g. “items”, “keys”, “values”. Now the only way to access such; fields is through the getitem syntax, e.g. “some_struct[‘items’]”.; It’s possible this could break existing code that uses such field; names.; (#13585) Fix bug; introduced in 0.2.121 where Query-on-Batch users could not make; requests to batch.hail.is without a domain configuration set. Version 0.2.121; Released 2023-09-06. New Features. (#13385) The VDS; combiner now supports arbitrary custom call fields via the; call_fields para",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:24778,optimiz,optimization,24778,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"f type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-ta",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39883,load,loadings,39883,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"ference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102511,load,load,102511,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"file to write. :param bool overwrite: If True, overwrite any existing KT file. Cannot be used ; to read from and write to the same path. """""". self._jkt.write(output, overwrite). [docs] @handle_py4j; def cache(self):; """"""Mark this key table to be cached in memory. :py:meth:`~hail.KeyTable.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. :rtype: :class:`.KeyTable`. """"""; return KeyTable(self.hc, self._jkt.cache()). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this key table to memory and/or disk. **Examples**. Persist the key table to both memory and disk:. >>> kt = kt.persist() # doctest: +SKIP. **Notes**. The :py:meth:`~hail.KeyTable.persist` and :py:meth:`~hail.KeyTable.cache` methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.KeyTable.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.KeyTable.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP; ; :rtype: :class:`.KeyTable`; """""". return KeyTable(self.hc, self._jkt.persist(storage_level)). [docs] @handle_py4j; def unpersist(self):; """"""; Unpersists this table from memory/disk.; ; **Notes**; This function will have no effect on a table that was not previously persisted.; ; There's nothing stopping you from continuing to use a table that has been unpersisted, but doing so will result in; all previous steps taken to compute the table being performed again since the table must be recomputed. Only unpersist;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/keytable.html:23926,cache,cache,23926,docs/0.1/_modules/hail/keytable.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/keytable.html,1,['cache'],['cache']
Performance,"file={; 'bed': '{root}.bed',; 'bim': '{root}.bim',; 'fam': '{root}.fam',; 'assoc': '{root}.assoc'; }); g.command(f'''; python3 /run_gwas.py \; --vcf {vcf} \; --phenotypes {phenotypes} \; --output-file {g.ofile} \; --cores {cores}; '''); return g. A couple of things to note about this function:. The image is the image created in the previous step. We copied the run_gwas.py; script into the root directory /. Therefore, to execute the run_gwas.py script, we; call /run_gwas.py.; The run_gwas.py script takes an output-file parameter and then creates files ending with; the extensions .bed, .bim, .fam, and .assoc. In order for Batch to know the script is; creating files as a group with a common file root, we need to use the BashJob.declare_resource_group(); method. We then pass g.ofile as the output file root to run_gwas.py as that represents the temporary file; root given to all files in the resource group ({root} when declaring the resource group). Clumping By Chromosome; The second function performs clumping for a given chromosome. The input arguments are the Batch; for which to create a new BashJob, the PLINK binary file root, the association results; with at least two columns (SNP and P), and the chromosome for which to do the clumping for.; The return value is the new BashJob created.; def clump(batch, bfile, assoc, chr):; """"""; Clump association results with PLINK; """"""; c = batch.new_job(name=f'clump-{chr}'); c.image('hailgenetics/genetics:0.2.37'); c.memory('1Gi'); c.command(f'''; plink --bfile {bfile} \; --clump {assoc} \; --chr {chr} \; --clump-p1 0.01 \; --clump-p2 0.01 \; --clump-r2 0.5 \; --clump-kb 1000 \; --memory 1024. mv plink.clumped {c.clumped}; '''); return c. A couple of things to note about this function:. We use the image hailgenetics/genetics which is a publicly available Docker; image from Docker Hub maintained by the Hail team that contains many useful bioinformatics; tools including PLINK.; We explicitly tell PLINK to only use 1Gi of memory becaus",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/cookbook/clumping.html:7847,perform,performs,7847,docs/batch/cookbook/clumping.html,https://hail.is,https://hail.is/docs/batch/cookbook/clumping.html,2,['perform'],['performs']
Performance,"finite; precision, the zero eigenvalues of :math:`X^T X` or :math:`X X^T` will; only be approximately zero. If the rank is not known ahead, examining the relative sizes of the; trailing singular values should reveal where the spectrum switches from; non-zero to ""zero"" eigenvalues. With 64-bit floating point, zero; eigenvalues are typically about 1e-16 times the largest eigenvalue.; The corresponding singular vectors should be sliced away **before** an; action which realizes the block-matrix-side singular vectors. :meth:`svd` sets the singular values corresponding to negative; eigenvalues to exactly ``0.0``. Warning; -------; The first and third stages invoke distributed matrix multiplication with; parallelism bounded by the number of resulting blocks, whereas the; second stage is executed on the leader (master) node. For matrices of; large minimum dimension, it may be preferable to run these stages; separately. The performance of the second stage depends critically on the number of; leader (master) cores and the NumPy / SciPy configuration, viewable with; ``np.show_config()``. For Intel machines, we recommend installing the; `MKL <https://anaconda.org/anaconda/mkl>`__ package for Anaconda. Consequently, the optimal value of `complexity_bound` is highly; configuration-dependent. Parameters; ----------; compute_uv: :obj:`bool`; If False, only compute the singular values (or eigenvalues).; complexity_bound: :obj:`int`; Maximum value of :math:`\sqrt[3]{nmr}` for which; :func:`scipy.linalg.svd` is used. Returns; -------; u: :class:`numpy.ndarray` or :class:`BlockMatrix`; Left singular vectors :math:`U`, as a block matrix if :math:`n > m` and; :math:`\sqrt[3]{nmr}` exceeds `complexity_bound`.; Only returned if `compute_uv` is True.; s: :class:`numpy.ndarray`; Singular values from :math:`\Sigma` in descending order.; vt: :class:`numpy.ndarray` or :class:`BlockMatrix`; Right singular vectors :math:`V^T``, as a block matrix if :math:`n \leq m` and; :math:`\sqrt[3]{nmr}` excee",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:76341,perform,performance,76341,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['perform'],['performance']
Performance,"first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v': '1:1:A:C', 's': 'a', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:1:A:C', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:1:A:C', 's': 'd', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'b', 'GT': hl.Call([1, 1])},; ... {'v': '1:2:G:T', 's': 'c', 'GT': hl.Call([0, 1])},; ... {'v': '1:2:G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd',",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:19217,load,loadings,19217,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"formatted string representation of the type. Parameters; ----------; indent : :obj:`int`; Spaces to indent. Returns; -------; :class:`str`; """"""; b = []; b.append(' ' * indent); self._pretty(b, indent, increment); return ''.join(b). def _pretty(self, b, indent, increment):; b.append(str(self)). @abc.abstractmethod; def _parsable_string(self) -> str:; raise NotImplementedError. def typecheck(self, value):; """"""Check that `value` matches a type. Parameters; ----------; value; Value to check. Raises; ------; :obj:`TypeError`; """""". def check(t, obj):; t._typecheck_one_level(obj); return True. self._traverse(value, check). @abc.abstractmethod; def _typecheck_one_level(self, annotation):; raise NotImplementedError. def _to_json(self, x):; converted = self._convert_to_json_na(x); return json.dumps(converted). def _convert_to_json_na(self, x):; if x is None:; return x; else:; return self._convert_to_json(x). def _convert_to_json(self, x):; return x. def _from_json(self, s):; x = json.loads(s); return self._convert_from_json_na(x). def _convert_from_json_na(self, x, _should_freeze: bool = False):; if x is None:; return x; else:; return self._convert_from_json(x, _should_freeze). def _convert_from_json(self, x, _should_freeze: bool = False):; return x. def _from_encoding(self, encoding):; return self._convert_from_encoding(ByteReader(memoryview(encoding))). def _to_encoding(self, value) -> bytes:; buf = bytearray(); self._convert_to_encoding(ByteWriter(buf), value); return bytes(buf). def _convert_from_encoding(self, byte_reader, _should_freeze: bool = False):; raise ValueError(""Not implemented yet""). def _convert_to_encoding(self, byte_writer, value):; raise ValueError(""Not implemented yet""). @staticmethod; def _missing(value):; return value is None or value is pd.NA. def _traverse(self, obj, f):; """"""Traverse a nested type and object. Parameters; ----------; obj : Any; f : Callable[[HailType, Any], bool]; Function to evaluate on the type and object. Traverse children if; the f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/types.html:5224,load,loads,5224,docs/0.2/_modules/hail/expr/types.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/types.html,2,['load'],['loads']
Performance,"func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:167581,queue,queue,167581,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['queue'],['queue']
Performance,"g``. :param str bed: PLINK BED file. :param str bim: PLINK BIM file. :param str fam: PLINK FAM file. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param str missing: The string used to denote missing values **only** for the phenotype field. This is in addition to ""-9"", ""0"", and ""N/A"" for case-control phenotypes. :param str delimiter: FAM file field delimiter regex. :param bool quantpheno: If True, FAM phenotype is interpreted as quantitative. :return: Variant dataset imported from PLINK binary file.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jhc.importPlink(bed, bim, fam, joption(min_partitions), delimiter, missing, quantpheno). return VariantDataset(self, jvds). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; drop_samples=bool,; drop_variants=bool); def read(self, path, drop_samples=False, drop_variants=False):; """"""Read .vds files as variant dataset. When loading multiple VDS files, they must have the same; sample IDs, genotype schema, split status and variant metadata. :param path: VDS files to read.; :type path: str or list of str. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations; or gneotypes. :param bool drop_variants: If True, create samples-only variant; dataset (no variants or genotypes). :return: Variant dataset read from disk.; :rtype: :class:`.VariantDataset`. """""". return VariantDataset(; self,; self._jhc.readAll(jindexed_seq_args(path), drop_samples, drop_variants)). [docs] @handle_py4j; @typecheck_method(path=strlike); def write_partitioning(self, path):; """"""Write partitioning.json.gz file for legacy VDS file. :param str path: path to VDS file.; """""". self._jhc.writePartitioning(path). [docs] @handle_py4j; @typecheck_method(path=oneof(strlike, listof(strlike)),; force=bool,; force_bgz=bool,; header_file=nullable(strlike),; min_partitions=nullable(integral),; drop_samples=bool,; store_gq=bool,; pp_as_pl=bool,; skip_ba",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:19081,load,loading,19081,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['loading']
Performance,"generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single argument as 0..N.; (#6903) Teach; BlockMatrix how to checkpoint. Performance. (#6895) Improved; performance of hl.import_bgen(...).count().; (#6948) Fixed; performance bug in BlockMatrix filtering functions.; (#6943) Improved; scaling of Table.union.; (#6980) Reduced; compute time for split_multi_hts by as much as 40%. hailctl dataproc. (#6904) Added; --dry-run option to submit.; (#6951) Fixed; --max-idle and --max-age arguments to",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86449,perform,performance,86449,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ggered by the MatrixWriteBlockMatrix; WriteBlocksRDD method. Version 0.2.45; Release 2020-06-15. Bug fixes. (#8948) Fix integer; overflow error when reading files >2G with hl.import_plink.; (#8903) Fix Python; type annotations for empty collection constructors and; hl.shuffle.; (#8942) Refactored; VCF combiner to support other GVCF schemas.; (#8941) Fixed; hl.import_plink with multiple data partitions. hailctl dataproc. (#8946) Fix bug when; a user specifies packages in hailctl dataproc start that are also; dependencies of the Hail package.; (#8939) Support; tuples in hailctl dataproc describe. Version 0.2.44; Release 2020-06-06. New Features. (#8914); hl.export_vcf can now export tables as sites-only VCFs.; (#8894) Added; hl.shuffle function to randomly permute arrays.; (#8854) Add; composable option to parallel text export for use with; gsutil compose. Bug fixes. (#8883) Fix an issue; related to failures in pipelines with force_bgz=True. Performance. (#8887) Substantially; improve the performance of hl.experimental.import_gtf. Version 0.2.43; Released 2020-05-28. Bug fixes. (#8867) Fix a major; correctness bug ocurring when calling BlockMatrix.transpose on; sparse, non-symmetric BlockMatrices.; (#8876) Fixed; “ChannelClosedException: null” in {Table, MatrixTable}.write. Version 0.2.42; Released 2020-05-27. New Features. (#8822) Add optional; non-centrality parameter to hl.pchisqtail.; (#8861) Add; contig_recoding option to hl.experimental.run_combiner. Bug fixes. (#8863) Fixes VCF; combiner to successfully import GVCFs with alleles called as .; (#8845) Fixed issue; where accessing an element of an ndarray in a call to Table.transmute; would fail.; (#8855) Fix crash in; filter_intervals. Version 0.2.41; Released 2020-05-15. Bug fixes. (#8799)(#8786); Fix ArrayIndexOutOfBoundsException seen in pipelines that reuse a; tuple value. hailctl dataproc. (#8790) Use; configured compute zone as default for hailctl dataproc connect; and hailctl dataproc modify",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:70008,perform,performance,70008,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"gh the same; effect can be achieved for * by using @. Warning; For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for + and *, place the; block matrix operand first; for -, /, and @, first convert; the ndarray to a block matrix using from_numpy(). Warning; Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product.; The \((i, j)\)-block in the product a @ b is computed by summing; the products of corresponding blocks in block row \(i\) of a and; block column \(j\) of b. So overall, in addition to this; multiplication and addition, the evaluation of a @ b realizes each; block of a as many times as the number of block columns of b; and realizes each block of b as many times as the number of; block rows of a.; This becomes a performance and resilience issue whenever a or b; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating a @ (c @ d) will; effectively evaluate c @ d as many times as the number of block rows; in a.; To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:; >>> c = BlockMatrix.read('c.bm') ; >>> d = BlockMatrix.read('d.bm') ; >>> (c @ d).write('cd.bm') ; >>> a = BlockMatrix.read('a.bm') ; >>> e = a @ BlockMatrix.read('cd.bm') . Indexing and slicing; Block matrices also support NumPy-style 2-dimensional; indexing and slicing,; with two differences.; First, slices start:stop:step must be non-empty with positive step.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional.; For example, for a block matrix bm with 10 rows and 10 columns:. bm[0, 0] is the element in row 0 and column 0 of bm.; bm[0:1, 0] is a block matrix with 1 row, 1 column,; and element bm[0, 0].; bm[2, :] is a block matri",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:3523,perform,performance,3523,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"gin` option. Optional. Overrides `hail.vep.lof.human_ancestor` and `hail.vep.lof.conservation_file`.; - **hail.vep.lof.human_ancestor** -- Location of the human ancestor file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise.; - **hail.vep.lof.conservation_file** -- Location of the conservation file for the LOFTEE plugin. Ignored if `hail.vep.plugin` is set. Required otherwise. Here is an example `vep.properties` configuration file. .. code-block:: text. hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /path/to/vep; hail.vep.lof.human_ancestor = /path/to/loftee_data/human_ancestor.fa.gz; hail.vep.lof.conservation_file = /path/to/loftee_data//phylocsf.sql. **VEP Invocation**. .. code-block:: text. <hail.vep.perl>; <hail.vep.location>; --format vcf; --json; --everything; --allele_number; --no_stats; --cache --offline; --dir <hail.vep.cache_dir>; --fasta <hail.vep.fasta>; --minimal; --assembly <hail.vep.assembly>; --plugin LoF,human_ancestor_fa:$<hail.vep.lof.human_ancestor>,filter_position:0.05,min_intron_size:15,conservation_file:<hail.vep.lof.conservation_file>; -o STDOUT. **Annotations**. Annotations with the following schema are placed in the location specified by ``root``.; The full resulting dataset schema can be queried with :py:attr:`~hail.VariantDataset.variant_schema`. .. code-block:: text. Struct{; assembly_name: String,; allele_string: String,; colocated_variants: Array[Struct{; aa_allele: String,; aa_maf: Double,; afr_allele: String,; afr_maf: Double,; allele_string: String,; amr_allele: String,; amr_maf: Double,; clin_sig: Array[String],; end: Int,; eas_allele: String,; eas_maf: Double,; ea_allele: String,,; ea_maf: Double,; eur_allele: String,; eur_maf: Double,; exac_adj_allele: String,; exac_adj_maf: Double,; exac_allele: String,; exac_afr_all",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:223848,cache,cache,223848,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"gions, please contact us on discuss.hail.is. File Format. The native file format version is now 1.4.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77382,perform,performance,77382,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"given by hard call genotypes (g.gt).; If use_dosages=True, then genotype values for per-variant association are defined by the dosage; \(\mathrm{P}(\mathrm{Het}) + 2 \cdot \mathrm{P}(\mathrm{HomVar})\). For Phred-scaled values,; \(\mathrm{P}(\mathrm{Het})\) and \(\mathrm{P}(\mathrm{HomVar})\) are; calculated by normalizing the PL likelihoods (converted from the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:96885,perform,performant,96885,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['perform'],"['performance', 'performant']"
Performance,"h in vds_paths:; vds = hl.vds.read_vds(; path,; _assert_reference_type=dataset_type.reference_type,; _assert_variant_type=dataset_type.variant_type,; _warn_no_ref_block_max_length=False,; ); n_samples = vds.n_samples(); vdses.append(VDSMetadata(path, n_samples)). vdses.sort(key=lambda x: x.n_samples, reverse=True). combiner = VariantDatasetCombiner(; save_path=save_path,; output_path=output_path,; temp_path=temp_path,; reference_genome=reference_genome,; dataset_type=dataset_type,; branch_factor=branch_factor,; target_records=target_records,; gvcf_batch_size=gvcf_batch_size,; contig_recoding=contig_recoding,; call_fields=call_fields,; vdses=vdses,; gvcfs=gvcf_paths,; gvcf_import_intervals=intervals,; gvcf_external_header=gvcf_external_header,; gvcf_sample_names=gvcf_sample_names,; gvcf_info_to_keep=gvcf_info_to_keep,; gvcf_reference_entry_fields_to_keep=gvcf_reference_entry_fields_to_keep,; ); combiner._raise_if_output_exists(); return combiner. [docs]def load_combiner(path: str) -> VariantDatasetCombiner:; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; return VariantDatasetCombiner.load(path). class Encoder(json.JSONEncoder):; def default(self, o):; if isinstance(o, VariantDatasetCombiner):; return o.to_dict(); if isinstance(o, HailType):; return str(o); if isinstance(o, tmatrix):; return o.to_dict(); return json.JSONEncoder.default(self, o). class Decoder(json.JSONDecoder):; def __init__(self, **kwargs):; super().__init__(object_hook=Decoder._object_hook, **kwargs). @staticmethod; def _object_hook(obj):; if 'name' not in obj:; return obj; name = obj['name']; if name == VariantDatasetCombiner.__name__:; del obj['name']; obj['vdses'] = [VDSMetadata(*x) for x in obj['vdses']]; obj['dataset_type'] = CombinerOutType(*(tmatrix._from_json(ty) for ty in obj['dataset_type'])); if 'gvcf_type' in obj and obj['gvcf_type']:; obj['gvcf_type'] = tmatrix._from_json(obj['gvcf_type']). rg = hl.get_reference(obj['reference_genome']); obj['reference_genome'] = rg; interva",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:33024,Load,Load,33024,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,1,['Load'],['Load']
Performance,"h minor allele frequency below some cutoff.) The factor :math:`1/m` gives each sample row approximately unit total variance (assuming linkage equilibrium) and yields the sample correlation or genetic relationship matrix (GRM) as simply :math:`MM^T`. PCA then computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:165002,load,loadings,165002,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"hail.experimental.datasets. 	Fill out the Community Feedback Survey!; . Query Docs. Batch Docs. Forum. Science. Blog. Hail Docs; ; ; ; (0.2); ; ; . ; . Installation; Hail on the Cloud; Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.datasets. Source code for hail.experimental.datasets; from typing import Optional, Union. import hail as hl; from hail.matrixtable import MatrixTable; from hail.table import Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, `",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1007,load,load,1007,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['load'],['load']
Performance,"he FILTER field of a VCF is contained in the va.filters annotation.; This annotation is a Set and can be queried for filter membership with expressions ; like va.filters.contains(""VQSRTranche99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23056,load,load,23056,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"he original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (mt._af < 1)). gt_norm = (mt._call.n_alt_alleles() - 2 * mt._af) / hl.sqrt(n_variants * 2 * mt._af * (1 - mt._af)). return mt.select_cols(scores=hl.agg.array_sum(mt._loadings * gt_norm)).cols(). def _get_expr_or_join(expr, source, other_source, loc):",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1970,load,loadings,1970,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance,"he99.5...""). Variants that are flagged as “PASS” ; will have no filters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23247,load,load,23247,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"heck(; ds=oneof(MatrixTable, lambda: hl.vds.VariantDataset),; min_af=numeric,; max_af=numeric,; min_dp=int,; max_dp=int,; min_gq=int,; ref_AF=nullable(expr_float64),; ); def compute_charr(; ds: Union[MatrixTable, 'hl.vds.VariantDataset'],; min_af: float = 0.05,; max_af: float = 0.95,; min_dp: int = 10,; max_dp: int = 100,; min_gq: int = 20,; ref_AF: Optional[Float64Expression] = None,; ):; """"""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """""". # Determine whether the input data is in the VDS format; if not, convert matrixtable to VDS and extract only the variant call information; if isinstance(ds, hl.vds.VariantDataset):; mt = ds.variant_data; else:; mt = ds. if all(x in mt.entry for x in ['LA', 'LAD', 'LGT', 'GQ']):; ad_field = 'LAD'; gt_field = 'LGT'; elif all(x in mt.entry for x in ['AD', 'GT', '",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/qc.html:60443,load,loaded,60443,docs/0.2/_modules/hail/methods/qc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/qc.html,4,['load'],"['load', 'loaded']"
Performance,"hema; Returns the signature of the sample annotations contained in this VDS. variant_schema; Returns the signature of the variant annotations contained in this VDS. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate by user-defined key and aggregation expressions to produce a KeyTable. annotate_alleles_expr; Annotate alleles with expression. annotate_genotypes_expr; Annotate genotypes with expression. annotate_global; Add global annotations from Python objects. annotate_global_expr; Annotate global with expression. annotate_samples_expr; Annotate samples with expression. annotate_samples_table; Annotate samples with a key table. annotate_variants_db; Annotate variants using the Hail annotation database. annotate_variants_expr; Annotate variants with expression. annotate_variants_table; Annotate variants with a key table. annotate_variants_vds; Annotate variants with variant annotations from .vds file. cache; Mark this variant dataset to be cached in memory. concordance; Calculate call concordance with another variant dataset. count; Returns number of samples and variants in the dataset. count_variants; Count number of variants in variant dataset. deduplicate; Remove duplicate variants. delete_va_attribute; Removes an attribute from a variant annotation field. drop_samples; Removes all samples from variant dataset. drop_variants; Discard all variants, variant annotations and genotypes. export_gen; Export variant dataset as GEN and SAMPLE file. export_genotypes; Export genotype-level information to delimited text file. export_plink; Export variant dataset as PLINK2 BED, BIM and FAM. export_samples; Export sample information to delimited text file. export_variants; Export variant information to delimited text file. export_vcf; Export variant dataset as a .vcf or .vcf.bgz file. file_version; File version of variant dataset. filter_alleles; Filter a user-defined set of alternate alleles for each variant. filter_gen",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:2476,cache,cache,2476,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['cache'],"['cache', 'cached']"
Performance,"hile this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors :math:`U_k` instead of the component scores; :math:`U_k S_k`. The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _hwe_normalized_blanczos(call_expr, k, compute_loadings). return pca(hwe_normalize(call_expr), k, compute_loadings). [docs]@typecheck(entry_expr=expr_float64, k=int, compute_loadings=bool); def pca(entry_expr, k=10, compute_loadings=False) -> Tuple[List[float], Table, Table]:; r""""""Run principal component analysis (PCA) on numeric columns derived from a; matrix table. Examples; --------. For a matrix table with variant rows, sample columns, and genotype entries,; compute the top 2 PC sample scores and eigenvalues of the matrix of 0s and; 1s encoding missingness of genotype calls. >>> eigenvalues, scores, _ = hl.pca(hl.int(hl.is_defined(dataset.GT)),; ... k=2). Warning; -------; This method does **not** automatically mean-center or normalize each column.; If desired, such transformations should be incorporated in `entry_expr`",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:4204,load,loadings,4204,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"hose versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for non-default reference genomes. Experimental. (#6488) Exposed; table.multi_way_zip_join. This takes a list of tables of; iden",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90498,perform,performance,90498,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"i, n_rows, n_cols, block_size=None, *, _assert_type=None):; """"""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__,; reads a binary file of float64 values in row-major order, such as that; produced by `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__; or :meth:`BlockMatrix.tofile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. A NumPy ndarray must have type float64 for the output of; func:`numpy.tofile` to be a valid binary input to :meth:`.fromfile`.; This is not checked. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary input file.; n_rows: :obj:`int`; Number of rows.; n_cols: :obj:`int`; Number of columns.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`default_block_size`. See Also; --------; :meth:`.from_numpy`; """""". if not block_size:; block_size = BlockMatrix.default_block_size(). return cls(; BlockMatrixRead(BlockMatrixBinaryReader(uri, [n_rows, n_cols], block_size), _assert_type=_assert_type); ). [docs] @classmethod; @typecheck_method(ndarray=np.ndarray, block_size=nullable(int)); def from_numpy(cls, ndarray, block_size=None):; """"""Distributes a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__; as a b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:11121,load,load,11121,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['load'],['load']
Performance,"identity-by-descent two; statistics, 'phik2k0' will compute the kinship; statistics and both identity-by-descent two and; zero, 'all' computes the kinship statistic and; all three identity-by-descent statistics. :return: A :py:class:`.KeyTable` mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two.; :rtype: :py:class:`.KeyTable`. """""". intstatistics = { ""phi"" : 0, ""phik2"" : 1, ""phik2k0"" : 2, ""all"" : 3 }[statistics]. return KeyTable(self.hc, self._jvdf.pcRelate(k, maf, block_size, min_kinship, intstatistics)). [docs] @handle_py4j; @typecheck_method(storage_level=strlike); def persist(self, storage_level=""MEMORY_AND_DISK""):; """"""Persist this variant dataset to memory and/or disk. **Examples**. Persist the variant dataset to both memory and disk:. >>> vds_result = vds.persist(). **Notes**. The :py:meth:`~hail.VariantDataset.persist` and :py:meth:`~hail.VariantDataset.cache` methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines. :py:meth:`~hail.VariantDataset.cache` is an alias for ; :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`. Most users will want ""MEMORY_AND_DISK"".; See the `Spark documentation <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__ ; for a more in-depth discussion of persisting data.; ; .. warning ::; ; Persist, like all other :class:`.VariantDataset` functions, is functional.; Its output must be captured. This is wrong:; ; >>> vds = vds.linreg('sa.phenotype') # doctest: +SKIP; >>> vds.persist() # doctest: +SKIP; ; The above code does NOT persist ``vds``. Instead, it copies ``vds`` and persists that result. ; The proper usage is this:; ; >>> vds = vds.pca().persist() # doctest: +SKIP. :param storage_level: Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DI",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:178055,cache,cache,178055,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"ile); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built with Sphinx using a; theme; provided by Read the Docs.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:2082,concurren,concurrent,2082,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,4,['concurren'],['concurrent']
Performance,"ime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Generate more; efficient IR for Table.flatten.; (#7740) Method; wrapping large let bindings to keep method size down. New features. (#7686) Added; comment argument to import_matrix_table, allowing lines with; certain prefixes to be ignored.; (#7688",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79611,perform,performance,79611,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"imental.import_gtf; (#7386)(#7394); Add {Table, MatrixTable}.tail.; (#7467) Added; hl.if_else as an alias for hl.cond; deprecated hl.cond.; (#7453) Add; hl.parse_int{32, 64} and hl.parse_float{32, 64}, which can; parse strings to numbers and return missing on failure.; (#7475) Add; row_join_type argument to MatrixTable.union_cols to support; outer joins on rows. Bug fixes. (#7479)(#7368)(#7402); Fix optimizer bugs.; (#7506) Updated to; latest htsjdk to resolve VCF parsing problems. hailctl dataproc. (#7460) The Spark; monitor widget now automatically collapses after a job completes. Version 0.2.26; Released 2019-10-24. New Features. (#7325) Add; string.reverse function.; (#7328) Add; string.translate function.; (#7344) Add; hl.reverse_complement function.; (#7306) Teach the VCF; combiner to handle allele specific (AS_*) fields.; (#7346) Add; hl.agg.approx_median function. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:82728,perform,performance,82728,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"in action!. count; Aggregators live in the hl.agg module. The simplest aggregator is count. It takes no arguments and returns the number of values aggregated. [1]:. import hail as hl; from bokeh.io import output_notebook,show; output_notebook(); hl.init(). hl.utils.get_movie_lens('data/'); users = hl.read_table('data/users.ht'). Loading BokehJS ... Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log; 2024-10-04 20:09:01.799 Hail: INFO: Movie Lens files found!. [2]:. users.aggregate(hl.agg.count()). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. [2]:. 943. [3]:. users.count(). [3]:. 943. stats; stats computes useful statistics about a numeric expression at once. There are also aggregators for mean, min, max, sum, product and array_sum. [4]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F""""other""""15213""; 642""M""""executive""""98101""; 757""M""""administrator""""91344""; 836""M""""administrator""""05201""; 929""M""""student""""01002""; 1053""M""""lawyer""""90703""; showing top 10 rows. [5]:. users.aggregate(hl.agg.stats(users.age)). [5]:. Struct(mean=34.05196182396607, stdev=12.186273150937211, min=7.0, max=73.0, n=943, sum=32111.0). counter; What about non-numeric data, like the occupation field?; counter is modeled on the Python Counter object: it",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/04-aggregation.html:2684,load,load,2684,docs/0.2/tutorials/04-aggregation.html,https://hail.is,https://hail.is/docs/0.2/tutorials/04-aggregation.html,1,['load'],['load']
Performance,"index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:79484,cache,cache,79484,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['cache'],['cache']
Performance,"info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """""". paths = wrap_to_list(paths). if file_per_partition and min_partitions is not None:; if min_partitions > len(paths):; raise FatalError(; f'file_per_partition is True while min partitions is {min_partitions} ,which is greater'; f' than the number of files, {len(paths)}'; ). st_reader = ir",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:65858,load,load,65858,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"ing data from VCF; Getting to know our data; Adding column fields; Query functions and the Hail Expression Language; Quality Control; Let’s do a GWAS!; Confounded!; Rare variant analysis; Epilogue. Table Tutorial; Aggregation Tutorial; Filtering and Annotation Tutorial; Table Joins Tutorial; MatrixTable Tutorial; Plotting Tutorial; GGPlot Tutorial. Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail Tutorials; GWAS Tutorial. View page source. GWAS Tutorial; This notebook is designed to provide a broad overview of Hail’s functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2003-0.2.133-4c60fddb171a.log. If the above cell ran without error, we’re ready to go!; Before using Hail, we import some standard Python libraries for use throughout the notebook. [2]:. from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(). Loading BokehJS ... Download public 1000 Genomes data; We use a small chunk of the public 1000 Genomes dataset, created by downsampling the genotyped SNPs in the full VCF to about 20 MB. We will also integrate sample and variant metadata from separate text files.; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html:1284,load,load,1284,docs/0.2/tutorials/01-genome-wide-association-study.html,https://hail.is,https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html,1,['load'],['load']
Performance,"ing the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143337,load,loadings,143337,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"ing, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chained = y_is_list and isinstance(y[0], list); if is_chained and any(len(lst) ==",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12210,perform,performance,12210,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['performance']
Performance,"ingular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t = None; return hl.eval(g.eigenvalues), scores, None if t is None else t.drop('eigenvalues', 'scores'). class TallSkinnyMatrix:; def __init__(self, blo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7574,load,loadings,7574,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"ion on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str or list of str. Returns:Key table with dropped columns",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:5138,cache,cache,5138,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['cache'],['cache']
Performance,"ions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_reference_genome. hail.expr.functions.min_rep(locus, alleles)[source]; Computes the minimal representation of a (l",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:22550,load,load,22550,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['load']
Performance,"its, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; Table – Table where each row corresponds to a row in the block matrix. tofile(uri)[source]; Collects and writes data to a binary file.; Examples; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') . To create a numpy.ndarray of the same dimensions:; >>> a = np.fromfile('/local/file').reshape((10, 20)) . Notes; This method, analogous to numpy.tofile,; produces a binary file of float64 values in row-major order, which can; be read by functions such as numpy.fromfile; (if a local file) and BlockMatrix.fromfile().; Binary files produced and consumed by tofile() and; fromfile() are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; BlockMatrix.write() and BlockMatrix.read() to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent.; The number of entries must be less than \(2^{31}\). Parameters:; uri (str, optional) – URI of binary output file. See also; to_numpy(). tree_matmul(b, *, splits, path_prefix=None)[source]; Matrix multiplication in situations with large inner dimension.; This function splits a single matrix multiplication into split_on_inner smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by file_name_prefix, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters:. b (numpy.ndarray or BlockMatrix); splits (int (keyword only argument)) – The number of smaller multiplications to do.; path_prefix (str (keyword only argument)) – The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns:; BlockMatrix. unpersist()[source]; Unpersists this block matrix from memory/dis",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:43533,load,load,43533,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['load']
Performance,"ive customization. Let’s start with an example. We are going to plot y = x^2 for x from 0 to 10. First we make a hail table representing that data:. [2]:. ht = hl.utils.range_table(10); ht = ht.annotate(squared = ht.idx**2). Every plot starts with a call to ggplot, and then requires adding a geom to specify what kind of plot you’d like to create. [3]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_line(); fig.show(). Initializing Hail with default parameters...; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2013-0.2.133-4c60fddb171a.log; SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details. aes creates an “aesthetic mapping”, which maps hail expressions to aspects of the plot. There is a predefined list of aesthetics supported by every geom. Most take an x and y at least.; With this interface, it’s easy to change out our plotting representation separate from our data. We can plot bars:. [4]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_col(); fig.show(). Or points:. [5]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared)) + geom_point(); fig.show(). There are optional aesthetics too. If we want, we could color the points based on whether they’re even or odd:. [6]:. fig = ggplot(ht, aes(x=ht.idx, y=ht.squared, color=hl.if_else(ht.idx % 2 == 0, ""even"", ""odd""))) + geom_point(); fig.show(). Note that the color aesthetic by default just takes in an expression that evaluates to str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/09-ggplot.html:2294,load,load,2294,docs/0.2/tutorials/09-ggplot.html,https://hail.is,https://hail.is/docs/0.2/tutorials/09-ggplot.html,1,['load'],['load']
Performance,"ix. classmethod from_entry_expr(entry_expr, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Creates a block matrix using a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use write_from_entry_expr() directly to; avoid writing the result twice. See write_from_entry_expr() for; further documentation. Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use write_from_entry_expr() to write to external storage.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). classmethod from_ndarray(ndarray_expression, block_size=4096)[source]; Create a BlockMatrix from an ndarray. classmet",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:22125,concurren,concurrently,22125,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['concurren'],['concurrently']
Performance,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7018,load,loadings,7018,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20717,load,loadings,20717,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"ks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of import_matrix_table.; (#7033)(#7049); Various improvements leading to overall 10-15% improvement. hailctl dataproc. (#7003) Pass through; extra arguments for hailctl dataproc list and; hailctl dataproc stop. Version 0.2.21; Released 2019-09-03. Bug fixes. (#6945) Fixed; expand_types to preserve ordering by key, also affects; to_pandas and to_spark.; (#6958) Fixed stack; overflow errors when counting the result of a Table.union. New features. (#6856) Teach; hl.agg.counter to weigh each value differently.; (#6903) Teach; hl.range to treat a single a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:86009,perform,performance,86009,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"l an; accuracy of 1e-6 is achieved. Hence a reported p-value of zero with no; issues may truly be as large as 1e-6. The accuracy and maximum number of; iterations may be controlled by the corresponding function parameters.; In general, higher accuracy requires more iterations. Caution; To process a group with \(m\) rows, several copies of an; \(m \times m\) matrix of doubles must fit in worker memory. Groups; with tens of thousands of rows may exhaust worker memory causing the; entire job to fail. In this case, use the max_size parameter to skip; groups larger than max_size. Warning; skat() considers the same set of columns (i.e., samples, points) for; every group, namely those columns for which all covariates are defined.; For each row, missing values of x are mean-imputed over these columns.; As in the example, the intercept covariate 1 must be included; explicitly if desired. Notes; This method provides a scalable implementation of the score-based; variance-component test originally described in; Rare-Variant Association Testing for Sequencing Data with the Sequence Kernel Association Test.; Row weights must be non-negative. Rows with missing weights are ignored. In; the R package skat—which assumes rows are variants—default weights; are given by evaluating the Beta(1, 25) density at the minor allele; frequency. To replicate these weights in Hail using alternate allele; frequencies stored in a row-indexed field AF, one can use the expression:; >>> hl.dbeta(hl.min(ds2.AF), 1.0, 25.0) ** 2. In the logistic case, the response y must either be numeric (with all; present values 0 or 1) or Boolean, in which case true and false are coded; as 1 and 0, respectively.; The resulting Table provides the group’s key (id), thenumber of; rows in the group (size), the variance component score q_stat, the SKAT; p-value, and a fault flag. For the toy example above, the table has the; form:. id; size; q_stat; p_value; fault. geneA; 2; 4.136; 0.205; 0. geneB; 1; 5.659; 0.195; 0. geneC",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:79389,scalab,scalable,79389,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['scalab'],['scalable']
Performance,"l pointer exception (manifesting as; scala.MatchError: null) when reading data from requester pays; buckets.; (#12739) Fix; hl.plot.cdf, hl.plot.pdf, and hl.plot.joint_plot which; were broken by changes in Hail and changes in bokeh.; (#12735) Fix; (#11738) by allowing; user to override default types in to_pandas.; (#12760) Mitigate; some JVM bytecode generation errors, particularly those related to; too many method parameters.; (#12766) Fix; (#12759) by; loosening parsimonious dependency pin.; (#12732) In Query on; Batch, fix bug that sometimes prevented terminating a pipeline using; Control-C.; (#12771) Use a; version of jgscm whose version complies with PEP 440. Version 0.2.109; Released 2023-02-08. New Features. (#12605) Add; hl.pgenchisq the cumulative distribution function of the; generalized chi-squared distribution.; (#12637); Query-on-Batch now supports hl.skat(..., logistic=False).; (#12645) Added; hl.vds.truncate_reference_blocks to transform a VDS to checkpoint; reference blocks in order to drastically improve interval filtering; performance. Also added hl.vds.merge_reference_blocks to merge; adjacent reference blocks according to user criteria to better; compress reference data. Bug Fixes. (#12650) Hail will; now throw an exception on hl.export_bgen when there is no GP; field, instead of exporting null records.; (#12635) Fix bug; where hl.skat did not work on Apple M1 machines.; (#12571) When using; Query-on-Batch, hl.hadoop* methods now properly support creation and; modification time.; (#12566) Improve; error message when combining incompatibly indexed fields in certain; operations including array indexing. Version 0.2.108; Released 2023-1-12. New Features. (#12576); hl.import_bgen and hl.export_bgen now support compression; with Zstd. Bug fixes. (#12585); hail.ggplots that have more than one legend group or facet are; now interactive. If such a plot has enough legend entries that the; legend would be taller than the plot, the legend will now be; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:39300,perform,performance,39300,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"lds:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['GT', 'GP'],; ... sample_file=""data/example.8bits.sample""). Import a BGEN file as a matrix table with genotype dosage entry field:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample""). Load a single variant from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=[hl.eval(hl.parse_variant('1:2000:A:G'))]). Load a set of variants specified by a table expression from a BGEN file:. >>> variants = hl.import_table(""data/bgen-variants.txt""); >>> variants = variants.annotate(v=hl.parse_variant(variants.v)).key_by('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:dat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:38995,Load,Load,38995,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Load'],['Load']
Performance,"le JVM Bytecode errors.; (#8645) Ease; unnecessarily strict assertion that caused errors when aggregating by; key (e.g. hl.experimental.spread).; (#8621); hl.nd.array now supports arrays with no elements; (e.g. hl.nd.array([]).reshape((0, 5))) and, consequently, matmul; with an inner dimension of zero. New features. (#8571); hl.init(skip_logging_configuration=True) will skip configuration; of Log4j. Users may use this to configure their own logging.; (#8588) Users who; manually build Python wheels will experience less unnecessary output; when doing so.; (#8572) Add; hl.parse_json which converts a string containing JSON into a Hail; object. Performance Improvements. (#8535) Increase; speed of import_vcf.; (#8618) Increase; speed of Jupyter Notebook file listing and Notebook creation when; buckets contain many objects.; (#8613); hl.experimental.export_entries_by_col stages files for improved; reliability and performance. Documentation. (#8619) Improve; installation documentation to suggest better performing LAPACK and; BLAS libraries.; (#8647) Clarify that; a LAPACK or BLAS library is a requirement for a complete Hail; installation.; (#8654) Add link to; document describing the creation of a Microsoft Azure HDInsight Hail; cluster. Version 0.2.38; Released 2020-04-21. Critical Linreg Aggregator Correctness Bug. (#8575) Fixed a; correctness bug in the linear regression aggregator. This was; introduced in version 0.2.29. See; https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; for more details. Performance improvements. (#8558) Make; hl.experimental.export_entries_by_col more fault tolerant. Version 0.2.37; Released 2020-04-14. Bug fixes. (#8487) Fix incorrect; handling of badly formatted data for hl.gp_dosage.; (#8497) Fix handling; of missingness for hl.hamming.; (#8537) Fix; compile-time errror.; (#8539) Fix compiler; error in Table.multi_way_zip_join.; (#8488) Fix; hl.agg.call_stats to appropriately throw an error for",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:72572,perform,performing,72572,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performing']
Performance,"le, Table, Table, Table). hail.methods.de_novo(mt, pedigree, pop_frequency_prior, *, min_gq=20, min_p=0.05, max_parent_ab=0.05, min_child_ab=0.2, min_dp_ratio=0.1, ignore_in_sample_allele_frequency=False)[source]; Call putative de novo events from trio data. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; Call de novo events:; >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree, pop_frequency_prior=priors[dataset.row_key].AF). Notes; This method assumes the GATK high-throughput sequencing fields exist:; GT, AD, DP, GQ, PL.; This method replicates the functionality of Kaitlin Samocha’s de novo; caller. The version; corresponding to git commit bde3e40 is implemented in Hail with her; permission and assistance.; This method produces a Table with the following fields:. locus (locus) – Variant locus.; alleles (array<str>) – Variant alleles.; id (str) – Proband sample ID.; prior (float64) – Site frequency prior. It is the maximum of:; the computed dataset alternate allele frequency, the; pop_frequency_prior parameter, and the global prior; 1 / 3e7. If the ignore_in_sample_allele_frequency parameter is True,; then the computed allele frequency is not included in the calculation, and the; prior is the maximum of the pop_frequency_prior and 1 / 3e7.; proband (struct) – Proband column fields from mt.; father (struct) – Father column fields from mt.; mother (struct) – Mother column fields from mt.; proband_entry (struct) – Proband entry fields from mt.; father_entry",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:52807,throughput,throughput,52807,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['throughput'],['throughput']
Performance,"le. rename(mapping)[source]; Rename fields of the table.; Examples; Rename C1 to col1 and C2 to col2:; >>> table_result = table1.rename({'C1' : 'col1', 'C2' : 'col2'}). Parameters:; mapping (dict of str, str) – Mapping from old field names to new field names. Notes; Any field that does not appear as a key in mapping will not be; renamed. Returns:; Table – Table with renamed fields. repartition(n, shuffle=True)[source]; Change the number of partitions.; Examples; Repartition to 500 partitions:; >>> table_result = table1.repartition(500). Notes; Check the current number of partitions with n_partitions().; The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; \(M\) rows is first imported, each of the \(k\) partitions will; contain about \(M/k\) of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it’s recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see their documentation; for details.; When shuffle=True, Hail does a full shuffle of the data; and creates equal sized partitions. When shuffle=False,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the repartition and; coalesce commands in Spark, respectively. In particular,; when shuffle=False, n_partitions cannot exceed current; number of partitions. Parameters:. n (int) – Desired number of partitions.; shuffle (bool) – If True, use full shuffle to repartition. Returns:; Table – Repartitioned table. property row; Returns a struct expression of all row-indexed fields, including keys.; Examples; The data type of the row struct:; >>> table1.row.dtype; d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.Table.html:55761,perform,performance,55761,docs/0.2/hail.Table.html,https://hail.is,https://hail.is/docs/0.2/hail.Table.html,1,['perform'],['performance']
Performance,"lf._fields = other._fields; self._fields_inverse = other._fields_inverse. [docs]class GroupedTable(ExprContainer):; """"""Table grouped by row that can be aggregated into a new table. There are only two operations on a grouped table, :meth:`.GroupedTable.partition_hint`; and :meth:`.GroupedTable.aggregate`.; """""". def __init__(self, parent: 'Table', key_expr):; super(GroupedTable, self).__init__(); self._key_expr = key_expr; self._parent = parent; self._npartitions = None; self._buffer_size = 50. self._copy_fields_from(parent). [docs] def partition_hint(self, n: int) -> 'GroupedTable':; """"""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""; self._npartitions = n; return self. def _set_buffer_size(self, n: int) -> 'GroupedTable':; """"""Set the map-side combiner buffer size (in rows). Parameters; ----------; n : int; Buffer size. Returns; -------; :class:`.GroupedTable`; Same grouped table with a buffer size.; """"""; if n <= 0:; raise ValueError(n); self._buffer_size = n; return self. [docs] @typecheck_method(named_exprs=expr_any); def aggregate(self, **named_exprs) -> 'Table':; """"""Aggregate by group, used after :meth:`.Table.gro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:5285,optimiz,optimizer,5285,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['optimiz'],['optimizer']
Performance,"line can; be rewritten to use annotation instead of entry filtering. See also; filter_entries(), compute_entry_filter_stats(). union_cols(other, row_join_type='inner', drop_right_row_fields=True)[source]; Take the union of dataset columns. Warning; This method does not preserve the global fields from the other matrix table. Examples; Union the columns of two datasets:; >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; In order to combine two datasets, three requirements must be met:. The row keys must match.; The column key schemas and column schemas must match.; The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match.; This method creates a MatrixTable which contains all columns; from both input datasets. The set of rows included in the result is; determined by the row_join_type parameter. With the default value of 'inner', an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; With row_join_type set to 'outer', an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling distinct_by_row() on each dataset first).; This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters:. other (MatrixTable) – Dataset to concatenate.; row_join_type (str) – If outer, perform an outer join on rows; if ‘inner’, perform an; inner join. Default inner.; drop_right_row_fields (bool) – If true, non-key row fields of other are dropped. Otherwise,; non-key row fields in the two datase",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.MatrixTable.html:65925,perform,performed,65925,docs/0.2/hail.MatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.MatrixTable.html,1,['perform'],['performed']
Performance,"list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:117955,load,load,117955,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"lit before being passed to ld_prune().; >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient \(r^2\) of any pair at most bp_window_size; base pairs apart is strictly less than r2. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, phase information is; ignored. Variants that do not vary across samples are dropped.; The method prunes variants in linkage disequilibrium in three stages. The first, “local pruning” stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; memory_per_core. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions.; The second, “global correlation” stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within bp_window_size base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regar",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:44522,queue,queue,44522,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"loadings and scores as features and projections.; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; even ignoring the above discrepancy that means the left singular vectors \(U_k\) instead of the component scores \(U_k S_k\). While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143241,load,loadings,143241,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"loat`; Probability of keeping each row.; seed : :obj:`int`; Random seed. Returns; -------; :class:`.Table`; Table with approximately ``p * n_rows`` rows.; """""". if not 0 <= p <= 1:; raise ValueError(""Requires 'p' in [0,1]. Found p={}"".format(p)). return self.filter(hl.rand_bool(p, seed)). [docs] @typecheck_method(n=int, shuffle=bool); def repartition(self, n, shuffle=True) -> 'Table':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:92697,perform,performance,92697,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['perform'],['performance']
Performance,"ls of genes or transcripts. export_entries_by_col(mt, path[, ...]); Export entries of the mt by column as separate text files. pc_project(call_expr, loadings_expr, af_expr); Projects genotypes onto pre-computed PCs. dplyr-inspired Methods. gather(ht, key, value, *fields); Collapse fields into key-value pairs. separate(ht, field, into, delim); Separate a field into multiple fields by splitting on a delimiter character or position. spread(ht, field, value[, key]); Spread a key-value pair of fields across multiple fields. Functions. hail.experimental.load_dataset(name, version, reference_genome, region='us-central1', cloud='gcp')[source]; Load a genetic dataset from Hail’s repository.; Example; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters:. name (str) – Name of the dataset to load.; version (str, optional) – Version of the named dataset to load (see available versions in; documentation). Possibly None for some datasets.; reference_genome (str, optional) – Reference genome build, 'GRCh37' or 'GRCh38'. Possibly None; for some datasets.; region (str) – Specify region for bucket, 'us', 'us-central1', or 'europe-west1', (default is; 'us-central1').; cloud (str) – Specify if using Google Cloud Platform or Amazon Web Services,; 'gcp' or 'aws' (default is 'gcp'). Note; The 'aws' cloud platform is currently only available for the 'us'; region. Returns:; Table, MatrixTable, or BlockMatrix. hail.experimental.ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None)[source]; Calculate LD scores.; Example; >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:4270,load,load,4270,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['load']
Performance,"ls. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where the filter_intervals optimization could make; scans return incorrect results.; (#6459)(#6458); Fixed rare correctness bug in the filter_intervals optimization; which could result too many rows being kept.; (#6496) Fixed html; output of show methods to truncate long field contents.; (#6478) Fixed the; broken documentation for the experimental approx_cdf and; approx_quantiles aggregators.; (#6504) Fix; Table.show collecting data twice while running in Jupyter; notebooks.; (#6571) Fixed the; message printed in hl.concordance to print the number of; overlapping samples, not the full list of overlapping sample IDs.; (#6583) Fixed; hl.plot.manhattan for no",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:90370,perform,performance,90370,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ls; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Module code; hail.experimental.ldscore. Source code for hail.experimental.ldscore; import hail as hl; from hail.expr.expressions import expr_float64, expr_locus, expr_numeric; from hail.linalg import BlockMatrix; from hail.table import Table; from hail.typecheck import nullable, oneof, sequenceof, typecheck; from hail.utils import new_temp_file, wrap_to_list. [docs]@typecheck(; entry_expr=expr_float64,; locus_expr=expr_locus(),; radius=oneof(int, float),; coord_expr=nullable(expr_float64),; annotation_exprs=nullable(oneof(expr_numeric, sequenceof(expr_numeric))),; block_size=nullable(int),; ); def ld_score(entry_expr, locus_expr, radius, coord_expr=None, annotation_exprs=None, block_size=None) -> Table:; """"""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | b",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html:1183,Load,Load,1183,docs/0.2/_modules/hail/experimental/ldscore.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/ldscore.html,1,['Load'],['Load']
Performance,"ls_by()'""; ); Env.hc()._warn_cols_order = False. return Table(ir.MatrixColsTable(self._mir)). [docs] def entries(self) -> Table:; """"""Returns a matrix in coordinate table form. Examples; --------; Extract the entry table:. >>> entries_table = dataset.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:88772,perform,performance,88772,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['perform'],['performance']
Performance,"lters applied; for these variants, va.filters.isEmpty() is true. Thus, ; filtering to PASS variants can be done with VariantDataset.filter_variants_expr(); as follows:; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). Annotations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cann",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23328,load,load,23328,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"lumns.; - r: ndarray of float64; The upper-triangular matrix R.; - (h, tau): ndarrays of float64; The array h contains the Householder reflectors that generate q along with r.; The tau array contains scaling factors for the reflectors; """""". assert nd.ndim == 2, f""QR decomposition requires 2 dimensional ndarray, found: {nd.ndim}"". if mode not in [""reduced"", ""r"", ""raw"", ""complete""]:; raise ValueError(f""Unrecognized mode '{mode}' for QR decomposition""). float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArrayQR(float_nd._ir, mode); indices = nd._indices; aggs = nd._aggregations; if mode == ""raw"":; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1)), indices, aggs); elif mode == ""r"":; return construct_expr(ir, tndarray(tfloat64, 2), indices, aggs); elif mode in [""complete"", ""reduced""]:; return construct_expr(ir, ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 2)), indices, aggs). [docs]@typecheck(nd=expr_ndarray(), full_matrices=bool, compute_uv=bool); def svd(nd, full_matrices=True, compute_uv=True):; """"""Performs a singular value decomposition. Parameters; ----------; nd : :class:`.NDArrayNumericExpression`; A 2 dimensional ndarray, shape(M, N).; full_matrices: :class:`.bool`; If True (default), u and vt have dimensions (M, M) and (N, N) respectively. Otherwise, they have dimensions; (M, K) and (K, N), where K = min(M, N); compute_uv : :class:`.bool`; If True (default), compute the singular vectors u and v. Otherwise, only return a single ndarray, s. Returns; -------; - u: :class:`.NDArrayNumericExpression`; The left singular vectors.; - s: :class:`.NDArrayNumericExpression`; The singular values.; - vt: :class:`.NDArrayNumericExpression`; The right singular vectors.; """"""; float_nd = nd.map(lambda x: hl.float64(x)); ir = NDArraySVD(float_nd._ir, full_matrices, compute_uv). return_type = (; ttuple(tndarray(tfloat64, 2), tndarray(tfloat64, 1), tndarray(tfloat64, 2)); if compute_uv; else tndarray(tfloat64, 1); ); return construct_expr(ir, return_",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:11320,Perform,Performs,11320,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Perform'],['Performs']
Performance,"m old to new sample IDs. :return: Dataset with remapped sample IDs.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.renameSamples(mapping); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(num_partitions=integral,; shuffle=bool); def repartition(self, num_partitions, shuffle=True):; """"""Increase or decrease the number of variant dataset partitions. **Examples**. Repartition the variant dataset to have 500 partitions:. >>> vds_result = vds.repartition(500). **Notes**. Check the current number of partitions with :py:meth:`.num_partitions`. The data in a variant dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. When a variant dataset with :math:`M` variants is first imported, each of the :math:`k` partition will contain about :math:`M/k` of the variants. Since each partition has some computational overhead, decreasing the number of partitions can improve performance after significant filtering. Since it's recommended to have at least 2 - 4 partitions per core, increasing the number of partitions can allow one to take advantage of more cores. Partitions are a core concept of distributed computation in Spark, see `here <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__ for details. With ``shuffle=True``, Hail does a full shuffle of the data and creates equal sized partitions. With ``shuffle=False``, Hail combines existing partitions to avoid a full shuffle. These algorithms correspond to the ``repartition`` and ``coalesce`` commands in Spark, respectively. In particular, when ``shuffle=False``, ``num_partitions`` cannot exceed current number of partitions. :param int num_partitions: Desired number of partitions, must be less than the current number if ``shuffle=False``. :param bool shuffle: If true, use full shuffle to repartition. :return: Variant dataset wi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:192087,perform,performance,192087,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performance']
Performance,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBack",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6686,load,loadings,6686,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"mal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; ---",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20385,load,loadings,20385,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"mal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.row_correlation(entry_expr, block_size=None)[source]; Computes the correlation matrix between row vectors.; Examples; Consider the following dataset with three variants and four samples:; >>> data = [{'v'",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18553,load,loadings,18553,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,2,['load'],['loadings']
Performance,"many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with; missingness. Bug fixes. (#5088) Fixed name; separator in MatrixTable.make_table.; (#5104) Fixed; optimizer bug related to experimental functionality.; (#5122) Fixed error; constructing Table or MatrixTable objects with fields with; certain character patterns like $. Version 0.2.7; Released 2019-01-03. New features. (#5046)(experimental); Added option to BlockMatrix.export_rectangles to export as; NumPy-compatible binary. Performance improvements. (#5050) Short-circuit; iteration in logistic_regression_rows and; poisson_regression_rows if NaNs appear. Version 0.2.6; Released 2018-12-17. New features. (#4962) Expanded; comparison operators (==, !=, <, <=, >, >=); to support expressions of every type.; (#4927) Expanded; functionality of Table.order_by to support ordering by arbitrary; expressions, instead of just top-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:101798,perform,performance,101798,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"minator \(m\) is replaced with the number of terms in; the sum \(\lvert\mathcal{C}_i\cap\mathcal{C}_j\rvert\), i.e. the; number of variants where both samples have non-missing genotypes. While this; is arguably a better estimator of the true GRM (trading shrinkage for; noise), it has the drawback that one loses the clean interpretation of the; loadings and scores as features and projections; Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM, i.e.; the left singular vectors \(U_k\) instead of the component scores; \(U_k S_k\). The scores have the advantage of representing true; projections of the data onto features with the variance of a score; reflecting the variance explained by the corresponding feature. In PC; bi-plots this amounts to a change in aspect ratio; for use of PCs as; covariates in regression it is immaterial. Parameters:. call_expr (CallExpression) – Entry-indexed call expression.; k (int) – Number of principal components.; compute_loadings (bool) – If True, compute row loadings. Returns:; (list of float, Table, Table) – List of eigenvalues, table with column scores, table with row loadings. hail.methods.genetic_relatedness_matrix(call_expr)[source]; Compute the genetic relatedness matrix (GRM).; Examples; >>> grm = hl.genetic_relatedness_matrix(dataset.GT). Notes; The genetic relationship matrix (GRM) \(G\) encodes genetic correlation; between each pair of samples. It is defined by \(G = MM^T\) where; \(M\) is a standardized version of the genotype matrix, computed as; follows. Let \(C\) be the \(n \times m\) matrix of raw genotypes; in the variant dataset, with rows indexed by \(n\) samples and columns; indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the; number of alternate alleles of variant \(j\) carried by sample; \(i\), which can be 0, 1, 2, or missing. For each variant \(j\),; the sample alternate allele frequency \(p_j\) is computed as half the; mean of the non-missing entries of column \(j\). Entries of \(M\",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:30598,load,loadings,30598,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['load'],['loadings']
Performance,"ming of a bi-allelic site leading to a change in position; `1:10000:AATAA,AAGAA` => `1:10002:T:G`. :param int max_shift: maximum number of base pairs by which; a split variant can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162164,load,loadings,162164,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"mmended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped.; If generic equals True, the genotype schema is a TStruct with field names equal to the IDs of the FORMAT fields.; The GT field is automatically read in as a TCall type. To specify additional fields to import as a; TCall type, use the call_fields parameter. All other fields are imported as the type specified in the FORMAT header field.; An example genotype schema after importing a VCF with generic=True is; Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. Warning. The variant dataset generated with generic=True will have significantly slower performance.; Not all VariantDataset methods will work with a generic genotype schema.; The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. import_vcf() does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant.; Since Hail’s genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If generic=False, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS “genotype schema.; Below are two example haploid genotypes and diploid equivalents that Hail sees.; Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:0,0,9:9:24:24,1000,40,1000:1000:0. Note; Using the FILTER field:; Th",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:21062,perform,performance,21062,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['perform'],['performance']
Performance,"model with parameters \((\beta^0_v, \beta^1_v, \ldots, \beta^c_v), \sigma_{g_v}^2)\), with \(\delta\) fixed at \(\hat\delta\) in both. The latter fit is simply that of the global model, \(((0, \hat{\beta}^1, \ldots, \hat{\beta}^c), \hat{\sigma}_g^2)\). The likelihood ratio test statistic is given by. \[\chi^2 = n \, \mathrm{ln}\left(\frac{\hat{\sigma}^2_g}{\hat{\sigma}_{g,v}^2}\right)\]; and follows a chi-squared distribution with one degree of freedom. Here the ratio \(\hat{\sigma}^2_g / \hat{\sigma}_{g,v}^2\) captures the degree to which adding the variant \(v\) to the global model reduces the residual phenotypic variance.; Kinship Matrix; FastLMM uses the Realized Relationship Matrix (RRM) for kinship. This can be computed with rrm(). However, any instance of KinshipMatrix may be used, so long as sample_list contains the complete samples of the caller variant dataset in the same order.; Low-rank approximation of kinship for improved performance; lmmreg() can implicitly use a low-rank approximation of the kinship matrix to more rapidly fit delta and the statistics for each variant. The computational complexity per variant is proportional to the number of eigenvectors used. This number can be specified in two ways. Specify the parameter n_eigs to use only the top n_eigs eigenvectors. Alternatively, specify dropped_variance_fraction to use as many eigenvectors as necessary to capture all but at most this fraction of the sample variance (also known as the trace, or the sum of the eigenvalues). For example, dropped_variance_fraction=0.01 will use the minimal number of eigenvectors to account for 99% of the sample variance. Specifying both parameters will apply the more stringent (fewest eigenvectors) of the two.; Further background; For the history and mathematics of linear mixed models in genetics, including FastLMM, see Christoph Lippert’s PhD thesis. For an investigation of various approaches to defining kinship, see Comparison of Methods to Account for Relatedness ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:106649,perform,performance,106649,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"mple_file=nullable(str),; tolerance=numeric,; min_partitions=nullable(int),; chromosome=nullable(str),; reference_genome=nullable(reference_genome_type),; contig_recoding=nullable(dictof(str, str)),; skip_invalid_loci=bool,; ); def import_gen(; path,; sample_file=None,; tolerance=0.2,; min_partitions=None,; chromosome=None,; reference_genome='default',; contig_recoding=None,; skip_invalid_loci=False,; ) -> MatrixTable:; """"""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:48011,load,load,48011,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"mples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include after the locus. ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89697,load,load,89697,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['load'],['load']
Performance,"mported from a text file or Spark DataFrame with import_table(); or from_dataframe(), generated from a variant dataset; with aggregate_by_key(), make_table(),; samples_table(), or variants_table().; In the examples below, we have imported two key tables from text files (kt1 and kt2).; >>> kt1 = hc.import_table('data/kt_example1.tsv', impute=True). ID; HT; SEX; X; Z; C1; C2; C3. 1; 65; M; 5; 4; 2; 50; 5. 2; 72; M; 6; 3; 2; 61; 1. 3; 70; F; 7; 3; 10; 81; -5. 4; 60; F; 8; 2; 11; 90; -10. >>> kt2 = hc.import_table('data/kt_example2.tsv', impute=True). ID; A; B. 1; 65; cat. 2; 72; dog. 3; 70; mouse. 4; 60; rabbit. Variables:hc (HailContext) – Hail Context. Attributes. columns; Names of all columns. key; List of key columns. num_columns; Number of columns. schema; Table schema. Methods. __init__; x.__init__(…) initializes x; see help(type(x)) for signature. aggregate_by_key; Aggregate columns programmatically. annotate; Add new columns computed from existing columns. cache; Mark this key table to be cached in memory. collect; Collect table to a local list. count; Count the number of rows. drop; Drop columns. exists; Evaluate whether a boolean expression is true for at least one row. expand_types; Expand types Locus, Interval, AltAllele, Variant, Genotype, Char, Set and Dict. explode; Explode columns of this key table. export; Export to a TSV file. export_cassandra; Export to Cassandra. export_elasticsearch; Export to Elasticsearch. export_mongodb; Export to MongoDB. export_solr; Export to Solr. filter; Filter rows. flatten; Flatten nested Structs. forall; Evaluate whether a boolean expression is true for all rows. from_dataframe; Convert Spark SQL DataFrame to key table. from_pandas; Convert Pandas DataFrame to key table. from_py. import_bed; Import a UCSC .bed file as a key table. import_fam; Import PLINK .fam file into a key table. import_interval_list; Import an interval list file in the GATK standard format. indexed; Add the numerical index of each row as a new column.",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:1504,cache,cache,1504,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,['cache'],"['cache', 'cached']"
Performance,"n \times n\) diagonal matrix of eigenvalues of \(K\) in descending order. \(S_{ii}\) is the eigenvalue of eigenvector \(U_{:,i}\); \(U^T = n \times n\) orthonormal matrix, the transpose (and inverse) of \(U\). A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. \[U^Ty \sim \mathrm{N}\left(U^TX\beta, \sigma_g^2 (S + \delta I)\right)\]; for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (\(y\)) and covariate vectors (columns of \(X\)) in \(\mathbb{R}^n\) by \(U^T\) transforms the model to one with independent residuals. For any particular value of \(\delta\), the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in \(n\). In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over \(\delta\) to find the REML estimate \((\hat{\delta}, \hat{\beta}, \hat{\sigma}_g^2)\) of the triple \((\delta, \beta, \sigma_g^2)\), which in turn determines \(\hat{\sigma}_e^2\) and \(\hat{h}^2\).; We first compute the maximum log likelihood on a \(\delta\)-grid that is uniform on the log scale, with \(\mathrm{ln}(\delta)\) running from -8 to 8 by 0.01, corresponding to \(h^2\) decreasing from 0.9995 to 0.0005. If \(h^2\) is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when \(\hat{h}^2\) is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing “lmmreg: table of delta”.; If the optimal grid point falls in the interior of the grid as expected, we then use Brent’s method to find the precise location of the maximum over the same range, with initial",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:101280,optimiz,optimization,101280,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['optimiz'],['optimization']
Performance,"n `par` must have contigs in; either `xContigs` or `yContigs` and must have positions between 0 and; the contig length given in `contigs`. Parameters; ----------; path : :class:`str`; Path to JSON file. Returns; -------; :class:`.ReferenceGenome`; """"""; with hl.hadoop_open(path) as f:; return ReferenceGenome._from_config(json.load(f)). [docs] @typecheck_method(output=str); def write(self, output):; """""" ""Write this reference genome to a file in JSON format. Examples; --------. >>> my_rg = hl.ReferenceGenome(""new_reference"", [""x"", ""y"", ""z""], {""x"": 500, ""y"": 300, ""z"": 200}); >>> my_rg.write(f""output/new_reference.json""). Notes; -----. Use :meth:`~hail.genetics.ReferenceGenome.read` to reimport the exported; reference genome in a new HailContext session. Parameters; ----------; output : :class:`str`; Path of JSON file to write.; """"""; with hl.utils.hadoop_open(output, 'w') as f:; json.dump(self._config, f). [docs] @typecheck_method(fasta_file=str, index_file=nullable(str)); def add_sequence(self, fasta_file, index_file=None):; """"""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:9256,Load,Load,9256,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,1,['Load'],['Load']
Performance,"n requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """""". if not reference_genome.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; reference_genome.name; ); ). return _func(""getReferenceSequence"", tstr, contig, position, before, after, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_genome_type); def is_valid_contig(contig, reference_genome='default') -> BooleanExpression:; """"""Returns ``True`` if `contig` is a valid contig name in `reference_genome`. Examples; --------. >>> hl.eval(hl.is_valid_contig('1', reference_genome='GRCh37')); True. >>> hl.eval(hl.is_valid_contig('chr1', reference_genome='GRCh37')); False. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :class:`.BooleanExpression`; """"""; return _func(""isValidContig"", tbool, contig, type_args=(tlocus(reference_genome),)). [docs]@typecheck(contig=expr_str, reference_genome=reference_ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:161576,load,loaded,161576,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['load'],['loaded']
Performance,"n self. if self.n_rows == 1:; index_expr = [0]; elif self.n_cols == 1:; index_expr = [1]; else:; index_expr = [1, 0]. return BlockMatrix(BlockMatrixBroadcast(self._bmir, index_expr, [self.n_cols, self.n_rows], self.block_size)). [docs] def densify(self):; """"""Restore all dropped blocks as explicit blocks of zeros. Returns; -------; :class:`.BlockMatrix`; """"""; return BlockMatrix(BlockMatrixDensify(self._bmir)). [docs] def cache(self):; """"""Persist this block matrix in memory. Notes; -----; This method is an alias for :meth:`persist(""MEMORY_ONLY"") <hail.linalg.BlockMatrix.persist>`. Returns; -------; :class:`.BlockMatrix`; Cached block matrix.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK'):; """"""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""; return Env.backend().persist_blockmatrix(self). [docs] def unpersist(self):; """"""Unpersists this block matrix from memory/disk. Notes; -----; This function will have no effect on a block matrix that was not previously; persisted. Returns; -------; :class:`.BlockMatrix`; Unpe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:42316,cache,cache,42316,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,4,"['cache', 'perform']","['cache', 'performance']"
Performance,"n to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a Hail session.; (#7823) Added; datetime functions hl.experimental.strptime and; hl.experimental.strftime.; (#7888) Added; hl.nd.array constructor from nested standard arrays. File size. (#7923) Fixed; compression problem since 0.2.23 resulting in larger-than-expected; matrix table files for datasets with few entry fields (e.g. GT-only; datasets). Performance. (#7867) Fix; performance regression leading to extra scans of data when; order_by and key_by appeared close together.; (#7901) Fix; performance regression leading to extra scans of data when; group_by/aggregate and key_by appeared close together.; (#7830) Improve; performance of array arithmetic. Bug fixes. (#7922) Fix; still-not-well-understood serialization error about; ApproxCDFCombiner.; (#7906) Fix optimizer; error by relaxing unnecessary assertion.; (#7788) Fix possible; memory leak in ht.tail and ht.head.; (#7796) Fix bug in; ingesting numpy arrays not in row-major orientation. Version 0.2.30; Released 2019-12-20. Performance. (#7771) Fixed extreme; performance regression in scans.; (#7764) Fixed; mt.entry_field.take performance regression. New features. (#7614) Added; experimental support for loops with hl.experimental.loop. Miscellaneous. (#7745) Changed; export_vcf to only use scientific notation when necessary. Version 0.2.29; Released 2019-12-17. Bug fixes. (#7229) Fixed; hl.maximal_independent_set tie breaker functionality.; (#7732) Fixed; incompatibility with old files leading to incorrect data read when; filtering intervals after read_matrix_table.; (#7642) Fixed crash; when constant-folding functions that throw errors.; (#7611) Fixed; hl.hadoop_ls to handle glob patterns correctly.; (#7653) Fixed crash; in ld_prune by unfiltering missing GTs. Performance improvements. (#7719) Gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:79351,optimiz,optimizer,79351,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"n_impute=False, center=False, normalize=False, axis='rows', block_size=None; ):; """"""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""; path = new_temp_file(); cls.write_from_entry_e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:14571,concurren,concurrently,14571,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['concurren'],['concurrently']
Performance,"ndarray(hl.tfloat64, 2), failed=hl.tbool); ir = Apply(""linear_triangular_solve_no_crash"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.annotate(solution=result.solution.reshape((-1))); return result. return_type = hl.tndarray(hl.tfloat64, 2); ir = Apply(""linear_triangular_solve"", return_type, A._ir, b._ir, lower._ir); result = construct_expr(ir, return_type, indices, aggregations); if nd_dep_ndim_orig == 1:; result = result.reshape((-1)); return result. def solve_helper(nd_coef, nd_dep, nd_dep_ndim_orig):; assert nd_coef.ndim == 2; assert nd_dep_ndim_orig in {1, 2}. if nd_dep_ndim_orig == 1:; nd_dep = nd_dep.reshape((-1, 1)). if nd_coef.dtype.element_type != hl.tfloat64:; nd_coef = nd_coef.map(lambda e: hl.float64(e)); if nd_dep.dtype.element_type != hl.tfloat64:; nd_dep = nd_dep.map(lambda e: hl.float64(e)); return nd_coef, nd_dep. [docs]@typecheck(nd=expr_ndarray(), mode=str); def qr(nd, mode=""reduced""):; r""""""Performs a QR decomposition. If K = min(M, N), then:. - `reduced`: returns q and r with dimensions (M, K), (K, N); - `complete`: returns q and r with dimensions (M, M), (M, N); - `r`: returns only r with dimensions (K, N); - `raw`: returns h, tau with dimensions (N, M), (K,). Notes; -----. The reduced QR, the default output of this function, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times n} \\; R : \mathbb{R}^{n \times n} \\; \\; Q^T Q = \mathbb{1}. The complete QR, has the following properties:. .. math::. m \ge n \\; nd : \mathbb{R}^{m \times n} \\; Q : \mathbb{R}^{m \times m} \\; R : \mathbb{R}^{m \times n} \\; \\; Q^T Q = \mathbb{1}; Q Q^T = \mathbb{1}. Parameters; ----------; nd : :class:`.NDArrayExpression`; A 2 dimensional ndarray, shape(M, N); mode : :class:`.str`; One of ""reduced"", ""complete"", ""r"", or ""raw"". Defaults to ""reduced"". Returns; -------; - q: ndarray of float64; A matrix with ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/nd/nd.html:9287,Perform,Performs,9287,docs/0.2/_modules/hail/nd/nd.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/nd/nd.html,1,['Perform'],['Performs']
Performance,"ndition_ab = '''let ab = g.ad[1] / g.ad.sum() in; ((g.isHomRef && ab <= 0.1) ||; (g.isHet && ab >= 0.25 && ab <= 0.75) ||; (g.isHomVar && ab >= 0.9))'''; vds = vds.filter_genotypes(filter_condition_ab). In [38]:. post_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)'); print('post QC call rate is %.3f' % post_qc_call_rate). post QC call rate is 0.955. Variant QC is a bit more of the same: we can use the; variant_qc; method to produce a variety of useful statistics, plot them, and filter. In [39]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; }; }. The; cache; is used to optimize some of the downstream operations. In [40]:. vds = vds.variant_qc().cache(). In [41]:. pprint(vds.variant_schema). Struct{; rsid: String,; qual: Double,; filters: Set[String],; pass: Boolean,; info: Struct{; AC: Array[Int],; AF: Array[Double],; AN: Int,; BaseQRankSum: Double,; ClippingRankSum: Double,; DP: Int,; DS: Boolean,; FS: Double,; HaplotypeScore: Double,; InbreedingCoeff: Double,; MLEAC: Array[Int],; MLEAF: Array[Double],; MQ: Double,; MQ0: Int,; MQRankSum: Double,; QD: Double,; ReadPosRankSum: Double,; set: String; },; qc: Struct{; callRate: Double,; AC: Int,; AF: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rHeterozygosity: Double,; rHetHomVar: Double,; rExpectedHetFrequency: Double,; pHWE: Double; }; }. In [42]:. variant_df = vds.variants_table().to_pandas(). plt.clf(); plt.subplot(2, 2, 1); variantgq_means = variant_df[""va.qc.gqMean""]; plt.hist(variantg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:18573,cache,cache,18573,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,2,"['cache', 'optimiz']","['cache', 'optimize']"
Performance,"ner'); .key_by(index_uid); .drop(*uids); ); result = MatrixTable(; ir.MatrixAnnotateColsTable(; (left.add_col_index(index_uid).key_cols_by(index_uid)._mir), joined._tir, uid; ); ).key_cols_by(*prev_key); return result. join_ir = ir.Join(ir.ProjectedTopLevelReference('sa', uid, new_schema), all_uids, exprs, joiner); return construct_expr(join_ir, new_schema, indices, aggregations); else:; raise NotImplementedError(); else:; raise TypeError(""Cannot join with expressions derived from '{}'"".format(src.__class__)). [docs] def index_globals(self) -> 'StructExpression':; """"""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""; return construct_expr(ir.TableGetGlobals(self._tir), self.globals.dtype). def _process_joins(self, *exprs) -> 'Table':; return process_joins(self, exprs). [docs] def cache(self) -> 'Table':; """"""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level='MEMORY_AND_DISK') -> 'Table':; """"""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apach",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:79358,cache,cache,79358,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['cache'],['cache']
Performance,"nfinity on any entry. Notes; -----. PCA is run on the columns of the numeric matrix obtained by evaluating; `entry_expr` on each entry of the matrix table, or equivalently on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr :",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6238,load,loadings,6238,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"nfo_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. to_dict()[source]; A serializable representation of this combiner. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:6021,load,load,6021,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['load'],['load']
Performance,"ng glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of partitions.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition (bool) – If True, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if True and min_partitions is less than; the number of files. Returns:; Table – Table constructed from imported data. hail.methods.import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, call_fields=['PGT'], reference_genome='default', contig_recoding=None, array_elements_required=True, skip_invalid_loci=False, entry_float_type=dtype('float64'), filter=None, find_replace=None, n_partitions=None, block_size=None, _create_row_uids=False, _create_col_uids=False)[source]; Import VCF file(s) as a MatrixTable.; Examples; Import a standard bgzipped VCF with GRCh37 as the reference genome.; >>> ds = hl.import_vcf('data/example2.vcf.bgz', reference_genome='GRCh37'). Impo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:37696,load,load,37696,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ng time; Estimating cost. Query-on-Batch; Google Cloud; Microsoft Azure; Amazon Web Services; Databricks. Tutorials; Reference (Python API); Configuration Reference; Overview; How-To Guides; Cheatsheets; Datasets; Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Hail on the Cloud; General Advice. View page source. General Advice. Start Small; The cloud has a reputation for easily burning lots of money. You don’t want to be the person who; spent ten thousand dollars one night without thinking about it. Luckily, it’s easy to not be that person!; Always start small. For Hail, this means using a two worker Spark cluster and experimenting on a small; fraction of the data. For genetic data, make sure your scripts work on chromosome 22 (the 2nd smallest autosomal chromosome) before; you try running on the entire genome! If you have a matrix table you can limit to chromosome 22 with filter_rows.; Hail will make sure not to load data for other chromosomes.; import hail as hl. mt = hl.read_matrix_table('gs://....'); mt = mt.filter_rows(mt.locus.contig == '22'). Hail’s hl.balding_nichols_model creates a random genotype dataset with configurable numbers of rows and columns.; You can use these datasets for experimentation.; As you’ll see later, the smallest Hail cluster (on GCP) costs about 3 dollars per hour. Each time you think you need to double; the size of your cluster ask yourself: am I prepared to spend twice as much money per hour?. Estimating time; Estimating the time and cost of a Hail operation is often simple. Start a small cluster and use filter_rows to read a small fraction of the data:; test_mt = mt.filter_rows(mt.locus.contig == '22'); print(mt.count_rows() / test_mt.count_rows()). Multiply the time spent computing results on this smaller dataset by the number printed. This yields a reasonable expectation of the time; to compute results on the full dataset using a cluster of the same size. Howe",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/cloud/general_advice.html:1221,load,load,1221,docs/0.2/cloud/general_advice.html,https://hail.is,https://hail.is/docs/0.2/cloud/general_advice.html,1,['load'],['load']
Performance,"nment variables to values to add to the environment when invoking the command.; cloud (str) – The cloud where the Batch Service is located.; image (str) – The docker image to run VEP.; data_bucket_is_requester_pays (bool) – True if the data bucket is requester pays.; regions (list of str) – A list of regions the VEP jobs can run in. In addition, the method command must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are consequence and tolerate_parse_error which are user-defined parameters to vep(),; part_id which is the partition ID, input_file which is the path to the input file where the input data can be found, and; output_file is the path to the output file where the VEP annotations are written to. An example is shown below:; def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} --format vcf {vcf_or_json} --everything --allele_number --no_stats --cache --offline --minimal --assembly GRCh37 --dir={self.data_mount} --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz -o STDOUT; '''. The following environment variables are added to the job’s environment:. VEP_BLOCK_SIZE - The maximum number of variants provided as input to each invocation of VEP.; VEP_PART_ID - Partition ID.; VEP_DATA_MOUNT - Location where the vep data is mounted (same as data_mount in the config).; VEP_CONSEQUENCE - Integer equal to 0 or 1 on whether csq is False or True.; VEP_TOLERATE_PARSE_ERROR - Integer equal to 0 or 1 on whether tolerate_parse_error is False or True.; VEP_OUTPUT_FILE - Str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:5620,cache,cache,5620,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['cache'],['cache']
Performance,"no.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female]). As above but with at most 100 Newton iterations and a stricter-than-default tolerance of 1e-8:; >>> result_ht = hl.logistic_regression_rows(; ... test='wald',; ... y=[dataset.pheno.is_case, dataset.pheno.is_case], # where pheno values are 0, 1, or missing; ... x=dataset.GT.n_alt_alleles(),; ... covariates=[1, dataset.pheno.age, dataset.pheno.is_female],; ... max_iterations=100,; ... tolerance=1e-8). Warning; logistic_regression_rows() considers the same set of; columns (i.e., samples, points) for every row, namely those columns for; which all response variables and covariates are defined. For each row, missing values of; x are mean-imputed over these columns. As in the example, the; intercept covariate 1 must be included explicitly if desired. Notes; This method performs, for each row, a significance test of the input; variable in predicting a binary (case-control) response variable based; on the logistic regression model. The response variable type must either; be numeric (with all present values 0 or 1) or Boolean, in which case; true and false are coded as 1 and 0, respectively.; Hail supports the Wald test (‘wald’), likelihood ratio test (‘lrt’),; Rao score test (‘score’), and Firth test (‘firth’). Hail only includes; columns for which the response variable and all covariates are defined.; For each row, Hail imputes missing input values as the mean of the; non-missing values.; The example above considers a model of the form. \[\mathrm{Prob}(\mathrm{is\_case}) =; \mathrm{sigmoid}(\beta_0 + \beta_1 \, \mathrm{gt}; + \beta_2 \, \mathrm{age}; + \beta_3 \, \mathrm{is\_female} + \varepsilon),; \quad; \varepsilon \sim \mathrm{N}(0, \sigma^2)\]; where \(\mathrm{sigmoid}\) is the sigmoid function, the genotype; \(\mathrm{gt}\) is coded as 0 for HomRef, 1 for Het, and 2 for; HomVar, and the Boolean covariate \(\mathrm{i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:7917,perform,performs,7917,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['perform'],['performs']
Performance,"ns. Return type:KeyTable. vep(config, block_size=1000, root='va.vep', csq=False)[source]¶; Annotate variants with VEP.; vep() runs Variant Effect Predictor with; the LOFTEE plugin; on the current variant dataset and adds the result as a variant annotation.; Examples; Add VEP annotations to the dataset:; >>> vds_result = vds.vep(""data/vep.properties"") . Configuration; vep() needs a configuration file to tell it how to run; VEP. The format is a .properties file.; Roughly, each line defines a property as a key-value pair of the form key = value. vep supports the following properties:. hail.vep.perl – Location of Perl. Optional, default: perl.; hail.vep.perl5lib – Value for the PERL5LIB environment variable when invoking VEP. Optional, by default PERL5LIB is not set.; hail.vep.path – Value of the PATH environment variable when invoking VEP. Optional, by default PATH is not set.; hail.vep.location – Location of the VEP Perl script. Required.; hail.vep.cache_dir – Location of the VEP cache dir, passed to VEP with the –dir option. Required.; hail.vep.fasta – Location of the FASTA file to use to look up the reference sequence, passed to VEP with the –fasta option. Required.; hail.vep.assembly – Genome assembly version to use. Optional, default: GRCh37; hail.vep.plugin – VEP plugin, passed to VEP with the –plugin option. Optional. Overrides hail.vep.lof.human_ancestor and hail.vep.lof.conservation_file.; hail.vep.lof.human_ancestor – Location of the human ancestor file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise.; hail.vep.lof.conservation_file – Location of the conservation file for the LOFTEE plugin. Ignored if hail.vep.plugin is set. Required otherwise. Here is an example vep.properties configuration file; hail.vep.perl = /usr/bin/perl; hail.vep.path = /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin; hail.vep.location = /path/to/vep/ensembl-tools-release-81/scripts/variant_effect_predictor/variant_effect_predictor.pl; hail.vep.cache_dir = /pa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:175075,cache,cache,175075,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"nt dataset is divided into chunks called partitions, which may be stored together or across a network, so that each partition may be read and processed in parallel by available cores. Partitions are a core concept of distributed computation in Spark, see here for details. Return type:int. num_samples¶; Number of samples. Return type:int. pc_relate(k, maf, block_size=512, min_kinship=-inf, statistics='all')[source]¶; Compute relatedness estimates between individuals using a variant of the; PC-Relate method. Danger; This method is experimental. We neither guarantee interface; stability nor that the results are viable for any particular use. Examples; Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:; >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024.; >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more efficient than producing the full key table and; filtering using filter().; >>> rel = vds.pc_relate(5, 0.01, min_kinship=0.1). Method; The traditional estimator for kinship between a pair of individuals; \(i\) and \(j\), sharing the set \(S_{ij}\) of; single-nucleotide variants, from a population with allele frequencies; \(p_s\), is given by:. \[\widehat{\phi_{ij}} := \frac{1}{|S_{ij}|}\sum_{s \in S_{ij}}\frac{(g_{is} - 2 p_s) (g_{js} - 2 p_s)}{4 * \sum_{s \in S_{ij} p_s (1 - p_s)}}\]; This estimator is true under the model that the sharing of common; (relative to the population) alleles is not very informative to; relatedness (because they’re common) and the sharing of rare alleles; suggests a recent common ancestor from which the allele was inherited by; descent.; When multiple ancestry grou",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:130027,perform,performing,130027,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performing']
Performance,"nt_map = hl.literal({(c[0], c[1], c[2], c[3]): [c[4], c[5]] for c in config_counts}). tri = trio_matrix(dataset, pedigree, complete_trios=True). # this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het; father_is_het = tri.father_entry.GT.is_het(); parent_is_valid_het = (father_is_het & tri.auto_or_x_par) | (tri.mother_entry.GT.is_het() & ~father_is_het). copy_state = hl.if_else(tri.auto_or_x_par | tri.is_female, 2, 1). config = (; tri.proband_entry.GT.n_alt_alleles(),; tri.father_entry.GT.n_alt_alleles(),; tri.mother_entry.GT.n_alt_alleles(),; copy_state,; ). tri = tri.annotate_rows(counts=agg.filter(parent_is_valid_het, agg.array_sum(count_map.get(config)))). tab = tri.rows().select('counts'); tab = tab.transmute(t=tab.counts[0], u=tab.counts[1]); tab = tab.annotate(chi_sq=((tab.t - tab.u) ** 2) / (tab.t + tab.u)); tab = tab.annotate(p_value=hl.pchisqtail(tab.chi_sq, 1.0)). return tab.cache(). [docs]@typecheck(; mt=MatrixTable,; pedigree=Pedigree,; pop_frequency_prior=expr_float64,; min_gq=int,; min_p=numeric,; max_parent_ab=numeric,; min_child_ab=numeric,; min_dp_ratio=numeric,; ignore_in_sample_allele_frequency=bool,; ); def de_novo(; mt: MatrixTable,; pedigree: Pedigree,; pop_frequency_prior,; *,; min_gq: int = 20,; min_p: float = 0.05,; max_parent_ab: float = 0.05,; min_child_ab: float = 0.20,; min_dp_ratio: float = 0.10,; ignore_in_sample_allele_frequency: bool = False,; ) -> Table:; r""""""Call putative *de novo* events from trio data. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. Call de novo events:. >>> pedigree = hl.Pedigree.read('data/trios.fam'); >>> priors = hl.import_table('data/gnomadFreq.tsv', impute=True); >>> priors = priors.transmute(**hl.parse_variant(priors.Variant)).key_by('locus', 'alleles'); >>> de_novo_results = hl.de_novo(dataset, pedigree,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html:19453,cache,cache,19453,docs/0.2/_modules/hail/methods/family_methods.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/family_methods.html,2,['cache'],['cache']
Performance,"nterval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid arguments here except:; `no_header`, `delimiter`, `impute`, `skip_blank_lines`, `types`, and `comment` as these; are used by import_bed. Returns; -------; :class:`.Table`; Interval-keyed table.; """""". # UCSC BED spec defined here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1. t = import_table(; path,; no_header=True,; delimiter=r""\s+"",; impute=False,; skip_blank_lines=True,; types={'f0': tstr, 'f1': tint32, 'f2': tint32, 'f3': tstr, 'f4': tstr},; comment=[""""""^browser.*"""""", """"""^track.*"""""", r""""""^\w+=(""[\w\d ]+""|\d+).*""""""],; **kwargs,; ). if contig_recoding is not None:; contig_recoding = hl.literal(contig_recoding). def recode_contig(x):; if contig_recoding is None:; return x; return contig_recoding.get(x, x). i",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:30662,load,loaded,30662,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['loaded']
Performance,"nterval:; >>> vds_result = vds.filter_intervals(Interval.parse('17:38449840-38530994')). Another way of writing this same query:; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))). Two identical ways of parsing a list of intervals:; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]. Use this interval list to filter:; >>> vds_result = vds.filter_intervals(intervals). Notes; This method takes an argument of Interval or list of Interval.; Based on the keep argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; 15:100000 but not 15:101000.; >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap any supplied interval will not be loaded at all. This property; enables filter_intervals to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with filter_variants_expr(); may come to mind first:; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'). However, it is much faster (and easier!) to use this method:; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). Note; A KeyTable keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for filter_variants_table() for an example. This is useful for; using interval files to filter a dataset. Parameters:; intervals (Interval or list of Interval) – Interval(s) to keep or remove.; keep (bool) – Keep variants overlapping an interval if True, remove varian",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:54206,perform,performs,54206,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ntly on the rows; of the **transposed** numeric matrix :math:`M` referenced below. PCA computes the SVD. .. math::. M = USV^T. where columns of :math:`U` are left singular vectors (orthonormal in; :math:`\mathbb{R}^n`), columns of :math:`V` are right singular vectors; (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2,; \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`.; Typically one computes only the first :math:`k` singular vectors and values,; yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute ro",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6365,load,loadings,6365,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,4,['load'],['loadings']
Performance,"nts considered; - **nCalled** (*Long*) -- Number of variants with a genotype call; - **expectedHoms** (*Double*) -- Expected number of homozygotes; - **observedHoms** (*Long*) -- Observed number of homozygotes. :param float maf_threshold: Minimum minor allele frequency threshold. :param bool include_par: Include pseudoautosomal regions. :param float female_threshold: Samples are called females if F < femaleThreshold. :param float male_threshold: Samples are called males if F > maleThreshold. :param str pop_freq: Variant annotation for estimate of MAF.; If None, MAF will be computed. :return: Annotated dataset.; :rtype: :py:class:`.VariantDataset`; """""". jvds = self._jvdf.imputeSex(maf_threshold, include_par, female_threshold, male_threshold, joption(pop_freq)); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(right=vds_type); def join(self, right):; """"""Join two variant datasets. **Notes**. This method performs an inner join on variants,; concatenates samples, and takes variant and; global annotations from the left dataset (self). The datasets must have distinct samples, the same sample schema, and the same split status (both split or both multi-allelic). :param right: right-hand variant dataset; :type right: :py:class:`.VariantDataset`. :return: Joined variant dataset; :rtype: :py:class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvds.join(right._jvds)). [docs] @handle_py4j; @typecheck(datasets=tupleof(vds_type)); def union(*datasets):; """"""Take the union of datasets vertically (include all variants). **Examples**. .. testsetup::. vds_autosomal = vds; vds_chromX = vds; vds_chromY = vds. Union two datasets:. >>> vds_union = vds_autosomal.union(vds_chromX). Given a list of datasets, union them all:. >>> all_vds = [vds_autosomal, vds_chromX, vds_chromY]. The following three syntaxes are equivalent:. >>> vds_union1 = vds_autosomal.union(vds_chromX, vds_chromY); >>> vds_union2 = all_vds[0].union(*all_vds[1:]); >>> vds_union3 = Va",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:90574,perform,performs,90574,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performs']
Performance,"o partition the GVCF files. The same partitioning is used; for all GVCF files. Finer partitioning yields more parallelism but less work per task.; gvcf_info_to_keep (list of str or None) – GVCF INFO fields to keep in the gvcf_info entry field. By default, all fields; except END and DP are kept.; gvcf_reference_entry_fields_to_keep (list of str or None) – Genotype fields to keep in the reference table. If empty, the first 10,000 reference block; rows of mt will be sampled and all fields found to be defined other than GT, AD,; and PL will be entry fields in the resulting reference matrix in the dataset. Attributes. default_exome_interval_size; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size; A reasonable partition size in basepairs given the density of genomes. finished; Have all GVCFs and input Variant Datasets been combined?. gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. Methods. load; Load a VariantDatasetCombiner from path. run; Combine the specified GVCFs and Variant Datasets. save; Save a VariantDatasetCombiner to its save_path. step; Run one layer of combinations. to_dict; A serializable representation of this combiner. __eq__(other)[source]; Return self==value. default_exome_interval_size = 60000000; A reasonable partition size in basepairs given the density of exomes. default_genome_interval_size = 1200000; A reasonable partition size in basepairs given the density of genomes. property finished; Have all GVCFs and input Variant Datasets been combined?. property gvcf_batch_size; The number of GVCFs to combine into a Variant Dataset at once. static load(path)[source]; Load a VariantDatasetCombiner from path. run()[source]; Combine the specified GVCFs and Variant Datasets. save()[source]; Save a VariantDatasetCombiner to its save_path. step()[source]; Run one layer of combinations.; run() effectively runs step() until all GVCFs and Variant Datasets have been; combined. t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html:5329,load,load,5329,docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,https://hail.is,https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html,1,['load'],['load']
Performance,"oadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference g",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:34155,perform,performs,34155,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['perform'],['performs']
Performance,"ocally-indexed array to globally-indexed. store_ref_block_max_length(vds_path); Patches an existing VDS file to store the max reference block length for faster interval filters. Variant Dataset Combiner. VDSMetadata; The path to a Variant Dataset and the number of samples within. VariantDatasetCombiner; A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. new_combiner(*, output_path, temp_path[, ...]); Create a new VariantDatasetCombiner or load one from save_path. load_combiner(path); Load a VariantDatasetCombiner from path. The data model of VariantDataset; A VariantDataset is the Hail implementation of a data structure called the; “scalable variant call representation”, or SVCR. The Scalable Variant Call Representation (SVCR); Like the project VCF (multi-sample VCF) representation, the scalable variant; call representation is a variant-by-sample matrix of records. There are two; fundamental differences, however:. The scalable variant call representation is sparse. It is not a dense; matrix with every entry populated. Reference calls are defined as intervals; (reference blocks) exactly as they appear in the original GVCFs. Compared to; a VCF representation, this stores less data but more information, and; makes it possible to keep reference information about every site in the; genome, not just sites at which there is variation in the current cohort. A; VariantDataset has a component table of reference information,; vds.reference_data, which contains the sparse matrix of reference blocks.; This matrix is keyed by locus (not locus and alleles), and contains an; END field which denotes the last position included in the current; reference block.; The scalable variant call representation uses local alleles. In a VCF,; the fields GT, AD, PL, etc contain information that refers to alleles in the; VCF by index. At highly multiallelic sites, the number of elements in the; AD/PL lists explodes to huge numbers, even though the inf",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/vds/index.html:4060,scalab,scalable,4060,docs/0.2/vds/index.html,https://hail.is,https://hail.is/docs/0.2/vds/index.html,1,['scalab'],['scalable']
Performance,"ocally-pruned matrix; table as a block matrix is n_locally_pruned_variants / block_size.; The third, “global pruning” stage applies maximal_independent_set(); to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; keep_higher_maf is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on BlockMatrix.from_entry_expr with; regard to memory and Hadoop replication errors. Parameters:. call_expr (CallExpression) – Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 (float) – Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size (int) – Window size in base pairs (inclusive upper bound).; memory_per_core (int) – Memory in MB per core for local pruning queue.; keep_higher_maf (int) – If True, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size (int, optional) – Block size for block matrices in the second stage.; Default given by BlockMatrix.default_block_size(). Returns:; Table – Table of a maximal independent set of variants. hail.methods.compute_charr(ds, min_af=0.05, max_af=0.95, min_dp=10, max_dp=100, min_gq=20, ref_AF=None)[source]; Compute CHARR, the DNA sample contamination estimator. Danger; This functionality is experimental. It may not be tested as; well as other parts of Hail and the interface is subject to; change. Notes; The returned table has the sample ID field, plus the field:. charr (float64): CHARR contamination estimation. Note; It is possible to use gnomAD reference allele frequencies with the following:; >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') ; >>> charr_r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/genetics.html:45976,queue,queue,45976,docs/0.2/methods/genetics.html,https://hail.is,https://hail.is/docs/0.2/methods/genetics.html,1,['queue'],['queue']
Performance,"ocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; --properties 'core:fs.gs.io.buffersize.write=1048576. Parameters:. entry_expr (Float64Expression) – Entry expression for numeric matrix entries.; path (str) – Path for output.; overwrite (bool) – If True, overwrite an existing file at the destination.; mean_impute (bool) – If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center (bool) – If true, subtract the row mean.; normalize (bool) – If true and center=False, divide by the row magnitude.; If true and center=True, divide the centered value by the; centered row magnitude.; axis (str) – One of “rows” or “cols”: axis by which to normalize or center.; block_size (int, optional) – Block size. Default given by BlockMatrix.default_block_size(). Previous; N",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:46795,concurren,concurrently,46795,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['concurren'],['concurrently']
Performance,"ocus and alleles are; always included. _row_fields determines if varid and rsid are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the _row_fields parameter is considered an experimental; feature and may be removed without warning. locus (tlocus or tstruct) – Row key. The chromosome; and position. If reference_genome is defined, the type will be; tlocus parameterized by reference_genome. Otherwise, the type; will be a tstruct with two fields: contig with type; tstr and position with type tint32.; alleles (tarray of tstr) – Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; varid (tstr) – The variant identifier. The third field in; each variant identifying block.; rsid (tstr) – The rsID for the variant. The fifth field in; each variant identifying block. Entry Fields; Up to three entry fields are created, as determined by; entry_fields. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for entry_fields, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. GT (tcall) – The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; GP (tarray of tfloat64) – Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; dosage (tfloat64) – The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See also; index_bgen(). Parameters:. path (str or list of str) – BGEN file(s) to read.; entry_fields (list of str) – List of entry fields to cre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:11018,perform,performance,11018,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['perform'],['performance']
Performance,"od on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configuration options. Bug fixes. (#7070) Fix; unintentionally strict type error in MatrixTable.union_rows.; (#7170) Fix issues; created downstream of BlockMatrix.T.; (#7146) Fix bad; handling of edge cases in BlockMatrix.filter.; (#7182) Fix problem; parsing VCFs where lines end in an INFO field of type flag. Version 0.2.23; Released 2019-09-23. hailctl dataproc. (#7087) Added back; progress bar to notebooks, with links to the correct Spark UI url.; (#7104) Increased; disk requested when using --vep to address the “colony collapse”; cluster error mode. Bug fixes. (#7066) Fixed; generated code when methods from multiple reference genomes appear; together.; (#7077) Fixed crash; in hl.agg.group_by. New features. (#7009) Introduced; analysis pass in Python that mostly obviates the hl.bind and; hl.rbind operators; idiomatic Python that generates Hail; expressions will perform much better.; (#7076) Improved; memory management in generated code, add additional log statements; about allocated memory to improve debugging.; (#7085) Warn only; once about schema mismatches during JSON import (used in VEP,; Nirvana, and sometimes import_table.; (#7106); hl.agg.call_stats can now accept a number of alleles for its; alleles parameter, useful when dealing with biallelic calls; without the alleles array at hand. Performance. (#7086) Improved; performance of JSON import.; (#6981) Improved; performance of Hail min/max/mean operators. Improved performance of; split_multi_hts by an additional 33%.; (#7082)(#7096)(#7098); Improved performance of large pipelines involving many annotate; calls. Version 0.2.22; Released 2019-09-12. New features. (#7013) Added; contig_recoding to import_bed and import_locus_intervals. Performance. (#6969) Improved; performance of hl.agg.mean, hl.agg.stats, and; hl.agg.corr.; (#6987) Improved; performance of",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:85489,perform,perform,85489,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['perform']
Performance,"og; option to hl.plot.histogram2d.; (#5937) Added; all_matches parameter to Table.index and; MatrixTable.index_{rows, cols, entries}, which produces an array; of all rows in the indexed object matching the index key. This makes; it possible to, for example, annotate all intervals overlapping a; locus.; (#5913) Added; functionality that makes arrays of structs easier to work with.; (#6089) Added HTML; output to Expression.show when running in a notebook.; (#6172); hl.split_multi_hts now uses the original GQ value if the; PL is missing.; (#6123) Added; hl.binary_search to search sorted numeric arrays.; (#6224) Moved; implementation of hl.concordance from backend to Python.; Performance directly from read() is slightly worse, but inside; larger pipelines this function will be optimized much better than; before, and it will benefit improvements to general infrastructure.; (#6214) Updated Hail; Python dependencies.; (#5979) Added; optimizer pass to rewrite filter expressions on keys as interval; filters where possible, leading to massive speedups for point; queries. See the blog; post; for examples. Bug fixes. (#5895) Fixed crash; caused by -0.0 floating-point values in hl.agg.hist.; (#6013) Turned off; feature in HTSJDK that caused crashes in hl.import_vcf due to; header fields being overwritten with different types, if the field; had a different type than the type in the VCF 4.2 spec.; (#6117) Fixed problem; causing Table.flatten() to be quadratic in the size of the; schema.; (#6228)(#5993); Fixed MatrixTable.union_rows() to join distinct keys on the; right, preventing an unintentional cartesian product.; (#6235) Fixed an; issue related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broke",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:93519,optimiz,optimizer,93519,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"ogle cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""; if index_file is None:; index_file = re.sub(r'\.[^.]*$', '.fai', fasta_file); Env.backend().add_sequence(self.name, fasta_file, index_file); self._sequence_files = (fasta_file, index_file). [docs] def has_sequence(self):; """"""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""; return self._sequence_files is not None. [docs] def remove_sequence(self):; """"""Remove the reference sequence.""""""; self._sequence_files = None; Env.backend().remove_sequence(self.name). [docs] @classmethod; @typecheck_method(; name=str,; fasta_file=str,; index_file=str,; x_contigs=oneof(str, sequenceof(str)),; y_contigs=oneof(str, sequenceof(str)),; mt_contigs=oneof(str, sequenceof(str)),; par=sequenceof(sized_tupleof(str, int, int)),; ); def from_fasta_file(cls, name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[]):; """"""Create reference genome from a FASTA file. Parameters; ----------; name: :class:`str`; Name for new reference genome.; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :class:`str`; Path to FASTA index file. Must be uncompressed.; x_contigs : :class:`str` or :obj:`list` of :obj:`str`; Contigs to be treated as X ch",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html:10975,load,loaded,10975,docs/0.2/_modules/hail/genetics/reference_genome.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/genetics/reference_genome.html,2,['load'],['loaded']
Performance,"oject(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}; ). if isinstance(loadings_source, hl.MatrixTable):; n_variants = loadings_source.count_rows(); else:; n_variants = loadings_source.count(). mt = mt.filter_rows(hl.is_defined(mt._loadings) & hl.is_defined(mt._af) & (mt._af > 0) & (m",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1807,load,loadings,1807,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance,"ollowing four variant annotations are added.; The indexing of the array annotations corresponds to that of y. va.linreg.nCompleteSamples (Int) – number of samples used; va.linreg.AC (Double) – sum of the genotype values x; va.linreg.ytx (Array[Double]) – array of dot products of each phenotype vector y with the genotype vector x; va.linreg.beta (Array[Double]) – array of fit genotype coefficients, \(\hat\beta_1\); va.linreg.se (Array[Double]) – array of estimated standard errors, \(\widehat{\mathrm{se}}\); va.linreg.tstat (Array[Double]) – array of \(t\)-statistics, equal to \(\hat\beta_1 / \widehat{\mathrm{se}}\); va.linreg.pval (Array[Double]) – array of \(p\)-values. Parameters:; ys – list of one or more response expressions.; covariates (list of str) – list of covariate expressions.; root (str) – Variant annotation path to store result of linear regression.; use_dosages (bool) – If true, use dosage genotypes rather than hard call genotypes.; variant_block_size (int) – Number of variant regressions to perform simultaneously. Larger block size requires more memmory. Returns:Variant dataset with linear regression variant annotations. Return type:VariantDataset. linreg_burden(key_name, variant_keys, single_key, agg_expr, y, covariates=[])[source]¶; Test each keyed group of variants for association by aggregating (collapsing) genotypes and applying the; linear regression model. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Run a gene burden test using linear regression on the maximum genotype per gene. Here va.genes is a variant; annotation of type Set[String] giving the set of genes containing the variant (see Extended example below; for a deep dive):; >>> linreg_kt, sample_kt = (hc.read('data/example_burden.vds'); ... .linreg_burden(key_name='gene',; ... variant_keys='va.genes',; ... single_key=False,; ... agg_expr='gs.map(g => g.gt).max()',; ... y='sa.burden.pheno',; ... covariates=['sa.burden.cov1', 'sa.burden.co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:83908,perform,perform,83908,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['perform']
Performance,"om the Phred-scale) to sum to 1.; Performance; Hail’s initial version of lmmreg() scales beyond 15k samples and to an essentially unbounded number of variants, making it particularly well-suited to modern sequencing studies and complementary to tools designed for SNP arrays. Analysts have used lmmreg() in research to compute kinship from 100k common variants and test 32 million non-rare variants on 8k whole genomes in about 10 minutes on Google cloud.; While lmmreg() computes the kinship matrix \(K\) using distributed matrix multiplication (Step 2), the full eigendecomposition (Step 3) is currently run on a single core of master using the LAPACK routine DSYEVD, which we empirically find to be the most performant of the four available routines; laptop performance plots showing cubic complexity in \(n\) are available here. On Google cloud, eigendecomposition takes about 2 seconds for 2535 sampes and 1 minute for 8185 samples. If you see worse performance, check that LAPACK natives are being properly loaded (see “BLAS and LAPACK” in Getting Started).; Given the eigendecomposition, fitting the global model (Step 4) takes on the order of a few seconds on master. Association testing (Step 5) is fully distributed by variant with per-variant time complexity that is completely independent of the number of sample covariates and dominated by multiplication of the genotype vector \(v\) by the matrix of eigenvectors \(U^T\) as described below, which we accelerate with a sparse representation of \(v\). The matrix \(U^T\) has size about \(8n^2\) bytes and is currently broadcast to each Spark executor. For example, with 15k samples, storing \(U^T\) consumes about 3.6GB of memory on a 16-core worker node with two 8-core executors. So for large \(n\), we recommend using a high-memory configuration such as highmem workers.; Linear mixed model; lmmreg() estimates the genetic proportion of residual phenotypic variance (narrow-sense heritability) under a kinship-based linear mixed model,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:97129,perform,performance,97129,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,"['load', 'perform']","['loaded', 'performance']"
Performance,"ome, min_match=0.95, include_strand=False):; """"""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval co",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:170350,load,load,170350,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['load'],['load']
Performance,"ome=reference_genome_type,; min_match=builtins.float,; include_strand=builtins.bool,; ); def liftover(x, dest_reference_genome, min_match=0.95, include_strand=False):; """"""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Ot",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:170267,load,loaded,170267,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['load'],['loaded']
Performance,"ompute_uv (bool) – If False, only compute the singular values (or eigenvalues).; complexity_bound (int) – Maximum value of \(\sqrt[3]{nmr}\) for which; scipy.linalg.svd() is used. Returns:. u (numpy.ndarray or BlockMatrix) – Left singular vectors \(U\), as a block matrix if \(n > m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True.; s (numpy.ndarray) – Singular values from \(\Sigma\) in descending order.; vt (numpy.ndarray or BlockMatrix) – Right singular vectors \(V^T`\), as a block matrix if \(n \leq m\) and; \(\sqrt[3]{nmr}\) exceeds complexity_bound.; Only returned if compute_uv is True. to_matrix_table_row_major(n_partitions=None, maximum_cache_memory_in_bytes=None)[source]; Returns a matrix table with row key of row_idx and col key col_idx, whose; entries are structs of a single field element. Parameters:. n_partitions (int or None) – Number of partitions of the matrix table.; maximum_cache_memory_in_bytes (int or None) – The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; Does not support block-sparse matrices. Returns:; MatrixTable – Matrix table where each entry corresponds to an entry in the block matrix. to_ndarray()[source]; Collects a BlockMatrix into a local hail ndarray expression on driver. This should not; be done for large matrices. Returns:; NDArrayExpression. to_numpy(_force_blocking=False)[source]; Collects the block matrix into a NumPy ndarray.; Examples; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; The resulting ndarray will have the same shape as the block matrix",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:40343,cache,cache,40343,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['cache'],['cache']
Performance,"on Reference; Advanced UI Search Help; Python Version Compatibility Policy; Change Log. Batch. Python Version Compatibility Policy. View page source. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility policy on Python; versions. In particular, Hail officially supports:. All minor versions of Python released 42 months prior to the project, and at minimum the two; latest minor versions.; All minor versions of numpy released in the 24 months prior to the project, and at minimum the; last three minor versions. Change Log; Version 0.2.132. (#14576) Fixed bug where; submitting many Python jobs would fail with RecursionError. Version 0.2.131. (#14544) batch.read_input; and batch.read_input_group now accept os.PathLike objects as well as strings.; (#14328) Job resource usage; data can now be retrieved from the Batch API. Version 0.2.130. (#14425) A job’s ‘always run’; state is rendered in the Job and Batch pages. This makes it easier to understand; why a job is queued to run when others have failed or been cancelled.; (#14437) The billing page now; reports users’ spend on the batch service. Version 0.2.128. (#14224) hb.Batch now accepts a; default_regions argument which is the default for all jobs in the Batch. Version 0.2.124. (#13681) Fix hailctl batch init and hailctl auth login for; new users who have never set up a configuration before. Version 0.2.123. (#13643) Python jobs in Hail Batch that use the default image now support; all supported python versions and include the hail python package.; (#13614) Fixed a bug that broke the LocalBackend when run inside a; Jupyter notebook.; (#13200) hailtop.batch will now raise an error by default if a pipeline; attempts to read or write files from or two cold storage buckets in GCP. Version 0.2.122. (#13565) Users can now use VEP images from the hailgenetics DockerHub; in Hail Batch. Version 0.2.121. (#13396) Non-spot instances can be requested via the Job.spot() method. Version 0.2.117. (#13007) Memo",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/change_log.html:1173,queue,queued,1173,docs/batch/change_log.html,https://hail.is,https://hail.is/docs/batch/change_log.html,1,['queue'],['queued']
Performance,"on used to order nodes with equal degree. Returns:a list of vertices in a maximal independent set. Return type:list of elements with the same type as i and j. num_columns¶; Number of columns.; >>> kt1.num_columns; 8. Return type:int. num_partitions()[source]¶; Returns the number of partitions in the key table. Return type:int. order_by(*cols)[source]¶; Sort by the specified columns. Missing values are sorted after non-missing values. Sort by the first column, then the second, etc. Parameters:cols – Columns to sort by. Type:str or asc(str) or desc(str). Returns:Key table sorted by cols. Return type:KeyTable. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this key table to memory and/or disk.; Examples; Persist the key table to both memory and disk:; >>> kt = kt.persist() . Notes; The persist() and cache() methods ; allow you to store the current table on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Parameters:storage_level – Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Return type:KeyTable. query(exprs)[source]¶; Performs aggregation queries over columns of the table, and returns Python object(s).; Examples; >>> mean_value = kt1.query('C1.stats().mean'). >>> [hist, counter] = kt1.query(['HT.hist(50, 80, 10)', 'SEX.counter()']). Notes; This method evaluates Hail expressions over the rows of the key table.; The exprs argument requires either a single string or a list of; strings. If a single string was passed, then a single result is; returned. If a list is passed, a list is returned.; The namespace of the expressions includes one aggregable for each column; of the key table",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:22343,cache,cache,22343,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,1,['cache'],['cache']
Performance,"on. Bug Fixes. (#7361) Fix AD; calculation in sparse_split_multi. Performance Improvements. (#7355) Improve; performance of IR copying. File Format. The native file format version is now 1.3.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.25; Released 2019-10-14. New features. (#7240) Add; interactive schema widget to {MatrixTable, Table}.describe. Use; this by passing the argument widget=True.; (#7250); {Table, MatrixTable, Expression}.summarize() now summarizes; elements of collections (arrays, sets, dicts).; (#7271) Improve; hl.plot.qq by increasing point size, adding the unscaled p-value; to hover data, and printing lambda-GC on the plot.; (#7280) Add HTML; output for {Table, MatrixTable, Expression}.summarize().; (#7294) Add HTML; output for hl.summarize_variants(). Bug fixes. (#7200) Fix VCF; parsing with missingness inside arrays of floating-point values in; the FORMAT field.; (#7219) Fix crash due; to invalid optimizer rule. Performance improvements. (#7187) Dramatically; improve performance of chained BlockMatrix multiplies without; checkpoints in between.; (#7195)(#7194); Improve performance of group[_rows]_by / aggregate.; (#7201) Permit code; generation of larger aggregation pipelines. File Format. The native file format version is now 1.2.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.24; Released 2019-10-03. hailctl dataproc. (#7185) Resolve issue; in dependencies that led to a Jupyter update breaking cluster; creation. New features. (#7071) Add; permit_shuffle flag to hl.{split_multi, split_multi_hts} to; allow processing of datasets with both multiallelics and duplciate; loci.; (#7121) Add; hl.contig_length function.; (#7130) Add; window method on LocusExpression, which creates an interval; around a locus.; (#7172) Permit; hl.init(sc=sc) with pip-installed packages, given the right; configurati",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:83628,optimiz,optimizer,83628,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimizer']
Performance,"onger. If ``generic`` equals False (default), Hail makes certain assumptions about the genotype fields, see :class:`Representation <hail.representation.Genotype>`. On import, Hail filters; (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are; silently dropped. If ``generic`` equals True, the genotype schema is a :py:class:`~hail.type.TStruct` with field names equal to the IDs of the FORMAT fields.; The ``GT`` field is automatically read in as a :py:class:`~hail.type.TCall` type. To specify additional fields to import as a; :py:class:`~hail.type.TCall` type, use the ``call_fields`` parameter. All other fields are imported as the type specified in the FORMAT header field. An example genotype schema after importing a VCF with ``generic=True`` is. .. code-block:: text. Struct {; GT: Call,; AD: Array[Int],; DP: Int,; GQ: Int,; PL: Array[Int]; }. .. warning::. - The variant dataset generated with ``generic=True`` will have significantly slower performance. - Not all :py:class:`.VariantDataset` methods will work with a generic genotype schema. - The Hail call representation does not support partially missing calls (e.g. 0/.). Partially missing calls will be treated as (fully) missing. :py:meth:`~hail.HailContext.import_vcf` does not perform deduplication - if the provided VCF(s) contain multiple records with the same chrom, pos, ref, alt, all; these records will be imported and will not be collapsed into a single variant. Since Hail's genotype representation does not yet support ploidy other than 2,; this method imports haploid genotypes as diploid. If ``generic=False``, Hail fills in missing indices; in PL / PP arrays with 1000 to support the standard VCF / VDS ""genotype schema. Below are two example haploid genotypes and diploid equivalents that Hail sees. .. code-block:: text. Haploid: 1:0,6:7:70:70,0; Imported as: 1/1:0,6:7:70:70,1000,0. Haploid: 2:0,0,9:9:24:24,40,0; Imported as: 2/2:",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:22593,perform,performance,22593,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['perform'],['performance']
Performance,"ons.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; Using the global summary result; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. No Data (missing variant); No Call (missing genotype call); Hom Ref; Heterozygous; Hom Var. The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_right_homvar = summary[2][4]; >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:30627,perform,performs,30627,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performs']
Performance,"ool_executor.BatchPoolFuture(executor, batch, job, output_file); Bases: object; Methods. add_done_callback; NOT IMPLEMENTED. async_cancel; Asynchronously cancel this job. async_result; Asynchronously wait until the job is complete. cancel; Cancel this job if it has not yet been cancelled. cancelled; Returns True if cancel() was called before a value was produced. done; Returns True if the function is complete and not cancelled. exception; Block until the job is complete and raise any exceptions. result; Blocks until the job is complete. running; Always returns False. add_done_callback(_); NOT IMPLEMENTED. async async_cancel(); Asynchronously cancel this job.; True is returned if the job is cancelled. False is returned if; the job has already completed. async async_result(timeout=None); Asynchronously wait until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; :class”.concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. cancel(); Cancel this job if it has not yet been cancelled.; True is returned if the job is cancelled. False is returned if; the job has already completed. cancelled(); Returns True if cancel() was called before a value was produced. done(); Returns True if the function is complete and not cancelled. exception(timeout=None); Block until the job is complete and raise any exceptions. result(timeout=None); Blocks until the job is complete.; If the job has been cancelled, this method raises a; concurrent.futures.CancelledError.; If the job has timed out, this method raises an; concurrent.futures.TimeoutError. Parameters:; timeout (Union[int, float, None]) – Wait this long before raising a timeout error. running(); Always returns False.; This future can always be cancelled, so this function always returns False. Previous; Next . © Copyright 2024, Hail Team. Built ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html:1453,concurren,concurrent,1453,docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,https://hail.is,https://hail.is/docs/batch/api/batch_pool_executor/hailtop.batch.batch_pool_executor.BatchPoolFuture.html,2,['concurren'],['concurrent']
Performance,"or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means t",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39943,load,loadings,39943,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"or Query-on-Batch and Batch use. Bug Fixes. (#13573) Fix; (#12936) in which; VEP frequently failed (due to Docker not starting up) on clusters; with a non-trivial number of workers.; (#13485) Fix; (#13479) in which; hl.vds.local_to_global could produce invalid values when the LA; field is too short. There were and are no issues when the LA field; has the correct length.; (#13340) Fix; copy_log to correctly copy relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27164,perform,performance,27164,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"order. :math:`S_{ii}` is the eigenvalue of eigenvector :math:`U_{:,i}`; - :math:`U^T = n \\times n` orthonormal matrix, the transpose (and inverse) of :math:`U`. A bit of matrix algebra on the multivariate normal density shows that the linear mixed model above is mathematically equivalent to the model. .. math::. U^Ty \\sim \mathrm{N}\\left(U^TX\\beta, \sigma_g^2 (S + \delta I)\\right). for which the covariance is diagonal (e.g., unmixed). That is, rotating the phenotype vector (:math:`y`) and covariate vectors (columns of :math:`X`) in :math:`\mathbb{R}^n` by :math:`U^T` transforms the model to one with independent residuals. For any particular value of :math:`\delta`, the restricted maximum likelihood (REML) solution for the latter model can be solved exactly in time complexity that is linear rather than cubic in :math:`n`. In particular, having rotated, we can run a very efficient 1-dimensional optimization procedure over :math:`\delta` to find the REML estimate :math:`(\hat{\delta}, \\hat{\\beta}, \\hat{\sigma}_g^2)` of the triple :math:`(\delta, \\beta, \sigma_g^2)`, which in turn determines :math:`\\hat{\sigma}_e^2` and :math:`\\hat{h}^2`. We first compute the maximum log likelihood on a :math:`\delta`-grid that is uniform on the log scale, with :math:`\\mathrm{ln}(\delta)` running from -8 to 8 by 0.01, corresponding to :math:`h^2` decreasing from 0.9995 to 0.0005. If :math:`h^2` is maximized at the lower boundary then standard linear regression would be more appropriate and Hail will exit; more generally, consider using standard linear regression when :math:`\\hat{h}^2` is very small. A maximum at the upper boundary is highly suspicious and will also cause Hail to exit. In any case, the log file records the table of grid values for further inspection, beginning under the info line containing ""lmmreg: table of delta"". If the optimal grid point falls in the interior of the grid as expected, we then use `Brent's method <https://en.wikipedia.org/wiki/Brent%27s_me",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:130282,optimiz,optimization,130282,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['optimiz'],['optimization']
Performance,"org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""; return Env.backend().execute(ir.MatrixToValueApply(self._mir, {'name': 'NPartitionsMatrixTable'})). [docs] @typecheck_method(n_partitions=int, shuffle=bool); def repartition(self, n_partitions: int, shuffle: bool = True) -> 'MatrixTable':; """"""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Reparti",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:107658,perform,performance,107658,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,2,['perform'],['performance']
Performance,ormance regression fix; Performance; Bug fixes; New features; Cheat sheets. Version 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail com,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:6478,perform,performance,6478,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ort Table. from .datasets_metadata import get_datasets_metadata. def _read_dataset(path: str) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; if path.endswith('.ht'):; return hl.read_table(path); elif path.endswith('.mt'):; return hl.read_matrix_table(path); elif path.endswith('.bm'):; return hl.linalg.BlockMatrix.read(path); raise ValueError(f'Invalid path: {path}. Can only load datasets with .ht, .mt, or .bm extensions.'). [docs]def load_dataset(; name: str, version: Optional[str], reference_genome: Optional[str], region: str = 'us-central1', cloud: str = 'gcp'; ) -> Union[Table, MatrixTable, hl.linalg.BlockMatrix]:; """"""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """""". valid_regions = {'us', 'us-central1', 'europe-west1'}; if region not in valid_regions:; raise ValueError(; f'Specify valid region parameter,'; f' received: region={region!r}.\n'; f'Valid region values are {valid_reg",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html:1655,load,load,1655,docs/0.2/_modules/hail/experimental/datasets.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/datasets.html,2,['load'],['load']
Performance,"ory where rectangles were written.; binary: :obj:`bool`; If true, reads the files as binary, otherwise as text delimited. Returns; -------; :class:`numpy.ndarray`; """""". def parse_rects(fname):; rect_idx_and_bounds = [int(i) for i in re.findall(r'\d+', fname)]; if len(rect_idx_and_bounds) != 5:; raise ValueError(f'Invalid rectangle file name: {fname}'); return rect_idx_and_bounds. rect_files = [file['path'] for file in hl.utils.hadoop_ls(path) if not re.match(r'.*\.crc', file['path'])]; rects = [parse_rects(os.path.basename(file_path)) for file_path in rect_files]. n_rows = max(rects, key=lambda r: r[2])[2]; n_cols = max(rects, key=lambda r: r[4])[4]. nd = np.zeros(shape=(n_rows, n_cols)); with with_local_temp_file() as f:; uri = local_path_uri(f); for rect, file_path in zip(rects, rect_files):; hl.utils.hadoop_copy(file_path, uri); if binary:; rect_data = np.reshape(np.fromfile(f), (rect[2] - rect[1], rect[4] - rect[3])); else:; rect_data = np.loadtxt(f, ndmin=2); nd[rect[1] : rect[2], rect[3] : rect[4]] = rect_data; return nd. [docs] @typecheck_method(compute_uv=bool, complexity_bound=int); def svd(self, compute_uv=True, complexity_bound=8192):; r""""""Computes the reduced singular value decomposition. Examples; --------. >>> x = BlockMatrix.from_numpy(np.array([[-2.0, 0.0, 3.0],; ... [-1.0, 2.0, 4.0]])); >>> x.svd(); (array([[-0.60219551, -0.79834865],; [-0.79834865, 0.60219551]]),; array([5.61784832, 1.56197958]),; array([[ 0.35649586, -0.28421866, -0.89001711],; [ 0.6366932 , 0.77106707, 0.00879404]])). Notes; -----; This method leverages distributed matrix multiplication to compute; reduced `singular value decomposition; <https://en.wikipedia.org/wiki/Singular-value_decomposition>`__ (SVD); for matrices that would otherwise be too large to work with locally,; provided that at least one dimension is less than or equal to 46300. Let :math:`X` be an :math:`n \times m` matrix and let; :math:`r = \min(n, m)`. In particular, :math:`X` can have at most; :math:`r` non-zero",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:72330,load,loadtxt,72330,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['load'],['loadtxt']
Performance,"otate_cols(**{name: hl.literal(new_ids)[hl.int(hl.scan.count())]}). [docs]@typecheck(ds=oneof(Table, MatrixTable), intervals=expr_array(expr_interval(expr_any)), keep=bool); def filter_intervals(ds, intervals, keep=True) -> Union[Table, MatrixTable]:; """"""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """""". if isinstance(ds, MatrixTable):; k_type = ds.row_key.dtype; else:; assert isinstance(ds, Table); k_type = ds.key.dtype. point_type = intervals.dtype.element_type.point_type. def is_struct_prefix(partial, full):; if list(partial) != list(full)[: len(partial)]:; return False; for k, v in partial.items():; if full[k] != v:; return False; return True. if point_type == k_type[0]:; needs_wrapper = True; k_name = k_type.f",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/misc.html:12580,latency,latency,12580,docs/0.2/_modules/hail/methods/misc.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/misc.html,2,['latency'],['latency']
Performance,"otations. va.filters (Set[String]) – Set containing all filters applied to a variant.; va.rsid (String) – rsID of the variant.; va.qual (Double) – Floating-point number in the QUAL field.; va.info (Struct) – All INFO fields defined in the VCF header; can be found in the struct va.info. Data types match the type; specified in the VCF header, and if the declared Number is not; 1, the result will be stored as an array. Parameters:; path (str or list of str) – VCF file(s) to read.; force (bool) – If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB.; force_bgz (bool) – If True, load .gz files as blocked gzip files (BGZF); header_file (str or None) – File to load VCF header from. If not specified, the first file in path is used.; min_partitions (int or None) – Number of partitions.; drop_samples (bool) – If True, create sites-only variant; dataset. Don’t load sample ids, sample annotations or; genotypes.; store_gq (bool) – If True, store GQ FORMAT field instead of computing from PL. Only applies if generic=False.; pp_as_pl (bool) – If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if generic=False.; skip_bad_ad (bool) – If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if generic=False.; generic (bool) – If True, read the genotype with a generic schema.; call_fields (str or list of str) – FORMAT fields in VCF to treat as a TCall. Only applies if generic=True. Returns:Variant dataset imported from VCF file(s). Return type:VariantDataset. index_bgen(path)[source]¶; Index .bgen files. HailContext.import_bgen() cannot run without these indices.; Example; >>> hc.index_bgen(""data/example3.bgen""). Warning; While this method parallelizes over a list of BGEN files, each file is; indexed serially by one core. Indexing several BGEN files on a large; cluster is a wast",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:23528,load,load,23528,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"ow(). Python makes it easy to make a Q-Q (quantile-quantile); plot. In [47]:. qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),; 5, 6). Confounded!¶; The observed p-values drift away from the expectation immediately.; Either every SNP in our dataset is causally linked to caffeine; consumption (unlikely), or there’s a confounder.; We didn’t tell you, but sample ancestry was actually used to simulate; this phenotype. This leads to a; stratified; distribution of the phenotype. The solution is to include ancestry as a; covariate in our regression.; The; linreg; method can also take sample annotations to use as covariates. We already; annotated our samples with reported ancestry, but it is good to be; skeptical of these labels due to human error. Genomes don’t have that; problem! Instead of using reported ancestry, we will use genetic; ancestry by including computed principal components in our model.; The; pca; method produces sample PCs in sample annotations, and can also produce; variant loadings and global eigenvalues when asked. In [48]:. pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen'). 2018-10-18 01:26:55 Hail: INFO: Running PCA with 5 components... In [49]:. pprint(pca.globals). {u'eigen': {u'PC1': 56.34707905481798,; u'PC2': 37.8109003010398,; u'PC3': 16.91974301822238,; u'PC4': 2.707349935634387,; u'PC5': 2.0851252187821174}}. In [50]:. pprint(pca.sample_schema). Struct{; Population: String,; SuperPopulation: String,; isFemale: Boolean,; PurpleHair: Boolean,; CaffeineConsumption: Int,; qc: Struct{; callRate: Double,; nCalled: Int,; nNotCalled: Int,; nHomRef: Int,; nHet: Int,; nHomVar: Int,; nSNP: Int,; nInsertion: Int,; nDeletion: Int,; nSingleton: Int,; nTransition: Int,; nTransversion: Int,; dpMean: Double,; dpStDev: Double,; gqMean: Double,; gqStDev: Double,; nNonRef: Int,; rTiTv: Double,; rHetHomVar: Double,; rInsertionDeletion: Double; },; pca: Struct{; PC1: Double,; PC2: Double,; PC3: Double,; PC4: Double,; PC5: Doubl",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/tutorials/hail-overview.html:24438,load,loadings,24438,docs/0.1/tutorials/hail-overview.html,https://hail.is,https://hail.is/docs/0.1/tutorials/hail-overview.html,1,['load'],['loadings']
Performance,"p-level fields.; (#4926) Expanded; default GRCh38 contig recoding behavior in import_plink. Performance improvements. (#4952) Resolved; lingering issues related to; (#4909). Bug fixes. (#4941) Fixed; variable scoping error in regression methods.; (#4857) Fixed bug in; maximal_independent_set appearing when nodes were named something; other than i and j.; (#4932) Fixed; possible error in export_plink related to tolerance of writer; process failure.; (#4920) Fixed bad; error message in Table.order_by. Version 0.2.5; Released 2018-12-07. New features. (#4845) The; or_error; method in hl.case and hl.switch statements now takes a string; expression rather than a string literal, allowing more informative; messages for errors and assertions.; (#4865) We use this; new or_error functionality in methods that require biallelic; variants to include an offending variant in the error message.; (#4820) Added; hl.reversed; for reversing arrays and strings.; (#4895) Added; include_strand option to the; hl.liftover; function. Performance improvements. (#4907)(#4911); Addressed one aspect of bad scaling in enormous literal values; (triggered by a list of 300,000 sample IDs) related to logging.; (#4909)(#4914); Fixed a check in Table/MatrixTable initialization that scaled O(n^2); with the total number of fields. Bug fixes. (#4754)(#4799); Fixed optimizer assertion errors related to certain types of; pipelines using group_rows_by.; (#4888) Fixed; assertion error in BlockMatrix.sum.; (#4871) Fixed; possible error in locally sorting nested collections.; (#4889) Fixed break; in compatibility with extremely old MatrixTable/Table files.; (#4527)(#4761); Fixed optimizer assertion error sometimes encountered with; hl.split_multi[_hts]. Version 0.2.4: Beginning of history!; We didn’t start manually curating information about user-facing changes; until version 0.2.4.; The full commit history is available; here. Previous. © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:104041,optimiz,optimizer,104041,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,2,['optimiz'],['optimizer']
Performance,"pectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of samples, using 5 prinicpal; components to correct for ancestral populations, and a minimum minor; allele frequency filter of 0.01:. >>> rel = vds.pc_relate(5, 0.01). Calculate values as above, but when performing distributed matrix; multiplications use a matrix-block-size of 1024 by 1024. >>> rel = vds.pc_relate(5, 0.01, 1024). Calculate values as above, excluding sample-pairs with kinship less; than 0.1. This is more effi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:168458,load,loadings,168458,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"pects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no data” in the other.; This method returns a tuple of three o",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:29856,cache,cache,29856,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['cache'],['cache']
Performance,"phi_{ij}} \le 2^{-5/2}; \end{cases}. The estimator for identity-by-descent one is given by:. .. math::. \widehat{k^{(1)}_{ij}} \coloneqq; 1 - \widehat{k^{(2)}_{ij}} - \widehat{k^{(0)}_{ij}}. Note that, even if present, phase information is ignored by this method. The PC-Relate method is described in ""Model-free Estimation of Recent; Genetic Relatedness"". Conomos MP, Reiner AP, Weir BS, Thornton TA. in; American Journal of Human Genetics. 2016 Jan 7. The reference; implementation is available in the `GENESIS Bioconductor package; <https://bioconductor.org/packages/release/bioc/html/GENESIS.html>`_ . :func:`.pc_relate` differs from the reference implementation in a few; ways:. - if ``k`` is supplied, samples scores are computed via PCA on all samples,; not a specified subset of genetically unrelated samples. The latter; can be achieved by filtering samples, computing PCA variant loadings,; and using these loadings to compute and pass in scores for all samples. - the estimators do not perform small sample correction. - the algorithm does not provide an option to use population-wide; allele frequency estimates. - the algorithm does not provide an option to not use ""overall; standardization"" (see R ``pcrelate`` documentation). Under the PC-Relate model, kinship, :math:`\phi_{ij}`, ranges from 0 to; 0.5, and is precisely half of the; fraction-of-genetic-material-shared. Listed below are the statistics for; a few pairings:. - Monozygotic twins share all their genetic material so their kinship; statistic is 0.5 in expection. - Parent-child and sibling pairs both have kinship 0.25 in expectation; and are separated by the identity-by-descent-zero, :math:`k^{(2)}_{ij}`,; statistic which is zero for parent-child pairs and 0.25 for sibling; pairs. - Avuncular pairs and grand-parent/-child pairs both have kinship 0.125; in expectation and both have identity-by-descent-zero 0.5 in expectation. - ""Third degree relatives"" are those pairs sharing; :math:`2^{-3} = 12.5 %` of their gene",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html:8191,perform,perform,8191,docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/relatedness/pc_relate.html,2,['perform'],['perform']
Performance,"ple, if you have the string “07/08/09” and the format string “%Y.%m.%d”, this method will fail, since that’s not specific; enough to determine seconds from. You can fix this by adding “00:00:00” to your date string and “%H:%M:%S” to your format string. Parameters:. time (str or Expression of type tstr) – The string from which to parse the time.; format (str or Expression of type tstr) – The format string describing how to parse the time.; zone_id (str or Expression of type tstr) – An id representing the timezone. See notes above. Returns:; Int64Expression – The Unix timestamp associated with the given time string. hail.experimental.pc_project(call_expr, loadings_expr, af_expr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive functi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:39567,load,loadings,39567,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"plied; for these variants, ``va.filters.isEmpty()`` is true. Thus, ; filtering to PASS variants can be done with :py:meth:`.VariantDataset.filter_variants_expr`; as follows:; ; >>> pass_vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). **Annotations**. - **va.filters** (*Set[String]*) -- Set containing all filters applied to a variant. ; - **va.rsid** (*String*) -- rsID of the variant.; - **va.qual** (*Double*) -- Floating-point number in the QUAL field.; - **va.info** (*Struct*) -- All INFO fields defined in the VCF header; can be found in the struct ``va.info``. Data types match the type; specified in the VCF header, and if the declared ``Number`` is not; 1, the result will be stored as an array. :param path: VCF file(s) to read.; :type path: str or list of str. :param bool force: If True, load .gz files serially. This means that no downstream operations; can be parallelized, so using this mode is strongly discouraged for VCFs larger than a few MB. :param bool force_bgz: If True, load .gz files as blocked gzip files (BGZF). :param header_file: File to load VCF header from. If not specified, the first file in path is used.; :type header_file: str or None. :param min_partitions: Number of partitions.; :type min_partitions: int or None. :param bool drop_samples: If True, create sites-only variant; dataset. Don't load sample ids, sample annotations or; genotypes. :param bool store_gq: If True, store GQ FORMAT field instead of computing from PL. Only applies if ``generic=False``. :param bool pp_as_pl: If True, store PP FORMAT field as PL. EXPERIMENTAL. Only applies if ``generic=False``. :param bool skip_bad_ad: If True, set AD FORMAT field with; wrong number of elements to missing, rather than setting; the entire genotype to missing. Only applies if ``generic=False``. :param bool generic: If True, read the genotype with a generic schema. :param call_fields: FORMAT fields in VCF to treat as a :py:class:`~hail.type.TCall`. Only applies if ``generic=True``",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/context.html:24944,load,load,24944,docs/0.1/_modules/hail/context.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/context.html,1,['load'],['load']
Performance,"porated in entry_expr.; Hail will return an error if entry_expr evaluates to missing, nan, or; infinity on any entry. Notes; PCA is run on the columns of the numeric matrix obtained by evaluating; entry_expr on each entry of the matrix table, or equivalently on the rows; of the transposed numeric matrix \(M\) referenced below.; PCA computes the SVD. \[M = USV^T\]; where columns of \(U\) are left singular vectors (orthonormal in; \(\mathbb{R}^n\)), columns of \(V\) are right singular vectors; (orthonormal in \(\mathbb{R}^m\)), and \(S=\mathrm{diag}(s_1, s_2,; \ldots)\) with ordered singular values \(s_1 \ge s_2 \ge \cdots \ge 0\).; Typically one computes only the first \(k\) singular vectors and values,; yielding the best rank \(k\) approximation \(U_k S_k V_k^T\) of; \(M\); the truncations \(U_k\), \(S_k\) and \(V_k\) are; \(n \times k\), \(k \times k\) and \(m \times k\); respectively.; From the perspective of the rows of \(M\) as samples (data points),; \(V_k\) contains the loadings for the first \(k\) PCs while; \(MV_k = U_k S_k\) contains the first \(k\) PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; \(MM^T\) are the squares of the singular values \(s_1^2, s_2^2,; \ldots\), which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the loadings parameter is; specified.; Scores are stored in a Table with the column key of the matrix; table as key and a field scores of type array<float64> containing; the principal component scores.; Loadings are stored in a Table with the row key of the matrix; table as key and a field loadings of type array<float64> containing; the principal component loadings.; The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters:. entry_expr (Expression) – Numeric expression for matrix entries.; k (int)",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/stats.html:18125,load,loadings,18125,docs/0.2/methods/stats.html,https://hail.is,https://hail.is/docs/0.2/methods/stats.html,1,['load'],['loadings']
Performance,"port_vcf('data/example3.vcf.bgz', generic=True, call_fields=['GTA']); ... .annotate_genotypes_expr('g = g.GTA.toGenotype()')). Notes; annotate_genotypes_expr() evaluates the expression given by expr and assigns; the result of the right hand side to the annotation path specified by the left-hand side (must; begin with g). This is analogous to annotate_variants_expr() and; annotate_samples_expr() where the annotation paths are va and sa respectively.; expr is in genotype context so the following symbols are in scope:. g: genotype annotation; v (Variant): Variant; va: variant annotations; s (Sample): sample; sa: sample annotations; global: global annotations. For more information, see the documentation on writing expressions; and using the Hail Expression Language. Warning. If the resulting genotype schema is not TGenotype,; subsequent function calls on the annotated variant dataset may not work such as; pca() and linreg().; Hail performance may be significantly slower if the annotated variant dataset does not have a; genotype schema equal to TGenotype.; Genotypes are immutable. For example, if g is initially of type Genotype, the expression; g.gt = g.gt + 1 will return a Struct with one field gt of type Int and NOT a Genotype; with the gt incremented by 1. Parameters:expr (str or list of str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. annotate_global(path, annotation, annotation_type)[source]¶; Add global annotations from Python objects.; Examples; Add populations as a global annotation:; >>> vds_result = vds.annotate_global('global.populations',; ... ['EAS', 'AFR', 'EUR', 'SAS', 'AMR'],; ... TArray(TString())). Notes; This method registers new global annotations in a VDS. These annotations; can then be accessed through expressions in downstream operations. The; Hail data type must be provided and must match the given annotation; parameter. Parameters:; path (str) – annotation path starting in ‘global’; annotation – annotat",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:11292,perform,performance,11292,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['performance']
Performance,"pr)[source]; Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; loadings_expr must have no missing data and reflect the rows; from the original PCA run for this method to be accurate.; Example; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) ; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) ; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) ; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) . Parameters:. call_expr (CallExpression) – Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr (ArrayNumericExpression) – Location of expression for loadings; af_expr (Float64Expression) – Location of expression for allele frequency. Returns:; Table – Table with scores calculated from loadings in column scores. hail.experimental.loop(f, typ, *args)[source]; Define and call a tail-recursive function with given arguments.; Notes; The argument f must be a function where the first argument defines the; recursive call, and the remaining arguments are the arguments to the; recursive function, e.g. to define the recursive function. \[f(x, y) = \begin{cases}; y & \textrm{if } x \equiv 0 \\; f(x - 1, y + x) & \textrm{otherwise}; \end{cases}\]; we would write:; >>> f = lambda recur, x, y: hl.if_else(x == 0, y, recur(x - 1, y + x)); Full recursion is not supported, and any non-tail-recursive methods will; throw an error when called.; This means that the result of any recursive call within the function must; also be the result of the entire function, without modification. Let’s; consider two different recursive definitions for the triangle function; \(f(x) = 0 + 1 + \dots + x\):; >>> def triangle1(x):; ... if x == 1:; ... return x; ... return x + triangle1(x - 1).",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/experimental/index.html:40290,load,loadings,40290,docs/0.2/experimental/index.html,https://hail.is,https://hail.is/docs/0.2/experimental/index.html,1,['load'],['loadings']
Performance,"pr_call,; expr_numeric,; raise_unless_entry_indexed,; raise_unless_row_indexed,; ); from hail.typecheck import typecheck. [docs]@typecheck(call_expr=expr_call, loadings_expr=expr_array(expr_numeric), af_expr=expr_numeric); def pc_project(call_expr, loadings_expr, af_expr):; """"""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""; raise_unless_entry_indexed('pc_project', call_expr); raise_unless_row_indexed('pc_project', loadings_expr); raise_unless_row_indexed('pc_project', af_expr). gt_source = call_expr._indices.source; loadings_source = loadings_expr._indices.source; af_source = af_expr._indices.source. loadings_expr = _get_expr_or_join(loadings_expr, loadings_source, gt_source, '_loadings'); af_expr = _get_expr_or_join(af_expr, af_source, gt_source, '_af'). mt = gt_source._annotate_all(; row_exprs={'_loadings': loadings_expr, '_af': af_expr}, entry_exprs={'_call': call_expr}",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html:1552,load,loadings,1552,docs/0.2/_modules/hail/experimental/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/pca.html,2,['load'],['loadings']
Performance,"put KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:; key_expr (str or list of str) – Named expression(s) for how to compute the keys of the new key table.; agg_expr (str or list of str) – Named aggregation expression(s). Returns:A new key table with the keys computed from the key_expr and the remaining columns computed from the agg_expr. Return type:KeyTable. annotate(expr)[source]¶; Add new columns computed from existing columns.; Examples; Add new column Y which is equal to 5 times X:; >>> kt_result = kt1.annotate(""Y = 5 * X""). Notes; The scope for expr is all column names in the input KeyTable.; For more information, see the documentation on writing expressions; and using the Hail Expression Language. Parameters:expr (str or list of str) – Annotation expression or multiple annotation expressions. Returns:Key table with new columns specified by expr. Return type:KeyTable. cache()[source]¶; Mark this key table to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:KeyTable. collect()[source]¶; Collect table to a local list.; Examples; >>> id_to_sex = {row.ID : row.SEX for row in kt1.collect()}. Notes; This method should be used on very small tables and as a last resort.; It is very slow to convert distributed Java objects to Python; (especially serially), and the resulting list may be too large; to fit in memory on one machine. Return type:list of hail.representation.Struct. columns¶; Names of all columns.; >>> kt1.columns; [u'ID', u'HT', u'SEX', u'X', u'Z', u'C1', u'C2', u'C3']. Return type:list of str. count()[source]¶; Count the number of rows.; Examples; >>> kt1.count(). Return type:int. drop(column_names)[source]¶; Drop columns.; Examples; Assume kt1 is a KeyTable with three columns: C1, C2 and; C3.; Drop columns:; >>> kt_result = kt1.drop('C1'). >>> kt_result = kt1.drop(['C1', 'C2']). Parameters:column_names – List of columns to be dropped. Type:str",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.KeyTable.html:5075,cache,cache,5075,docs/0.1/hail.KeyTable.html,https://hail.is,https://hail.is/docs/0.1/hail.KeyTable.html,2,['cache'],"['cache', 'cached']"
Performance,"r (CallExpression); mother (CallExpression); child (CallExpression). Returns:; Int32Expression. hail.expr.functions.liftover(x, dest_reference_genome, min_match=0.95, include_strand=False)[source]; Lift over coordinates to a different reference genome.; Examples; Lift over the locus coordinates from reference genome 'GRCh37' to; 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) ; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome 'GRCh37'; to 'GRCh38':; >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) ; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See Liftover variants from one coordinate system to another for more instructions on lifting over a Table; or MatrixTable.; Notes; This function requires the reference genome of x has a chain file loaded; for dest_reference_genome. Use ReferenceGenome.add_liftover() to; load and attach a chain file to a reference genome.; Returns None if x could not be converted. Warning; Before using the result of liftover() as a new row key or column; key, be sure to filter out missing values. Parameters:. x (Expression of type tlocus or tinterval of tlocus) – Locus or locus interval to lift over.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to.; min_match (float) – Minimum ratio of bases that must remap.; include_strand (bool) – If True, output the result as a StructExpression with the first field result being; the locus or locus interval and the second field is_negative_strand is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns:; Expression – A locus or locus interval converted to dest_refer",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/functions/genetics.html:22476,load,loaded,22476,docs/0.2/functions/genetics.html,https://hail.is,https://hail.is/docs/0.2/functions/genetics.html,1,['load'],['loaded']
Performance,"r Hail Table.; We’ve provided a method to download and import the MovieLens dataset of movie ratings in the Hail native format. Let’s read it!. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=https://dx.doi.org/10.1145/2827872. [1]:. import hail as hl; hl.init(). Loading BokehJS ... SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.5.0; SparkUI available at http://hostname-09f2439d4b:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.133-4c60fddb171a; LOGGING: writing to /io/hail/python/hail/docs/tutorials/hail-20241004-2008-0.2.133-4c60fddb171a.log. [2]:. hl.utils.get_movie_lens('data/'). SLF4J: Failed to load class ""org.slf4j.impl.StaticMDCBinder"".; SLF4J: Defaulting to no-operation MDCAdapter implementation.; SLF4J: See http://www.slf4j.org/codes.html#no_static_mdc_binder for further details.; [Stage 3:> (0 + 1) / 1]. [3]:. users = hl.read_table('data/users.ht'). Exploring Tables; The describe method prints the structure of a table: the fields and their types. [4]:. users.describe(). ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'id': int32; 'age': int32; 'sex': str; 'occupation': str; 'zipcode': str; ----------------------------------------; Key: ['id']; ----------------------------------------. You can view the first few rows of the table using show.; 10 rows are displayed by default. Try changing the code in the cell below to users.show(5). [5]:. users.show(). idagesexoccupationzipcodeint32int32strstrstr; 124""M""""technician""""85711""; 253""F""""other""""94043""; 323""M""""writer""""32067""; 424""M""""technician""""43537""; 533""F",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/tutorials/03-tables.html:2358,load,load,2358,docs/0.2/tutorials/03-tables.html,https://hail.is,https://hail.is/docs/0.2/tutorials/03-tables.html,1,['load'],['load']
Performance,"r some of Hail’s built-in; references:; GRCh37 to GRCh38; gs://hail-common/references/grch37_to_grch38.over.chain.gz; GRCh38 to GRCh37; gs://hail-common/references/grch38_to_grch37.over.chain.gz; Public download links are available; here. Parameters:. chain_file (str) – Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome (str or ReferenceGenome) – Reference genome to convert to. add_sequence(fasta_file, index_file=None)[source]; Load the reference sequence from a FASTA file.; Examples; Access the GRCh37 reference genome using get_reference():; >>> rg = hl.get_reference('GRCh37') . Add a sequence file:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') . Add a sequence file with the default index location:; >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') . Notes; This method can only be run once per reference genome. Use; has_sequence() to test whether a sequence is loaded.; FASTA and index files are hosted on google cloud for some of Hail’s built-in; references:; GRCh37. FASTA file: gs://hail-common/references/human_g1k_v37.fasta.gz; Index file: gs://hail-common/references/human_g1k_v37.fasta.fai. GRCh38. FASTA file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz; Index file: gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai. Public download links are available; here. Parameters:. fasta_file (str) – Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file (None or str) – Path to FASTA index file. Must be uncompressed. If None, replace; the fasta_file’s extension with fai. contig_length(contig)[source]; Contig length. Parameters:; contig (str) – Contig name. Returns:; int – Length of contig. property contigs; Contig names. Returns:; list of str. classmethod from_fasta_file(name, fasta_file, index_file, x_contigs=[], y_contigs=[], mt_contigs=[], par=[])[source]; Create reference ge",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html:5433,load,loaded,5433,docs/0.2/genetics/hail.genetics.ReferenceGenome.html,https://hail.is,https://hail.is/docs/0.2/genetics/hail.genetics.ReferenceGenome.html,1,['load'],['loaded']
Performance,"r) – Key column(s).; min_partitions (int or None) – Minimum number of partitions.; no_header (bool) – File has no header and the N columns are named f0, f1, … fN (0-indexed); impute (bool) – Impute column types from the file; comment (str or None) – Skip lines beginning with the given pattern; delimiter (str) – Field delimiter regex; missing (str) – Specify identifier to be treated as missing; types (dict with str keys and Type values) – Define types of fields in annotations files; quote (str or None) – Quote character. Returns:Key table constructed from text table. Return type:KeyTable. import_vcf(path, force=False, force_bgz=False, header_file=None, min_partitions=None, drop_samples=False, store_gq=False, pp_as_pl=False, skip_bad_ad=False, generic=False, call_fields=[])[source]¶; Import VCF file(s) as variant dataset.; Examples; >>> vds = hc.import_vcf('data/example2.vcf.bgz'). Notes; Hail is designed to be maximally compatible with files in the VCF v4.2 spec.; import_vcf() takes a list of VCF files to load. All files must have the same header and the same set of samples in the same order; (e.g., a variant dataset split by chromosome). Files can be specified as Hadoop glob patterns.; Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (.vcf) or block compressed; (.vcf.bgz). If you have a large compressed VCF that ends in .vcf.gz, it is likely that the file is actually block-compressed,; and you should rename the file to “.vcf.bgz” accordingly. If you actually have a standard gzipped file, it is possible to import; it to Hail using the force optional parameter. However, this is not recommended – all parsing will have to take place on one node because; gzip decompression is not parallelizable. In this case, import could take significantly longer.; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters; (sets to no-call) any genotype that violate",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:19392,load,load,19392,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"r); Returns True if the two expressions are equal.; Examples; >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; This method will fail with an error if the two expressions are not; of comparable types. Parameters:; other (Expression) – Expression for equality comparison. Returns:; BooleanExpression – True if the two expressions are equal. __floordiv__(other)[source]; Positionally divide by a ndarray or a scalar using floor division. Parameters:; other (NumericExpression or NDArrayNumericExpression). Returns:; NDArrayNumericExpression. __ge__(other); Return self>=value. __gt__(other); Return self>value. __le__(other); Return self<=value. __lt__(other); Return self<value. __matmul__(other)[source]; Matrix multiplication: a @ b, semantically equivalent to NumPy matmul. If a and b are vectors,; the vector dot product is performed, returning a NumericExpression. If a and b are both 2-dimensional; matrices, this performs normal matrix multiplication. If a and b have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if a has shape (3, 4, 5) and b has shape (3, 5, 6), a is treated; as a stack of three matrices of shape (4, 5) and b as a stack of three matrices of shape (5, 6). a @ b; would then have shape (3, 4, 6).; Notes; The last dimension of a and the second to last dimension of b (or only dimension if b is a vector); must have the same length. The dimensions to the left of the last two dimensions of a and b (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters:; other (numpy.ndarray NDArrayNumericExpression). Returns:; NDArrayNumericExpression or NumericExpression. __mul__(other)[source]; Positionally multiply by a ndarray or a scalar. Parameters:; other (",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html:3016,perform,performs,3016,docs/0.2/hail.expr.NDArrayNumericExpression.html,https://hail.is,https://hail.is/docs/0.2/hail.expr.NDArrayNumericExpression.html,1,['perform'],['performs']
Performance,r.aggregators). array_windows() (in module hail.linalg.utils). ArrayExpression (class in hail.expr). ArrayNumericExpression (class in hail.expr). asc() (in module hail). ascertainment_bias() (in module hail.experimental.ldscsim). available_datasets (hail.experimental.DB property). B. balding_nichols_model() (in module hail.methods). binarize() (in module hail.experimental.ldscsim). binary_search() (in module hail.expr.functions). bind() (in module hail.expr.functions). binom_test() (in module hail.expr.functions). bit_and() (in module hail.expr.functions). bit_count() (in module hail.expr.functions). bit_lshift() (in module hail.expr.functions). bit_not() (in module hail.expr.functions). bit_or() (in module hail.expr.functions). bit_rshift() (in module hail.expr.functions). bit_xor() (in module hail.expr.functions). block_size (hail.linalg.BlockMatrix property). BlockMatrix (class in hail.linalg). bool() (in module hail.expr.functions). BooleanExpression (class in hail.expr). C. cache() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). calculate_phenotypes() (in module hail.experimental.ldscsim). Call (class in hail.genetics). call() (in module hail.expr.functions). call_stats() (in module hail.expr.aggregators). CallExpression (class in hail.expr). case() (in module hail.expr.functions). CaseBuilder (class in hail.expr.builders). cdf() (in module hail.plot). ceil() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). checkpoint() (hail.linalg.BlockMatrix method). (hail.MatrixTable method). (hail.Table method). (hail.vds.VariantDataset method). chi_squared_test() (in module hail.expr.functions). choose_cols() (hail.MatrixTable method). citation() (in module hail). coalesce() (in module hail.expr.functions). cochran_mantel_haenszel_test() (in module hail.expr.functions). col (hail.MatrixTable property). col_key (hail.MatrixTable property). col_value (hail.MatrixTable property). collect() (hail.expr.ArrayExpression method).,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:12758,cache,cache,12758,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['cache'],['cache']
Performance,"r; to the specified annotation path.; The expr argument expects an annotation assignment whose scope; includes, va, the variant annotations in the current VDS, and vds,; the variant annotations in other.; VDSes with multi-allelic variants may produce surprising results because; all alternate alleles are considered part of the variant key. For; example:. The variant 22:140012:A:T,TTT will not be annotated by; 22:140012:A:T or 22:140012:A:TTT; The variant 22:140012:A:T will not be annotated by; 22:140012:A:T,TTT. It is possible that an unsplit variant dataset contains no multiallelic; variants, so ignore any warnings Hail prints if you know that to be the; case. Otherwise, run split_multi() before annotate_variants_vds(). Parameters:; other (VariantDataset) – Variant dataset to annotate with.; root (str) – Sample annotation path to add variant annotations.; expr (str) – Annotation expression. Returns:Annotated variant dataset. Return type:VariantDataset. cache()[source]¶; Mark this variant dataset to be cached in memory.; cache() is the same as persist(""MEMORY_ONLY""). Return type:VariantDataset. colkey_schema¶; Returns the signature of the column key (sample) contained in this VDS.; Examples; >>> print(vds.colkey_schema). The pprint module can be used to print the schema in a more human-readable format:; >>> from pprint import pprint; >>> pprint(vds.colkey_schema). Return type:Type. concordance(right)[source]¶; Calculate call concordance with another variant dataset. Important; The genotype_schema() must be of type TGenotype in order to use this method. Example; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). Notes; This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as “no d",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:29787,cache,cache,29787,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['cache'],"['cache', 'cached']"
Performance,"rabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in addition to gs (Google Cloud Storage). Version 0.2.79; Release 2021-11-17. Bug fixes. (#11023) Fixed bug; in call decoding that was introduced in version 0.2.78. New features. (#10993) New; function p_value_excess_het. Version 0.2.78; Release 2021-10-19. Bug fixes. (#10766) Don’t throw; out of memory error when broadcasting more than 2^(31) - 1 bytes.; (#10910) Filters on; key field won’t be slowed down by uses of; MatrixTable.localize_entries or Table.rename.; (#10959) Don’t throw; an error in certain situations where some key fields are optimized; away. New features. (#10855) Arbitrary; aggregations can be implemented using hl.agg.fold. Performance Improvements. (#10971); Substantially improve the speed of Table.collect when collecting; large amounts of data. Version 0.2.77; Release 2021-09-21. Bug fixes. (#10888) Fix crash; when calling hl.liftover.; (#10883) Fix crash /; long compilation times writing matrix tables with many partitions. Version 0.2.76; Released 2021-09-15. Bug fixes. (#10872) Fix long; compile times or method size errors when writing tables with many; partitions; (#10878) Fix crash; importing or sorting tables with empty data partitions. Version 0.2.75; Released 2021-09-10. Bug fixes. (#10733) Fix a bug; in tabix parsing when the size of the list of all sequences is large.; (#10765) Fix rare; bug where valid pipelines would fail to compile if intervals were; created conditionally.; (#10746) Various; compiler improvements, decrease likelihood of ClassTooLarge; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:56341,optimiz,optimized,56341,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimized']
Performance,"rage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of col",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22593,perform,performance,22593,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['perform'],['performance']
Performance,"rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:6919,Load,Loadings,6919,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['Load'],['Loadings']
Performance,"rank :math:`k` approximation :math:`U_k S_k V_k^T` of; :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are; :math:`n \times k`, :math:`k \times k` and :math:`m \times k`; respectively. From the perspective of the rows of :math:`M` as samples (data points),; :math:`V_k` contains the loadings for the first :math:`k` PCs while; :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each; sample. The loadings represent a new basis of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:20618,Load,Loadings,20618,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,1,['Load'],['Loadings']
Performance,"rce for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; performance.; This method opens n_cols / block_size files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45993,perform,performance,45993,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['perform'],['performance']
Performance,"re checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""; hl.current_backend().validate_file(path); self.write(path, overwrite, force_row_major, stage_locally); return BlockMatrix.read(path, _assert_type=self._bmir._type). [docs] @staticmethod; @typecheck(; entry_expr=expr_float64,; path=str,; overwrite=bool,; mean_impute=bool,; center=bool,; normalize=bool,; axis=nullable(enumeration('rows', 'cols')),; block_size=nullable(int),; ); def write_from_entry_expr(; entry_expr,; path,; overwrite=False,; mean_impute=False,; center=False,; normalize=False,; axis='rows',; block_size=None,; ):; """"""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:22343,load,loaded,22343,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['load'],['loaded']
Performance,"re=0, after=0):; """"""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """""". rg = self.dtype.reference_genome; if not rg.has_sequence():; raise TypeError(; ""Reference genome '{}' does not have a sequence loaded. Use 'add_sequence' to load the sequence from a FASTA file."".format(; rg.name; ); ); return hl.get_sequence(self.contig, self.position, before, after, rg). [docs] @typecheck_method(before=expr_int32, after=expr_int32); def window(self, before, after):; """"""Returns an interval of a specified number of bases around the locus. Examples; --------; Create a window of two megabases centered at a locus:. >>> locus = hl.locus('16', 29_500_000); >>> window = locus.window(1_000_000, 1_000_000); >>> hl.eval(window); Interval(start=Locus(contig=16, position=28500000, reference_genome=GRCh37), end=Locus(contig=16, position=30500000, reference_genome=GRCh37), includes_start=True, includes_end=True). Notes; -----; The returned interval is inclusive of both the `start` and `end`; endpoints. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`; Number of bases to include before the locus. Truncates at 1.; after : :class:`.Expressio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html:89667,load,loaded,89667,docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/expressions/typed_expressions.html,2,['load'],['loaded']
Performance,"related to aggregation inside MatrixTable.filter_cols.; (#6226) Restored lost; behavior where Table.show(x < 0) shows the entire table.; (#6267) Fixed cryptic; crashes related to hl.split_multi and MatrixTable.entries(); with duplicate row keys. Version 0.2.14; Released 2019-04-24; A back-incompatible patch update to PySpark, 2.4.2, has broken fresh pip; installs of Hail 0.2.13. To fix this, either downgrade PySpark to; 2.4.1 or upgrade to the latest version of Hail. New features. (#5915) Added; hl.cite_hail and hl.cite_hail_bibtex functions to generate; appropriate citations.; (#5872) Fixed; hl.init when the idempotent parameter is True. Version 0.2.13; Released 2019-04-18; Hail is now using Spark 2.4.x by default. If you build hail from source,; you will need to acquire this version of Spark and update your build; invocations accordingly. New features. (#5828) Remove; dependency on htsjdk for VCF INFO parsing, enabling faster import of; some VCFs.; (#5860) Improve; performance of some column annotation pipelines.; (#5858) Add unify; option to Table.union which allows unification of tables with; different fields or field orderings.; (#5799); mt.entries() is four times faster.; (#5756) Hail now uses; Spark 2.4.x by default.; (#5677); MatrixTable now also supports show.; (#5793)(#5701); Add array.index(x) which find the first index of array whose; value is equal to x.; (#5790) Add; array.head() which returns the first element of the array, or; missing if the array is empty.; (#5690) Improve; performance of ld_matrix.; (#5743); mt.compute_entry_filter_stats computes statistics about the; number of filtered entries in a matrix table.; (#5758) failure to; parse an interval will now produce a much more detailed error; message.; (#5723); hl.import_matrix_table can now import a matrix table with no; columns.; (#5724); hl.rand_norm2d samples from a two dimensional random normal. Bug fixes. (#5885) Fix; Table.to_spark in the presence of fields of tuples.; (#5882)(#5886);",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:95220,perform,performance,95220,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"relative file paths.; (#13364); hl.import_gvcf_interval now treats PGT as a call field.; (#13333) Fix; interval filtering regression: filter_rows or filter; mentioning the same field twice or using two fields incorrectly read; the entire dataset. In 0.2.121, these filters will correctly read; only the relevant subset of the data.; (#13368) In Azure,; Hail now uses fewer “list blobs” operations. This should reduce cost; on pipelines that import many files, export many of files, or use; file glob expressions.; (#13414) Resolves; (#13407) in which; uses of union_rows could reduce parallelism to one partition; resulting in severely degraded performance.; (#13405); MatrixTable.aggregate_cols no longer forces a distributed; computation. This should be what you want in the majority of cases.; In case you know the aggregation is very slow and should be; parallelized, use mt.cols().aggregate instead.; (#13460) In; Query-on-Spark, restore hl.read_table optimization that avoids; reading unnecessary data in pipelines that do not reference row; fields.; (#13447) Fix; (#13446). In all; three submit commands (batch, dataproc, and hdinsight),; Hail now allows and encourages the use of – to separate arguments; meant for the user script from those meant for hailctl. In hailctl; batch submit, option-like arguments, for example “–foo”, are now; supported before “–” if and only if they do not conflict with a; hailctl option.; (#13422); hailtop.hail_frozenlist.frozenlist now has an eval-able repr.; (#13523); hl.Struct is now pickle-able.; (#13505) Fix bug; introduced in 0.2.117 by commit c9de81108 which prevented the; passing of keyword arguments to Python jobs. This manifested as; “ValueError: too many values to unpack”.; (#13536) Fixed; (#13535) which; prevented the use of Python jobs when the client (e.g. your laptop); Python version is 3.11 or later.; (#13434) In QoB,; Hail’s file systems now correctly list all files in a directory, not; just the first 1000. This could manifest in an ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:27476,optimiz,optimization,27476,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['optimiz'],['optimization']
Performance,"res_asj; gnomad_ld_scores_eas; gnomad_ld_scores_est; gnomad_ld_scores_fin; gnomad_ld_scores_nfe; gnomad_ld_scores_nwe; gnomad_ld_scores_seu; gnomad_ld_variant_indices_afr; gnomad_ld_variant_indices_amr; gnomad_ld_variant_indices_asj; gnomad_ld_variant_indices_eas; gnomad_ld_variant_indices_est; gnomad_ld_variant_indices_fin; gnomad_ld_variant_indices_nfe; gnomad_ld_variant_indices_nwe; gnomad_ld_variant_indices_seu; gnomad_lof_metrics; gnomad_mnv_genome_d01; gnomad_mnv_genome_d02; gnomad_mnv_genome_d03; gnomad_mnv_genome_d04; gnomad_mnv_genome_d05; gnomad_mnv_genome_d06; gnomad_mnv_genome_d07; gnomad_mnv_genome_d08; gnomad_mnv_genome_d09; gnomad_mnv_genome_d10; gnomad_pca_variant_loadings; Schema (3.1, GRCh38). gnomad_plof_metrics_gene; gnomad_plof_metrics_transcript; gnomad_variant_co-occurrence; ldsc_baselineLD_annotations; ldsc_baselineLD_ldscores; panukb_ld_scores_AFR; panukb_ld_scores_AMR; panukb_ld_scores_CSA; panukb_ld_scores_EAS; panukb_ld_scores_EUR; panukb_ld_scores_MID; panukb_ld_variant_indices_AFR; panukb_ld_variant_indices_AMR; panukb_ld_variant_indices_CSA; panukb_ld_variant_indices_EAS; panukb_ld_variant_indices_EUR; panukb_ld_variant_indices_MID; panukb_meta_analysis_all_ancestries; panukb_meta_analysis_high_quality; panukb_summary_stats. Annotation Database; Libraries; For Software Developers; Other Resources; Change Log And Version Policy. menu; Hail. Datasets; Schemas; gnomad_pca_variant_loadings. View page source. gnomad_pca_variant_loadings. Versions: 2.1, 3.1; Reference genome builds: GRCh37, GRCh38; Type: hail.Table. Schema (3.1, GRCh38); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh38>; 'alleles': array<str>; 'loadings': array<float64>; 'pca_af': float64; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. Previous; Next . © Copyright 2015-2024, Hail Team.; Last updated on Oct 04, 2024.; . ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html:9195,load,loadings,9195,docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,https://hail.is,https://hail.is/docs/0.2/datasets/schemas/gnomad_pca_variant_loadings.html,1,['load'],['loadings']
Performance,"ression. Returns; -------; :class:`.Expression` of type :py:data:`.tint64` or :py:data:`.tfloat64`; Product of records of `expr`.; """""". return _agg_func('Product', [expr], expr.dtype). [docs]@typecheck(predicate=expr_bool); def fraction(predicate) -> Float64Expression:; """"""Compute the fraction of records where `predicate` is ``True``. Examples; --------; Compute the fraction of rows where `SEX` is ""F"" and `HT` > 65:. >>> table1.aggregate(hl.agg.fraction((table1.SEX == 'F') & (table1.HT > 65))); 0.25. Notes; -----; Missing values for `predicate` are treated as ``False``. Parameters; ----------; predicate : :class:`.BooleanExpression`; Boolean predicate. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Fraction of records where `predicate` is ``True``.; """"""; return hl.bind(; lambda n: hl.if_else(n == 0, hl.missing(hl.tfloat64), hl.float64(filter(predicate, count())) / n), count(); ). [docs]@typecheck(expr=expr_call, one_sided=expr_bool); def hardy_weinberg_test(expr, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html:30890,Perform,Performs,30890,docs/0.2/_modules/hail/expr/aggregators/aggregators.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/aggregators/aggregators.html,1,['Perform'],['Performs']
Performance,"ri):; """"""Collects and writes data to a binary file. Examples; --------; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') # doctest: +SKIP. To create a :class:`numpy.ndarray` of the same dimensions:. >>> a = np.fromfile('/local/file').reshape((10, 20)) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__,; produces a binary file of float64 values in row-major order, which can; be read by functions such as `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__; (if a local file) and :meth:`BlockMatrix.fromfile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary output file. See Also; --------; :meth:`.to_numpy`; """"""; hl.current_backend().validate_file(uri). _check_entries_size(self.n_rows, self.n_cols). writer = BlockMatrixBinaryWriter(uri); Env.backend().execute(BlockMatrixWrite(self._bmir, writer)). [docs] @typecheck_method(_force_blocking=bool); def to_numpy(self, _force_blocking=False):; """"""Collects the block matrix into a `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html>`__. Examples; --------; >>> bm = BlockMatrix.random(10, 20); >>> a = bm.to_numpy(). Notes; -----; The resulting ndarray will have the same shape as the block matrix. Returns; -------; :class:`numpy.ndarray`; """"""; from hail.backend.service_backend import ServiceBackend. if self.n_rows * self.n_cols > 1 << 31 or _force_blocking:; path = new_temp_file(",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:39142,load,load,39142,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['load'],['load']
Performance,"riant): Variant; va: variant annotations; global: global annotations; gs (Aggregable[Genotype]): aggregable of Genotype for variant v. For more information, see the Overview and the Expression Language. Caution; When expr evaluates to missing, the variant will be removed regardless of whether keep=True or keep=False. Parameters:; expr (str) – Boolean filter expression.; keep (bool) – Keep variants where expr evaluates to true. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_list(variants, keep=True)[source]¶; Filter variants with a list of variants.; Examples; Filter VDS down to a list of variants:; >>> vds_filtered = vds.filter_variants_list([Variant.parse('20:10626633:G:GC'), ; ... Variant.parse('20:10019093:A:G')], keep=True). Notes; This method performs predicate pushdown when keep=True, meaning that data shards; that don’t overlap with any supplied variant will not be loaded at all. This property; enables filter_variants_list to be used for reasonably low-latency queries of one; or more variants, even on large datasets. Parameters:; variants (list of Variant) – List of variants to keep or remove.; keep (bool) – If true, keep variants in variants, otherwise remove them. Returns:Filtered variant dataset. Return type:VariantDataset. filter_variants_table(table, keep=True)[source]¶; Filter variants with a Variant keyed key table.; Example; Filter variants of a VDS to those appearing in a text file:; >>> kt = hc.import_table('data/sample_variants.txt', key='Variant', impute=True); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Keep all variants whose chromosome and position (locus) appear in a file with ; a chromosome:position column:; >>> kt = hc.import_table('data/locus-table.tsv', impute=True).key_by('Locus'); >>> filtered_vds = vds.filter_variants_table(kt, keep=True). Remove all variants which overlap an interval in a UCSC BED file:; >>> kt = KeyTable.import_bed('data/file2.bed'); >>> filtered_vds = vds.filter_variant",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:59906,latency,latency,59906,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['latency'],['latency']
Performance,"rite=False, force_row_major=False, stage_locally=False)[source]; Writes the block matrix. Danger; Do not write or checkpoint to a path that is already an input source for the query. This can cause data loss. Parameters:. path (str) – Path for output file.; overwrite (bool) – If True, overwrite an existing file at the destination.; force_row_major (bool) – If True, transform blocks in column-major format; to row-major format before writing.; If False, write blocks in their current format.; stage_locally (bool) – If True, major output will be written to temporary local storage; before being copied to output. static write_from_entry_expr(entry_expr, path, overwrite=False, mean_impute=False, center=False, normalize=False, axis='rows', block_size=None)[source]; Writes a block matrix from a matrix table entry expression.; Examples; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; The resulting file can be loaded with BlockMatrix.read().; Blocks are stored row-major.; If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance.; By default, this method will fail if any values are missing (to be clear,; special float values like nan are not missing values). Set mean_impute to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is nan.; Set center to shift each row to have mean zero before possibly; normalizing.; Set normalize to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set center and normalize and then multiply; the result by sqrt(n_cols). Warning; If the rows of the matrix table have been filtered to a small fraction,; then MatrixTable.repartition() before this method to improve; perfor",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html:45748,load,loaded,45748,docs/0.2/linalg/hail.linalg.BlockMatrix.html,https://hail.is,https://hail.is/docs/0.2/linalg/hail.linalg.BlockMatrix.html,1,['load'],['loaded']
Performance,"rns; -------; :class:`.Expression`; """"""; a, b, success = unify_exprs(a, b); if not success:; raise TypeError(; f""'or_else' requires the 'a' and 'b' arguments to have the same type\n""; f"" a: type '{a.dtype}'\n""; f"" b: type '{b.dtype}'""; ); return coalesce(a, b). [docs]@typecheck(predicate=expr_bool, value=expr_any); def or_missing(predicate, value):; """"""Returns `value` if `predicate` is ``True``, otherwise returns missing. Examples; --------. >>> hl.eval(hl.or_missing(True, 5)); 5. >>> hl.eval(hl.or_missing(False, 5)); None. Parameters; ----------; predicate : :class:`.BooleanExpression`; value : :class:`.Expression`; Value to return if `predicate` is ``True``. Returns; -------; :class:`.Expression`; This expression has the same type as `b`.; """""". return hl.if_else(predicate, value, hl.missing(value.dtype)). [docs]@typecheck(; x=expr_int32, n=expr_int32, p=expr_float64, alternative=enumeration(""two.sided"", ""two-sided"", ""greater"", ""less""); ); def binom_test(x, n, p, alternative: str) -> Float64Expression:; """"""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(h",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:60727,Perform,Performs,60727,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Perform'],['Performs']
Performance,"rough files, selected annotations from each file; for db_file, expr in file_exprs.iteritems():. # if database file is a VDS; if db_file.endswith('.vds'):. # annotate analysis VDS with database VDS; self = self.annotate_variants_vds(self.hc.read(db_file), expr=expr). # if database file is a keytable; elif db_file.endswith('.kt'):. # join on gene symbol for gene annotations; if db_file == 'gs://annotationdb/gene/gene.kt':; if gene_key:; vds_key = gene_key; else:; vds_key = 'va.gene.transcript.gene_symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:40978,cache,cache,40978,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['cache'],['cache']
Performance,"rove; performance of hl.split_multi and hl.split_multi_hts.; (#6644) Optimize core; code generation primitives, leading to across-the-board performance; improvements.; (#6775) Fixed a major; performance problem related to reading block matrices. hailctl dataproc. (#6760) Fixed the; address pointed at by ui in connect, after Google changed; proxy settings that rendered the UI URL incorrect. Also added new; address hist/spark-history. Version 0.2.18; Released 2019-07-12. Critical performance bug fix. (#6605) Resolved code; generation issue leading a performance regression of 1-3 orders of; magnitude in Hail pipelines using constant strings or literals. This; includes almost every pipeline! This issue has exists in versions; 0.2.15, 0.2.16, and 0.2.17, and any users on those versions should; update as soon as possible. Bug fixes. (#6598) Fixed code; generated by MatrixTable.unfilter_entries to improve performance.; This will slightly improve the performance of hwe_normalized_pca; and relatedness computation methods, which use unfilter_entries; internally. Version 0.2.17; Released 2019-07-10. New features. (#6349) Added; compression parameter to export_block_matrices, which can be; 'gz' or 'bgz'.; (#6405) When a matrix; table has string column-keys, matrixtable.show uses the column; key as the column name.; (#6345) Added an; improved scan implementation, which reduces the memory load on; master.; (#6462) Added; export_bgen method.; (#6473) Improved; performance of hl.agg.array_sum by about 50%.; (#6498) Added method; hl.lambda_gc to calculate the genomic control inflation factor.; (#6456) Dramatically; improved performance of pipelines containing long chains of calls to; Table.annotate, or MatrixTable equivalents.; (#6506) Improved the; performance of the generated code for the Table.annotate(**thing); pattern. Bug fixes. (#6404) Added; n_rows and n_cols parameters to Expression.show for; consistency with other show methods.; (#6408)(#6419); Fixed an issue where ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:89690,perform,performance,89690,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"rs (orthonormal in :math:`\mathbb{R}^m`), and :math:`S=\mathrm{diag}(s_1, s_2, \ldots)` with ordered singular values :math:`s_1 \ge s_2 \ge \cdots \ge 0`. Typically one computes only the first :math:`k` singular vectors and values, yielding the best rank :math:`k` approximation :math:`U_k S_k V_k^T` of :math:`M`; the truncations :math:`U_k`, :math:`S_k` and :math:`V_k` are :math:`n \\times k`, :math:`k \\times k` and :math:`m \\times k` respectively. From the perspective of the samples or rows of :math:`M` as data, :math:`V_k` contains the variant loadings for the first :math:`k` PCs while :math:`MV_k = U_k S_k` contains the first :math:`k` PC scores of each sample. The loadings represent a new basis of features while the scores represent the projected data on those features. The eigenvalues of the GRM :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2, \ldots`, which represent the variances carried by the respective PCs. By default, Hail only computes the loadings if the ``loadings`` parameter is specified. *Note:* In PLINK/GCTA the GRM is taken as the starting point and it is computed slightly differently with regard to missing data. Here the :math:`ij` entry of :math:`MM^T` is simply the dot product of rows :math:`i` and :math:`j` of :math:`M`; in terms of :math:`C` it is. .. math::. \\frac{1}{m}\sum_{l\in\mathcal{C}_i\cap\mathcal{C}_j}\\frac{(C_{il}-2p_l)(C_{jl} - 2p_l)}{2p_l(1-p_l)}. where :math:`\mathcal{C}_i = \{l \mid C_{il} \\text{ is non-missing}\}`. In PLINK/GCTA the denominator :math:`m` is replaced with the number of terms in the sum :math:`\\lvert\mathcal{C}_i\cap\\mathcal{C}_j\\rvert`, i.e. the number of variants where both samples have non-missing genotypes. While this is arguably a better estimator of the true GRM (trading shrinkage for noise), it has the drawback that one loses the clean interpretation of the loadings and scores as features and projections. Separately, for the PCs PLINK/GCTA output the eigenvectors of the GRM; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:165440,load,loadings,165440,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,['load'],['loadings']
Performance,"rs or patterns should be passed as a list.; delimiter (str) – Field delimiter regex.; missing (str or list [str]) – Identifier(s) to be treated as missing.; types (dict mapping str to HailType) – Dictionary defining field types.; quote (str or None) – Quote character.; skip_blank_lines (bool) – If True, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz (bool) – If True, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not '.bgz', but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter (str, optional) – Line filter regex. A partial match results in the line being removed; from the file. Applies before find_replace, if both are defined.; find_replace ((str, str)) – Line substitution regex. Functions like re.sub, but obeys the exact; semantics of Java’s; String.replaceAll.; force (bool) – If True, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field (str, optional) – If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns. Returns:; Table. hail.methods.import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False)[source]; Import lines of file(s) as a Table of strings.; Examples; To import a file as a table of strings:; >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters:. paths (str or list of str) – Files to import.; min_partitions (int or None) – Minimum number of par",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/impex.html:36365,load,load,36365,docs/0.2/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/methods/impex.html,1,['load'],['load']
Performance,"ruct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @typecheck_method(k=integral,; maf=numeric,; block_size=integral,; min_kinship=numeric,; statistics=enumeration(""phi"", ""phik2"", ""phik2k0"", ""all"")); def pc_relate(self, k, maf, block_size=512, min_kinship=-float(""inf""), statistics=""all""):; """"""Compute relatedness estimates between individuals using a variant of the; PC-Relate method. .. include:: experimental.rst. **Examples**. Estimate kinship, identity-by-descent two, identity-by-descent one, and; identity-by-descent zero for every pair of sa",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:168051,load,loadings,168051,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"rval.parse('17:38449840-38530994')); ; Another way of writing this same query:; ; >>> vds_result = vds.filter_intervals(Interval(Locus('17', 38449840), Locus('17', 38530994))); ; Two identical ways of parsing a list of intervals:; ; >>> intervals = map(Interval.parse, ['1:50M-75M', '2:START-400000', '3-22']); >>> intervals = [Interval.parse(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; ; Use this interval list to filter:; ; >>> vds_result = vds.filter_intervals(intervals); ; **Notes**; ; This method takes an argument of :class:`.Interval` or list of :class:`.Interval`. Based on the ``keep`` argument, this method will either restrict to variants in the; supplied interval ranges, or remove all variants in those ranges. Note that intervals; are left-inclusive, and right-exclusive. The below interval includes the locus; ``15:100000`` but not ``15:101000``. >>> interval = Interval.parse('15:100000-101000'). This method performs predicate pushdown when ``keep=True``, meaning that data shards; that don't overlap any supplied interval will not be loaded at all. This property; enables ``filter_intervals`` to be used for reasonably low-latency queries of small ranges; of the genome, even on large datasets. Suppose we are interested in variants on ; chromosome 15 between 100000 and 200000. This implementation with :py:meth:`.filter_variants_expr`; may come to mind first:; ; >>> vds_filtered = vds.filter_variants_expr('v.contig == ""15"" && v.start >= 100000 && v.start < 200000'); ; However, it is **much** faster (and easier!) to use this method:; ; >>> vds_filtered = vds.filter_intervals(Interval.parse('15:100000-200000')). .. note::. A :py:class:`.KeyTable` keyed by interval can be used to filter a dataset efficiently as well.; See the documentation for :py:meth:`.filter_variants_table` for an example. This is useful for; using interval files to filter a dataset. :param intervals: Interval(s) to keep or remove.; :type intervals: :class:`.Interval` or list of :class:`.In",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:74346,perform,performs,74346,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,2,"['load', 'perform']","['loaded', 'performs']"
Performance,"ry('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; The memory expression must be of the form {number}{suffix}; where valid optional suffixes are K, Ki, M, Mi,; G, Gi, T, Ti, P, and Pi. Omitting a suffix means; the value is in bytes.; For the ServiceBackend, the values ‘lowmem’, ‘standard’,; and ‘highmem’ are also valid arguments. ‘lowmem’ corresponds to; approximately 1 Gi/core, ‘standard’ corresponds to approximately; 4 Gi/core, and ‘highmem’ corresponds to approximately 7 Gi/core.; The default value is ‘standard’. Parameters:; memory (Union[str, int, None]) – Units are in bytes if memory is an int. If None,; use the default value for the ServiceBackend (‘standard’). Return type:; Self. Returns:; Same job object with memory requirements set. regions(regions); Set the cloud regions a job can run in.; Notes; Can only be used with the backend.ServiceBackend.; This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency.; Examples; Require the job to run in ‘us-central1’:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:; >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters:; regions (Optional[List[str]]) – The cloud region(s) to run this job in. Use None to signify; the job can run in any available region. Use py:staticmethod:.ServiceBackend.supported_regions; to list the available regions to choose from. The default is the job can run in; any region. Return type:; Self. Returns:; Same job object with the cloud regions the job can run in set. spot(is_spot); Set whether a job is run on spot instances. By default, all jobs run on spot instances.; Examples; Ensure a job only runs on non-spot instances:; >>> b = Batch(backend=backend.ServiceBackend('test')); >",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html:6844,latency,latency,6844,docs/batch/api/batch/hailtop.batch.job.Job.html,https://hail.is,https://hail.is/docs/batch/api/batch/hailtop.batch.job.Job.html,2,['latency'],['latency']
Performance,"s : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:59652,load,load,59652,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"s :math:`U_k S_k`. While this is just a matter of the scale on each PC, the scores have the advantage of representing true projections of the data onto features with the variance of a score reflecting the variance explained by the corresponding feature. (In PC bi-plots this amounts to a change in aspect ratio; for use of PCs as covariates in regression it is immaterial.). **Annotations**. Given root ``scores='sa.scores'`` and ``as_array=False``, :py:meth:`~hail.VariantDataset.pca` adds a Struct to sample annotations:. - **sa.scores** (*Struct*) -- Struct of sample scores. With ``k=3``, the Struct has three field:. - **sa.scores.PC1** (*Double*) -- Score from first PC. - **sa.scores.PC2** (*Double*) -- Score from second PC. - **sa.scores.PC3** (*Double*) -- Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the ``loadings`` and ``eigenvalues`` arguments, respectively. Given roots ``scores='sa.scores'``, ``loadings='va.loadings'``, and ``eigenvalues='global.evals'``, and ``as_array=True``, :py:meth:`~hail.VariantDataset.pca` adds the following annotations:. - **sa.scores** (*Array[Double]*) -- Array of sample scores from the top k PCs. - **va.loadings** (*Array[Double]*) -- Array of variant loadings in the top k PCs. - **global.evals** (*Array[Double]*) -- Array of the top k eigenvalues. :param str scores: Sample annotation path to store scores. :param loadings: Variant annotation path to store site loadings.; :type loadings: str or None. :param eigenvalues: Global annotation path to store eigenvalues.; :type eigenvalues: str or None. :param k: Number of principal components.; :type k: int or None. :param bool as_array: Store annotations as type Array rather than Struct; :type k: bool or None. :return: Dataset with new PCA annotations.; :rtype: :class:`.VariantDataset`; """""". jvds = self._jvdf.pca(scores, k, joption(loadings), joption(eigenvalues), as_array); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @ty",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:167544,load,loadings,167544,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"s and Variant Datasets.""""""; flagname = 'no_ir_logging'; prev_flag_value = hl._get_flags(flagname).get(flagname); hl._set_flags(**{flagname: '1'}). vds_samples = sum(vds.n_samples for vdses in self._vdses.values() for vds in vdses); info(; 'Running VDS combiner:\n'; f' VDS arguments: {self._num_vdses} datasets with {vds_samples} samples\n'; f' GVCF arguments: {len(self._gvcfs)} inputs/samples\n'; f' Branch factor: {self._branch_factor}\n'; f' GVCF merge batch size: {self._gvcf_batch_size}'; ); while not self.finished:; self.save(); self.step(); self.save(); info('Finished VDS combiner!'); hl._set_flags(**{flagname: prev_flag_value}). [docs] @staticmethod; def load(path) -> 'VariantDatasetCombiner':; """"""Load a :class:`.VariantDatasetCombiner` from `path`.""""""; fs = hl.current_backend().fs; with fs.open(path) as stream:; combiner = json.load(stream, cls=Decoder); combiner._raise_if_output_exists(); if combiner._save_path != path:; warning(; 'path/save_path mismatch in loaded VariantDatasetCombiner, using '; f'{path} as the new save_path for this combiner'; ); combiner._save_path = path; return combiner. def _raise_if_output_exists(self):; if self.finished:; return; fs = hl.current_backend().fs; ref_success_path = os.path.join(VariantDataset._reference_path(self._output_path), '_SUCCESS'); var_success_path = os.path.join(VariantDataset._variants_path(self._output_path), '_SUCCESS'); if fs.exists(ref_success_path) and fs.exists(var_success_path):; raise FatalError(; f'combiner output already exists at {self._output_path}\n' 'move or delete it before continuing'; ). [docs] def to_dict(self) -> dict:; """"""A serializable representation of this combiner.""""""; intervals_typ = hl.tarray(hl.tinterval(hl.tlocus(self._reference_genome))); return {; 'name': self.__class__.__name__,; 'save_path': self._save_path,; 'output_path': self._output_path,; 'temp_path': self._temp_path,; 'reference_genome': str(self._reference_genome),; 'dataset_type': self._dataset_type,; 'gvcf_type': self._gv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html:13238,load,loaded,13238,docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/vds/combiner/variant_dataset_combiner.html,2,['load'],['loaded']
Performance,"s at RegExr. Parameters:; regex (str) – The regular expression to match.; path (str or list of str) – The files to search.; max_count (int) – The maximum number of matches to return. import_bgen(path, tolerance=0.2, sample_file=None, min_partitions=None)[source]¶; Import .bgen file(s) as variant dataset.; Examples; Importing a BGEN file as a VDS (assuming it has already been indexed).; >>> vds = hc.import_bgen(""data/example3.bgen"", sample_file=""data/example3.sample""). Notes; Hail supports importing data in the BGEN file format. For more information on the BGEN file format,; see here. Note that only v1.1 and v1.2 BGEN files; are supported at this time. For v1.2 BGEN files, only unphased and diploid genotype probabilities are allowed and the; genotype probability blocks must be either compressed with zlib or uncompressed.; Before importing, ensure that:. The sample file has the same number of samples as the BGEN file.; No duplicate sample IDs are present. To load multiple files at the same time, use Hadoop Glob Patterns.; Genotype probability (``gp``) representation:; The following modifications are made to genotype probabilities in BGEN v1.1 files:. Since genotype probabilities are understood to define a probability distribution, import_bgen() automatically sets to missing those genotypes for which the sum of the probabilities is a distance greater than the tolerance parameter from 1.0. The default tolerance is 0.2, so a genotype with sum .79 or 1.21 is filtered out, whereas a genotype with sum .8 or 1.2 remains.; import_bgen() normalizes all probabilities to sum to 1.0. Therefore, an input distribution of (0.98, 0.0, 0.0) will be stored as (1.0, 0.0, 0.0) in Hail. Annotations; import_bgen() adds the following variant annotations:. va.varid (String) – 2nd column of .gen file if chromosome present, otherwise 1st column.; va.rsid (String) – 3rd column of .gen file if chromosome present, otherwise 2nd column. Parameters:; path (str or list of str) – .bgen files to import",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.HailContext.html:9224,load,load,9224,docs/0.1/hail.HailContext.html,https://hail.is,https://hail.is/docs/0.1/hail.HailContext.html,1,['load'],['load']
Performance,"s interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval converted to `dest_reference_genome`.; """""". if not 0.0 <= min_match <= 1.0:; raise TypeError(""'liftover' requires 'min_match' is in the range [0, 1]. Got {}"".format(min_match)). if isinstance(x.dtype, tlocus):; rg = x.dtype.reference_genome; method_name = ""liftoverLocus""; rtype = tstruct(result=tlocus(dest_reference_genome), is_negative_strand=tbool); else:; rg = x.dtype.point_type.reference_genome; method_name = ""liftoverLocusInterval""; rtype = tstruct(result=tinterval(tlocus(dest_reference_genome)), is_negative_strand=tbool). if not rg.has_liftover(dest_reference_genome.name):; raise TypeError(; """"""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file."""""".format(rg.name, dest_reference_genome.name); ). expr = _func(method_name, rtype, x, to_expr(min_match, tfloat64)); if not include_strand:; expr = expr.result; return expr. [docs]@typecheck(; f=func_spec(1, expr_float64),; min=expr_float64,; max=expr_float64,; max_iter=builtins.int,; epsilon=builtins.float,; tolerance=builtins.float,; ); def uniroot(f: Callable, min, max, *, max_iter=1000, epsilon=2.2204460492503131e-16, tolerance=1.220703e-4):; """"""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expressio",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:172050,load,load,172050,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,2,['load'],['load']
Performance,"s method. A hard-called variant dataset is about two orders of magnitude; smaller than a standard sequencing dataset. Use this; method to create a smaller, faster; representation for downstream processing that only; requires the GT field. Returns:Variant dataset with no genotype metadata. Return type:VariantDataset. ibd(maf=None, bounded=True, min=None, max=None)[source]¶; Compute matrix of identity-by-descent estimations. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the variant dataset itself:; >>> vds.ibd(). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in [0.2, 0.9], using minor allele frequencies stored in; va.panel_maf:; >>> vds.ibd(maf='va.panel_maf', min=0.2, max=0.9). Notes; The implementation is based on the IBD algorithm described in the PLINK; paper.; ibd() requires the dataset to be; bi-allelic (otherwise run split_multi() or otherwise run filter_multi()); and does not perform LD pruning. Linkage disequilibrium may bias the; result so consider filtering variants first.; The resulting KeyTable entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output key table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:; maf (str or None) – Expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2,; and PI_HAT to take on biologically meaningful values; (in the range [0,1]).; min (float or None) – Sample pairs with a PI_HAT below this v",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:66954,perform,perform,66954,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['perform'],['perform']
Performance,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; from hail.backend.service_backend import ServiceBackend. if isinstance(hl.current_backend(), ServiceBackend):; return _blanczos_pca(entry_expr, k, compute_loadings). raise_unless_entry_indexed('pca/entry_expr', entry_expr). mt = matrix_table_source('pca/entry_expr', entry_expr). # FIXME: remove once select_entries on a field is free; if entry_expr in mt._fields_inverse:; field = mt._fields_inverse[entry_expr]; else:; field = Env.get_uid(); mt = mt.select_entries(**{field: entry_expr}); mt = mt.select_cols().select_rows().select_globals(). t = Table(; ir.MatrixToTableApply(; mt._mir, {'name': 'PCA', 'entryField': field, 'k': k, 'computeLoadings': compute_loadings}; ); ).persist(). g = t.index_globals(); scores = hl.Table.parallelize(g.scores, key=list(mt.col_key)); if not compute_loadings:; t =",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:7419,load,loadings,7419,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"s of features while the scores; represent the projected data on those features. The eigenvalues of the Gramian; :math:`MM^T` are the squares of the singular values :math:`s_1^2, s_2^2,; \ldots`, which represent the variances carried by the respective PCs. By; default, Hail only computes the loadings if the ``loadings`` parameter is; specified. Scores are stored in a :class:`.Table` with the column key of the matrix; table as key and a field `scores` of type ``array<float64>`` containing; the principal component scores. Loadings are stored in a :class:`.Table` with the row key of the matrix; table as key and a field `loadings` of type ``array<float64>`` containing; the principal component loadings. The eigenvalues are returned in descending order, with scores and loadings; given the corresponding array order. Parameters; ----------; entry_expr : :class:`.Expression`; Numeric expression for matrix entries.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings.; q_iterations : :obj:`int`; Number of rounds of power iteration to amplify singular values.; oversampling_param : :obj:`int`; Amount of oversampling to use when approximating the singular values.; Usually a value between `0 <= oversampling_param <= k`. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; if not isinstance(A, TallSkinnyMatrix):; raise_unless_entry_indexed('_blanczos_pca/entry_expr', A); A = _make_tsm(A, block_size). if oversampling_param is None:; oversampling_param = k. compute_U = (not transpose and compute_loadings) or (transpose and compute_scores); U, S, V = _reduced_svd(A, k, compute_U, q_iterations, k + oversampling_param); info(""blanczos_pca: SVD Complete. Computing conversion to PCs.""). def numpy_to_rows_table(X, field_name):; t = A.source_table.select(); t = t.annotate_globals(X=X); idx_name = '_tmp_pca_loading_index'; ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:21118,load,loadings,21118,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"s of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""; if gene_symbols is None and gene_ids is None and transcript_ids is None:; raise ValueError('get_gene_intervals requires at least one of gene_symbols, gene_ids, or transcript_ids'); ht = _load_gencode_gtf(gtf_file, reference_genome); criteria = []; if gene_symbols:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_name == y), gene_symbols)); if gene_ids:; criteria.append(hl.any(lambda y: (ht.feature == 'gene') & (ht.gene_id == y.split('\\.')[0]), gene_ids)); if transcript_ids:; criteria.append(; hl.any(lambda y: (ht.feature == 'transcript') & (ht.transcript_id == y.split('\\.')[0]), transcript_ids); ). ht = ht.filter(functools.reduce(operator.ior, criteria)); gene_info = ht.aggregate(hl.agg.collect((ht.feature, ht.gene_name, ht.gene_id, ht.transcript_id, ht.interv",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html:7850,load,load,7850,docs/0.2/_modules/hail/experimental/import_gtf.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/experimental/import_gtf.html,2,['load'],['load']
Performance,"s written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument to hl.init, permitting configuration of; Spark runtime for a ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77577,perform,performance,77577,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"s().cols, hail_array).map(; lambda tup: tup[0].annotate(**{field_name: tup[1]}); ); t = hl.Table.parallelize(cols_and_X, key=A.col_key); return t. st = None; lt = None; eigens = hl.eval(S * S); if transpose:; if compute_loadings:; lt = numpy_to_cols_table(V, 'loadings'); if compute_scores:; st = numpy_to_rows_table(U * S, 'scores'); else:; if compute_scores:; st = numpy_to_cols_table(V * S, 'scores'); if compute_loadings:; lt = numpy_to_rows_table(U, 'loadings'). return eigens, st, lt. @typecheck(; call_expr=expr_call,; k=int,; compute_loadings=bool,; q_iterations=int,; oversampling_param=nullable(int),; block_size=int,; ); def _hwe_normalized_blanczos(; call_expr, k=10, compute_loadings=False, q_iterations=10, oversampling_param=None, block_size=128; ):; r""""""Run randomized principal component analysis approximation (PCA) on the; Hardy-Weinberg-normalized genotype call matrix. Implements the Blanczos algorithm found by Rokhlin, Szlam, and Tygert. Examples; --------. >>> eigenvalues, scores, loadings = hl._hwe_normalized_blanczos(dataset.GT, k=5). Notes; -----; This method specializes :func:`._blanczos_pca` for the common use case; of PCA in statistical genetics, that of projecting samples to a small; number of ancestry coordinates. Variants that are all homozygous reference; or all homozygous alternate are unnormalizable and removed before; evaluation. See :func:`._blanczos_pca` for more details. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression.; k : :obj:`int`; Number of principal components.; compute_loadings : :obj:`bool`; If ``True``, compute row loadings. Returns; -------; (:obj:`list` of :obj:`float`, :class:`.Table`, :class:`.Table`); List of eigenvalues, table with column scores, table with row loadings.; """"""; raise_unless_entry_indexed('_blanczos_pca/entry_expr', call_expr); A = _make_tsm_from_call(call_expr, block_size, hwe_normalize=True). return _blanczos_pca(; A,; k,; compute_loadings=compute_loadings,; q_itera",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/pca.html:23359,load,loadings,23359,docs/0.2/_modules/hail/methods/pca.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/pca.html,2,['load'],['loadings']
Performance,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html:5153,perform,performance,5153,docs/0.2/_modules/hail/linalg/blockmatrix.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html,2,['perform'],['performance']
Performance,"s. Hail Query provides powerful, easy-to-use data science tools. Interrogate data at every scale: small datasets on a; laptop through to biobank-scale datasets (e.g. UK; Biobank, gnomAD, TopMed, FinnGen, and; Biobank Japan) in the cloud.; . Genomic Dataframes. Modern data science is driven by numeric matrices (see Numpy) and tables; (see R dataframes; and Pandas). While sufficient for many tasks, none of these tools adequately; capture the structure of genetic data. Genetic data combines the multiple axes of a matrix (e.g. variants and samples); with the structured data of tables (e.g. genotypes). To support genomic analysis, Hail introduces a powerful and; distributed data structure combining features of matrices and dataframes called; MatrixTable.; . Input Unification. The Hail; MatrixTable unifies a wide range of input formats (e.g. vcf, bgen, plink, tsv, gtf, bed files), and supports; scalable queries, even on petabyte-size datasets. Hail's MatrixTable abstraction provides an integrated and scalable; analysis platform for science.; . Learn More. Hail Batch. Arbitrary Tools. Hail Batch enables massively parallel execution and composition of arbitrary GNU/Linux tools like PLINK, SAIGE, sed,; and even Python scripts that use Hail Query!; . Cost-efficiency and Ease-of-use. Hail Batch is cost-efficient and easy-to-use because it automatically and cooperatively manages cloud resources for; all users. As an end-user you need only describe which programs to run, with what arguments, and the dependencies; between programs.; . Scalability and Cost Control. Hail Batch automatically scales to fit the needs of your job. Instead of queueing for limited resources on a; fixed-size cluster, your jobs only queue while the service requests more cores from the cloud. Hail Batch also; optionally enforces spending limits which protect users from cost overruns.; . Learn More. Acknowledgments. The Hail team has several sources of funding at the Broad Institute:. The Stanley Center for P",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/index.html:1996,scalab,scalable,1996,index.html,https://hail.is,https://hail.is/index.html,1,['scalab'],['scalable']
Performance,"s; Aggregate to a matrix with genes as row keys, computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; All complex expressions must be passed as named expressions. Parameters:. exprs (args of str or Expression) – Row fields to group by.; named_exprs (keyword args of Expression) – Row-indexed expressions to group by. Returns:; GroupedMatrixTable – Grouped matrix. Can be used to call GroupedMatrixTable.aggregate(). partition_hint(n)[source]; Set the target number of partitions for aggregation.; Examples; Use partition_hint in a MatrixTable.group_rows_by() /; GroupedMatrixTable.aggregate() pipeline:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; Until Hail’s query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints.; The default number of partitions for GroupedMatrixTable.aggregate() is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters:; n (int) – Number of partitions. Returns:; GroupedMatrixTable – Same grouped matrix table with a partition hint. result()[source]; Return the result of aggregating by group.; Examples; Aggregate to a matrix with genes as row keys, collecting the functional; consequences per gene as a row field and computing the number of; non-reference calls as an entry field:; >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .aggregate_rows(consequences = hl.agg.collect_as_set(dataset.consequence)); ... .aggregate_entries(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref())); ... .result()). Aggregate to a matrix w",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html:5172,optimiz,optimizer,5172,docs/0.2/hail.GroupedMatrixTable.html,https://hail.is,https://hail.is/docs/0.2/hail.GroupedMatrixTable.html,1,['optimiz'],['optimizer']
Performance,"s; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:data:`.tfloat32` or :py:data:`.tfloat64`. Default:; :py:data:`.tfloat64`.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:102797,load,loaded,102797,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['loaded']
Performance,"s=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate allele frequency :math:`p_j` is computed as half the mean of the non-missing entries of column :math:`j`. Entries of :math:`M` are then mean-centered and variance-normalized as. .. math::. M_{ij} = \\frac{C_{ij}-2p_j}{\sqrt{2p_j(1-p_j)m}},. with :math:`M_{ij} = 0` for :math:`C_{ij}` missing (i.e. mean genotype imputation). This scaling normalizes genotype variances to a common value :math:`1/m` for variants in Hardy-Weinberg e",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162760,load,loadings,162760,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
Performance,"s[field] = hl.tfloat64; else:; imputed_types[field] = hl.tstr; reasons[field] = 'imputed'. strs.append('Finished type imputation'). all_types = dict(**types, **imputed_types). for f_idx, field in enumerate(fields):; strs.append(f' Loading field {field!r} as type {all_types[field]} ({reasons[field]})'); fields_to_value[field] = parse_type(ht.split_text[f_idx], all_types[field]); else:; strs.append('Reading table without type imputation'); for f_idx, field in enumerate(fields):; reason = 'user-supplied' if field in types else 'not specified'; t = types.get(field, hl.tstr); fields_to_value[field] = parse_type(ht.split_text[f_idx], t); strs.append(f' Loading field {field!r} as type {t} ({reason})'). ht = ht.annotate(**fields_to_value).drop('split_text'); if source_file_field is not None:; source_file = {source_file_field: ht.file}; ht = ht.annotate(**source_file); ht = ht.drop('file'). if len(fields) < 30:; hl.utils.info('\n'.join(strs)); else:; from collections import Counter. strs2 = [f'Loading {ht.row} fields. Counts by type:']; for name, count in Counter(ht[f].dtype for f in fields).most_common():; strs2.append(f' {name}: {count}'); hl.utils.info('\n'.join(strs2)). if key:; key = wrap_to_list(key); ht = ht.key_by(*key); return ht. [docs]@typecheck(; paths=oneof(str, sequenceof(str)), min_partitions=nullable(int), force_bgz=bool, force=bool, file_per_partition=bool; ); def import_lines(paths, min_partitions=None, force_bgz=False, force=False, file_per_partition=False) -> Table:; """"""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`;",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:64727,Load,Loading,64727,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,1,['Load'],['Loading']
Performance,"s` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:101817,load,load,101817,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['load'],['load']
Performance,"sce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""; return MatrixTable(ir.MatrixRepartition(self._mir, max_partitions, ir.RepartitionStrategy.NAIVE_COALESCE)). [docs] def cache(self) -> 'MatrixTable':; """"""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""; return self.persist('MEMORY_ONLY'). [docs] @typecheck_method(storage_level=storage_level); def persist(self, storage_level: str = 'MEMORY_AND_DISK') -> 'MatrixTable':; """"""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY,",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/matrixtable.html:110586,Cache,Cached,110586,docs/0.2/_modules/hail/matrixtable.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/matrixtable.html,1,['Cache'],['Cached']
Performance,"scent estimates. Note; Requires the column key to be one field of type tstr. Note; Requires the dataset to have a compound row key:. locus (type tlocus); alleles (type tarray of tstr). Note; Requires the dataset to contain no multiallelic variants.; Use split_multi() or split_multi_hts() to split; multiallelic sites, or MatrixTable.filter_rows() to remove; them. Examples; To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:; >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; PI_HAT in \([0.2, 0.9]\), using minor allele frequencies stored in; the row field panel_maf:; >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; The dataset must have a column field named s which is a StringExpression; and which uniquely identifies a column.; The implementation is based on the IBD algorithm described in the PLINK; paper.; identity_by_descent() requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first.; The resulting Table entries have the type: { i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }. The key list is: *i: String, j:; String*.; Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form; i j ibd.Z0 ibd.Z1 ibd.Z2 ibd.PI_HAT ibs0 ibs1 ibs2; sample1 sample2 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample3 1.0000 0.0000 0.0000 0.0000 ...; sample1 sample4 0.6807 0.0000 0.3193 0.3193 ...; sample1 sample5 0.1966 0.0000 0.8034 0.8034 ... Parameters:. dataset (MatrixTable) – Variant-keyed and sample-keyed MatrixTable containing genotype information.; maf (Float64Expression, optional) – Row-indexed expression for the minor allele frequency.; bounded (bool) – Forces the estimations for Z0, Z1, Z2, and PI_HAT to take; on biologically meani",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/methods/relatedness.html:4003,perform,perform,4003,docs/0.2/methods/relatedness.html,https://hail.is,https://hail.is/docs/0.2/methods/relatedness.html,1,['perform'],['perform']
Performance,"self):; """"""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'NPartitionsTable'})). [docs] def count(self):; """"""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""; return Env.backend().execute(ir.TableCount(self._tir)). async def _async_count(self):; return await Env.backend()._async_execute(ir.TableCount(self._tir)). def _force_count(self):; return Env.backend().execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). async def _async_force_count(self):; return await Env.backend()._async_execute(ir.TableToValueApply(self._tir, {'name': 'ForceCountTable'})). @typecheck_method(caller=str, row=expr_struct()); def _select(self, caller, row) -> 'Table':; analyze(caller, row, self._row_indices); base, cleanup = self._process_joins(row); return cleanup(Table(ir.TableMapRows(base._tir, row._ir))). @typecheck_method(caller=str, s=expr_struct()); def _select_globals(self, caller, s) -> 'Table':; base",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/table.html:13155,load,loaded,13155,docs/0.2/_modules/hail/table.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/table.html,2,['load'],['loaded']
Performance,"set of statistics to compute, ‘phi’ will only; compute the kinship statistic, ‘phik2’ will; compute the kinship and identity-by-descent two; statistics, ‘phik2k0’ will compute the kinship; statistics and both identity-by-descent two and; zero, ‘all’ computes the kinship statistic and; all three identity-by-descent statistics. Returns:A KeyTable mapping pairs of samples to estimations; of their kinship and identity-by-descent zero, one, and two. Return type:KeyTable. pca(scores, loadings=None, eigenvalues=None, k=10, as_array=False)[source]¶; Run Principal Component Analysis (PCA) on the matrix of genotypes. Important; The genotype_schema() must be of type TGenotype in order to use this method. Examples; Compute the top 5 principal component scores, stored as sample annotations sa.scores.PC1, …, sa.scores.PC5 of type Double:; >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations sa.scores, va.loadings, and global.evals of type Array[Double]:; >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). Notes; Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure Patterson, Price and Reich, 2006. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively.; PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix \(M\), computed as follows. An \(n \times m\) matrix \(C\) records raw genotypes, with rows indexed by \(n\) samples and columns indexed by \(m\) bialellic autosomal variants; \(C_{ij}\) is the number of alternate alleles of variant \(j\) carried by sample \(i\), which can be 0, 1, 2, or missing. For each variant \(j\), the sample alternate allele frequency \(p_j\) is compu",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:138452,load,loadings,138452,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,1,['load'],['loadings']
Performance,"sible issues about having too many open file handles. New features. (#11300); geom_histogram infers min and max values automatically.; (#11317) Add support; for alpha aesthetic and identity position to; geom_histogram. Version 0.2.83; Release 2022-02-01. Bug fixes. (#11268) Fixed; log argument in hail.plot.histogram.; (#11276) Fixed; log argument in hail.plot.pdf.; (#11256) Fixed; memory leak in LD Prune. New features. (#11274) Added; geom_col to hail.ggplot. hailctl dataproc. (#11280) Updated; dataproc image version to one not affected by log4j vulnerabilities. Version 0.2.82; Release 2022-01-24. Bug fixes. (#11209); Significantly improved usefulness and speed of Table.to_pandas,; resolved several bugs with output. New features. (#11247) Introduces; a new experimental plotting interface hail.ggplot, based on R’s; ggplot library.; (#11173) Many math; functions like hail.sqrt now automatically broadcast over; ndarrays. Performance Improvements. (#11216); Significantly improve performance of parse_locus_interval. Python and Java Support. (#11219) We no; longer officially support Python 3.6, though it may continue to work; in the short term.; (#11220) We support; building hail with Java 11. File Format. The native file format version is now 1.6.0. Older versions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.81; Release 2021-12-20. hailctl dataproc. (#11182) Updated; Dataproc image version to mitigate yet more Log4j vulnerabilities. Version 0.2.80; Release 2021-12-15. New features. (#11077); hl.experimental.write_matrix_tables now returns the paths of the; written matrix tables. hailctl dataproc. (#11157) Updated; Dataproc image version to mitigate the Log4j vulnerability.; (#10900) Added; --region parameter to hailctl dataproc submit.; (#11090) Teach; hailctl dataproc describe how to read URLs with the protocols; s3 (Amazon S3), hail-az (Azure Blob Storage), and file; (local file system) in additi",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:54795,perform,performance,54795,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"sing a crash.; (#5268) Fix; Table.export writing a file called ‘None’ in the current; directory.; (#5265) Fix; hl.get_reference raising an exception when called before; hl.init().; (#5250) Fix crash in; pc_relate when called on a MatrixTable field other than ‘GT’.; (#5278) Fix crash in; Table.order_by when sorting by fields whose names are not valid; Python identifiers.; (#5294) Fix crash in; hl.trio_matrix when sample IDs are missing.; (#5295) Fix crash in; Table.index related to key field incompatibilities. Version 0.2.9; Released 2019-01-30. New features. (#5149) Added bitwise; transformation functions:; hl.bit_{and, or, xor, not, lshift, rshift}.; (#5154) Added; hl.rbind function, which is similar to hl.bind but expects a; function as the last argument instead of the first. Performance improvements. (#5107) Hail’s Python; interface generates tighter intermediate code, which should result in; moderate performance improvements in many pipelines.; (#5172) Fix; unintentional performance deoptimization related to Table.show; introduced in 0.2.8.; (#5078) Improve; performance of hl.ld_prune by up to 30x. Bug fixes. (#5144) Fix crash; caused by hl.index_bgen (since 0.2.7); (#5177) Fix bug; causing Table.repartition(n, shuffle=True) to fail to increase; partitioning for unkeyed tables.; (#5173) Fix bug; causing Table.show to throw an error when the table is empty; (since 0.2.8).; (#5210) Fix bug; causing Table.show to always print types, regardless of types; argument (since 0.2.8).; (#5211) Fix bug; causing MatrixTable.make_table to unintentionally discard non-key; row fields (since 0.2.8). Version 0.2.8; Released 2019-01-15. New features. (#5072) Added; multi-phenotype option to hl.logistic_regression_rows; (#5077) Added support; for importing VCF floating-point FORMAT fields as float32 as well; as float64. Performance improvements. (#5068) Improved; optimization of MatrixTable.count_cols.; (#5131) Fixed; performance bug related to hl.literal on large values with",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:100848,perform,performance,100848,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"sion 0.2.31; New features; File size; Performance; Bug fixes. Version 0.2.30; Performance; New features; Miscellaneous. Version 0.2.29; Bug fixes; Performance improvements; New features; hailctl dataproc. Version 0.2.28; Critical correctness bug fix; Bug fixes; New Features; hailctl dataproc; Documentation. Version 0.2.27; New Features; Bug fixes; hailctl dataproc. Version 0.2.26; New Features; Bug Fixes; Performance Improvements; File Format. Version 0.2.25; New features; Bug fixes; Performance improvements; File Format. Version 0.2.24; hailctl dataproc; New features; Bug fixes. Version 0.2.23; hailctl dataproc; Bug fixes; New features; Performance. Version 0.2.22; New features; Performance; hailctl dataproc. Version 0.2.21; Bug fixes; New features; Performance; hailctl dataproc. Version 0.2.20; Critical memory management fix; Bug fixes; New features. Version 0.2.19; Critical performance bug fix; Bug fixes; Performance Improvements; hailctl dataproc. Version 0.2.18; Critical performance bug fix; Bug fixes. Version 0.2.17; New features; Bug fixes; Experimental; File Format. Version 0.2.16; hailctl; Bug fixes. Version 0.2.15; hailctl; New features; Bug fixes. Version 0.2.14; New features. Version 0.2.13; New features; Bug fixes; Experimental. Version 0.2.12; New features; Bug fixes; Experimental. Version 0.2.11; New features; Bug fixes. Version 0.2.10; New features; Performance improvements; Bug fixes. Version 0.2.9; New features; Performance improvements; Bug fixes. Version 0.2.8; New features; Performance improvements; Bug fixes. Version 0.2.7; New features; Performance improvements. Version 0.2.6; New features; Performance improvements; Bug fixes. Version 0.2.5; New features; Performance improvements; Bug fixes. Version 0.2.4: Beginning of history!. menu; Hail. Change Log And Version Policy. View page source. Change Log And Version Policy. Python Version Compatibility Policy; Hail complies with NumPy’s compatibility; policy; on Python versions. In particular, Ha",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:6579,perform,performance,6579,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,sions of Hail; will not be able to read tables or matrix tables written by this; version of Hail. Version 0.2.33; Released 2020-02-27. New features. (#8173) Added new; method hl.zeros. Bug fixes. (#8153) Fixed; complier bug causing MatchError in import_bgen.; (#8123) Fixed an; issue with multiple Python HailContexts running on the same cluster.; (#8150) Fixed an; issue where output from VEP about failures was not reported in error; message.; (#8152) Fixed an; issue where the row count of a MatrixTable coming from; import_matrix_table was incorrect.; (#8175) Fixed a bug; where persist did not actually do anything. hailctl dataproc. (#8079) Using; connect to open the jupyter notebook browser will no longer crash; if your project contains requester-pays buckets. Version 0.2.32; Released 2020-02-07. Critical performance regression fix. (#7989) Fixed; performance regression leading to a large slowdown when; hl.variant_qc was run after filtering columns. Performance. (#7962) Improved; performance of hl.pc_relate.; (#8032) Drastically; improve performance of pipelines calling hl.variant_qc and; hl.sample_qc iteratively.; (#8037) Improve; performance of NDArray matrix multiply by using native linear algebra; libraries. Bug fixes. (#7976) Fixed; divide-by-zero error in hl.concordance with no overlapping rows; or cols.; (#7965) Fixed; optimizer error leading to crashes caused by; MatrixTable.union_rows.; (#8035) Fix compiler; bug in Table.multi_way_zip_join.; (#8021) Fix bug in; computing shape after BlockMatrix.filter.; (#7986) Fix error in; NDArray matrix/vector multiply. New features. (#8007) Add; hl.nd.diagonal function. Cheat sheets. (#7940) Added cheat; sheet for MatrixTables.; (#7963) Improved; Table sheet sheet. Version 0.2.31; Released 2020-01-22. New features. (#7787) Added; transition/transversion information to hl.summarize_variants.; (#7792) Add Python; stack trace to array index out of bounds errors in Hail pipelines.; (#7832) Add; spark_conf argument,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:77518,perform,performance,77518,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"spect ratio; for use of PCs as covariates in regression it is immaterial.); Annotations; Given root scores='sa.scores' and as_array=False, pca() adds a Struct to sample annotations:. sa.scores (Struct) – Struct of sample scores. With k=3, the Struct has three field:. sa.scores.PC1 (Double) – Score from first PC; sa.scores.PC2 (Double) – Score from second PC; sa.scores.PC3 (Double) – Score from third PC. Analogous variant and global annotations of type Struct are added by specifying the loadings and eigenvalues arguments, respectively.; Given roots scores='sa.scores', loadings='va.loadings', and eigenvalues='global.evals', and as_array=True, pca() adds the following annotations:. sa.scores (Array[Double]) – Array of sample scores from the top k PCs; va.loadings (Array[Double]) – Array of variant loadings in the top k PCs; global.evals (Array[Double]) – Array of the top k eigenvalues. Parameters:; scores (str) – Sample annotation path to store scores.; loadings (str or None) – Variant annotation path to store site loadings.; eigenvalues (str or None) – Global annotation path to store eigenvalues.; k (bool or None) – Number of principal components.; as_array (bool) – Store annotations as type Array rather than Struct. Returns:Dataset with new PCA annotations. Return type:VariantDataset. persist(storage_level='MEMORY_AND_DISK')[source]¶; Persist this variant dataset to memory and/or disk.; Examples; Persist the variant dataset to both memory and disk:; >>> vds_result = vds.persist(). Notes; The persist() and cache() methods ; allow you to store the current dataset on disk or in memory to avoid redundant computation and ; improve the performance of Hail pipelines.; cache() is an alias for ; persist(""MEMORY_ONLY""). Most users will want “MEMORY_AND_DISK”.; See the Spark documentation ; for a more in-depth discussion of persisting data. Warning; Persist, like all other VariantDataset functions, is functional.; Its output must be captured. This is wrong:; >>> vds = vds.linre",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/hail.VariantDataset.html:143715,load,loadings,143715,docs/0.1/hail.VariantDataset.html,https://hail.is,https://hail.is/docs/0.1/hail.VariantDataset.html,2,['load'],['loadings']
Performance,"ss than or equal to `x`. Examples; --------. >>> hl.eval(hl.floor(3.1)); 3.0. Parameters; ----------; x : :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""floor"", x.dtype, x). [docs]@typecheck(x=expr_oneof(expr_float32, expr_float64, expr_ndarray(expr_float64))); @ndarray_broadcasting; def ceil(x):; """"""The smallest integral value that is greater than or equal to `x`. Examples; --------. >>> hl.eval(hl.ceil(3.1)); 4.0. Parameters; ----------; x : :class:`.Float32Expression`,:class:`.Float64Expression` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Float32Expression`, :class:`.Float64Expression`, or :class:`.NDArrayNumericExpression`; """"""; return _func(""ceil"", x.dtype, x). [docs]@typecheck(n_hom_ref=expr_int32, n_het=expr_int32, n_hom_var=expr_int32, one_sided=expr_bool); def hardy_weinberg_test(n_hom_ref, n_het, n_hom_var, one_sided=False) -> StructExpression:; """"""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/expr/functions.html:33807,Perform,Performs,33807,docs/0.2/_modules/hail/expr/functions.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/expr/functions.html,1,['Perform'],['Performs']
Performance,"ssage from hadoop_ls when file does not exist. Performance Improvements. (#10068) Make; certain array copies faster.; (#10061) Improve; code generation of hl.if_else and hl.coalesce. Version 0.2.62; Released 2021-02-03. New features. (#9936) Deprecated; hl.null in favor of hl.missing for naming consistency.; (#9973) hl.vep; now includes a vep_proc_id field to aid in debugging unexpected; output.; (#9839) Hail now; eagerly deletes temporary files produced by some BlockMatrix; operations.; (#9835) hl.any; and hl.all now also support a single collection argument and a; varargs of Boolean expressions.; (#9816); hl.pc_relate now includes values on the diagonal of kinship,; IBD-0, IBD-1, and IBD-2; (#9736) Let; NDArrayExpression.reshape take varargs instead of mandating a tuple.; (#9766); hl.export_vcf now warns if INFO field names are invalid according; to the VCF 4.3 spec. Bug fixes. (#9976) Fixed; show() representation of Hail dictionaries. Performance improvements. (#9909) Improved; performance of hl.experimental.densify by approximately 35%. Version 0.2.61; Released 2020-12-03. New features. (#9749) Add or_error; method to SwitchBuilder (hl.switch). Bug fixes. (#9775) Fixed race; condition leading to invalid intermediate files in VCF combiner.; (#9751) Fix bug where; constructing an array of empty structs causes type error.; (#9731) Fix error and; incorrect behavior when using hl.import_matrix_table with int64; data types. Version 0.2.60; Released 2020-11-16. New features. (#9696); hl.experimental.export_elasticsearch will now support; Elasticsearch versions 6.8 - 7.x by default. Bug fixes. (#9641) Showing hail; ndarray data now always prints in correct order. hailctl dataproc. (#9610) Support; interval fields in hailctl dataproc describe. Version 0.2.59; Released 2020-10-22. Datasets / Annotation DB. (#9605) The Datasets; API and the Annotation Database now support AWS, and users are; required to specify what cloud platform they’re using. hailctl datapr",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/change_log.html:62530,perform,performance,62530,docs/0.2/change_log.html,https://hail.is,https://hail.is/docs/0.2/change_log.html,1,['perform'],['performance']
Performance,"ssary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; - `dosage` (:py:data:`.tfloat64`) -- The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See Also; --------; :func:`.index_bgen`. Parameters; ----------; path : :class:`s",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/impex.html:41496,perform,performance,41496,docs/0.2/_modules/hail/methods/impex.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/impex.html,2,['perform'],['performance']
Performance,ssion method). king() (in module hail.methods). L. lambda_gc() (in module hail.methods). last() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). ld_matrix() (in module hail.methods). ld_prune() (in module hail.methods). ld_score() (in module hail.experimental). ld_score_regression() (in module hail.experimental). len() (in module hail.expr.functions). length() (hail.expr.ArrayExpression method). (hail.expr.ArrayNumericExpression method). (hail.expr.CollectionExpression method). (hail.expr.SetExpression method). (hail.expr.StringExpression method). lengths (hail.genetics.ReferenceGenome property). lgt_to_gt() (in module hail.vds). liftover() (in module hail.expr.functions). linear_mixed_model() (in module hail.methods). linear_mixed_regression_rows() (in module hail.methods). linear_regression_rows() (in module hail.methods). LinearMixedModel (class in hail.stats). linreg() (in module hail.expr.aggregators). literal() (in module hail.expr.functions). load() (hail.vds.combiner.VariantDatasetCombiner static method). load_combiner() (in module hail.vds.combiner). load_dataset() (in module hail.experimental). local_to_global() (in module hail.vds). localize_entries() (hail.MatrixTable method). Locus (class in hail.genetics). locus() (in module hail.expr.functions). locus_from_global_position() (hail.genetics.ReferenceGenome method). (in module hail.expr.functions). locus_interval() (in module hail.expr.functions). locus_windows() (in module hail.linalg.utils). LocusExpression (class in hail.expr). log() (hail.linalg.BlockMatrix method). (in module hail.expr.functions). log10() (in module hail.expr.functions). logistic_regression_rows() (in module hail.methods). logit() (in module hail.expr.functions). loop() (in module hail.experimental). lower() (hail.expr.StringExpression method). ls() (in module hailtop.fs). M. make_betas() (in module hail.experimental.ldscsim). make_table() (hail.MatrixTable method). manhattan() (in module hail.plot). ma,MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/genindex.html:32872,load,load,32872,docs/0.2/genindex.html,https://hail.is,https://hail.is/docs/0.2/genindex.html,1,['load'],['load']
Performance,"ssion model is derived in Section; 3.2 of `The Elements of Statistical Learning, 2nd Edition; <http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf>`__.; See equation 3.12 for the t-statistic which follows the t-distribution with; :math:`n - k - 1` degrees of freedom, under the null hypothesis of no; effect, with :math:`n` samples and :math:`k` covariates in addition to; ``x``. Note; ----; Use the `pass_through` parameter to include additional row fields from; matrix table underlying ``x``. For example, to include an ""rsid"" field, set; ``pass_through=['rsid']`` or ``pass_through=[mt.rsid]``. Parameters; ----------; y : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; One or more column-indexed response expressions.; x : :class:`.Float64Expression`; Entry-indexed expression for input variable.; covariates : :obj:`list` of :class:`.Float64Expression`; List of column-indexed covariate expressions.; block_size : :obj:`int`; Number of row regressions to perform simultaneously per core. Larger blocks; require more memory but may improve performance.; pass_through : :obj:`list` of :class:`str` or :class:`.Expression`; Additional row fields to include in the resulting table.; weights : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Optional column-indexed weighting for doing weighted least squares regression. Specify a single weight if a; single y or list of ys is specified. If a list of lists of ys is specified, specify one weight per inner list. Returns; -------; :class:`.Table`; """"""; if not isinstance(Env.backend(), SparkBackend) or weights is not None:; return _linear_regression_rows_nd(y, x, covariates, block_size, weights, pass_through). mt = matrix_table_source('linear_regression_rows/x', x); raise_unless_entry_indexed('linear_regression_rows/x', x). y_is_list = isinstance(y, list); if y_is_list and len(y) == 0:; raise ValueError(""'linear_regression_rows': found no values for 'y'""); is_chain",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html:12126,perform,perform,12126,docs/0.2/_modules/hail/methods/statgen.html,https://hail.is,https://hail.is/docs/0.2/_modules/hail/methods/statgen.html,2,['perform'],['perform']
Performance,"symbol'; else:; vds_key = None. # annotate analysis VDS with database keytable; self = self.annotate_variants_table(self.hc.read_table(db_file), expr=expr, vds_key=vds_key). else:; continue. return self. [docs] @handle_py4j; def cache(self):; """"""Mark this variant dataset to be cached in memory. :py:meth:`~hail.VariantDataset.cache` is the same as :func:`persist(""MEMORY_ONLY"") <hail.VariantDataset.persist>`.; ; :rtype: :class:`.VariantDataset`; """""". return VariantDataset(self.hc, self._jvdf.cache()). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(right=vds_type); def concordance(self, right):; """"""Calculate call concordance with another variant dataset. .. include:: requireTGenotype.rst. **Example**; ; >>> comparison_vds = hc.read('data/example2.vds'); >>> summary, samples, variants = vds.concordance(comparison_vds). **Notes**. This method computes the genotype call concordance between two bialellic variant datasets. ; It performs an inner join on samples (only samples in both datasets will be considered), and an outer join; on variants. If a variant is only in one dataset, then each genotype is treated as ""no data"" in the other.; This method returns a tuple of three objects: a nested list of list of int with global concordance; summary statistics, a key table with sample concordance statistics, and a key table with variant concordance ; statistics.; ; **Using the global summary result**; ; The global summary is a list of list of int (conceptually a 5 by 5 matrix), ; where the indices have special meaning:. 0. No Data (missing variant); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var; ; The first index is the state in the left dataset (the one on which concordance was called), and the second; index is the state in the right dataset (the argument to the concordance method call). Typical uses of ; the summary list are shown below.; ; >>> summary, samples, variants = vds.concordance(hc.read('data/example2.vds')); >>> left_homref_righ",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:41428,perform,performs,41428,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['perform'],['performs']
Performance,"t can move. Affects memory usage, and will; cause Hail to throw an error if a variant that moves further; is encountered. :rtype: :class:`.VariantDataset`; """""". jvds = self._jvds.minRep(max_shift); return VariantDataset(self.hc, jvds). [docs] @handle_py4j; @requireTGenotype; @typecheck_method(scores=strlike,; loadings=nullable(strlike),; eigenvalues=nullable(strlike),; k=integral,; as_array=bool); def pca(self, scores, loadings=None, eigenvalues=None, k=10, as_array=False):; """"""Run Principal Component Analysis (PCA) on the matrix of genotypes. .. include:: requireTGenotype.rst. **Examples**. Compute the top 5 principal component scores, stored as sample annotations ``sa.scores.PC1``, ..., ``sa.scores.PC5`` of type Double:. >>> vds_result = vds.pca('sa.scores', k=5). Compute the top 5 principal component scores, loadings, and eigenvalues, stored as annotations ``sa.scores``, ``va.loadings``, and ``global.evals`` of type Array[Double]:. >>> vds_result = vds.pca('sa.scores', 'va.loadings', 'global.evals', 5, as_array=True). **Notes**. Hail supports principal component analysis (PCA) of genotype data, a now-standard procedure `Patterson, Price and Reich, 2006 <http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190>`__. This method expects a variant dataset with biallelic autosomal variants. Scores are computed and stored as sample annotations of type Struct by default; variant loadings and eigenvalues can optionally be computed and stored in variant and global annotations, respectively. PCA is based on the singular value decomposition (SVD) of a standardized genotype matrix :math:`M`, computed as follows. An :math:`n \\times m` matrix :math:`C` records raw genotypes, with rows indexed by :math:`n` samples and columns indexed by :math:`m` bialellic autosomal variants; :math:`C_{ij}` is the number of alternate alleles of variant :math:`j` carried by sample :math:`i`, which can be 0, 1, 2, or missing. For each variant :math:`j`, the sample alternate a",MatchSource.WIKI,hail-is,hail,0.2.133,https://hail.is/docs/0.1/_modules/hail/dataset.html:162332,load,loadings,162332,docs/0.1/_modules/hail/dataset.html,https://hail.is,https://hail.is/docs/0.1/_modules/hail/dataset.html,1,['load'],['loadings']
